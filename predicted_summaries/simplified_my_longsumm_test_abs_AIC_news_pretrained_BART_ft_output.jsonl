{"ground_truth": "BEAT: asynchronous BFT made practical Duan et al., CCS\u201918  Reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still. That\u2019s the world of Byzantine fault tolerance (BFT). We\u2019ve looked at Practical BFT (PBFT) and HoneyBadger on previous editions of The Morning Paper. Today\u2019s paper, BEAT, builds on top of HoneyBadger to offer BFT with even better latency and throughput. Asynchronous BFT protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet. The best performing asynchronous BFT protocol, HoneyBadger , still lags behind the partially synchronous PBFT protocol in terms of throughput and latency. BEAT is actually a family of five different asynchronous BFT protocols that start from the HoneyBadger baseline and make improvements targeted at different application scenarios. Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers). The BEAT protocols divide into two groups: those supporting full (general) state-machine replication (SMR), as required e.g. for smart contract use cases (BEAT0, BEAT1, BEAT2); and those that support BFT storage (append-only ledger) use cases only (BEAT3, BEAT4). The following table summarises the BEAT family and the key distinguishing features of each member. ( Enlarge )  There\u2019s a lot of ground to cover here, but I\u2019ll do my best to give you an overview. Alongside the BEAT protocols themselves, the paper also includes two new building blocks: the generalized fingerprinted cross-checksum and an asynchronous verifiable information dispersal (AVID) algorithm. The HoneyBadger baseline  HoneyBadger supports ACS (the asynchronous common subset) meaning that it provides these guarantees:  Validity: if a correct server delivers a set  , then  and  contains the inputs of at least  correct servers. Agreement: if a correct server delivers a set  , then all correct servers deliver  . Totality: if  correct servers submit an input, then all correct servers deliver an output. HoneyBadger uses reliable broadcast (RBC) and asynchronous Byzantine binary agreement (ABA) protocols to achieve its aims. Threshold signatures are used to provide common coins for ABA, and threshold encryption is used to avoid censorship and achieve liveness. In a threshold scheme the partial outputs (e.g. decryption shares) of at least t participants need to be combined in order to recover (decrypt) the intended value. BEAT0: improved security and performance  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption, a direct instantiation of threshold coin-flipping (instead of using threshold signatures), and more flexible and efficient erasure-coding support. BEAT0\u2019s threshold encryption uses the TDH2 scheme by Shoup and , providing 128-bit security under elliptic curve cryptography. This gives stronger security and better performance than the scheme used in HoneyBadger. In place of the zfec erasure coding library used by HoneyBadger, which supports only Reed-Solomon codes and at most 128 servers, BEAT uses the Jerasure library giving access to more efficient erasure coding schemes and lifting the replica restriction. BEAT1: lower latency  Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that (1) most of the latency comes from threshold encryption and threshold signatures, and (2) somewhat surprisingly, when the load is small and there is low contention, erasure-coded reliable broadcast (AVID broadcast) causes significant latency. BEAT1 swaps out the AVID broadcast protocol of BEAT0 for a replication-based reliable broadcast protocol, Bracha\u2019s broadcast . Under small loads BEAT1 has lower latency. With small batch sizes BEAT1\u2019s throughput is higher than HoneyBadger / BEAT0, but with larger batch sizes throughput is down by 20-30%. BEAT2: causal ordering  BEAT2 builds on BEAT1 and also opportunistically moves the use of threshold encryption to the client side. In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions. Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients. BEAT2 can be combined with anonymous communication networks to achieve full liveness. BEAT2 additionally achieves causal order, which prevents the adversary from inserting derived transactions before the original, causally prior transactions. BEAT3: higher throughput for storage use cases  BEAT3 is the first member of the BEAT family targeted for BFT-storage use cases (as opposed to general SMR). Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded). BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric. Whereas so far we\u2019ve been using a reliable broadcast protocol (AVID), BEAT3 replaces this with a bandwidth-efficient information dispersal scheme called AVID-FP. To disperse a block  , AVID requires bandwidth  , whereas AVID-FP can do it in  . To order transactions of size  , the communication complexity of BEAT0 is  , of BEAT1 and BEAT2 is  , and of BEAT3 is  . AVID-FP is a bandwidth-efficient AVID (asynchronous verifiable information dispersal) protocol using fingerprinted cross-checksum. In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, where  and  \u2026 then it generates the corresponding fingerprinted cross-checksum for B with respect to the erasure coding scheme. Each server verifies the correctness of its fragment with respect to the fingerprint cross-checksum, \u201cand then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol.\u201d  An (n,m) fingerprinted cross-checksum contains a cross-checksum array of n values, and a fingerprint array of m values. The ith entry in the checksum array contains the hash of the ith coded fragment. See section 4 in the paper for details of the fingerprint array usage. BEAT4: partial reads  BEAT4 further reduces read bandwidth using a novel erasure-coded reliable broadcast protocol called AVID-FP-Pyramid. This supports use cases where clients only need to read a fraction of a data block. AVID-FD-Pyramid is based on pyramid codes, which trade space for access efficiency in erasure-coded storage systems (about 10% extra space requirement for a 50% drop in access overhead). Pyramid codes can be efficiently built from any (n, m) systematic and MDS (maximum distance separable) code. See section 4 in the paper for brief details, or Huang et al. for an in-depth treatment. BEAT4 uses a 2-level pyramid scheme which can tolerate one failure in each level, and is able to reduce read bandwidth by 50%. Full details are in section 9 of the paper. Evaluation  The evaluation is conducted on EC2 with up to 92 nodes from ten different regions in five different continents, using a variety of network sizes and batch sizes. In the figures that follow,  represents the network size such that BEAT0,1,2 & 3 require  nodes and BEAT4 requires  nodes. When f=1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2x faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger. When f = 1, BEAT4 is about as fast as HoneyBadger\u2026 As f increases, HoneyBadger is much slower than BEAT4. For throughput, BEAT0 slightly outperforms HoneyBadger. BEAT1 and BEAT2 achieve higher throughput than HoneyBadger with small batch sizes, but have 20-30% lower throughput at larger batch sizes. BEAT3 and BEAT4 outperform all the other protocols consistently. If this write-up has captured your interest, I highly encourage you to go an and read the full paper which contains significantly more detail than I was able to convey here.", "prediction": "BEAT: A set of practical asynchronous BFT protocols for wide-area (WAN) blockchains Gog et al., CCS\u201918  Asynchronous blockchains (BFTs) have recently gained in popularity and protection from DDoS attacks and state machine replication (SMR). This paper introduces BEAT, a set of five protocols that are designed to meet different goals (e.g., different application scenarios). The five protocols are:  BEAT  An implementation of BEAT on Amazon EC2 (92 instances)  An evaluation of HoneyBadgerBFT (70), the most recent BFT protocol to be evaluated  A reference to the full BEAT set here:  ( Enlarge )  In part, this is due to the fact that the use of expensive threshold cryptography (specifically threshold encryptions and threshold signatures) has been hard to achieve (as has been argued in various works, e.g. , see our previous post on the topic here  ), and partially synchronous BFT  A combination of these two approaches allows for building permissioned blockchains where the distributed ledgers know each other identities but may not trust one another. Asynchronous BFT ensures liveness of the protocol without depending on any timing assumptions, which is prudent when the network is controlled by an adversary. (See our previous piece on PBFT here )  There are still a number of challenges and opportunities associated with implementing BFT, as well as new opportunities arising with the rise of blockchains. The Hyperledger umbrella has become a global open-source project under the Linux Foundation, now with more than 250 members. Meanwhile, there are also new opportunities for asynchronous B FT protocols, as a result of the Internet\u2019s role as a communications medium, and the proliferation of gateways (IPs) to the Internet . It was shown in  [70] that a fully synchronous PBFT protocol would achieve zero throughput against an adversarial asynchronous scheduler, and that partially synchronized PBFT would be unable to cope with a DDoS attack mounted over the Internet. Enter BEAT! BEAT is comprised of five different bFT protocols, all of which can be implemented on top of each other. There are application scenarios in which each of the five protocols can be used, and different trade-offs can be achieved between application-level functionality and performance. For example, in the EC2 case, there is a large number of jobs that require multiple replica servers, and each replica server has limited computation power. The following table shows the performance of different BEAT instances (highlighted in blue) over a one-week period (July-September 2018):  In the table below, we can see that in this case, the baseline ( PBFT) achieved around 80-90% throughput, and in the second category (BEAT), the performance was significantly worse (between 60-80%). The table below shows the trade-off (latency) and throughput (throughput) of two competing BFTs:  In this example, BEAT (left) and BEAT-All (right), both of which use threshold-encoders and use different combinations of these to achieve lower latency and lower throughput. Note that BEAT uses both threshold-and-signals, whereas PBFT uses only one (per application) and uses both (per server) the second (per user) to guarantee liveness. Here we have a situation in which a server is communicating with multiple other servers, but only one of its servers is synchronous (and therefore it can provide liveness to the rest of the servers). In this situation, and given the latency and throughput", "simplified_prediction": "BEAT : A set of practical protocols for wide-area ( WAN ) blockchains Gog et al . The BFT protocol has recently gained in popularity and protection from DDoS attacks and state machine replication ( SMR ) . This paper shows BEAT , a set of five protocols that are designed to meet different goals , different application scenarios . The five protocols are : BEAT An evaluation of BEAT on Amazon EC2 ( 92 instances ) An evaluation of HoneyBadgerBT ( 70 ) , the most recent BFT protocol to be evaluated A reference to the full BEAT set here : ( Enlarge ) In part , this is due to the fact that the use of expensive threshold cryptography ( specifically threshold encryptions and threshold has been hard to signature ) . , see our previous post on the topic here , and partially synchronous BFT A combination of these two approaches allows for building blockchains where the distributed ledgers know each other , but may not trust one another . Asynchronous BFT ensures liveness of the protocol without depending on any assumption that the network is controlled by an adversary device . There are still a number of challenges and opportunities with implementing BFT , as well as new opportunities arising with the rise of blockchains ( which comes after ) . The Hyperledger umbrella has become a global open-source project under the Linux Foundation , now with more than 250 members . Meanwhile , there are also new opportunities for asynchronous B FT protocols , as a result of the IPs role as a communications medium , and using gateways ( IPs ) to the Internet . It was shown in 70 ] that a fully synchronous PBFT protocol would achieve zero throughput against an adversarial asynchronous scheduler , and that partly synchronized PBFT would be unable to cope with a DDoS attack over the Internet . Enter BEAT ! BEAT is made up of five different bFT protocols , all of which can be used on top of each other . There are application scenarios in which each of the five protocols can be used , and different trade-offs can be done between the computer and the computer . For example , in the EC2 case , there is a large number of jobs that have many different servers , and each server has a lot of power . The following table shows the performance of different BEAT instances ( highlighted in blue ) over a one-week period ( July-September 2018 ) : In the table below , we can see that in this case , the baseline ( PBFT ) achieved around 80 - 90 % throughput , and in the second category ( BEAT ) , the performance was much worse ( between 60 - 80 % ) . The table below shows the trade-off ( latency ) and throughput ( throughput ) of two competing BFTs : In this example , BEAT ( left ) and BEAT-All ( right ) , both of which use different combinations of these to achieve lower latency and lower throughput . Note that BEAT uses both ( per server ) the second ( per user ) to guarantee liveness , and uses both ( per server ) the second ( per user ) to guarantee liveness . Here we have a situation in which a server is talking with multiple other servers , but only one of its servers can provide liveness to the rest of the server . In this situation , and given the latency and throughput"}
{"ground_truth": "This paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. Only a finite set of a priori known policies are considered. Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle. A statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. Evidence  Anomaly detection was explored by recording three trajectories corresponding to two bikes and a bus. The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn. System was able to detect these trajectories as anomalous (Not within the set of known policies)  Evaluated in simulated driving environment  Notes  Bayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed. Then, the likelihood of all available policies for the target car given the distribution over the car\u2019s potential policies at the current timestep can be computed (sounds like HMM). The CHAMP algorithm infers the maximum a posteriori set of times at which changepoints between policies have occurred, yielding a set of segments. Given a segment from time s to t and a policy pi, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)  Viterbi path is found for the most likely sequence of latent policies  For decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car. For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon. This yields a set of simulated trajectories. The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen. This repeats continuously in a receding horizon manner. Reward function  distance to the goal at the end of the evaluation horizon  minimum distance to obstacles to evaluate safety  lane choice bias to add a preference for the right lane  maximum yaw rate and longitudinal jerk to measure passenger comfort", "prediction": "This paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a result of closed-loop policies that react to the actions of other agents. Each policy is designed to capture a different high-level behavior, such as following a lane, changing lanes, or turning at an intersection. At each timestep of the model\u2019s run, it estimates the distribution over the policies that other traffic agents\u2019 actions may be, using information from the observed history of states of nearby cars. It also uses changepoint detection to estimate which policy a given vehicle was executing at each point in its history of actions, and then infer the likelihood of each potential intention of the vehicle. At the last step, it uses the policy that maximizes the expected reward given the sampled outcomes of the behavioral prediction system, and executes the policy with the maximum expected reward value. \u201cDecision-making for autonomous cars is hard due to uncertainty on the continuous state of nearby vehicles and, in particular, due to their discrete potential intentions\u2026\u201d  Previous approaches have employed hand-tuned heuristics and numerical optimization to account for the future actions of interacting traffic agents, but these methods do not scale up to real-world scenarios. In contrast, this paper presents a fully integrated behavioral anticipation and decision making approach that combines the benefits of partially observable Markov decision process (POMDP) solvers to partially incorporate the effects of other traffic agent actions with the uncertainty of the interaction between vehicles. The key to this work is the use of behavioral anticipation to estimate the probability of actions taken by each traffic agent, in addition to the known policy options available for our vehicle. Approach  The proposed in this paper is a combination of two approaches:  The first approach uses a changepoint-based behavioral prediction approach that equates the trajectory of actions of each traffic vehicle with the changepoint of its neighbors. This approach is made tractable by considering only a finite set of a priori known policies. The second approach, based on behavioral anticipation, considers the potential actions that each vehicle could take given the policy available to it and the policy options it has available for it. The choice of policies is made by taking the policy as a whole and combining it with the policy for the current vehicle, the policy of the other vehicles, and any other policies that our vehicle may have available, to produce the final policy for our car. The system proceeds in two stages:  (1) Estimate the distribution of actions that other vehicles may be taking. (2) Sample policies from the distribution to obtain high-likelihood actions for each participating vehicle. (3) Find out which policies are most likely to be followed by our vehicle, and (4) execute the policy based on those policies. During the sampling phase, we use a Bayesian changepoint detector (based on the policy distribution of other vehicles\u2019 trajectories) to determine which of the policy trajectories of our vehicle was being executed at each time step t and t-1. This allows us to infer which policy was being followed, and to use this information to compute the changepoints of other trajectories. The changepoints are then used to compute a statistical test to detect anomalous actions (changing lanes, driving in the wrong direction, etc). The authors use the results of this phase to evaluate the performance of the proposed system online, using traffic data from our autonomous vehicle platform. Results  The key takeaway is that given a set of trajectories and trajectories, it is possible to accurately predict which policies our vehicle will follow given an understanding of the trajectory. This insight forms the basis for the proposed behavioral model", "simplified_prediction": "This paper presents an integrated behavioral behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a result of closed-loop policies that make the actions of other agents . Each policy has been designed to capture a different high-level behavior , such as following a lane , changing lanes , or turning at an area . The distribution estimates the distribution over the policies that other traffic agents can be , using information from the observed history of states of nearby cars to be found . It also uses changepoint detection to estimate which policy a given vehicle was executing at each point in its history of actions , and then infer that could not be used for the vehicle . At the last step , it uses the policy that gives the expected reward given the sampled outcomes of the behavioral prediction system , and the policy with the maximum expected reward value is expected . A lot of cars are hard due to uncertainty on the continuous state of nearby vehicles and , in particular , due to their difference in account for interacting traffic agents , but these methods do not scale up to real-world scenarios . This is because of the future actions of interacting traffic agents , but these methods do not scale up to real-world scenarios . In contrast , this paper presents a fully integrated behavioral anticipation and decision making approach that combines the benefits of partially observable Markov decision process ( POMDP ) solvers to produce the effects of other traffic agent actions with the interaction between vehicles . The key to this work is the use of behavioral anticipation to estimate the probability of actions taken by each traffic agent , in addition to the known policy options available for our vehicle . The proposed in this paper is a combination of two approaches : The first approach uses a changepoint-based behavioral prediction approach that the trajectory of actions of each traffic vehicle with the changepoint of its neighbors . This approach is done by considering only a set of a priori known policies . The second approach , based on behavioral anticipation , considers the potential actions that each vehicle could take given the policy available to it and the policy options it has available for it . The choice of policies is made by taking the policy as a whole and combining it with the policy for the current vehicle , the policy of the other vehicles , and any other policies that our vehicle may have available , to make the final policy for our car . The system comes in two stages : ( 1 ) Estimate the distribution of actions that other vehicles may be taking . 2 ) Sample policies from the distribution to get actions for each taking part in the car . ( 3 ) Find out which policies are most likely to be followed by our vehicle , and ( 4 ) do not have the policy based on those policies . During the sampling phase , we use a Bayesian changepoint detector ( based on the policy distribution of other vehicles ) to find out which the policy of our vehicle was being executed at each time step t and t-1 . This allows us to infer which policy was being followed , and to use this information to change the changepoints of other policy . They are then used to compute a statistical test to detect actions ( changing lanes , driving in the wrong direction , etc. ) . The authors use the results of this phase to evaluate the performance of the proposed system online , using traffic data from our vehicle platform . The key takeaway is that given a set of trajectories and trajectories , it is possible to accurately predict which policies our vehicle will follow the understanding of the trajectory . This forms the basis for the proposed behavioral model ."}
{"ground_truth": "The paper presents the task of abductive NLP (pronounced as alpha NLP) where the model needs to perform abductive reasoning. Abductive reasoning is the inference to the most plausible explanation. Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse. A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided. The dataset models the task as multiple-choice questions to make the evaluation process easy. Task Setup  Given a pair of observations O1 and O2 and two hypothesis h1 and h2, the task is to select the most plausible hypothesis. In general, P(h | O1, O2) is propotional to P(h |O1)P(O2|h, O1). Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the \u201cfully connected\u201d assumption would jointly model both the observations and the hypothesis. Dataset  Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis. Observation pairs are created using the ROCStories dataset which is a collection of short, manually crafted stories of 5 sentences. The average word length for both the content and the hypothesis is between 8 to 9. To collect plausible hypothesis, the crowd workers were asked to fill in a plausible \u201cin-between\u201d sentence in natural language. Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words. Adversarial filtering approach from Zellers et al. is used with BERT as the adversary. A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration. Key Observations  Human performance: 91.4%  Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%  Entailment NLI baseline: 59%. This highlights the additional complexity of abductive NLI as compared to entailment NLI. BERT: 68.9%  GPT: 63.1%  Numerical and spatial knowledge-based data points are particularly hard. The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible", "prediction": "The paper presents a new dataset of 20K commonsense narratives and 200K explanations based on the hypothesis that a thief caused the mess at home (see Figure 1 for an example). In addition, a new conditional generation task, Abductive NLI (abductive reasoning in natural language), is introduced which requires the user to choose between one of the two explanations given an observation and a given pair of hypotheses, given a pair of hypothesis and a set of questions (given the question and the question). Questions  Given a question (question), answer the question with the most plausible explanation while the answers to the questions are selected using a binary cross-entropy scoring function which aims to minimise the discrepancy between the answers provided by the hypothesis and the facts stated in the question. Abduction is the only logical operation which introduces any new ideas, which contrasts with other types of inference such as entailment which focuses on inferring only such information that is already provided in the premise. This shift from logic-based to language-based reasoning draws inspirations from a significant body of work on language based entailment (including work by Bowman et al., 2015, and Williams and Wojciechowski, 2018). Approach  Most previous work on abductive reasoning has focused on formal logic, which has proven to be too rigid and hard to generalize to the full complexity of natural language. The main change proposed in this work is the use of language as the representation medium, with the paradigm shift from formal to language based reasoning. Two tasks are considered:  Question Answering - multiple choice task for choosing the most likely explanation  Conditional generation task for explaining observations given a given hypothesis (AbductiveNLI). Conditional Generation - conditional generation of the hypothesis  Dataset  Created a new multi-choice question answering dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, two new tasks are introduced:  AbductIVE NLI - Multiple choice task to choose the \u201cmore likely explanation\u201d given an hypothesis (given a question and pair of question-answering questions)  Abduction NLG - Conditional reasoning task to generate the hypothesis for the conditional generation  Notes  There has been a lot of work in this field recently (see \u201c Language-based Inference in Natural Language\u201d and \u201cLanguage-based Extrapolation\u201d) but little attention has been paid to the application to the natural language domain. This work aims to investigate the viability of language- based abductive inference and reasoning in the context of language and to provide new insights on the reasoning capabilities of deep pre-trained language models. One of the main observations made in the paper is that the current state-of-the-art models on the task of conditional generation NLI are sub-par when it comes to reasoning about natural language and perform poorly on the question answering task. This gap between human performance on NLI and BERT (based on BERT model) is much wider when the model is trained on the more difficult task of choosing the more likely explanation. The authors believe that this gap could be due to the fact that the models are trained on a task where the task is to predict the most \u201cbelievable\u201d (i.e., plausible) hypothesis and not just the average or \u201cexplanatory\u201d hypothesis. They propose two variants of the proposed models:  BERT: Standard BERT based model where the question is a question with a predefined set of question answering questions and the answers are generated by a binary classifier using a soft-attention scoring function  NLI", "simplified_prediction": "The paper presents a new dataset of 20K commonsense narratives based on the hypothesis that a thief caused the mess at home ( see Figure 1 for an example ) . In addition , a new conditional generation task , Abductive NLI , is introduced which requires the user to choose between one of the two explanations given an observation and a given pair of hypotheses , given a pair of hypothesis and a set of questions ( given the question and the question ) . Questions Given a question ( question ) , answer the question with the best explanation while the answers to the questions are selected using a binary cross-entropy scoring function which tries to stop the question . The answers between the answers provided by the hypothesis and the facts stated in the question . It is the only logical operation which shows any new ideas , which contrasts with other types of information which focuses on inferring only such information that is already provided in the premise . This draws inspirations from a significant body of work on language based entailment ( including work by Bowman et al . , 2015 , and Williams and Wojciechowski , 2018 ) . Most previous work on abductive reasoning has focused on formal logic , which has been too rigid and hard to make natural language more complex than any other thing . The main change proposed in this work is the use of language as the representation medium , and its formal language is based reasoning . Two tasks are considered : Question Answering - multiple choice task for choosing the most likely part of the task for explaining observations given a hypothesis ( AbductiveNLI ) . Created a new multi-choice question answering dataset , ART , that consists of over 20k commonsense narrative contexts and 200k explanations . Based on this dataset , two new tasks are introduced : AbductIVE NLI - Multiple choice task to choose the onlinemore likely explanation given an hypothesis ( given a question and pair of question-answering questions ) Abduction NLG - Conditional reasoning task to generate the hypothesis for the conditional generation Notes There has been a lot of work in this field recently , but it has been a lot of people who wanted to talk about it . This work wants to look at the viability of language- based abductive inference and reasoning in the context of language and to provide new features of deep pre-trained language models . One of the main sights made in the paper is that the current state-of-the-art models on the task of generation NLI are when it comes to reason about natural language and perform poorly on the question answer task . This gap between human performance on NLI and BERT is much wider when the model is trained on the more difficult task of choosing the more likely explanation . The authors believe that this gap could be due to the fact that the models are trained on a task where the task is to predict how many people talk about ( i.e. , plausible ) hypothesis and not just the average hypothesis . They propose two variants of the proposed models : Standard BERT based model where the question is a question with a set of question answering questions . The answers are generated by a group of people using a soft-attention scoring function NLI ."}
{"ground_truth": "`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript. The notes below were made based on v1.`  This paper considers the problem of Maximum Inner Product Search (MIPS). In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$. Recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation). In this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs. Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm). Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree. They propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS. The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS. In order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query. Their experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods.", "prediction": "MIPS (Maximum Inner Product Search) - solving a K-MIPS problem in sublinear time Gog et al., CIFAR\u201918  Yesterday we looked at some of the amazing properties of word vectors with word embeddings , today\u2019s paper introduces a very similar problem, Maximum Cosine Similarity Search (MIPS). The problem of predicting the top-K most likely class labels for a given data point, given the context of the few previous words in a vocabulary, can be cast as solving a \u201cKMIPS\u201d problem. With millions of candidate items to recommend, it is usually not possible to do a full linear search within the available time frame of only few milliseconds. Finding the k most likely words is not always easy. In many cases the retrieved result need not be exact: it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high, and thus highly likely (though not guaranteed) to contain some of \u201cthe exact KMIPS vectors\u201d The examples in the above examples motivate research on approximate MIPS algorithms. If we can obtain large speedups over a full \u201cloosely supervised\u201d search without sacrificing too much on precision, it will have a direct impact on such large-scale applications. The authors start out by training a softmax model to predict the softmax probabilities for all the words in the vocabulary, and then use that to rank the likelihood of specific words. The softmax is a non-linear function that allows the model to compute a matrix of softmax scores for each hidden layer representation, and from these scores an inner product score is obtained for the vocabulary words. Given a vocabulary word (e.g. \u201chorse\u201d) and a hidden representation  , an embedding dictionary  is constructed, where  is the set of all the possible words and  is a fixed size vector  . Each column  in the dictionary is a point, and  denotes the embedding of the vocabulary word in the hidden layer space  . An inner product  is computed between the column  and the hidden representation, to yield a score  for each word  . In the final layer  , the number of columns is  and  represents the predicted embedding for that word  To mark the points in the vector space, an addition is added, such that  now we have  . Given a set of k words,  ,  , and  , we can compute the following matrix  . The matrix  has as many columns as there are words in  The columns  are represented by  , with  representing the inner product scores  . Let  be the points and  represent the word vectors  . We want to predict which of the k words from the vocabulary is the most likely to be chosen as the correct label for  . To compute this matrix, we need to know which of  the k word vectors have the highest inner product, and which of them has the lowest. The paper introduces an extremely simple approach to this task, based on a k-means clustering algorithm. Specifically, we propose to train a spherical kmeans (i.e., k-MI PS problem) model, and after having reduced the MIPS problem to  , train this model to solve approximate K-PIPS. The k-Means model  The basic idea is to use locality-sensitive hashing (LSH) to reduce the time of solving MIPS. Localized LHS is a very common technique to solve MIPS, and has been used by famous Convolutional Neural Networks such as GoogLeNet  A variant of LHS called as Local", "simplified_prediction": "MI ( Maximum Inner Product Search ) - solving a K-MIPS problem in sublinear time Gog et al . Yesterday we looked at some of the amazing properties of word vectors with word embeddings , today a very similar problem , Maximum Cosine similarity similar Search ( MIPS ) . The problem of predicting the top-K most likely class labels for a given data point , which is given the context of the few previous words in a vocabulary , can be made by people . With millions of candidate items to recommend , it is usually not possible to do a full linear search within the available time frame of only two milliseconds . The most likely words are not always easy . In many cases the retrieved result need not be exact : it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high , and thus likely ( though not guaranteed ) to contain some of the usual KMIPS algorithms . The example is the same in the above examples motivate research on approximate MIPS algorithms . If we can get large speedups over a full name , it will have a direct impact on such large-scale applications ( such as sacrificing too much on precision ) . The authors start by training a softmax model to predict the softmax probabilities for all the words in the vocabulary , and then use that to change specific words . The softmax is a non-linear function that allows the model to compute a matrix of softmax scores for each hidden layer . From these scores an inner product score is taken for the words . Given a word ( e . g ) People who do this , and a hidden representation , an embedding dictionary is constructed , where all the possible words and is a fixed size vector . Each column in the dictionary is a point . Each column is the embedding of the word in the hidden layer space . An inner product is computed between the column and the hidden representation , to make a score for each word . In the final layer , the number of columns is and represents the predicted embedding for that word To mark the points in the vector space , an addition is added . Given a set of k words , , and , we can talk about the next matrix . The matrix has as many columns as there are words in The columns are represented by , for example the inner product scores . Let be the points and represent the word vectors . We want to predict which the k words from the vocabulary are the most likely to be chosen as the correct label for . To compute this matrix , we need to know which the k word vectors have the highest inner product , and which they have the lowest number . The paper has an extremely simple approach to this task , based on a very simple algorithm . After having reduced the MIPS problem to , train this model to solve approximate K-PIPS . The basic idea of this model is to use locality-sensitive hashing ( LSH ) to reduce the time of solving MIPS . Localized LHS is a very common technique to solve MIPS . It has been used by famous Networks such as the GoogLeNet A variant of LHS called as Local ."}
{"ground_truth": "Understanding lifecycle management complexity of datacenter topologies Zhang et al., NSDI\u201919  There has been plenty of interesting research on network topologies for datacenters, with Clos-like tree topologies and Expander based graph topologies both shown to scale using widely deployed hardware. This research tends to focus on performance properties such as throughput and latency, together with resilience to failures. Important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure. The great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible but still very important \u201clifecycle management\u201d implications of topology design. In networking, this translates into how easy it is to physically deploy the network, and how easy it to subsequently expand. They find a way to quantify the associated lifecycle management costs, and then use this to help drive the design of a new class of topologies, called FatClique. \u2026 we show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics. Now, there\u2019s probably only a relatively small subset of The Morning Paper readers involved in designing and deploying datacenter network topologies. So my challenge to you as you read through this paper, is to think about where the hidden complexity and costs are in your own systems. Would you do things differently if these were made more visible? It would be great to see more emphasis for example on things like developer experience (DX) and operational simplicity \u2013 in my experience these kinds of attributes can have an outsize impact on the long-term success of a system. Anyway, let\u2019s get back to cables and switches\u2026  Physically deploying network topologies  When it comes to laying out a network topology for real in a datacenter, you need to think about packaging, placement, and bundling. Packaging is how you group things together, e.g. the arrangement of switches in racks, and placement concerns how these racks are physically placed on the datacenter floor. Placement in turn determines the kinds of cabling you need, and for optical cables the power of the transceivers. Within a rack we might package several connected switches into a single chassis using a backplane. At the other end of the scale, blocks are larger units of co-placement and packaging that combine several racks. With all those connections, it makes things a lot easier to group together multiple fibres all connecting the same two endpoints (racks) into bundles, which contain a fixed number of identical length fibres. Manufacturing bundles is simpler than manufacturing individual fibres, and handling such bundles significantly simplifies operational complexity. Patch panels make bundling easier by providing a convenient aggregation point to create and route bundles. Bundles and fibres are physically routed through the datacenter on cable trays. The trays themselves have capacity constraints of course. Here\u2019s an example of a logical Clos topology and its physical instantiation:  The authors identify three key metrics that together capture much of the deployment complexity in a topology:  The number of switches. More switches equals more packaging complexity. The number of patch panels, which is a function of topological structure and a good proxy for wiring complexity. The number of bundle types. This metric captures the other important part of wiring complexity \u2013 how many distinct bundle types are needed. A bundle type is represented by its capacity (how how many fibres) and its length. These complexity measures are complete. The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which are inputs to the topology design). The number of cables and transceivers can be derived from the number of patch panels. Here\u2019s how Clos and Expander (Jellyfish) representative topologies for the same number of servers stack up against these metrics:  The expander graph topology shows much higher deployment complexity in terms of the number of bundle types. Clos also exposes far fewer ports outside of a rack (it has better port hiding). Expanding existing networks  When you want to expand an existing network first you need to buy all the new gear and lay it out on the datacenter floor, and then you can begin a re-wiring process. This is all going on with live traffic flowing, so expansion is carried out in steps. During each step the capacity of the topology is guaranteed to be at least some percentage of the existing topology capacity. The percentage is sometimes known as the expansion SLO. During a step existing links to be re-wired are drained, then human operators physical rewire links at patch panels. The new links are tested and then undrained (strange word! ), i.e., brought into service. For example, here\u2019s a logical expansion (top row) and its physical realisation:  The most time-consuming part of all this is the physical rewiring. The two metrics that capture expansion complexity are therefore:  The number of expansion steps, and  The average number of rewired links in a patch panel rack. Here\u2019s how Clos and Expander stack up on those metrics for the same networks we saw earlier:  This time the victory goes to Expander (Jellyfish). Jellyfish has a much higher north-to-south capacity ratio. Northbound links exit a block, and southbound links are to/from servers within a block. \u201cFat edges\u201d have more northbound than southbound links, and the extra capacity means you can accomplish more movement in each step. Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio. Enter the FatClique  Inspired by these insights, the authors define a new class of topologies called FatClique, which combine the hierarchical structure of Clos with the edge expansion capabilities of expander graphs. There are three levels in the hierarchy. A clique of switches form a sub-block. Cliques of sub-blocks come together to form blocks. And cliques of blocks come together to from the full FatClique topology. Four key design variables determine the particular instantiation of a FatClique topology: the number of ports in a switch that connect to other servers; the number of ports in a switch that connect to other sub-blocks in a block; the number of switches in a sub-block; and the number of sub-blocks in a block. A synthesis algorithm  takes a set of six input constraints (see \u00a75.1) and determines the values for these four design variables. There is plenty more detail in section 5 of the paper which I don\u2019t have the space to do justice too here. FatClique vs Clos vs Expander  The evaluation compares FatClique to Clos, Xpander, and Jellyfish at different network topology sizes, as shown in the table below. ( Enlarge )  Here\u2019s how they stack up against the complexity metrics:  Number of switches  Number of patch panels  Number of bundle types  and associated cabling costs:  Number of expansion steps  Average number of rewired links  We find that FatClique is the best at most scales by all our complexity metrics. (The one exception is that at small and medium scales, Clos has slightly fewer patch panels). It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we were able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5x longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher. The one thing I couldn\u2019t find in the evaluation is any data to back up the opening claim that FatClique achieves all of this \u201cwhile being performance-equivalent to existing topologies.\u201d  The last word  As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction\u2026", "prediction": "FatClique: Understanding lifecycle management and expanding a datacenter\u2019s network Gog et al., HotCloud\u201918  Previously on The Morning Paper we\u2019ve looked at some of the characteristics of datacentre topologies, and how they can be optimised for varying degree of flexibility (e.g. flexibility to add more capacity, flexibility to reconfigure at different points in the lifecycle). This paper introduces a new dimension, life cycle management complexity (LMC), which attempts to understand the complexity of deploying a topology and expanding it. It attempts to devise metrics that indicate the time-scale and scale of changes needed to support the needs of a constantly increasing set of services over time, and to provide a measure of how much effort and resources need to be spent on managing this lifecycle. The key to understanding is this:  \u2026 the costs of the large array of components needed for deployment such as switches, transceivers, cables, racks, patch panels, switch-types, and cable trays, are proprietary and change over time. An alternative approach is to develop complexity measures (as opposed to dollar costs) for lifecycle Management, but as far as we know, no prior work has addressed this topic with this prior work. In part, this is due to the fact that intuitions about lifcycle management are developed over time and with operations experience, and these lessons are not made available universally. In this paper, the authors identify the following key factors:  The number of switches required to deploy a data center topology  The amount of effort needed to pack a set of different types of switches into homogeneous racks into a single unit (i.e. a \u201cmasterpiece\u201d of switches, switches, transformers, routers, and associated cables). The need to choose among different type of switch types, and the number of different kinds of cables to connect to them  How many switch-related components are needed to package a given set of switches? This can vary from one provider to another, and depends on a number of factors related to the particular case. For instance, in a typical commercial data center there are three types:  Number of switches in the topology determines how \u201chard\u201d it is to pack switches into racks. This number determines the \u201cneed to\u201d to be able to efficiently place switches in racks in a space efficient manner. Number of racks determines the amount of \u201cclearing\u201d required to pack individual switches, for example, a large number of racks is likely to require the use of three or more of them. Wiring complexity is related to both these factors as well, and can be influenced by the choice of switch type, and also by the type of cables used in the racks. Increasing the total number of links (\u201cconnections\u201d) influences the \u2018need to re-wire\u201d factor, as increasing the number means that the overall amount of re-wiring needs to go up. The authors identify factors such as \u201cwiring patterns\u201d and \u201cpatch panels\u201d, and propose a new class of topologies called FatClique, which combines the benefits of both of these factors with the benefits (and the benefits) of reducing the overall number of switch component requirements, making it comparable to, but better than, other topologies. A key insight from the analysis is that Closings and Expanders are both susceptible to the effects of time-cycle management, and that the effects (and benefits) from these effects are especially prominent at the edges, where the edges are the most sensitive to the timing of connection failures and connection degradation.", "simplified_prediction": "FatClique : Understanding lifecycle management and expanding a datacenter placings network Gog et al . , HotCloud can be given for varying degree of flexibility ( e g ) , and how they can be optimized for varying degree of flexibility ( e g . This is because add more capacity , flexibility to reconfigure at different points in the lifecycle . This paper shows a new dimension , life cycle management complexity ( LMC ) , which tries to understand the complexity of making it difficult to understand . It tries to get metrics that show the time-scale and scale of changes needed to support the needs of a set of services over time , and to provide a measure of how much effort and resources need to be spent on managing this lifecycle . The key to understanding is this : ... the costs of the large array of parts needed for deployment such as switches , transceivers , cables , racks , patch panels , switch-types , and cable trays . An alternative approach is to develop complexity measures ( as opposed to dollar costs ) for lifecycle Management , but as far as we know , no work has called this before work . In part , this is due to the fact that people intuitions about lifcycle management are developed over time and with operations experience , and these lessons are not available . In this paper , the authors identify the following key factors : The number of switches needed to deploy a data center '' The amount of effort '' needed to pack a set of different types of switches into a single unit . There are many kinds of switches , switches , transformers , routers , and associated cables . The need to choose among different type of switch types . The number of different kinds of cables to connect to them How many parts are needed to make a given set of switches ? This can be different from one provider to another , and depends on a number of things related to the particular case . For example , in a typical commercial data center there are three types : Number of switches in the topology tells how many people know it is to pack change into racks . This number decides to change the name of the name to be able to efficiently place switches in a space efficient manner . Number of racks decide how many people wanted to pack individual switches , for example , a large number of racks is likely to require the use of three or more of them . It is related to both these factors as well , and can be influenced by the choice of switch type , and also by the type of cables used in the racks . Increasing the total number of links influences the people who want to change the name to re-wire entrance factor , as increasing the number means that the amount of changes to go up . The authors identify things such as '' main things '' , '' a new class of topologies called FatClique '' , which combines the benefits of both of these factors with the benefits ( and the benefits ) of reducing the overall number of switch requirements , making it comparable to , but better than , other things . A key insight from the analysis is that Closings and Expanders are especially prominent at the edges , where the edges are the most sensitive to the timing of connection failures and connection degradation , where the edges are the most important to the timing of connection failures ."}
{"ground_truth": "KV-Direct: High-performance in-memory key-value store with programmable NIC Li et al., SOSP\u201917  We\u2019ve seen some pretty impressive in-memory datastores in past editions of The Morning Paper, including FaRM , RAMCloud , and DrTM . But nothing that compares with KV-Direct:  With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Check out the bottom line in this comparison table from the evaluation:  ( Enlarge )  In addition to sheer speed, you might also notice that KV-Direct is 3x more power efficient than other systems, and the first general purpose KVS system to achieve 1 million KV operations per watt on commodity servers. Since the server CPU can also be used to run other workloads at the same time, you can make a case for KV-Direct being as much as 10x more power efficient than CPU-based systems. What we\u2019re seeing here is a glimpse of how large-scale systems software of the future may well be constructed. As the power ceiling puts a limit on multi-core scaling, people are now turning to domain-specific architectures for better performance. A first generation of key-value stores were built in a straightforward manner on top of traditional operating systems and TCP/IP stacks. More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. KV-Direct however, goes one step beyond. To support network virtualisation, more and more servers in data centers are now equipped with programmable NICS containing field-programmable gate arrays (FPGA). An embedded NIC chip connects to the network, and a PCIe connector attaches to the server. KV-Direct uses the FPGA in the NIC to implement key-value primitives directly. Like one-sided RDMA (Fig 1b below), KV-Direct bypasses the remote CPU. But it also extends the RDMA primitives from simple memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops) \u2014 Fig 1c below. Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues on the server-side, thus removing computation overhead in the client, and reducing network traffic. In addition, to support vector-based operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages and delegate certain computations to programmable NIC for efficiency. Design goals and challenges  Use cases for in-memory key-value stores have evolved beyond caching to things such as storing data indices, machine learning model parameters, nodes and edges in graph computing, and sequencers in distributed synchronisation. The role of the store shifts from object caching to a generic data structure store (c.f. Redis). This leads to the following design goals:  High batch throughput for small key-value pairs (e.g., model parameters, graph node neighbours). Predictable low-latency (e.g., for data-parallel computation,where tail latency matters)  High efficiency under write-intensive workloads (e.g., graph computations, and parameter servers)  Fast atomic operations  (e.g., for centralized schedulers, sequencers , counters and so on). Vector-type operations (for machine learning and graph computing workloads that often require operating on every element in a vector). The throughput constraint ends up being PCIe bandwidth:  In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET. Getting to this level involves work on three fronts:  Minimising DMA (direct memory access) requests per KV operation. The two major components that drive random memory access are hash tables and memory allocation. Hiding PCIe latency while maintaining consistency, which entails pipelining requests. Care must be taken to respect causal dependencies here though. Balancing load between NIC DRAM and host memory. The NIC itself has a small amount of DRAM available, but it turns out not to be much faster than going over PCIe. So the trick turns out to be to use both in order to utilise the joint bandwidth. KV-Direct  KV-Direct enables remote direct key-value access. Clients send operation requests to the KVS server, and the programmable NIC processes requests and sends back results, bypassing the CPU. The following table shows the supported operations. The most interesting of course are the vector operations. KV-Direct supports two types of vector operations: sending a scalar to the NIC on the server, where the NIC applies the update to each element in the vector; and sending a vector to the server, where the NIC updates the original vector element-by-element. Furthermore, KV-Direct supports user-defined update functions as a generalisation to atomic operations. The update functions needs to be pre-registered and compiled to hardware logic before executing. When the user supplies an update function, the KV-Direct toolchain duplicates it several times to leverage FPGA parallelism and match computation with PCIe throughput, and then compiles it into reconfigurable hardware logic using a high-level synthesis (HLS) tool. These functions can be used for general stream processing on a vector value. The programmable NIC on the KVS server is reconfigured as a KV processor, which receives packets from the network, decodes vector operations, and buffers KV operations in a reservation station. The out-of-order engine then issues independent KV operations from the reservation station into the decoder. To minimise memory accesses, small KV pairs are stored inline in the hash table, while others are stored in dynamically allocated memory from a slab memory allocator. After a KV operation completes, the result is sent back to the out-of-order execution engine to find and execute matching KV operations in the reservation station. The reservation station is used to avoid dependencies between two KV operations leading to data hazards and a stalled pipeline. We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context. To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed. However, comparing 256 16-byte keys in parallel would take 40% of the logic resource of our FPGA. Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key. When a KV operation completes, the latest value is forwarded to the reservation station, where pending operations in the same hash slot are checked. Those with a matching key are executed immediately and removed from the station. Further design and implementation details can be found in sections 3 and 4 of the paper. Evaluation  The evaluation section contains a suite of microbenchmarks, followed by a system benchmark based on the YCSB workload. To simulate a skewed Zipf workload, skewness 0.99 was chosen. This is referred to as the long-tail workload in the figures. The testbed comprises eight servers with two 8-core CPUS per server,and one Arista switch. There is a total of 128 GiB of host memory per server. A programmable NIC is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch. The NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector. Here\u2019s the overall throughput achieved by the system. The throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores. Without network batching, the tail latency ranges from 3-9  s depending on KV size, operation type, and key distribution. Network batching adds less than 1  s latency, but significantly improves performance. It is possible to attach multiple NICs per server. With 10 KV-Direct NICs on a server, one billion KV ops/s is readily achievable on a commodity server. Each NIC owns a disjoin partition of the keys. Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation, but for a relatively small number of partitions (e.g. 10) the load imbalance is not too great \u2013 1.5x of the average in the highest loaded NIC even for the long-tail highly skewed workload. KV-Direct throughput scales almost linearly with the number of NICS on a server. The last word:  After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction.", "prediction": "The in-memory key-value store (KVS) at scale Li et al., SOSP\u201917  Key-value stores, or KVSs, are a key component in many data centers. Performance of the KVS is the key factor in the overall system efficiency. A significant amount of research has been done over the years to improve KVS performance. Today\u2019s paper introduces KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct access to the main host memory. It achieves 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems. This is equivalent to the throughput of tens of CPU cores. It also reduces the average power usage by 3x and reduces the latency of the connection between the NIC and the host memory by up to 10%. The key to this success is the use of two RDMA-capable NICs, one for the CPU and another for the servers. The CPU is the original bottleneck in KVS until recently, but as the network bandwidth to the CPU has grown, this has become a non-trivial factor in performance. The RDMA abstraction has been largely replaced by RDMA since the introduction of RDMA in the mid-2000s, but primitives for accessing the main memory of the CPU have been somewhat limited. This paper exploits two new RDMA technologies:  Remote Direct Memory Access (RDMA) \u2013 a term coined to refer to the RDMA interface  RDMA bypass bypassing the CPU  In-memory RDMA (or RDMA) can be used to access the CPU directly, but is expensive and doesn\u2019t provide full access to main memory. The authors exploit this by using two common RDMA techniques, one-sided RDMA to bypass CPU and remote RDMA access to KVS servers\u2019 main memory, and another-side RDMA on the servers themselves. This enables access to a shared key value hash table among distributed clients and speeds up their accesses to the table. At the heart of the paper is a discussion on the benefits of using RDMA for KVS, and the challenges presented by moving the workload away from the CPU to the network in this manner. RDMA is an important factor in moving workloads from the network to the data center, and as we\u2019ve seen before, its benefits are limited if the network itself is not fast enough. To address this issue, the authors propose to use two common approaches:  The first approach (more familiar to regular RDMA users) is to use the CPU as the link point between the server and the in- memory RDMA server. This approach degrades PUT (per-time access time) for transactional operations due to high communication and PUT overhead. The second approach (achieving near linear scalability with multiple NICs) exploits the second-level of abstraction provided by the programmable networking layer of the RDDMA abstraction and uses it to support in-network processing. With two NICs in a server, a typical KVS can achieve up to 180 M operations a second, beating the previous best of up to 1.2 billion (1.7 billion) with a single CPU core. Note that the figure in the table below is for the table created by a vanilla Memcached implementation. You\u2019ll find out more details of the hardware involved in the implementation in the video below. The full paper can be found on the link-base at  [url]", "simplified_prediction": "The key-value store ( KVS ) at scale Li et al . , SOSP store17 Key-value stores , or KVSs , is a key component in many data centers . Performance of the KVS is the most important factor in the system . A big amount of research has been done over the years to make KVS performance . Today 's paper introduces KV-Direct , a high performance KVS that does not allow NIC to extend RDMA primitives and allow remote direct access to the main host memory . It achieves 1.22 billion KV operations per second , which is almost the same as the order-of-magnitude improvement . This is the same as the throughput of tens of cores . It also reduces the average power usage by 3x and reduces the latency of the connection between the NIC memory and the memory by up to 10 % . It is also the use of two RDMA-capable NICs , one for the CPU and another one for the servers . The CPU is the original bottleneck in KVS until recently , but as the network bandwidth to the CPU has grown , this has become a good factor in performance . The RDMA abstraction has been replaced by RDMA since the introduction of RDMA in the mid-2000s , but primitives for accessing the main memory of the CPU have been very little limited . It uses two new RDMA technologies : Remote Direct Memory Access ( RDMA ) Access ( RDMA ) is a term used to refer to the RDMA interface RDMA bypassing the CPU In-memory RDMA . It can be used to access the CPU directly to access the CPU , but is expensive and cost full access to main memory . It is used by using two common RDMA techniques , one-sided RDMA to bypass CPU and remote RDMA access to KVS servers , and another RDMA on the servers themselves . This allows access to a shared key value of the table among distributed clients and speeds up their accesses to the table . At the heart of the paper is a discussion on the benefits of using RDMA for KVS . The challenges are given by moving the workload away from the CPU to the network . RDMA is an important factor in moving workloads from the network to the data center , and its benefits are limited if the network itself is not fast enough . The first approach to use the CPU as the link point between the server and the in- memory RDMA server is to use the CPU as the link point between the server and the in memory RDMA server . This makes PUT per-time access time for transactional operations because of high communication and PUT overhead at the same time . The second approach ( making near linear scalability with multiple NICs ) uses the second-level of abstraction that is provided by the programmable networking layer of the RDDMA abstraction and uses it to support in-network processing . With two NICs in a server , a normal KVS can achieve up to 180 M operations a second , break the previous best of up to 1.2 billion ( 1.7 billion ) with one CPU core . Note that the figure in the table below is for the table that is created by the vanilla Memcached implementation . You can find out more details of the hardware in the video below of the video . The full paper can be found on the link and link ."}
{"ground_truth": "The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns. Approach  Given a motif M , the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S). Mathematically, the aim is to minimise the motif conductance metric given as cutM(S, S\u2019) / min[volM(S), volM(S\u2019)] where S\u2019 is complement of S, cutM(S, S\u2019) = number of instances of M which have atleast one node from both S and S\u2019 and volM(S) = Number of nodes in instances of M that belong only to S.  Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices. The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal. Algorithm  Given the network and motif M, form a motif adjacency matrix WM where WM(i, j) is the number of instances of M that contains i and j.  Compute spectral ordering of the nodes from normalized motif laplacian matrix. Compute prefix set of spectral ordering with small motif conductance. Scalability  Worst case O(m1.5), based on experiments O(m1.2) where m is the number of edges. Advantages  Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well). In case the motif is not known beforehand, the framework can be used to compute significant motifs. The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks.", "prediction": "The paper presents a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimisation of obtained clusters and scales to networks with billions of edges. Link to the code  Approach  Given a network of nodes  , the goal is to find a cluster of nodes S such that the nodes participate in many instances of M (with respect to M) and that the set S minimizes the incidence of cutting (when only a subset of the nodes from M are in S). The set S should minimise the following ratio:  Joining the nodes in S should result in low incidence of cut-offs. Avoiding cutting instances  In practice, it is difficult to determine which subset of nodes from S should be in S given the landscape. The paper uses the following objective function:  Where  S is the set of all the nodes S and  M is the number of node instances in S that reside in S. To identify clusters, the paper uses an objective function called Equation 1 (equation 1) which asks:  The most common clustering pattern is chosen and the nodes are connected in such a way that their average number of connections is small (i.e., few connections between them are fewest). This is known as clustering by factorization. Networks with large number of nodes are more susceptible to falling into one of the two problems (see Equation 2). The paper shows that information propagation units in neuronal networks and in transport networks exhibit higher order organization. Network motif  The idea of a \u201cnetwork of building blocks\u201d is derived from the fact that small subgraphs are building blocks for understanding the behavior of the brain. Given a small set of nodes and a set of connections (connected to the same node), the task is to cluster the nodes according to the given network motif so that the connections between a node and its neighbors are close to each other and avoid cutting out any nodes that are not part of S. This requires finding a cluster with low incidence (low incidence) of cutting instances, but high incidence (high incidence). The key to this objective function is the following equation:  Let H = (H - H), where H is the fraction of nodes that belong in S and H denotes how many nodes should be connected to S. The answer is given in Equation 3 and Equation 4  Given H, compute H(H) where H and H are the nodes of S and S are the edges of H.  H is a factorized graph and H is an element-wise product of H and S. Given H (here, H) and S (here in S), compute the following factorizations:  Equation 5  For each node in S, compute the average H, H, and S-factorized by the factorization factorization  H = H (H, S)  H - H, S, and H-factor  H-style factorizations are those nodes that have high H-frequency interactions with other nodes and the type of H that they belong to (here H-type interactions are those with H- or H-like interactions). For S-style interactions, compute a factorization of H with respect to H such that H = M(H), M(M), and S(S)  The final factorization is the combination of H, M, and the remaining factorizations of S, S.  This factorization can be thought of as the \u201ccluster-of-h\u201d factorization which means that the cluster consists of nodes connected to H (and also to the nodes that don\u2019t belong to H).", "simplified_prediction": "The paper has a good framework for clustering networks based on higher-order connectivity patterns . This framework provides mathematical guarantees on the optimisation of obtained clusters and scales to networks with billions of edges . Link to the code Approach Given a network of nodes , the goal is to find a cluster of nodes S such that the nodes take part in many instances of M ( with respect to M ) . The goal is to find a cluster of cutting ( when only a subset of the nodes from M are in S ) . The set of S should minimise the following ratio : Joining the nodes in S should result in low cut-offs . It is difficult to determine which subset of nodes from S should be in S given the landscape , because it is difficult to find . The paper uses the following objective function : Where S is the set of all the nodes S and M is the number of node instances in S/O. The paper uses an objective function called Equation 1 ( equation 1 ) which asks : The most common clustering pattern is chosen and the nodes are connected in such a way that their average number of connections is small ( i few connections ) . This is called clustering by factorization . Networks with large number of nodes are more susceptible to falling into one of the two problems ( see Equation 2 ) . The paper shows that information using neuronal networks and in transport networks have higher order organization . Network motif The idea of a building blocks name comes from the fact that small subgraphs are building blocks for understanding the behavior of the brain . Given a small set of nodes and a set of connections ( connected to the same node ) , the task is to cluster the nodes according to the given network motif so that the connections between a node and its neighbors are close to each other and avoid cutting out any nodes that are not part of S/O. This requires finding a cluster with low incidence ( low incidence ) of cutting instance . The key to this objective function is the following equation : Let H = ( H - H ) , where H is the fraction of nodes that belong in S and H denotes how many nodes should be connected to S/O. The answer is given in Equation 3 and Equation 4 Given H is a factorized by H. H. H. H. H. H. H. H. H. H. H. H. H. However , the H. H. H. H. However , the H. H. H. H. H. However , the H. H. H. H. H. H. H. H. H. H. However , the H. However , the H. H. H. H. H. However , the H. However , the H. H. However , the However , the However , the H. However , the H For-style interactions , compute a factorization of H with respect to H = M ( H ) , M ( M ) , and S ( S ( S ) The final factorization is the combination of H , M , and the remaining factorizations of S/O. This factorization can be thought of as the person who does not belong to a number of nodes connected to H."}
{"ground_truth": "They suggest a new method to train GANs. They start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions. Each new block of convolutions is slowly blended in, instead of being added from one batch to the next. Combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new CelebA-HQ dataset (CelebA in high resolution). They also suggest a new scoring method based on the approximated Wasserstein distance between real and generated image patches. According to that score, their progressive training method improves results significantly. What  They suggest a new, progressive training method for GANs. The method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images. They also introduce two new normalization techniques. They also suggest a new method to estimate/score the quality of the generated images. They introduce CelebA-HQ, a variation of CelebA containing high resolution images. How  Progressive growing/training  They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN). Visualization:  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images. Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator. They don't switch to the added convolutions instantly/suddenly, but give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B, where A are the features after just upscaling, B are the features after upscaling AND the convolutions and alpha is the overlay factor, which is gradually increased over time. This is done for both the generator and the discriminator and at all resolutions. Visualization:  Note that all layers are always trained (after they were added to the models). Training for the earlier layers does not stop. Training in this way focuses most of the computation on the earlier resolutions. It also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time. Minibatch Standard Deviation  They try to improve diversity by adding a method very similar to minibatch discrimination. They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers). They do this per example in each minibatch, resulting in B*H*W*C standard deviations. (B = batch size, H = height, W = width, C = channels/filters)  They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output. This adds a channel with one constant value to each example in the minibatch. The value is the same for all examples. Equalized Learning Rate  They use Adam for their training. Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight). They argue that this has the downside of equalizing all weight's stepsizes. But some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\"). As a result, the learning rate will be too small for some weights and too large for others. To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1). Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer. (TODO exact formula for c?) (This looks an aweful lot like weight normalization .) Using simpler weight initialization equalizes the dynamic range of parameters. Doing the normalization then fixes problems related to the simpler weight initialization. Pixelwise Feature Vector Normalization in the Generator  They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control. They fix this by normalizing feature vectors in the generator, similar to local response normalization. They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):  Scoring Images  They suggest a new method to score images generated by the generator. They perform the following steps:  Sample 16384 images from the generator and the dataset. Build a Laplacian Pyramid of each image. It begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution. Each level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band). Sample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level. Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches. Normalize each patch with respect to the computed means and standard deviations. Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real). The result is one value. Lower values are better. CelebA-HQ  They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces. They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images. They use an adversarially-trained superresolution model to upscale the images. They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation. They rescale the images to 1024x1024 using bilinear sampling and box filters. They manually select the 30k best looking images. Other stuff  They use Adam for training (alpha=0.001, beta1=0, beta2=0.99). They use the WGAN-WP method for training, but LSGAN also works. They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions. They also add regularization loss on the discriminator, punishing outputs that are very far away from 0. Their model for CelebA-HQ training is similar to a standard DCGAN model. The generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling. They start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator. They use leaky ReLUs in the generator and discriminator. They remove batch normalization everywhere. Results  Scores  Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:  So progressive growing (b) significantly improves results. Same -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g). Minibatch discrimination worsened the results. Using small batch sizes also worsened the results. In (d) they \"adjusted the hyperparameters\" (??) and removed batch normalization. They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models. They achieve an Inception Score of 8.80 on CIFAR-10. Images look improved. CelebA-HQ example results:  LSUN dining room, horse, kitchen, churches:", "prediction": "The basic idea is to introduce a curriculum into the GAN training procedure. One starts by training the generator to produce 4 x 4 images, progressively adding layers to increase the resolution. In the paper, they generated high-quality 1024 x 1024 samples from CelebA, LSUN, and CIFAR-10. This is a nice applied paper where the core idea is quite simple and explained clearly. They describe all of the challenges hidden under the surface of training large-scale GANs and tell the reader how they tackled them. Lots of good deep learning voodoo in this paper. They found that the progressive scheme helps the models to overall improve (in terms of model stability) and reduces total training time by about a factor of 2. They mainly use the following changes to their models:  (1) They now use the Wasserstein loss (a derivative of the Jensen-Shannon divergence that was previously used) to train the generator. Previously, the divergence was calculated based on the difference between the input image and the target image. This has been changed to  (2) Consider the following examples:  Let's start out with the standard WGAN loss that we saw in the original GAN paper (i.e. WGAN-GP). Then we can add the following modifications to our WGAN:  We now use a 1x1 convolutional loss that penalizes the difference in image quality between the target and the generated image. We now add a 2x2 convolution that normalizes the image quality to the expected value (subject to the learning rate and learning rate of the discriminator). We now have 3 input images per layer and 3 target images per layers. The first 2 images (high-level) are generated using the standard GAN loss. The 3rd and 4th image (shown in the figure below) are created using the progressive loss. Note that the 3rd image is an example of the final generated image for the test set. In this example, the target images are the images of CelebA (high level image) and LSUN (low level image). The white circles indicate the different activations of the different layers (e.g. 2.5, 4.0). The blue circles indicate activations from different layers in the training set. The following examples show the order in which the layers are added:  Inception score  This metric is introduced to evaluate the quality of the generated samples. It is based on a combination of the mean squared error (measured by the squared error with margin) and the variance squared error  Measuring the variation of generated samples is difficult due to the fact that they are generated from a uniform distribution, which makes them hard to interpret. So the paper introduces a very simple and effective method to try and improve their image quality. They start with low-level examples and gradually add layers, such that the images become progressively higher in resolution. They achieve an inception score of 8.80 in unsupervised mode. (That should probably be higher than the 8.4 achieved by the model in \" Inception with Convolutional Autoencoders\" in the paper). They also experiment with least-squares learning with their GAN, and note that it helps the model to deal with higher levels of detail. Overall, one of my favorite paper this year: it's original, tackles an important problem, proposes a simple and elegant solution to it, and is easy to understand.", "simplified_prediction": "The basic idea is to create a teaching teaching training procedure . One starts by training the generator to produce 4 x 4 images . This is because of adding layers to change . In the paper , they created a high-quality 1024 x 1024 samples from CelebA , LSUN and CIFAR-10 . This is applied paper where the core idea is quite simple and explained clearly . They describe all of the challenges hidden under the surface of the GANs . They tell how they tackle the challenge . The good deep learning voodoo in this paper is good . They found that the progressive scheme helps the models to make better ( in terms of model stability ) and reduces total training time by about a factor of 2 . They mainly use the following changes to their models : ( 1 ) They now use the Wasserstein loss ( a derivative of the Jensen-Shannon that was previously used ) to train the generator . Before that , the divergence was calculated based on the difference between the input image and the target image . This has been changed to ( 2 ) Consider the following examples : Let's start out with the standard WGAN loss that we saw in the original paper . WGAN-GP Then we can add the following changes to our WGAN : We now use a 1x1 convolutional loss that gives the difference in image quality between the target and the generated image . We now add a 2x2 convolution that makes the image quality to the expected value ( subject to the learning rate and learning rate of the discriminator ) . We now have 3 pictures per layer and 3 target images per layer . The first two images ( high-level ) have been made using the standard GAN loss . The 3rd and 4th image ( shown in the figure below ) is shown using a progressive loss . Note that the 3rd image is an example of the last image for the test set . In this example , the target images are the images of a ( high level image ) and LSUN ( low level image ) . The white circles indicate the different parts of the different layers . 2 , 2 . The blue circles show changes from different layers in the training set . The following examples show the order in which the layers are added . The following examples show the order in which the layers are added . It is based on a combination of the mean squared error ( measured by the squared error with margin ) and the squared error of the variation of generated samples is difficult due to the fact that they are generated from a uniform distribution , which makes them hard to interpret . The paper has a very simple and effective way to try and improve their image quality . They start with low-level examples and gradually add layers , such that the images become higher in resolution . They achieve an inception score of 8.80 people unsupervised mode . That should probably be higher than the 8.4 achieved by the model in people who wanted to buy Autoencoders . This was done in the paper . They also experiment with least learning with their GAN , and note that it helps the model to deal with the higher levels of detail . One of his favorite paper this year : it ''s original , tackles an important problem , proposes a simple and elegant solution to it , and is easy to understand ."}
{"ground_truth": "In this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error. Succinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by \u201cconvexifying\u201d them. They note that their method doesn\u2019t work with max pooling or very deep CNNs with lots of bells and whistles. This is a thought-provoking paper. I like how the authors pursued a theoretically interesting question, even though there isn\u2019t much practical use yet for this. I don\u2019t have personal experience writing theory papers, but I imagine that this is a good(?) representation of how they often go in ML. The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results.", "prediction": "The paper introduces a new model class called as Convexified Convolutional Neural Networks (CCNNs) which captures the effect of parameter sharing in a convex manner by using a low-rank matrix for the CNN parameters. Benefits  Sparsity  Non-linearity  Each non-linear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing that the same filter is applied to each patch. Caution  The model is hard to train, because the activation function of a CNN is nonlinear. In order to address this issue, the authors propose to reduce the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our previous work on RKHS filters, in which we developed this step for fully connected neural networks. Contribution  The authors show that the model can be trained in a layer-wise manner, i.e. that the parameters of the network are shared across all the layers, in such a way that the input-to-output mapping (IRM) transformation is non-zero and the learning rate is stochastic. They train the network on MNIST, CIFAR-10, and handwritten MNIST datasets and show that it converges to the state-of-the-art in terms of generalization error. They also show that their model is able to outperform SVMs, networks trained with backpropagation, stacked denoising auto-encoders, fully connected networks, and SVMs and other baseline methods. Notes  The paper mentions two possible reasons for their model being different:  One, that the inputs are not directly from the training set, but from the pre-trained models. Two, that they use a different learning rate for each layer (i.e., not one per layer, but multiple layers) and that there is a difference in learning rate between the \"best\" and \"average\" learning rate of the different layers. The paper also shows that the variance of the variance between the best and the average learning rate (based on the number of layers) is lower for the two-layer model, which supports the idea of \"best learning rate\"", "simplified_prediction": "The paper shows a new model class called '' Convexified Convolutional Neural Networks '' which captures the effect of parameter sharing in a convex manner by using a low-rank matrix for the CNN parameters . Each convolutional filter acts only on a local patch of the input , and ( ii ) parameter sharing that the same filter is applied to each patch ( ii ) parameter sharing the same filter . It is hard to train , because the activation function of a CNN is hard to train . In order to address this issue , the authors propose to reduce the class of CNN filters to a kernel called Hilbert space . This is inspired by our previous work on RKHS filters , in which we developed this step for a long time . The authors show that the model can be trained in a special way of using a layer-wise manner . The network 's parameters are shared across all the layers , in such a way that the input-to-output mapping ( IRM ) change is non-zero and the learning rate . They train the network on MNIST and the handwritten MNIST datasets and show that it meets the state-of-the-art in terms of generalization . They also show that their model is able to make the SVMs , networks trained with backpropagation , stacked auto-encoders , fully connected networks and other baseline methods . Notes The paper mentions two possible reasons for their model being different : One , that the inputs are not directly from the training set , but from the pre-trained models . Two , that they use a different learning rate for each layer ( i . e. , not one per layer , but many layers ) and that there is a difference in learning rate between the most people who live in the country . The paper also shows that the variance of the best and the average learning rate ( based on the number of layers ) is lower for the two-layer model , which supports the idea of people know about how people learn ."}
{"ground_truth": "Proposes a novel, end-to-end architecture for generating short email responses. Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses. . Challenges in deploying Smart Reply in a user-facing product  Responses must always be of high quality. Ensured by constructing a target response set to select responses from. The likelihood of choosing the responses must be maximised. Ensured by normalising the responses and enforcing diversity. The system should not add latency to emails. Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline. Computation time is further reduced by finding approximate best result instead of the best result. Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system. Architecture  Preprocess Email  Perform actions like language detection, tokenization, sentence segmentation etc on the input email. Triggering Model  A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses. Data  Training set of pairs (o, y) where o is the incoming message and y is a boolean variable to indicate if the message had a response. Features  Unigrams, bigrams from the messages. Signals like - is the recipient in the contact list of the sender. Response Selection  LSTM network to predict the approximate best response for an incoming message o  Network  Sequence to Sequence Learning. Reads the input message (token by token) and encode a vector representation. Compute softmax to get the probability of first output token given the input token sequence. Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token. During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach. Response Set Generation  Generate a set of high-quality responses that also capture the variability in the intent of the response. Canonicalize the email response by extracting the semantic structure using a dependency parser. Partition all response messages into \"semantic\" clusters. These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses. Semantic Intent Clustering  Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used. Graph Construction  Manually define a few clusters with a small number of example responses for each cluster. Construct a graph with frequent response messages (including the labelled nodes) as response nodes (VR). For each response node, extract a set of feature nodes (VF) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node. Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph. After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence. For validation, extract the top k members of each cluster and validate the quality with help of human evaluators. Suggestion Diversity  Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses. If the top two responses contain at least one positive (negative) response and none of the top three responses is negative (positive), the third response is replaced with a negative (positive) one. This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set. Strengths  The system is already in production and assists with around 10% of all mobile responses. This comment has been minimized. Sign in to view  Copy link  Quote reply  vegetakarthhik commented  Nov 26, 2018  hey do you have python implementation?", "prediction": "The paper presents some general characteristics that intelligent machines should possess, and a roadmap to develop such intelligent machines in small, realistic steps. Characteristics  Communication and Learning  The intelligent agent should be able to communicate with humans, preferably using language as the medium. Such a communication medium can be used to teach the machine basic tasks like spelling, grammar, vocabulary etc. A simulated environment to educate the agent to acquire these skills is suggested. The environment should be interactive and the agent should not be isolated from the outside world. Skills to Learn  Skills to learn should come from both the environment and the external environment. For example, the environment could provide a few examples to the agent so that the agent knows how to differentiate between them. The agent should also know how to distinguish between real words from the examples provided by the environment. Algorithmic Types  Learning of Algorithms  Given a learning environment and a learning agent, the following algorithms should be considered:  Simple - Assign a small number of facts and skills to the learning agent  Simple + Select from a set of known algorithms and use their weights as learning weights  Regular - Choose from a fixed set of chosen algorithms  Regular + select from a random set of candidate algorithms  General - Pick a learning algorithm and use its weights as a learning weight  L1 penalty  L2 penalty  Simultaneously - Choose an algorithm that maximizes the L1-likelihood of a given input (and use L2 as the learning weight)  Multi-threaded - Use multiple threads to transfer data and skills  Long-Term Memory  Use a long-term memory to store both the learned skills and the data  Simulated Environment  Environment  Agent  Learner  Start with a simple environment and teach the basics using natural language. This environment should encourage the learner to acquire \u201ccommon sense\u201d and \u201creasoning\u201d  Teacher  Provides a short-term supervised environment and an intermediate target which should lead to the true learning environment. The learner\u2019s goal is to master the basics so that it can access the full potential of the learning environment in a much smaller time-frame. The teacher  Provides an intermediate goal which encourages the Learner to explore the environment more and more and learns from the mistakes made by the Teacher  The intermediate goal should be a smaller version of the final machine which should act like a stand-alone intelligent system albeit one that will be initially very limited in what it can do. Future Work  The Learner should access the intermediate target and learn from it in a way that it learns from its mistakes and biases. Further, the Teacher should provide more information about the tasks and how to solve them. This information should be incorporated into the training and the Learning Agent  The Learning Agent should be free to use whatever programming language it likes and to interact with the Teacher whenever the Teacher is not around. It should be possible for the Teacher to provide additional information (for example, spelling or grammar) or to supervise the learning Agent. The Teacher should be aware of this and should not expect too much from the Teacher as it is a natural instinct to teach by example and not from pre-programming. Eventually, the Agent should acquire enough knowledge to perform many tasks independently and profitably. The Agent should have access to a large amount of data which should be stored in its own internal memory. The memory should be large enough that it should support many learning agents and not require copying across tasks. Learning  Learners should start out by memorizing facts and symbols. Then they learn to associate symbols and numbers (from the environment) and use these to generate new facts and numbers. They use the new knowledge to", "simplified_prediction": "Some people think that intelligent machines should possess , and a roadmap to develop such intelligent machines in small , realistic steps . Communication and Learning The intelligent agent should be able to communicate with humans . They should be able to communicate with the language . They can be used to teach the machine basic tasks like spelling , grammar , or other tasks . A simulated environment to get the agent to get these skills is suggested . The environment should change and the agent should not be isolated from the outside world . Skills to Learn Skills to learn should come from both the environment and the environment . For example , the environment could provide a few examples to the agent so that the agent knows how to differentiate between them . The agent should also know how to tell how real words come from the example '' environment '' . Algorithms Given a learning environment and a learning agent , the following algorithms should be considered : Simple - Assign a small number of facts and skills to the learning agent Simple + Select from a set of known algorithms and use their weights as learning weights , General - Choose from a fixed weight and a fixed set of using algorithms such as a single algorithm ( LSDLPLPL ) . This environment should encourage the learner to find the way people wanted to live , but they wanted a short-term supervised environment and an intermediate target which should lead to the true learning environment . It can access the full potential of the learning environment in a much smaller time-frame so that it can access the full potential of the learning environment . The teacher wants to explore the environment more and more and learns from the mistakes made by the Teacher The intermediate goal should be a smaller version of the final machine which should act like a stand-alone intelligent system that will be very limited in what it can do . Future The Learner should access the intermediate target and learn from it in a way that it learns from its mistakes and biases . Further , the Teacher should provide more information about the tasks and how to solve them . This information should be put into the training and the Learning Agent should be free to use whatever programming language it likes and to talk about the Teacher whenever the Teacher is not around . It should be possible for the Teacher to provide more information ( for example , spelling or grammar ) or to help people learn the learning Agent . The Teacher should be aware of this and should not expect too much from the Teacher as it is a natural way to teach by example and not from pre-programming . Eventually , the Agent should acquire enough knowledge to perform many tasks independently . The Agent should have access to a large amount of data which should be stored in the internal memory . The memory should be large enough that it should support many learning agents and not need money to learn it . Learning Learners should start out by using the fact and symbols . Then they learn to learn the symbols and numbers ( from the environment ) . They also use new facts and numbers . They use the new knowledge to see ."}
{"ground_truth": "Build a supervised reading comprehension data set using news corpus. Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task. Reading Comprehension  Estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query. Dataset Generation  Use online newspapers (CNN and DailyMail) and their matching summaries. Parse summaries and bullet points into Cloze style questions. Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder. Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers. The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence. Models  Baseline Models  Majority Baseline  Picks the most frequently observed entity in the context document. Exclusive Majority  Picks the most frequently observed entity in the context document which is not observed in the query. Symbolic Matching Models  Frame-Semantic Parsing  Parse the sentence to find predicates to answer questions like \"who did what to whom\". Extracting entity-predicate triples (e1,V, e2) from query q and context document d  Resolve queries using rules like exact match, matching entity etc. Word Distance Benchmark  Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity. Sum the distance of every word in q to their nearest aligned word in d  Neural Network Models  Deep LSTM Reader  Test the ability of Deep LSTM encoders to handle significantly longer sequences. Feed the document query pair as a single large document, one word at a time. Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output. Attentive Reader  Employ attention model to overcome the bottleneck of fixed width hidden vector. Encode the document and the query using separate bidirectional single layer LSTM. Query encoding is obtained by concatenating the final forward and backwards outputs. Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs). The weights can be interpreted as the degree to which the network attends to a particular token in the document. Model completed by defining a non-linear combination of document and query embedding. Impatient Reader  As an add-on to the attentive reader, the model can re-read the document as each query token is read. Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding. Result  Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling. Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query. Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure. Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document. The paper also includes heat maps over the context documents to visualise the attention mechanism. This comment has been minimized. Sign in to view  Copy link  Quote reply  yauhen-info commented  Apr 28, 2017  Thank you for sharing a good piece of work. Let me also ask if you had found a link to an implementation of the Attentive and Impatient Readers?", "prediction": "The paper introduces a new methodology that provides a large scale supervised reading comprehension data set which allows to train a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure. Link to the implementation  Approach  Traditional approaches to machine reading and comprehension based on hand engineered grammars or information extraction methods of detecting predicate argument triples that can later be queried as a relational database. This approach fails to manage transition from synthetic data to real environments as such closed worlds \u201cexpect to capture the complexity, richness, and noise\u201d of natural language. Attention Based Attention Mechanism  Recent developments for incorporating attention mechanisms into recurrent neural network architectures allow a model to focus on aspects of a document that it believes will help it answer a question, and also allows us to visualise its inference process. Attentive and Impatient Readers  The attention mechanism is just one instantiation of a very general idea which can be further exploited. However, the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not scale linearly with the data set size. The paper proposes to train the models using a combination of recurrent and attention based neural networks. The recurrent neural networks are adapted to take into account the fact that the attention mechanism cannot be directly incorporated into the model\u2019s architecture. Attention mechanism is incorporated via an attention scoring mechanism which allows the model to select the salient features of the input document such that these are the salient points in the document and use these to score the document as a feature vector. Impatiently  An attention scoring system is used where an attention score is assigned to each document point and a score between 0 and 1 indicates how much of the document is of interest to the model  Impatient  A patient reading model is trained to answer the question \u201cwhat\u201d  The document is fed to the attention scoring network and the score between 1 (high score) and 5 (low score) indicate how well the model understood the document  Attention scoring is based on a binary cross-entropy scoring function which aims to measure the degree of agreement between the document points and the question points  Paraphrasing (corresponding to the entities)  A paraphrasing sentence  A news story  The paper uses as input two news stories from CNN and DailyMail.com as the main source of information for the attention score. Two attention scoring models are trained and the results are presented below. Note that the datasets used for training and validation is much smaller than the one used in the state-of-the-art (SoTA) paper which uses 763 tokens and 27 entities and is comparable to the results of the Behavioural Imputation Corpus (BOC) paper . Dataset Generation  The main goal of the paper was to generate a large enough dataset to be able to test the proposed attention based models on which the models can learn to answer natural language questions. Two datasets were used to this end:  CNN Data  A short, short (shortly, short-term) version of each news story was converted to context query answer triplets using simple entity detection and anonymisation algorithms. The second dataset consisted of a longer, longer (more complex and longer) version that doubled the length of the original document triplet triplet and used as the context answer triplet. Data was post processed using a custom anonymisation algorithm which took as input a news story and an entity embedding vector and used the vectors to classify the entities in the embeddings. The input to the network was the embedding vectors and the output was a vector", "simplified_prediction": "The paper shows a new methodology that provides a large scale supervised reading data set which allows to train a class of attention based deep neural networks that learn to read real documents and answer complex questions with very little knowledge of language structure . Link to the implementation of Approach Traditional approaches to machine reading and comprehension based on hand engineered grammars or information extraction methods of detecting argument triples that can later be used as a relational database . This does not move from synthetic data to real environments as such closed worlds because they want to capture the complexity , richness , and noise about natural language . Recent developments for using attention mechanisms into recurrent neural network architectures allow a model to focus on aspects of a document that it believes will help it answer a question , and also allows us to visualise its inference process . Readers The attention mechanism is just one instantiation of a very general idea . This idea can also be used by using a very general idea . However , the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not have the same size as the data set size . The paper proposes to train the models using a combination of attention and attention based neural networks . The recurrent neural networks are to take into account the fact that the attention mechanism cannot be changed into the model making of the building . This allows the model to select the salient features of the document such that these are the salient points in the document and use these to score the document as a feature vector . Impatiently An attention scoring system is used where an attention score is assigned to each document point and a score between 0 and 1 indicates how much of the document is of interest to the model Impatient A patient reading model is trained to answer the question situation between the document is fed to the attention scoring network and the score between 1 ( high score ) and 5 ( low score ) indicate how well the model understood the document '' document '' attention is scoring based on a cross-tropary scale . Two of these models are trained and the results are given below . Note that the datasets used for training and validation is much smaller than the one used in the state-of-the-art ( SoTA ) paper which uses 763 tokens and 27 people who are compared to the results of a paper . The main goal of the paper was to make a large enough dataset to be able to test the proposed attention based models on which the models can learn to answer natural language questions . Two datasets were used to this end : CNN Data A short ( short , short-term ) version of each news story was changed to talk about using simple entity and anonymisation algorithm . The second dataset consisted of a longer , longer version that doubled the length of the original document triplet and used as the context answer triplet . Data post processed using a custom used algorithm which took as a news story and an embedding vector and used the vectors to show the algorithms in the embeddings . The input to the network was a vector and the output was a vector ."}
{"ground_truth": "RPCValet: NI-driven tail-aware balancing of \u00b5s-scale RPCs Daglis et al., ASPLOS\u201919  Last week we learned about the [increased tail-latency sensitivity of microservices based applications with high RPC fan-outs. Seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager. Today\u2019s paper choice, RPCValet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s). The net result of rapid advancements in the networking world is that inter-tier communications latency will approach the fundamental lower bound of speed-of-light propagation in the foreseeable future. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network. Furthermore, the evaluation shows that \u201cRPCValet leaves no significant room for improvement\u201d when compared against the theoretical ideal (it comes within 3-15%). So what we have here is a glimpse of the limits for low-latency RPCs under load. When it\u2019s no longer physically possible to go meaningfully faster, further application-level performance gains will have to come from eliminating RPCs altogether. RPCValet balances incoming RPC requests among the multiple cores of a server. Consider for example a Redis server maintaining a sorted array in memory\u2026  \u2026 an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few \u00b5s while new translations are installed. While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server. In theory, how fast could we go? Consider a 16-core server handling 16 requests. We could put anywhere from 1 to 16 queues in front of those cores. At one extreme we have a \u201816 x 1\u2019 architecture with 16 queues each with one associated processing unit. At the other extreme is a \u20181 x 16\u2019 architecture with one shared queue serving all 16 processing units. Or we could have e.g. a \u20184 x 4\u2019 with 4 queues each serving 4 units, and so on\u2026  If you model that out with Poisson arrivals and variety of different service time distributions (fixed, uniform, exponential, and generalised extreme value, GEV) what you\u2019ll find is that the \u20181\u00d716\u2019 architecture performs the best, and the \u201816\u00d71\u2019 architecture performs the worst. 1 x 16 significantly outperforms 16 x 1. 16 x 1\u2019s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than 1 x 16 under a tail latency SLO at 10x the mean service time\u2026 The theoretical results suggest that systems should implement a queueing configuration that is as close as possible to a single queue (1 x 16) configuration. In practice those models failed to account for a significant source of overhead: synchronizing across multiple cores on the single queue. When you take synchronisation into account, a dedicated queue per core starts to look like a good idea again. NICs supporting Receive-Side Scaling (RSS) can push messages into each core\u2019s queue, but while RSS can help to achieve load distribution, it can\u2019t truly achieving load balancing. Any resulting load imbalance after applying these [RSS] rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with \u00b5s-scale service times. Introducing RPCValet  RPCValet uses a push-based model, while also taking into account the current loading of the cores. It\u2019s designed for \u201cemerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.\u201d. The key hardware feature is that the network interface has direct access to the server\u2019s memory hierarchy, eliminating round trips over e.g. PCIe. Servers register a part of their DRAM into a partitioned global address space (PGAS), where every server can read and write memory in RDMA fashion. (We\u2019re not given any details on system reconfiguration etc.). An integrated NI can, with proper hardware support, monitor each core\u2019s state and steer RPCs to the least loaded cores. Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g. ~ 1.5 \u00b5s for a 3-hop posted PCIe transaction) would mean that the NI will make delayed\u2014hence sub-optimal, or even wrong\u2014 decisions until the information arrives. With N participants in the system, RPCValet first writes every incoming message into a single PGAS-resident message buffer of NxS slots. Then it notifies the selected core to process the request. Message arrival and memory location are thus decoupled from the assignment of a core for processing. We have the synchronization-free zero-copy behaviour of a partitioned multi-queue architecture, together with the load-balancing flexibility of a single queue system. In the implementation each node maintains a send and a receive buffer. Send buffers contain a valid bit indicating whether they are currently being used, and a pointer to a buffer in local memory containing the payload. Receive buffers on the other hand have slots that are size to accommodate message payloads directly. Overall, the messaging mechanism\u2019s memory footprint is 32 x N x S + (max_msg_size + 64) x N x S bytes. That is, a few tens of MB at most. The implementation uses a simple scheme to estimate core loads. RPCValet simply keeps track of the number of outstanding send requests assigned to each core. Allowing only one inflight request per core corresponds to a true single-system queue behaviour, but introduces a small inefficiency waiting for the notification of completed request processing. A practical compromise is to allow two outstanding requests per core. This results in marginal performance gains over the single request design for ultra-fast RPCs with service times of a few nanoseconds. All IN backends independently handle incoming network packets and access memory directly, but they hand off to a single NI dispatcher (over the on-chip interconnect) that is statically assigned to dispatch requests to cores for servicing. \u2026 for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible (just a few ns). Evaluation  The evaluation is conducted using against three different service implementations: a synthetic service that emulates different service time distributions; the HERD key-value store, and the Masstree data store. An SLO of less than or equal to 10x the mean service time is assumed in each case, and configurations are evaluated in terms of throughput under SLO. Compared to a software implementation, which requires a synchronisation primitive, the hardware implementation delivers 2.3x-2.7x higher throughput under SLO. The following plots show the performance of different queueing arrangements under the three workloads, with the \u20181\u00d716\u2019 arrangement that RPCValet simulates performing the best as expected. ( Enlarge )  Compared to a theoretically optimal 1 x 16 model, RPCValet gets within 3-15% (depending on the service time distribution). \u201cWe attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model.\u201d  At the end of the day though:  RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns. Throughput under tight tail latency goals is improved by up to 1.4x, and tail latency before saturation is reduced by up to 4x.", "prediction": "RPCValet: NI-Driven Tail-Aware Balancing of s-scale RPCs \u2013 Daglis et al. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS 19)  Today\u2019s paper choice is part of an ongoing theme in the \u201cTail at Scale\u201d challenge in the datacenter space: dealing with the tail of online services at scale. Tail-tolerant computing is one of the major ongoing challenges in the  hardware-driven solutions space, as long-tail events are rare and rooted in convoluted hardware-software interactions. A key contributor to the well-known \u201c Tail at Scale challenge\u2026 is the deployment of online service software stacks in numerous communicating tiers, where the interactions between a services software stacks take the form of Remote Procedure Calls (RPCs). Large-scale software is often built in this fashion to ensure modularity, portability, and development velocity. Not only does each incoming request result in a wide fan-out of inter-tier RPCs, each one lies directly on the critical path between the user and the service, and thus bound the slowest interaction a user may have with the service. The trend towards ephemeral and fungible software tiers has created a challenge to preserve the benefits of multi-tiered software while making it tail tolerant. It would be nice if we could get rid of the notion of \u201cuser-level\u201d protocols, and replace them with user-friendly user-terminated ones (e.g., InfiniBand/RDMA), but this seems to be a long-term trend\u2026  As it seems to us to be more practical to abandon user-level protocols in favour of integrated Network Interfaces (NIs). We are seeing a shift away from POSIX sockets and TCP/IP to lean towards user-specific protocols such as InfiniBands and RDMA. The net result of this is that the fundamental approach of communication latency will lower bound in the foreseeable future, and hence lower bound of speed-of-light propagation in the tail. The fundamental challenge exacerbates the difficulty in dealing with tail at scale as the number of concurrent requests increases. In response to this, we\u2019re seeing the emergence of \u2018inter-scale\u2019 (i.e., large-scale) NIs, and a shift towards more inter-scale communication between small and medium-scale NIs. The convergence of these two technologies will see a significant reduction in the time taken to send a request to the server, and an improvement in the average response time (latency). With a few more cores on the network (and more importantly, more compute power to compute them), the challenges of tail-tolerance will be even greater. We\u2019d like to get to the bottom of this as soon as possible, but it looks like this won\u2019t be possible\u2026  In \u2018tuck at scale\u2019 we have the following challenges:  We have a need to find a solution to the tail-latency problem we can both understand and control  Online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, \u201cshort\u201d RPCs. This means that a single \u201csingle-queue\u201d request could be one of a series of long, intermediate requests, causing the need for faster inter-server communication. We want to know which of these short requests are the most important, and therefore the most impactful, so that we can intelligently and efficiently handle", "simplified_prediction": "PCValet : NI-Driven Tail-Aware Balancing of s-scale RPCs , Daglis et al . Today a lot of people call him Support for Programming Languages and Operating Systems ( ASPLOS 19 ) . Today , it is part of an ongoing theme in the main theme of online services at scale . Tail-tolerant computing is one of the major challenges in the hardware-driven solutions space , as long-tail events are rare and rooted in hardware-software interactions . Tail at Scale challenge... is the use of online service software stacks in many communicating tiers , where the interactions between a services software stacks take the form of Remote Procedure Calls ( RPCs ) . Large-scale software is often built in this fashion to make modularity , portability , and development velocity more . Not only incoming request result in a wide fan-out of RPCs , each one is directly on the critical path between the user and the service , and the slowest interaction a user may have with the service . The difference between ephemeral and fungible software tiers has created a challenge to keep the benefits of software while making it tail tolerant . It would be nice if we could get rid of the notion of many things , and replace them with user-friendly user-terminated ones ( e . g , InfiniBand / RDMA ) , but this seems to be a long-term trend of NIs , as it seems to be more practical to leave user-level protocols in favour of the Network Interface . We seeing a shift away from POSIX sockets and TCP / IP to lean towards user-specific protocols like InfiniBands and RDMA . The net result is that the fundamental approach of communication latency will lower bound in the future . This is because lower borders between speed-of-light propagation in the tail . The fundamental challenge exacerbates the difficulty in dealing with tail at scale as the number of requests increase . In response to this , we can seeing the emergence of people who want to get food ( e . , large-scale NIs , and a shift towards more inter-scale communication between small and medium-scale NIs . These two technologies will see a significant reduction in the time taken to send a request to the server , and an improvement in the average response time ( latency ) to the server . With a few more cores on the network ( and more importantly , more compute power to compute them ) , the challenges of tail-tolerance will be even greater . We can get like to get to the bottom of this as soon as possible , but it looks like this won all the way the way they need to find a solution to find a solution to the tail-latency problem we can both understand and control Online services come with stringent quality in terms of response time tail latency . Because of their decomposition into good-grained communicating software layers , a single user asked fans out into a short way of making it better known . This means that a single number of people living there could be one of a series of long , intermediate requests , causing the need for faster inter-server communication . We want to know which of these short requests are the most important , so that we can intelligently and efficiently handle all of the time ."}
{"ground_truth": "Scalable Atomic Visibility with RAMP Transactions \u2013 Bailis et al. 2014  RAMP transactions came up last week as part of the secret sauce in Coordination avoidance in database systems that contributed to a 25x improvement on the TPC-C benchmark. So what exactly are RAMP transactions and why might we need them? As soon as you partition your database across multiple servers, things start to get interesting. We\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026  The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. A lot of implemented systems have chosen to go with the fast-and-furious option resulting in incorrect behaviour for cases where atomic visibility matters. The RAMP (Read Atomic Multiple Partition) transaction models introduced in this paper show that you can have performance and scalability of transactions spanning multiple partitions with atomic visibility. \u2026data stores like Bigtable, Dynamo, and many popular \u201cNoSQL\u201d and even some \u201cNewSQL\u201d stores do not provide transactional guarantees for multi-item operations. The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics. Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described. Under evaluation, the RAMP algorithms did not degrade substantially under contention, and scaled linearly to over 7.1 million operations per second on 100 servers. Bad things that can happen when you don\u2019t have atomic multi-partition isolation  Without atomic isolation foreign key constraints, secondary indexing, and materialized view maintenance can all break! Data models often represent bi-directional relationships as two distinct uni-directional relationships. \u201cFor example, in TAO, a user performing a \u2018like\u2019 action on a Facebook page produces updates to both the LIKES and LIKED_BY associations.\u201d  These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access. Without atomic isolation broken bi-directional relationships, and dangling or incorrect references can surface. With data partitioned across servers by primary key, access by secondary attributes becomes more challenging. There are two dominant strategies for distributed secondary indexing. First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server. This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads. Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data. This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two)  Real-world services tend to use either local secondary indexing (non-scalable but correct), or non-atomic (scalable but incorrect) global indexes. In the latter cases queries involving the secondary attributes can return records that shouldn\u2019t match, and omit ones that should. Without atomic isolation, materialized views can diverge from the base data. For example, a count may become inaccurate. With RAMP transactions, base data and views can be updated atomically. The physical maintenance of a view depends on its specification, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition. For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance). For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter. Scalability Requirements  Consider databases that are partitioned over multiple servers. Each item has a single logical copy stored on one of those partitions, which one can be calculated using the item itself (e.g. primary key). In order to achieve scalability the author\u2019s identify two key properties that must be preserved: synchronization independence, and partition independence. Synchronization independence ensures that one client\u2019s transactions cannot cause another client\u2019s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition). (Also known as transactional availability). Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access. Thus, a partition failure only affects transactions that access items contained on the partition. This also reduces load on servers not directly involved in a transaction\u2019s execution. In the distributed systems literature, partition independence for replicated data is called replica availability or genuine partial replication. A third constraint is that the metadata required to achieve synchronization and partition independence is not too large: \u201cthere are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state.\u201d  The RAMP transaction algorithms  You may be wondering why I keep referring to algorithms (plural). This is because the authors actually define three RAMP variants: RAMP-Fast, RAMP-Small, and RAMP-Hybrid. These trade-off between performance and the amount of metadata that needs to be kept. At a high level, RAMP transactions allow reads and writes to proceed concurrently. This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction\u2019s writes, violating RA (i.e., fractured reads might occur). Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions. To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions. RAMP-Fast stores metadata in the form of write sets (thus the overhead is linear in transaction size), and has one RTT for reads in the best case (two in the worst case). RAMP-Small uses constant size metadata (it only stores the transaction timestamp) but always requires two RTT for reads. RAMP-Hybrid takes the same write set information as RAMP-Fast, but encodes it in a Bloom filter. With no false positives from the filter, Ramp-Hybrid would therefore behave as RAMP-Fast. And with all false positives, it behaves as RAMP-Small. All of the variants require two RTTs/transaction for writes. The two-phase atomic commitment protocol used by RAMP ensures readers never block waiting for writes to arrive. It is known that every atomic commitment protocol may block during failures. Blocked writes instead act as \u201cresource leaks\u201d on partitions: partitions will retain prepared versions indefinitely unless action is taken. To \u201cfree\u201d these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP). CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT\u2026 Compared to alternatives (e.g. replicating clients), we have found CTP to be both lightweight and effective. There is of course much more detail in the full paper, which I encourage you to go on and read. Section 6 on Related Work contains a nice short summary of isolation guarantees in the wild. \u201cIn recent years, many \u2018NoSQL\u2019 designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation\u2026\u201d", "prediction": "Read Atomic Multi-Partition (RAMP) transactions that enforce atomic visibility while providing scalability, two-phase locking, and guaranteed commit despite partial failures Gog et al., SIGMOD\u201914  With growing amounts of data and unprecedented query volume, distributed databases increasingly split their data across multiple servers, or partitions, such that no one partition contains an entire copy of the database. This strategy succeeds in allowing near-unlimited scalability for operations that access single partitions. But for the many other applications that access multiple partitions and need to communicate across serversoften synchronously in order to provide correct behavior, it\u2019s not so easy. Designing systems and algorithms that tolerate these communication delays is a difficult task but is key to maintaining scalability. In this work, we address a largely underserved class of applications requiring multi-partition, atomically visible transactional access: cases where all or none of each transactions effects should be visible. The status quo for these types of atomic transactions provides an uncomfortable choice between algorithms that deliver consistent results but are slow and unavailable under failure, and those that provide fast and scalable results but don\u2019t provide proper semantics for operations on arbitrary sets of data items. Read Atomic (RA) isolation \u2013 all-or-nothing visibility of updates  Read Atomic  The core idea in this work is to use the ACID isolation effects of ACID atomicity to mediate atomic visibility of transactions\u2019 updates. This differs from uses of atomicity (e.g., linearizability) to denote serializability or ACID-induced atomicity in the context of transactions. Consider a set of transactions where either all (or many) of the transactions are observed by all other transactions, or none (and only those that cause no (or few) transactions to be observed by other transactions. At issue is the following scenario:  Suppose we have a database containing two sets of operations, one for reads and another for updates to the database, where reads can come from either one or more transactions. For the reads that require atomic visibility, we have the following trade-offs to consider:  Primary indexing (indexing on the database)  Key constraint enforcement (keeping the database up to date)  Primary foreign key (and related to it)  Store updates in the database  Support for materialized view maintenance  Any of the above three use cases can be implemented using the following approaches:  Read-Atomized RamP transactions (read-atomized RAMP)  RAMP transactions are non-serializable transactions that guarantee atomic visibility and scalability because they use multi-party locking  Non-serialized RAP transactions (non-atomic RAP) transactions are transactions where the transactions effects are not visible to other transactions and only visible to others (and hence can be seen by all or many other transactions). In this way, the effects of single-party atomic transactions can be modelled as a single-phase phenomenon. The authors show that under certain conditions (and in the presence of multiple transactions), RAP can provide high scalability (up to 100%) and low overheating (0.5-10%) if all parties involved in the transaction are involved in some way (readers, database operators, or both). If not involved in any way, then the effects can be masked (or masked) by the other parties (or even masked entirely). The authors develop three variants of RAMP transactions that achieve the above objectives:  Single-party RAMP (single-party)  RAMP Transactions that avoid the two phase locking problem  Ramp-agnostic (RPN) transactions  RPN transactions that don\ufffd", "simplified_prediction": "Read atomic Multi-Partition ( RAMP ) agreements that make atomic visibility while providing scalability , two-phase locking , and guaranteed commit despite partial failures Gog et al . , SIGMOD in its own amounts of data and unprecedented query volume , distributed databases across multiple servers , or partition , such that no one part of the data contains of the entire database . This strategy allows the idea to allow near-unlimited scalability for operations that access single partitions . But for the many other applications that access multiple partitions and need to communicate across serversoften synchronously in order to provide correct behavior , it gives correct behavior so easy . Designing systems and algorithms that have tolerate these communication delays is very difficult to keep the scalability system . In this work , we address a largely underserved class of applications needing multi-partition , which can be seen in a special way : cases where all or none of each transactions effects should be visible . The status quo for these types of atomic transactions provides an uncomfortable choice between algorithms that deliver consistent results but are slow and unavailable under failure , and those that provide fast and scalable results but do not have proper semantics for operations on arbitrary sets of data items . Read Atomic ( RA ) The core idea in this work is to use the ACID isolation effects of AC atomicity to mediate atomic visibility of transactions in the same work as the ACID isolation effects . This differs from uses of atomicity ( e . g . , linearizability ) to show serializability or which is used in the context of transactions . A set of transactions where either all ( or many ) of the transactions are observed by all other transactions , or none ( and only those that cause no ( or few ) transactions to be seen by other transactions . At issue is the following way : Suppose we have a database that contains two sets of operations , one for updates to the database , where reads can come from either one or more transactions . The reads that require vistrainibility , we have the following trade-offs to consider : Primary indexing ( indexing on the database ) Key constraint enforcement ( keeping the database up to date ) Primary foreign key ( and related to it ) Store updates in the database Support for materialized any of the above three use cases can be implemented using the following approaches : Read-Atomized RamP transactions ( read-a ) . In this way , the effects of single-party atomic transactions can be used as a single-phase phenomenon . The authors show that under certain conditions ( and in the presence of multiple transactions ) , RAP can provide high scalability and low overheating ( 0.5 - 10 % ) if all parties involved in the transaction are involved in some way ( readers , database operators , or both ) . If not involved in any way , then the effects can be made out by the other parties ( or even masked entirely ) . The authors develop three different kinds of RAMP transactions that achieve the above objectives : Single-party RAMP ( single-party ) RAMP Transactions that avoid the two phase locking problem ( RPN ) transactions that don ."}
{"ground_truth": "Uncertainty propagation in data processing systems Manousakis et al., SoCC\u201918  When I\u2019m writing an edition of The Morning Paper, I often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day. There are three levels of takeaway from today\u2019s paper choice:  If you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence. If you\u2019re savouring a cortado, then you might also want to dip into the techniques we can use to propagate uncertainty through a computation. If you\u2019re lingering over a latte, then the UP (Uncertainty Propagation) framework additionally shows how to integrate these techniques into a dataflow framework. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing, and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. Are you sure? Uncertainty can arise from a number of different sources including probabilistic modelling, machine learning, approximate computing, imprecise sensor data, and such like. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings. If you have a dataflow system with computation based on a DAG, then uncertainty in upstream data values needs to flow through the computation. For example, consider a simple 2-node DAG where an approximate query is used to produce an approximate count of the number of customers in different age groups (e.g., using BlinkDB ), and then we take a weighted average of those groups. The second node will by default produce a single value, but in reality it should result in a distribution. There may be meaningful parts of that distribution where the outcome would be disadvantageous (for example), but the probability of this is completely lost when reporting a single value. Uncertainty propagation  Our method offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation. Consider a function  , where  is an arbitrary function without side-effects representing the computation at a node in a dataflow,  is a set of random variables representing inputs with uncertainties, and  is a set of random variables representing outputs with uncertainties. Depending on the nature of  , we can use different statistical methods to approximate the mean and variance of each variable in the output. When  is a continuous differentiable function we can use first-order Differential Analysis:  The general strategy is to compute  by approximating  using its first-order Taylor series at the expected value of  . This approximation is accurate if  is roughly linear around the support (in other words, neighborhood) of  \u2026  When there are multiple inputs and multiple outputs, the calculation also needs to take into account the covariances between the outputs. When  is a semi-continuous function we have two possibilities. If the support of each input mostly or entirely falls within a continuous differentiable part of the function then we can use Differential Analysis (DA) as before. If it spans a discontinuity then we have to use Monte Carlo simulation. For example, consider the function  when  , and  otherwise. If each input is greater than  then we can use DA. We use Monte Carlo simulation to approximate  for functions  that do not meet (or the developers do not know whether they meet) the requirements for DA. is evaluated on  randomly drawn samples of the input, and the outputs are used as an approximation of  . To generate accurate samples, one must know the joint density of  and pay the heavy computational cost of any rejection-sampling algorithm. Unfortunately that cost grows exponentially with an increasing size of  and thus we resort to two approximations:  Given input distributions, generate samples accordingly and ignore covariances  In the absence of full distributional information, assume that each input is normally distributed with the same mean and covariance matrix as the unknown distribution. (This approximation works because the mean and variance estimation of Y depends solely on the mean and variance of  ). Uncertainty propagation in dataflows  As stated earlier, in a dataflow graph we need to perform uncertainty propagation at all nodes downstream of uncertain data. For Monte Carlo simulation-based uncertainty propagation (UP-MC) we can just treat a node as a black box, dynamically generate samples from the input set, and compute the mean and variance for each output using empirically derived distributions (or assume normal distributions in the absence of this information). The implementation of Differential Analysis (henceforth called UP-DA) is more challenging. Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the outputs\u2026 input covariances can require additional data flow to be added to the DAG for computing output variances and covariances. If the programmer can provide a partial derivative function, then using this often gives better performance than resorting to numerical differentiation. Observe that we might make a saving early in the dataflow by introducing uncertainty (e.g. by computing an approximate result), but then we have to pay more later for the resulting uncertainty propagation. The evaluation explores this trade-off. UP-MapReduce is an implementation of the above ideas in the in MapReduce. The UP-MapReduce extension includes three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions. The extension also introduce the uncertain type PV (Probabilistic Value) which contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution. The UP-DA Continuous Reducer class for example provides an abstract derivative method that a developer can implement to provide a closed-form derivative function. Uncertainty propagation in practice  We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. Baselines for the evaluation are established by running a large Monte Carlo experiment over a precise version of each application. When input errors are small (e.g. below 3%) then UP-MapReduce estimates means with very low error. The following figure shows the relative errors and execution times for the three variants of UP-MC as compared to the baseline. Enlarge  For six of the applications UP-MapReduce is highly accurate, but when input errors are significant (e.g. eig, svd) its estimated relative errors can deviate noticeably from baseline values. The best performance is obtained when using closed-form (user provided) derivatives. tsocial and latency are both multi-stage approximate workflows. The following chart shows the execution times and maximum relative errors for sampling rates from 0.1% to 100% (precise). For tsocial, a sampling rate of 80% or less is required before the overheads of uncertainty propagation are outweighed by the sampling benefits. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low\u2014 an average of 6% performance degradation\u2014 when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computational resources.", "prediction": "The paper presents a framework for processing uncertain data using a DAG-based data processing system called UP-MapReduce that allows developers to modify precise implementations of DAG nodes to process uncertain inputs with modest effort. Uncertainty propagation  Data should be represented as probability distributions or estimated values with error bounds rather than exact values. In many cases performing computations on uncertain data as if it were exact leads to incorrect results. This paper proposes and evaluates an approach for tackling this challenge in DAG based data processing systems. The approach is based on techniques such as Differential Analysis (DA) and Monte Carlo simulation which help programmers to propagate uncertainties through the nodes of the DAG safely and accurately. Datasets  Data is being produced and collected at a tremendous pace. There is an urgent need to process an exploding body of data with uncertainties (suspected values from sensors in IoT). Data uncertainties also arise in many other contexts such as probabilistic modeling, machine learning, approximate storage and sampling-based approximation. For example, data collected using sensors are always estimates but there can be uncertainties the difference between the estimated and true values due to sensor inaccuracies. Failure to properly account for this uncertainty may lead to incorrect result. For some applications, including AI/ML, trend analysis and image processing, it can reduce execution time by 2.3x and up to 5x in some cases. Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains. The framework  DAGs are directed acyclic graphs (DAGs) of side-effect free computation nodes, with data flowing through the edges for processing. At the nodes, the inputs are composed of continuous and discrete functions that are differentiable and continuous-differentiable functions. For discrete functions, the input distribution and the locations of the discontinuities (discontinuities) are chosen from a Gaussian distribution which determines the appropriate computation method to use for the given discrete function. For the discrete function inputs, the choice of computation is made by taking the gaussian distribution and comparing it with the distribution of the discrete functions and choosing the appropriate method from the set of all the inputs  Given the variables  , we know which of the continuous function inputs is the most likely to be at a low or high risk of being at low risk given the distribution  We want the distribution to be a mixture of low and high risk, i.e., a mixture that gives a misleading impression of accuracy. To achieve this, we use a combination of DA (differential analysis based on Monte Carlo simulations) and uncertainty propagation  We use the following approach:  We start from a continuous function input (say from sensors) and compute the following function outputs:  The function outputs are then propagated through the following steps:  Let\u2019s start out with some continuous function outputs (per node) from the (continuous and discrete) component  We select a random gaussian function to propagate the uncertainty through the continuous component (d) of the (d-dimensional) component by comparing it to  The uncertainty propagation algorithm uses two different approaches:  First, we compute the probability distribution (probabilities) using the (Continuous, Differential) part-of-the-constant-differential-approximation approach  The second approach uses the (Monte Carlo) part of the same approach  We then use the difference of the probabilities as the uncertainty distribution  The change of the confidence interval  The difference of confidence intervals  The confidence intervals are chosen by comparing the confidence intervals between  and  The first part uses the squared error", "simplified_prediction": "The paper presents a framework for processing uncertain data using a DAG-based data processing system called UP-MapReduce that allows developers to make different kinds of DAG to process uncertain inputs . Uncertainty Data should be represented as probability distributions or estimated values with error bounds rather than exact values . In many cases performing computations on uncertain data as if it was not true . This paper proposes an approach that tackling this challenge in DAG based data processing systems . The approach is based on techniques such as Differential Analysis ( DA ) and Monte Carlo simulation which help programmers to find uncertainties through the nodes of the DAG safely and accurately . Data is being produced and collected at a speed and collected at a pace . There is an urgent need to process a body of data with uncertainties ( suspected values from sensors in IoT ) . Data uncertainties also happen in many other contexts such as modeling , machine learning , approximate storage and sampling-based approximation . For example , data collected using sensors , but there can be certain differences between the estimated and true values due to sensor inaccuracies . Failure to properly account for this uncertainty may lead to incorrect result . For some applications , including AI / ML , the analysis and image processing , it can reduce execution time by 2.3x and up to 5x in some cases . Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains . The framework DAGs are directed ( DAGs ) of free computation nodes , with data flowing through the edges for processing . The inputs are composed of continuous and discrete functions . These functions are differentiable and differentiable functions . The distribution and the locations of the discontinuities ( discontinuities ) are chosen from a Gaussian distribution which determines the appropriate computation method to use for the given discrete function . For this example , the choice of computation is made by taking the gaussian distribution and comparing it with the distribution of the discrete functions and choosing the appropriate method from the set of all the inputs Given the variables , we know which the continuous function inputs is the most likely to be at a low risk of being at low risk given the distribution We want the distribution to be a low of a mixture . To achieve this , we use a combination of DA ( differential analysis based on Monte Carlo simulations ) and uncertainty propagation We use the following approach : We start from a continuous function input ( say from sensors ) and compute the following function outputs : The function outputs are then propagated through the following steps : Let initiates start out with some continuous function outputs ( perde ) from the continuous and discrete ( a random function of a random function 's ) to a random function ."}
{"ground_truth": "Mining High-Speed Data Streams \u2013 Domingos & Hulten 2000  This paper won a \u2018test of time\u2019 award at KDD\u201915 as an \u2018outstanding paper from a past KDD Conference beyond the last decade that has had an important impact on the data mining community.\u2019  Here\u2019s what the test-of-time committee have to say about it:  This paper proposes a decision tree learner for data streams, the Hoeffding Tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. This work constitutes a significant step in developing methodology suitable for modern \u2018big data\u2019 challenges and has initiated a lot of follow-up research. The Hoeffding Tree algorithm has been covered in various textbooks and is available in several public domain tools, including the WEKA Data Mining platform. The goal is to create a knowledge discovery system that can cope with large volumes of data (perhaps an unbounded stream) without needing to fit everything in memory (40MB was the allotted amount used in their evaluation tests \u2013 remember this was 2000). Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists. However, the available algorithms of this type have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm. Based on a statistical result known as the Hoeffding bound, the authors show how to create Hoeffding (decision) trees and build a Very Fast Decision Tree (VFDT) system based on them. A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node). In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces. In a classification problem, a set of N training examples of the form (x\u20d7,y) is given, where y is a discrete class label and x\u20d7 is a vector of d attributes. From these examples we need to produce a model y = f(x\u20d7) that will predict the class of future examples x\u20d7 with high accuracy. Decision tree learners create models in the form of decision trees, where each node contains a test on an attribute, each branch corresponds to a possible outcome of the test, and each leaf contains a class prediction. To learn a decision tree you recursively replace leaves by test nodes, starting at the root. Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets. This learner should require each example to be read at most once, and only a small constant time to process it. This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost. In Hoeffding trees, in order to find the best attribute to test at a given node, only a small subset of the training examples that pass through that node are used. The key of course, is to determine how small that subset can be, and what guarantees we can give concerning it. Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively. We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound). Given a real-valued random variable r with range R (e.g. 0-1 for a probability), and n independent observations of the variable, we can compute the mean of those observations, r\u0304. The Hoeffding bound tells us that with probability 1 \u2013 \u03b4, the true mean of the variable is at least r\u0304 \u2013 \u03b5, where:  The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations. The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same \u03b4 and \u03b5). If G(Xi) is the heuristic used to choose test attributes, then we want to ensure with high probability the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples. Suppose that we\u2019ve seen n examples so far, and the best attribute predicted by G is Xa and the second best is Xb. Call the difference between the observed heuristic values of Xa and Xb \u0394G\u0304  Now, given a desired \u03b4, the Hoeffding bound tells us that Xa is the correct choice with probability 1 \u2013 \u03b4 if n examples have been seen at this node and \u0394G\u0304 > \u03b52. Thus a node needs to accumulate examples from the streamuntil \u03b5 becomes smaller than \u2206G. (Notice that \u03b5 is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves. Pseudo-code for a Hoeffding tree algorithm based on this is given in table 1 of the paper. The VFDT system was built using this algorithm, and included a number of additional optimisations:  When two or more attributes have very similar scores, lots of examples may be needed to decide between them with confidence. But if they are very similar, it probably doesn\u2019t matter too much which one we choose, so let\u2019s just pick one after we reach some user-defined threshold and move on\u2026  We don\u2019t need to recompute G after every example since it is unlikely the decision to split will be made at that specific point. So we can micro-batch and accept a minimum number of new examples before recomputing G.  Under memory pressure, VFDT deactivates the least promising leaves in order to make room for new ones. Likewise VFDT can also drop early on attributes that do not look promising. VFDT can be initialised with a tree produced offline by a traditional batch learner. (Trained on a subset of the overall data). VFDT can rescan previously seen examples if desired. This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times. This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once.", "prediction": "VFDT: A Decision Tree Learning System for Data Mining \u2013 Klein et al. 2003  This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. It uses Hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. It does not store any examples in main memory, requiring space proportional to the size of the main memory (or parts thereof) or at all. It can learn an anytime-to-use model that is ready to use at any point in time (and hence can store any statistics or data points associated with any part of the tree) and can output a model which is identical to a batch version of the data points. The main reasons for including such data points is that they come from an open-ended data stream, and that the current KDD systems dealing with such data volumes are not equipped to cope with them. Ideally, we would like to have K DD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists\u2026 However, the available algorithms of this type have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are sensitive to the example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in terms of time and memory. Klein et. al. tackle these challenges head-on, developing a tree learning algorithm that learns to balance the above three main constraints: time, memory and sample size. Their decision tree, VF DT, combines the benefits of online learning with the benefits (a) of batch learning, (b) data dredging avoidance, and (c) reduced overfitting if the sample size is small enough to avoid overfitting and data overfitting. The basic idea is to let the tree grow exponentially with the amount of data it has to learn, while letting the model grow in size (i.e., the tree size and/time per example). To achieve this, the tree\u2019s size and timestamps are both controlled by a factor H (in this paper, H is defined as the number of nodes in the tree). The probability that a given node will produce an H-indexed node at any given timestep decreases with the more examples it has  We also describe how the decision-tree learning system can incorporate tens of thousands of examples per second using off-the-shelf hardware. The system is evaluated on a continuous access data set generated by the University of Washington main campus. In this data set, there are 10 million (millions) of unique classes of classes and categories. Each class is represented by a unique identifier, and each unique identifier is denoted by a 1-dimensional vector (denoted here as  ). The number of unique identifiers is represented as  , where  represents the unique identifier for that class and  denotes the set of all the other classes in the dataset  The input to the time-ordered vector  is a sequence of classes,  ,  , and  . The first  of these vectors is the first example in the class  , the second  of the class,  and  the third  . Each example  is fed to the decision tree as it comes in, and the tree  learns a binary tree-like structure", "simplified_prediction": "VFDT : A Decision Tree Learning System for Data Mining people et al . This paper says that VFDT is an anytime system that builds decision trees using constant memory and constant time per example . It uses a lot of bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner . It does not store any examples in main memory . This requires space to the size of the main memory ( or parts of memory ) or at all . It can learn an anytime model that is ready to use at any point in time ( and can store any statistics or data points associated with any part of the tree ) . They can output a model which is identical to a batch version of the data points . The main reasons for including such data points is that they come from an open-ended data stream . They also say that the current KDD systems deal with the data volumes with them . Ideally , we would like to have K DD systems that have continuously and indefinitely , including examples as they arrive , and never losing good information . The desiderata are fulfilled by learning methods ( also known as online , successive or sequential methods ) . On which a large literature exists , the available algorithms of this type have significant shortcomings from the KDD point of view . Some reasonably efficient , but do not guarantee that the model learned will be similar to the one made by learning on the same data in batch mode . They may be sensitive to the example ordering . This potentially never recovered from a set of early examples . Others produce the same model as the batch version , but at a high cost in terms of time and memory . et . al . The challenges head-on , developing a tree learning algorithm that learns to balance the three main algorithms : time , memory and sample size . Their decision tree , VF DT , combines the benefits of online learning with the benefits ( a ) of batch learning , ( b ) data dredging avoidance , and ( c ) is small enough to keep the sample size and data overfitting . The basic idea is to let the tree grow very quickly with the amount of data it has to learn , while letting the model grow in size ( i.e. the tree size and / time per example ) . To achieve this , the trees size and timestamps are both controlled by a factor H ( in this paper ) , and the number of nodes in the tree . The probability that a given node will produce an H-indexed node at any timestep decreases with the more examples it has also describe how the decision-tree learning system can use thousands of examples per second using a hardware system . The system is evaluated on a continuous access data set by the University of Washington . There are 10 million ( millions ) of different classes of classes and categories . Each class is represented by a unique identifier , and each identifier is denoted by a 1 - dimensional vector ( denoted here as ) . The number of unique identifiers is represented as , where represents the unique identifier for that class . The set of all the other classes in the dataset The input to the time-ordered vector is a sequence of classes . The first example is the first example of the class , the second of the class , and the third is the third . Each example is fed to the decision tree as it comes in , and the tree learns a structure of the tree ."}
{"ground_truth": "The paper looks at the problem of learning structured exploration policies for training RL agents. Structured Exploration  Consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters. To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence. Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes. The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution. Consider a distibution over the tasks p(T). At the start of any episode of the ith task, a latent variable zi is sampled from the distribution N(\u03bci, \u03c3i) where \u03bci and \u03c3i are the learned parameters of the distribution and are referred to as the variation parameters. Once sampled, the same zi is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution \u03c0\u03b8(a|s, zi). The intuition is that the latent variable zi would encode the notion of a task or goal that does not change arbitrarily during the episode. Model Agnostic Exploration with Structured Noise  The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks. A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the policy-parameters and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy. General meta-RL approaches have two limitations when it comes to \u201clearning to explore\u201d:  Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively. Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model. Idea behind MAESN is to meta-train policy-parameters so that they learn to use the task-specific latent variables for exploration and can quickly adapt to a new task. An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration. For every iteration of meta-training, an \u201cinner\u201d gradient update is performed on the variational parameters and the post-inner-update parameters are used to perform the meta-update. The authors report that performing the \u201cinner\u201d gradient update on the policy-parameters does not help the overall learning objective and that the step size for each parameter had to be meta-learned. The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case). After training, the variational parameters for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the variational parameters. Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training. To be fair to the baselines, reward shaping is used while training baselines as well. Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time). Experiments  Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion. Each task distribution has 100 meta-training tasks. In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions. In the Locomotion task distributions, the different tasks correspond to the different goal positions. The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy. \u2022 In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer.", "prediction": "The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of trajectories. Standard IRL approaches aim to learn reward functions that \u201cjustify\u201d the demonstration policy and hence those approaches cannot outperform the \u201cnear-optimal\u201d demonstration policy. The main flaw of current inverse RL approaches is that when the learning agent does not have access to the true reward function, it can fall prey to the effects of the imitation learning approach when the policy is not directly from the user\u2019s own intentions. In contrast, the proposed IRL approach aims to learn a parameterized reward function that explains the ranking demonstrations, allowing the evaluation to focus on the features correlated with the true intention and not just the quantity. Approach  Approach  Given a trajectory, start with a trajectory T, rank the trajectories according to their similarity with each other. Assume that the trajectory T-1, T-2, and T-3 are a sequence of trajectory trajectories T1, \u2026 T-4, where T=0.5 is the sequence of states corresponding to the trajectory and T=1 if the trajectory is unknown to the user. These trajectories are ranked using a binary classification loss function which aims to predict the reward for trajectories that are better than the one-hot trajectory. The reward function is trained by learning from observations where the input trajectory is the sum of rewards of all trajectories (from the training set) and the target reward is a sum of the rewards of the user chooses from a set of trajectory options. The user chooses the trajectory from a stable distribution. Given the trajectory, the reward function predicts which of the two given trajectory would be the better reward function. The sum of reward functions (corresponding to the two trajectories) is trained jointly with the user and the training objective. The two reward functions are trained jointly and then the user selects the preferred trajectory from the training dataset. The chosen trajectory and the chosen reward function are trained together with a learning agent which is trained separately from the learning the policy from the other trajectory. A learning agent is trained to predict both the reward functions and the preference between the two. Then, given a trajectory and a trajectory (given a trajectory), the model learns to predict which reward function would be better and how to use it to improve the other reward functions. The model is trained end-to-end using a stochasticity term which means that it learns to use the extrapolated reward function to estimate the difference between the expected reward and the generated reward function (from all the other trajectories). Results  Environments  Mujoco (Atari), Atari  Atari  Demonstrations  The proposed approach outperforms the baselines Behaviour Cloning from Observations (Biotic Cloning) and Imitation Learning (RL). In terms of reward extrapolation, the key observation is that the proposed approach is robust to noise and that the model does not overfit (in terms of performance) to the information provided by the training data. The experimental results are presented in the figure below. Ensemble of networks used to train and train the model  Ensemble  Model  Given an observation  , the model generates a trajectory  from a distribution  consisting of  trajectories  . The model predicts the reward  for each trajectory  and  . This prediction is fed to the model as a hidden parameter  . In practice, the model uses the following model:  A trajectory  is the trajectory which is expected to receive the highest reward  A reward function  from  , which", "simplified_prediction": "The paper proposes a new algorithm ( IRL ) called '' Trajectory-ranked Reward EXtrapolation '' that learns a reward function from a collection of trajectories . Standard IRL approaches aim to learn reward functions that the demonstration policy and those approaches cannot outperform the people who wanted to learn about how to use the IRL . The main part of current inverse RL approaches is that when the learning agent does not have access to the true reward function , it can fall to the effects of the imitation learning approach when the policy is not directly from the user . The proposed IRL approach aims to learn a reward function that explains the ranking demonstrations , allowing the evaluation to focus on the features correlated with the true intention and not just the quantity . The Approach Given a trajectory , start with a trajectory T , rank the trajectories according to their similarity with each other . Assume that the trajectory T-1 , T-2 , and T-3 are a sequence of trajectory trajectories T1 , ... T = 0.5 is the sequence of states that are related to the trajectory and T = 1 if the trajectory is unknown to the user . These trajectories are ranked using a binary classification loss function which tries to predict the reward for trajectories that are better than the one-hot system . The reward function is trained by learning from observations where the trajectory is the sum of rewards of all trajectories ( from the training set ) and the target reward is a sum of the rewards of the user chooses from a set of options . The user chooses the user from a stable distribution . The reward function predicts which of the two would be the better reward function would be the better reward function . The sum of reward functions ( meaning the two trajectories ) is trained together with the user and the training objective . The two reward functions are trained jointly and then the user selects the same thing as the training dataset . The chosen reward function are trained together with a learning agent which is trained separately from learning the policy from the other people to learn . A learning agent is trained to predict both the reward functions and the people know about the two . Then , given a trajectory and a trajectory , the model learns to predict which reward function would be better and how to use it to improve the other reward functions . The model is trained using a stochasticity term which means that it learns to use the reward function to estimate the difference between the expected reward and the generated reward function ( from all the other rewards ) . It is thought that Environments Mujoco ( Atari ) , Atari Atari Demonstrations The proposed approach outperforms the baselines Behaviour Cloning from Observations and Imitation Learning ( RL ) . In terms of reward extrapolation , the key observation is that the proposed approach to noise and that the model does not get better ( in terms of performance ) to the information provided by the training data . The experimental results are given in the figure below . Ensemble of networks used to train and train the model Ensemble Model Given an observation . The model generates a trajectory from a distribution that is made up of trajectories . The model predicts the reward for each person 's reward . This prediction is like the model as a hidden parameter . In practice , the model uses the following model : A trajectory is the trajectory which is expected to receive the highest reward A reward function ."}
{"ground_truth": "The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies. Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner. The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL). Idea  Instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives). Each model primitive will be good in only a small part of the state space (aka region of specialization). These model primitives can then be used to train a gating mechanism for selecting sub-policies to solve a given task. Since these model primitives are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches. Single Task Learning  A gating controller is trained to choose the sub-policy whose model primitive makes the best prediction. This requires modeling p(Mk | st, at, st+1) where p(Mk) denotes the probability of selecting the kth model primitive. This is hard to compute as the system does not have access to st+1 and at at time t before it has choosen the sub-policy. Properly marginalizing st+1 and at would require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce p(Mk | st). This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout. The paper notes that technique is biased but reports that it still works for the downstream tasks. The gating controller composes the sub-policies as a mixture of Gaussians. For learning, PPO algorithm is used with each model primitives gradient weighted by the probability from the gating controller. Lifelong Learning  Different tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO). Experiments  Domains:  Mujoco ant navigating different mazes. Stacker arm picking up and placing different boxes. Implementation Details:  Gaussian subpolicies  PPO as the baseline  Model primitives are hand-crafted using the true next state provided by the environment simulator. Single Task  Only maze task is considered with the start position (of the ant) and the goal position is fixed. Observation includes distance from the goal. Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task. Lifelong Learning  Maze  10 random Mujoco ant mazes used as the task distribution. MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks). Pick and Place  8 Pick and Place tasks are created with max 3 goal locations. Observation includes the position of the goal. Ablations  Overlapping model primitives can degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust. Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks. The order of tasks (in the 10-Mazz task) does not degrage the performance. Transfering the gating controller leads to negative transfer. Notes  I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.", "prediction": "The paper proposes a framework that uses diverse sub-optimal world models to decompose complex task solutions into simpler modular sub-tasks. This framework jointly learns the required modular subpolicies as well as a controller to coordinate them. Link to the code  Approach  The main idea in this paper is to use the agent to learn a gating controller that selects from a set of models such that each model primitive is only relatively better at predicting the next states within a certain region of the environment space. This assumption is based on the fact that successor representations decouple the state transitions from the rewards (representing the task or goals) and can only transfer across tasks with the same environment dynamics. In this setting, the agent must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning (multi-task learning), where the agent jointly trains on multiple task environments. Not only is this setting different, but also non-incremental, which allows the problem of discovering common structures between tasks easier, and allows the methods to ignore the issue of catastrophic forgetting (in which the agent forgets how to solve previous tasks after learning to solve new tasks)  Motivation  The authors hypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agent\u2019s solution increases if it can capture such structure. The world is complex and learning models consistent enough to plan with is not only hard, but planning with such one-step models is suboptimal  The requirement that these models be good predictors of the world state is unnecessary. A key ingredient of our proposal is the idea of a transition model that can predict future sensory data given the agents current actions. The assumption that a subset of model primitives are useful across a range of tasks and environments is non-negligible  It assumes that the following assumption:  That successor representations can decouple from the state transition model and act as a predictor for the agent  That the agent can accurately predict future state transitions given current state transitions  That transitions are transfer-dependent  The region of specialization in the transition model (region of specialization) is the set of all the possible states that the agent could be in given the current state and the current actions  This area is referred to as the \u201ctermmodel region\u201d  Since the termmodels are models that predict the next state (and related subtasks) for a certain set of inputs, they are assumed to be relatively well-tuned  It can be shown that a good transition model can be obtained by learning a permutation of the following permutation:  The permutation term is defined in the form of  where  is the permutation factor    and  is a learning rate   The number of permutation factors is represented as  The model is trained to predict for each input the probability of being in  a given region of  the transition state  The objective is to predict the transition from  to  from  The task of decomposition is to compress the input into smaller and smaller modules according to  . Modules  The agent is trained in a bottom-up manner by learning the required modules in a top-down manner, in such a way that the number of modules grows monotonically with the size of the input. For a given source task,  , the following modules are learned:  Modules 1, 2, 3, 4, 5, and 6  The modules are chosen based on their size  The regions of specialization are defined based on  ,  , and  . Each module is assigned a unique label  which", "simplified_prediction": "The paper proposes a framework that uses parts of the world to decompose complex task solutions into simpler modular sub-tasks . They also learn the required modular subpolicies as well as a controller to coordinate them . The main idea in this paper is to use the agent to learn a gating controller that selects from a set of models such that each model primitive is only relatively better at predicting the next states within a certain area of the environment space . This assumption is based on the fact that makes the state change from the rewards ( representing the task or goals ) . It can only transfer across tasks with the same environment dynamics . In this setting , the agent must be able to get knowledge in previous tasks to get better on future tasks . This is different from multi-task learning ( multi-task learning ) , where the agent can learn together to learn their own task environment . This allows the problem of discovering common structures between tasks easier , and allows the methods to ignore the issue of catastrophic forgetting ( in which the agent forgets how to solve previous tasks after learning to solve new tasks ) Motivation The authors hypothesize that many complex tasks are heavily structured and hierarchical in nature . The likelihood of transfer of an agent in a solution increases if it can capture the structure . The world is complex and learning models consistent enough to plan with is not only hard , but planning with such one-step models are needed . This means that these models are good predictors of the world state . A key ingredient of our proposal is the idea of a change model that can predict future sensory data given the agents current actions . This means that a subset of model primitives are useful across a range of tasks and environments is non-negligible It assumes that the following assumption : That successor representations can decouple from the state transition model and act as a predictor for the agent That can accurately predict future state transitions in the transition . This means that the change in order of the transition to the current state is the same as the current state in order of the transition , the current in the state of the state . Modules The agent is trained in a bottom-up manner by learning the required modules in a top-down manner , in such a way that the number of modules grow with the size of the input . The following modules are learned : Modules 1 , 2 , 3 , 4 , 5 , and 6 The modules are chosen based on their size The regions of different types are defined based on , and . Each module has a unique label which has one name ."}
{"ground_truth": "The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution. Adversarial Net  Two models - Generative Model(G) and Discriminative Model(D)  Both are multi-layer perceptrons. G takes as input a noise variable z and outputs data sample x(=G(z)). D takes as input a data sample x and predicts whether it came from true data or from G.  G tries to minimise log(1-D(G(z))) while D tries to maximise the probability of correct classification. Think of it as a minimax game between 2 players and the global optimum would be when G generates perfect samples and D can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data). Alternate between k steps of training D and 1 step of training G so that D is maintained near its optimal solution. When starting training, the loss log(1-D(G(z))) would saturate as G would be weak. Instead maximise log(D(G(z)))  The paper contains the theoretical proof for global optimum of the minimax game. Experiments  Datasets  MNIST, Toronto Face Database, CIFAR-10  Generator model uses RELU and sigmoid activations. Discriminator model uses maxout and dropout. Evaluation Metric  Fit Gaussian Parzen window to samples obtained from G and compare log-likelihood. Strengths  Computational advantages  Backprop is sufficient for training with no need for Markov chains or performing inference. A variety of functions can be used in the model. Since G is trained only using the gradients from D, fewer chances of directly copying features from the true data. Can represent sharp (even degenerate) distributions. Weakness  D must be well synchronised with G.  While G may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained. Possible Extensions  Conditional generative models. Inference network to predict z given x. Implement a stochastic extension of the deterministic Multi-Prediction Deep Boltzmann Machines  Using discriminator net or inference net for feature selection. Accelerating training by ensuring better coordination between G and D or by determining better distributions to sample z from during training.", "prediction": "The paper introduces a new training procedure for generative models where the model is trained to predict the probability of a given data distribution (i.e. the data distribution is fed into a discriminative model, e.g. a GAN) trained to estimate the accuracy of the discriminator (e.g., a DGAN). In this paper, the authors employ the concept of a two-player game in which a discriminator is pitted against an adversary in the form of G.  The adversarial process is modelled as a game between two teams of counterfeiters (G and D). One player, called as G, tries to produce fake samples and use them to evade detection, while the other player (here represented as D) tries to detect the counterfeit samples and correct the mistake caused by the fake samples. The paper demonstrates the potential of the framework through a quantitative evaluation of the generated samples. Questions for discussion  Why don't they use the reverse KL term for L1 units? It seems like they are using a KL term here for L2 units, which is equivalent to using two KL units (one for G and one for D). The paper also mentions that KL units should be used in place of KL units in D, since they seem to have a better gradient (more specifically, their gradient is lower with D). How effective is the method of training G and D? (This seems like a big question)  Why not just train G to predict (and test) D and then test both of them on the real data and then fine-tune G later on? (see Equation 5)  What justifies training two models simultaneously? (See Equation 10)  How do you mesh them together? Both models should be able to outperform each other in terms of accuracy? Generative models typically have a single hidden layer, while D models usually have multiple hidden layers. In the generative model setting, it would be ideal to train D to predict whether a sample came from the training data distribution or not (see Section 4.2.2 in the paper). But it is difficult to get rid of the hidden layers, and hence the name \"adversarial nets\". The paper proposes to train both models jointly using the backpropagation and dropout algorithms, with the last hidden layer being a MSE between the first and second hidden layer. The training procedure thus allows for fine-tuning of G to improve upon D, while ensuring that D does not make a mistake. Notes  The idea in itself is very simple and straight-forward to add to any existing model, but I find it hard to believe that it would not be easier to implement in practice as well. It would be interesting to see if the authors can add an auxiliary network to predict z given the training dataset. The auxiliary network could be trained after the model has finished training, for example, to take care of updating the weights of D after it has run. This is similar to the idea in the wake-sleep algorithm, but with the advantage that it can be trained for a fixed number of epochs (up to the point where D has run out of training, i.e., after p(x, c) has been fed into the model). One can train a family of conditional models to model all conditionals of x by training a pair of models (p(x | c) and training them jointly with each other. The family of models could be composed of models that share parameters, where p is a deterministic MP-DBM stochastic model, and c is a random noise model. I think one could also train a second network (d) to", "simplified_prediction": "The paper has a new training procedure for generative models where the model is trained to predict the probability of a given data distribution . The data distribution can be fed into a discriminative model . GAN trained to estimate the accuracy of the discriminator ( e.g. a DGAN ) . In this paper , the authors employ the concept of a two-player game in which a game is pitted against an adversary in the form of G. The process is modelled as a game between two teams of counterfeiters ( G and D ) . One player , called as G , tries to produce fake samples and use them to evade detection . The other player ( here represented as D ) tries to detect the samples and correct the mistake caused by the fake samples . The paper shows the potential of the framework through the evaluation of the generated samples . Questions for discussion Why don't they use the KL term for L1 units ? It seems like they are using a KL term here for L2 units , which is the same as two KL units ( one for G and one for D ) . The paper also mentions that KL units should be used in place of KL units in D.C. since they seem to have a better gradient ( more specifically , their gradient is lower with D ) . How it is the method of training G and D ? This seems like a big question ( like a big question ) Why not just train G to predict ( and test ) D and then test both of them on the real data . What justifies training two models at the same time ? How do you mesh them together ? Both models should be able to move each other in terms of accuracy ? These models usually have a single hidden layer . D models usually have many hidden layers . It would be ideal to train D to predict whether a sample came from the training data distribution or not ( see Section 4.2.2.2.2 in the paper ) . But it is difficult to get rid of the hidden layers . The name comes from the name of adversarial person . The paper proposes to train both models together with the last hidden layer being a MSE between the first and second hidden layer , with the last hidden layer being a MSE . The training procedure allows for fine-tuning of G to improve upon D , while so that D does not make a mistake . Notes The idea in itself is very simple and straight-forward to add to any existing model . I find it hard to believe that it would not be easier to use in practice as well . This would be interesting to see if the authors can add an auxiliary network to predict the training dataset . The network could be trained after the model has finished training . For example , the model could take care of the weights of D after it has run . This is similar to the idea in the wake-sleep algorithm , but with the advantage that it can be trained for a fixed number of algorithms ( up to the point where D has run out of training , i.e. x , c ) has been fed into the model . One can train a family of models to model all conditionals of x by training a pair of models ( p ( x | c ) and training them together together with each other . The family of models could be made up of models that share parameters , where p is a deterministic model , and c is a random noise model . I think one could also train a second network ."}
{"ground_truth": "An empirical analysis of anonymity in Zcash Kappos et al., USENIX Security\u201918  As we\u2019ve seen before, in practice Bitcoin offers little in the way of anonymity . Zcash on the other hand was carefully designed with privacy in mind. It offers strong theoretical guarantees concerning privacy. So in theory users of Zcash can remain anonymous. In practice though it depends on the way those users interact with Zcash. Today\u2019s paper choice, \u2018An empirical analysis of anonymity in Zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the study. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. The analysis also provides some interesting insights into who is using Zcash and for what as well. Founders and miners combined account for around 66% of the value drawn from the shielded pool. The code for the analysis is available online at  [url]", "prediction": "An empirical analysis of Zcash\u2019s transactions and associated with its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. Contribution  Since the introduction of Bitcoin in 2008, cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. Despite the broader positive potential of blockchain (i.e., the public decentralized ledger underlying almost all cryptocurrencies) is still unclear, despite the growing number of legitimate users there are today still many people using these cryptocurrencies for less legitimate purposes. These range from the purchase of drugs or other illicit goods on so-called dark markets such as Dream Market, to the payments from victims in ransomware attacks such as WannaCry, with many other crimes in between. Given the growing awareness that most cryptocurrencies do not have strong anonymity guarantees, a number of alternative cryptocurrencies or other privacy-enhancing techniques have been deployed with the goal of improving on these guarantees. The most notable cryptocurrencies that fall into this category are Dash, Monero, and Zcash  At the time of this writing all have a market capitalization of over 1 billion USD, although this figure is notoriously volatile, so should be taken with a grain of salt. Even within this category of privacy- enhanced cryptocurrencies, and despite its relative youth, and relatively youth, Zcash stands somewhat on its own. The reason for this is that Zcash is backed by a body of recent, highly regarded research on pseudonymous ad-dresses, and comes with seemingly combining strong privacy guarantees with a relatively small number of transactions. Indeed, the original papers cryptographically prove the security of the \u201cshielded pool\u201d privacy feature  in which users can spend shielded coins without revealing which coins they have spent. It does require, however, that all newly generated coins pass through the shielded pool before being spent further, thus ensuring that all coins have been shielded at least once. This requirement led the Zcash developers to conclude that the anonymity for users spending shielded coins is in fact all generated coins, and thus that the strategies that other cryptocurrencies use for anonymity are \u201csmall\u201d and \u201cimplemented\u201d in comparison to Zcash and  other cryptocurrencies in terms of transaction privacy. The main question this paper aims to address is this:  How much of the overall transactions in Zcash are really \u2018transparent\u2019? And, to what extent does this \u201ctransparency\u201d extend beyond the use of pseudonymous senders and receiveers, and the amount of the total amount of coins that have been spent? The main answer is that the vast majority of transactions are not transparent, meaning that they reveal the pseudonymous addresses of both the sender and the recipient (e.g., \u201ccoin_id\u201d) along with the amount being sent and the transaction amount. In fact, the authors found that the majority of exchanges typically engage in transactions that are essentially the same as transactions in Bitcoin in the transparent part of the blockchain, meaning it does not engage with the \u2018public part\u2019 part at all. How Zcash has evolved  The study begins by introducing a general analysis of how Zcash transactions have evolved over time, and shows that the main privacy features of the current Zcash model are:  A Shielded Pool  Shielded pool is a place in the blockchain where users can send and receive coins in pseudonymous transactions using only pseudonyms as identifiers.", "simplified_prediction": "An empirical analysis of Zcash honours and associated with its main feature , a shielded pool that acts as anonymity set for users wanting to spend some time in privately . We conclude that while it is possible to use Zcash in a private way , it is also possible to shrink the word '' Zcash '' set a lot by developing simple patterns of usage . Since the introduction of Bitcoin in 2008 , cryptocurrencies have become increasingly popular to the point of reaching a near-mania , with thousands of people using cryptocurrencies now , attracting trillions of dollars in investment . However , the public decentralized ledger underlying almost all cryptocurrencies , is still unclear , even though the growing number of legitimate users there are still many people using these cryptocurrencies for less legitimate purposes . These range from the purchase of drugs or other goods on so-called dark markets such as Dream Market , to the payments from victims had to stop attacks such as WannaCry , with many other crimes in between . Given the growing awareness that most cryptocurrencies do not have strong guarantees , a number of alternative cryptocurrencies or other techniques have been used with the goal of improving on these guarantees . The most notable cryptocurrencies that fall into this category are Dash , Monero , and Zcash At the time of this writing all have a market capitalization of over 1 billion USD , although this figure is well known as a grain of salt . Even within this category of youth , Zcash stands somewhat on its own , and despite its relative youth , and relatively youth , Zcash stands somewhat on its own . The reason for this is that Zcash is backed by a body of recent , highly regarded research on pseudonymous ad-dresses . It comes with seemingly combining strong privacy guarantees with a relatively small number of transactions . Indeed , the original papers cryptographically prove the security of the people who lived in the pool . There are many different kinds of privacy in which users can spend shielding coins without revealing which they have spent . However , that all newly created coins pass through the shielded pool before being spent further . All coins have been shielded at least once , so all coins have been made . This led the Zcash developers to make sure that the anonymity for users spending shielded coins is in fact all generated coins , and thus that the strategies that other cryptocurrencies use for a special type of cryptocurrencies in terms of transaction privacy . The main question this paper aims to address is this : How much of the overall transactions in Zcash can not be used for the first time . And to what extent does this main part of the use of pseudonymous senders and receiveers , and the amount of the total amount of coins that have been spent ? The main answer is that the vast majority of transactions are not transparent , meaning that they show the addresses of both the sender and the recipient ( e.g. the person who can not get the job ) along with the amount being sent and the transaction amount . In fact , the authors found that the majority of exchanges usually have transactions that are the same as transactions in Bitcoin . This means that it does not have the same part of the blockchain , but it does not take part at all . The study begins by introducing a general analysis of how Zcash transactions have evolved over time , and shows that the main privacy features of the current Zcash model are : A Shielded Pool Shielded pool is a place in the blockchain where users can send and receive coins using only pseudonyms as identifiers ."}
{"ground_truth": "Statiscal foundations of virtual democracy Kahng et al., ICML\u201919  This is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently. Consider a food bank that receives donations of food and distributes it to those in need. The goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it. We could hard code a set of rules, but what should they be? And who gets to decide? A democratic solution to this would be to give each of the stakeholders a vote on every decision. In the food bank setting, identified classes of stakeholders include the donors, the recipients, the volunteers (who pick up food from the donor and deliver it to the recipient), and employees. Their votes encode their own preferences and biases, perhaps in a way that even the voters themselves couldn\u2019t neatly codify in a set of explicit rules. It\u2019s not really practical to have an actual vote with all stakeholders participating every time a food donation is made though! One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand. As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues. One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote\u2014 hopefully an informed one\u2014 every time a new dilemma arises. The big idea behind virtual democracy is that we learn the voting preferences of each stakeholder, essentially creating an agent which is able to vote in their place, a virtual voter. Then when we need to make a decision we ask those virtual voters to cast their votes (in the form of a preference ranking). The central question in this paper is this: given a set of preference rankings, how should we combine them to produce an actual decision? The procedure for doing this is known as the voting rule. \u2026 the choice of voting rule can have a major impact on the efficacy of the system. In fact, the question of which voting rule to employ is one of the central questions in computational social choice. It\u2019s one thing to come up with a voting rule that works well when we have the actual true preference rankings of all of the stakeholders. In a virtual democracy setting though, where we have learned approximations to those preference rankings, a highly desirable feature of a voting rule is that it is robust to noise. I.e., we want a voting rule whereby\u2026  \u2026 the output on the true preferences is likely to coincide with the output on noisy estimates thereof. Learning preferences  To learn voter preferences, voters are asked to make a set of pairwise comparisons (about 100) between alternatives. I.e., given this donation, should it be sent to recipient A or recipient B? Each alternative is presented as a set of pre-determined features. In the case of the food bank question voters are given information about the type of donation, and seven additional features such as distance between the donor and recipient, and when the recipient last received a donation. At the end of this process, the training data is used to learn a model of the preferences of the voter. This model is then used to predict the voter\u2019s preference ranking over many hundreds of recipients for a given donation. The Mallows model  To be able to compare the efficacy of various voting rules, we\u2019re going to need a way to compare how good their outputs are. The Kendall tau (KT) distance between two rankings (permutations) of a set is defined as the number of pairs of alternatives on which the rankings disagree. By disagree we mean that given a pair  one ranks  ahead of  , and the other ranks  ahead of  . For example, the KT distance between  and  is 2. The Mallows (1957) model was originally designed for use in situations where there is true ranking of the alternatives, and assigns a probability that a given voter is associated with a given alternative ranking. The probability decreases exponentially with the number of pairs of alternatives on which the true and alternative ranking disagree, i.e., their KT distance. A Mallows model is parameterised by a  parameter  . Our technical approach relies on the observation that the classic Mallows (1957). model is an unusually good fit with our problem. In the problem at hand, instead of a single true ranking, each voter has their own true ranking. When validating a learned  model, the test for accuracy is done using pairwise comparisons, just like in Mallows. Given an observed prediction accuracy  , we can relate this accuracy to an underlying Mallows model through a parameter  , where pairwise comparisons are drawn from within the top  ranked items in the true ranking. (See \u00a73 in the paper). Voting rules and the Borda count  The next piece of the puzzle is the selection of a voting rule to combine rankings and produce a final decision. The main result in the paper concerns the Borda count voting rule. Borda count is a positional scoring rule. Positional scoring rules give a score vector that assigns points to each position in a ranking. E.g. 5 points for being ranked first, 3 points for being ranked second, and so on. The score of an alternative is the sum of its ranking points across all of the voters. The alternative with the biggest score wins (break ties via random selection). The Borda count uses a very straightforward score vector: if there are  alternatives in the ranking, the score vector is defined as  . The heart of the paper is \u00a74, where drawing on the properties of the Mallows model, it\u2019s relationship to the predicted accuracy, and the Borda count rule, the authors show the Borda count is surprisingly robust to noise. I\u2019m going to happily skip over the proofs here and leave you to follow up on those if you\u2019re interested! \u2026 it is intuitive that the separation in Borda scores has to depend on  , but it is encouraging (and, to us, surprising) that his dependence is almost linear\u2026 the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in  . Other rules  So far so good, but what about other voting rules? Are they also robust to noise or is there something special about the Borda count? The main alternative to positional scoring rules are pairwise-majority consistent (PMC) rules, of which there are many examples (e.g., the ranked pairs method). The key result in \u00a75 of the paper is that all rules in this class are not robust to noise. It is instructive to contrast our positive result, Theorem 1, with this negative result. On a very high level, the former result asserts that \u201cif Borda count says that the gaps between alternatives are signi\ufb01cant, then the alternatives will not \ufb02ip under Borda count,\u201d whereas the latter says \u201ceven if a PMC rule says that the gaps between alternatives are very signi\ufb01cant, some alternatives are likely to \ufb02ip under that rule.\u201d  Borda count FTW  So there you have it: if you need to robustly combine noisy rankings of alternatives to make a decision, use the Borda count! Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. Another important feature of the Borda count rule is that the decisions it takes can be easily explained. An explanation consists of two elements: first the average position in the predicted preferences of each of the stakeholder groups, and second the features that were most important in achieving that ranking position (possible since alternatives are presented as vectors of features).", "prediction": "Learning to learn voting rules based on the preferences of individuals Kahng et al., ICML\u201919  This is the third post in a mini-series on machine learning and social choice, and it\u2019s going strong! The question of which voting rule to use is one of the central questions in computational social choice (and in social choice theory more broadly). The central premise is that, in the context of virtual democracy, certain statistical considerations should guide the choice of voting rule, yet one might hope that it would still operate on the \u201ctrue\u201d predictions of the voters, yet still maintain the integrity of the voting rule. Indeed, Borda count is also compelling in terms of usability and explainability, and from a statistical viewpoint, explaining why it is attractive\u2026 which voting rules have the property that their output on the true preferences coincide with their \u201cdeeper\u201d estimates thereof? Our theoretical and empirical results identify two possible reasons for this:  The first is that the input to the system is a collection of (say) 100 pairwise comparisons of two food donations, where in each comparison, the voter is provided information about the type of the donation, as well as seven relevant features of the two alternatives that are being compared, e.g. the distance between the donor and the recipient, and when the recipient last received a donation. The second is that at runtime, the predicted preferences of each voter are used to compute a ranking over the alternatives, and this ranking is combined with the true preference scores of the other voters, to produce a single voting rule that aggregates the vote on the given dilemma. In short, given a set of (100) pair-of-choice examples, the model is trained to predict the voting preference scores for each of those examples, and a voting rule is applied to those predictions, ensuring that their true preferences align with the predictions of other voting rules (i.e., the voting rules are robust to noisy estimates). Building on the work of Noothigattu et al. (2018), who presented a model of voting preferences in a recent paper, \u201cAutomating Decisions with Automated Decision Making , Automated Empirical Evidence ,\u201d with the following structure:  We start out by collecting the voting preferences of the respondents (members of the public) on a variety of different dilemmas. We use these as a proxy for the voting process, and use the models to learn models of the actual voting process. The models are trained on a dataset of candidate preferences, and then we use those models to compute the voting score of each of the candidates (referred to as the \u2018puppeteer\u2019). At runtime, we combine the predicted voting score with the real voting score for each candidate, and produce the final voting decision by summing the results of those two processes. The paper aims to show that voting rules should be robust to prediction errors, in that the output of voting rules that operate on true preferences are likely to coincide with the output on noisy estimates. Specifically, the authors show that the classic Borda Count rule is robust to both noisy estimates and true preferences, whereas voting rules belonging to the wide family of pair-wisemajority consistent rules are not. The main results in this paper are in the following table, and the table below. You\u2019ll find it interesting to note that the figure below shows the effect of using noisy estimates in the case of the proposed voting rule:  In this case, the true voting preference score for a candidate is the score of the candidate whose preference ranking was predicted based on a combination of all the other voting votes, and only the preference", "simplified_prediction": "Learning to learn voting rules based on people who want to learn Kahng et al . ICML are the third post in a machine learning and social choice , and it is going strong ! The question of which voting rule to use is one of the central questions in social choice ( and in social choice theory more broadly ) . The central building is that , in the context of virtual democracy , certain statistical considerations should guide the choice of voting rule , yet one might hope that it would still work on the change of the voters , yet still keeping the voting rule . Borda count is also compelling in terms of usability and explainability , and from a statistical viewpoint , explaining why it is attractive , which voting rules have the property that their output is at the same time at the same time . Our and empirical results identify two possible reasons for this : The first is that the input to the system is a collection of ( say ) 100 pairwise comparisons of two food donations , where in each comparison , the voter is provided information about the type of donation , as well as seven of the two alternatives that are being compared , e . The distance between the person and the recipient , and when the person received a donation . The second is that at runtime , the predicted people of each voter are used to compute a ranking over the alternatives , and this ranking is combined with the true preference scores of the other voters . This makes a single voting rule that makes the vote on the given dilemma . In short , given a set of '' 100 '' pair-of-choice examples , the model is trained to predict the voting preference scores for each of those examples , and a voting rule is applied to those predictions . This means that their true preferences align with the predictions of other voting rules ( i.e. the voting rules are robust to noisy estimates ) . Building on the work of Noothigattu et al . ( 2018 ) , who presented a model of voting preferences in a recent paper , with the following structure : We start out by collecting the voting preferences of the respondents ( members of the public ) on a variety of different dilemmas ( members of the public ) . We use these as a proxy for the voting process , and use the models to learn models about the actual process . The models are trained on a dataset of candidate preferences , and then we use those models to compute the voting score of each of the candidates ( called the person who works ) . At runtime , we wanted voting score with the real voting score for each candidate . This made the final voting decision by making the results of those two processes . The paper wants to show that voting rules should be robust to prediction errors , in that the output of voting rules that are likely to be changed into the output because of the output . The authors show that the classic Borda Count rule is robust to both people who live in , whereas voting rules belonging to the wide family of pair-wisemajority consistent rules are not . The main part of this paper is in the following table , and the table below . You can find it interesting to note that the figure below shows the effect of using noisy estimates in the case of the proposed voting rule : In this case , the true voting score for a candidate is the score of the candidate whose preference ranking was shown based on a combination of all the other voting votes , and only the preference ."}
