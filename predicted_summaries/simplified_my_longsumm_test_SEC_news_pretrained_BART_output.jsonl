{"ground_truth": "The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns. Approach  Given a motif M , the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S). Mathematically, the aim is to minimise the motif conductance metric given as cutM(S, S\u2019) / min[volM(S), volM(S\u2019)] where S\u2019 is complement of S, cutM(S, S\u2019) = number of instances of M which have atleast one node from both S and S\u2019 and volM(S) = Number of nodes in instances of M that belong only to S.  Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices. The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal. Algorithm  Given the network and motif M, form a motif adjacency matrix WM where WM(i, j) is the number of instances of M that contains i and j.  Compute spectral ordering of the nodes from normalized motif laplacian matrix. Compute prefix set of spectral ordering with small motif conductance. Scalability  Worst case O(m1.5), based on experiments O(m1.2) where m is the number of edges. Advantages  Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well). In case the motif is not known beforehand, the framework can be used to compute significant motifs. The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks.", "prediction": "Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. Here we develop Higher-order organization of complex networks is largely unknown. Here we develop a framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher- The method is based on the graph Laplacian and cut and volume measures for sets of vertices in a graph. We then define network motifs in Section S1.2 and generalize the notions of cut andvolume to motifs. Finally, we relate conductance on the weighted graph defined by Equation S19. When |A| = 3, the motif cut and motif volume are both equal to half. For any motif with three anchor nodes, conductance is equal to the motif conductance. Theorem 5 says that the set of nodes S is within a quadratic factor of optimal. The second result provides a lower bound on the optimal motif conductance in terms of the eigenvalue. We use this bound to show that certain motifs do not provide the exact conductance with an additional penalty for splitting the four anchor nodes into two groups of two. This follows from Lemmas 1 and 8. When |A| > 4, we can derive similar penalized approximations to \u03c6(G)M (S We first provide a theoretical analysis of the computational complexity, which depends on motif. After, we empirically analyze the time to find clusters for triangular motifs on a variety of real-world networks. Finally, we show that we can practically compute the motif adjacency For several motifs, the motif adjacency matrix WM (Equation S19) has a simple formula. Table S6 lists the formula of WM for motifs M1, M2, M3, M4, M5, M6, and M For our experiments, we compare our spectral motif-based custering to the following methods:. Standard, edge-based spectral clustering. Infomap, which is based on the map equation (62) and the Louvain method (63) C. elegans network. In this network, the nodes are neurons and the edges are synapses. The network data was downloaded from suppl/celegans131.zip. The original network has 131 nodes and 764 edges. The largest connected component of the motif The nodes in the transportation reachability network are airports in the United States and Canada. There is an edge from city i to city j if the estimated travel time from i to j is less than some threshold (23). The network is not symmetric. The network with We next use motif-based clustering to analyze several additional networks. Motifs model energy flow patterns between several species. Low motif conductance (high-quality) clusters only exist for motif M6. Clusters based on motifs M5 or M8 have All data is available at our project web site at The web site includes links to datasets used for experiments throughout the supplementary material. See the Supplementary Material.", "simplified_prediction": "Networks are an important tool for understanding and modeling systems in physics , biology , engineering , and social science . Many networks have rich , lower-order connectivity patterns that can be captured at the level of individual nodes and edges . Here we develop Higher-order organization of complex networks . Here we develop a framework for clustering networks based on different types of patterns . This means that mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges . The framework shows higher- The method is based on the graph Laplacian and cut and volume measures for sets of vertices in a graph . We then define network motifs in Section S1.2 and generalize the notions of cut andvolume to motifs . Finally , we relate to the weighted graph defined by Equation S19 . When A | = 3 , the motif cut and motif volume are both equal to half . For any motif with three anchor nodes , he is equal to the motif 's . Theorem 5 says that the set of nodes S is within a certain factor of optimal . The second result provides a lower bound on the optimal motif 's in terms of eigenvalue . We use this to show that certain motifs do not provide the exact conductance with more penalty for splitting the four anchor nodes into two groups of two . This is from Lemmas 1 and 8 . When A | > 4 , we can get similar penalized approximations to  clothing ( G ) M ( S We first provide a theoretical analysis of the computational complexity , which depends on motif . After , we looked at the time to find clusters for triangular motifs on a variety of real-world networks . Finally , we show that we can practically compute the motif adjacency For several motifs , matrix WM ( Equation S19 ) has a simple formula . Table S6 lists the formula of WM for motifs M1 , M2 , M3 , M4 , M5 , and M For our experiments , we compared our spectral motif to the following methods : Standard edge-based spectral clustering It is based on the map equation ( 62 ) and the Louvain method ( 63 C. elegans network ) . The nodes are neurons and the edges are called synapses . The network has been downloaded from suppl / celegans131 . The original network has 131 nodes and 764 edges . The largest connected part of the motif The nodes in the transportation network are airports in the United States and Canada . There is an edge from city i to city j , if the estimated travel time from i to j is less than 23 years old . The network is not symmetric . The network with We next use motif-based clustering to look at several other networks . Motifs model energy flow patterns between several species . Low motif goes on high-quality clusters only for motif M6 . Clusters based on motifs M5 or M8 have All data is available at our website at The web site includes links to data used for experiments all over the world . See the Supplementary Materials ."}
{"ground_truth": "KV-Direct: High-performance in-memory key-value store with programmable NIC Li et al., SOSP\u201917  We\u2019ve seen some pretty impressive in-memory datastores in past editions of The Morning Paper, including FaRM , RAMCloud , and DrTM . But nothing that compares with KV-Direct:  With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Check out the bottom line in this comparison table from the evaluation:  ( Enlarge )  In addition to sheer speed, you might also notice that KV-Direct is 3x more power efficient than other systems, and the first general purpose KVS system to achieve 1 million KV operations per watt on commodity servers. Since the server CPU can also be used to run other workloads at the same time, you can make a case for KV-Direct being as much as 10x more power efficient than CPU-based systems. What we\u2019re seeing here is a glimpse of how large-scale systems software of the future may well be constructed. As the power ceiling puts a limit on multi-core scaling, people are now turning to domain-specific architectures for better performance. A first generation of key-value stores were built in a straightforward manner on top of traditional operating systems and TCP/IP stacks. More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. KV-Direct however, goes one step beyond. To support network virtualisation, more and more servers in data centers are now equipped with programmable NICS containing field-programmable gate arrays (FPGA). An embedded NIC chip connects to the network, and a PCIe connector attaches to the server. KV-Direct uses the FPGA in the NIC to implement key-value primitives directly. Like one-sided RDMA (Fig 1b below), KV-Direct bypasses the remote CPU. But it also extends the RDMA primitives from simple memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops) \u2014 Fig 1c below. Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues on the server-side, thus removing computation overhead in the client, and reducing network traffic. In addition, to support vector-based operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages and delegate certain computations to programmable NIC for efficiency. Design goals and challenges  Use cases for in-memory key-value stores have evolved beyond caching to things such as storing data indices, machine learning model parameters, nodes and edges in graph computing, and sequencers in distributed synchronisation. The role of the store shifts from object caching to a generic data structure store (c.f. Redis). This leads to the following design goals:  High batch throughput for small key-value pairs (e.g., model parameters, graph node neighbours). Predictable low-latency (e.g., for data-parallel computation,where tail latency matters)  High efficiency under write-intensive workloads (e.g., graph computations, and parameter servers)  Fast atomic operations  (e.g., for centralized schedulers, sequencers , counters and so on). Vector-type operations (for machine learning and graph computing workloads that often require operating on every element in a vector). The throughput constraint ends up being PCIe bandwidth:  In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET. Getting to this level involves work on three fronts:  Minimising DMA (direct memory access) requests per KV operation. The two major components that drive random memory access are hash tables and memory allocation. Hiding PCIe latency while maintaining consistency, which entails pipelining requests. Care must be taken to respect causal dependencies here though. Balancing load between NIC DRAM and host memory. The NIC itself has a small amount of DRAM available, but it turns out not to be much faster than going over PCIe. So the trick turns out to be to use both in order to utilise the joint bandwidth. KV-Direct  KV-Direct enables remote direct key-value access. Clients send operation requests to the KVS server, and the programmable NIC processes requests and sends back results, bypassing the CPU. The following table shows the supported operations. The most interesting of course are the vector operations. KV-Direct supports two types of vector operations: sending a scalar to the NIC on the server, where the NIC applies the update to each element in the vector; and sending a vector to the server, where the NIC updates the original vector element-by-element. Furthermore, KV-Direct supports user-defined update functions as a generalisation to atomic operations. The update functions needs to be pre-registered and compiled to hardware logic before executing. When the user supplies an update function, the KV-Direct toolchain duplicates it several times to leverage FPGA parallelism and match computation with PCIe throughput, and then compiles it into reconfigurable hardware logic using a high-level synthesis (HLS) tool. These functions can be used for general stream processing on a vector value. The programmable NIC on the KVS server is reconfigured as a KV processor, which receives packets from the network, decodes vector operations, and buffers KV operations in a reservation station. The out-of-order engine then issues independent KV operations from the reservation station into the decoder. To minimise memory accesses, small KV pairs are stored inline in the hash table, while others are stored in dynamically allocated memory from a slab memory allocator. After a KV operation completes, the result is sent back to the out-of-order execution engine to find and execute matching KV operations in the reservation station. The reservation station is used to avoid dependencies between two KV operations leading to data hazards and a stalled pipeline. We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context. To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed. However, comparing 256 16-byte keys in parallel would take 40% of the logic resource of our FPGA. Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key. When a KV operation completes, the latest value is forwarded to the reservation station, where pending operations in the same hash slot are checked. Those with a matching key are executed immediately and removed from the station. Further design and implementation details can be found in sections 3 and 4 of the paper. Evaluation  The evaluation section contains a suite of microbenchmarks, followed by a system benchmark based on the YCSB workload. To simulate a skewed Zipf workload, skewness 0.99 was chosen. This is referred to as the long-tail workload in the figures. The testbed comprises eight servers with two 8-core CPUS per server,and one Arista switch. There is a total of 128 GiB of host memory per server. A programmable NIC is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch. The NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector. Here\u2019s the overall throughput achieved by the system. The throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores. Without network batching, the tail latency ranges from 3-9  s depending on KV size, operation type, and key distribution. Network batching adds less than 1  s latency, but significantly improves performance. It is possible to attach multiple NICs per server. With 10 KV-Direct NICs on a server, one billion KV ops/s is readily achievable on a commodity server. Each NIC owns a disjoin partition of the keys. Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation, but for a relatively small number of partitions (e.g. 10) the load imbalance is not too great \u2013 1.5x of the average in the highest loaded NIC even for the long-tail highly skewed workload. KV-Direct throughput scales almost linearly with the number of NICS on a server. The last word:  After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction.", "prediction": "KV-Direct leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory. Compared with CPU based K We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory. Combined, these mechanisms allow a single NIC K In-memory key-value store (KVS) is a key distributed system component in many data centers. Historically, KVS such as Memcached gained popularity as Historically, KVS such as Memcached gained popularity as an object caching system for web services. The workload shifts from object cache to generic data structure store implies several High-performance KVS systems fall into three categories: on the CPU of KVS server, on KVS clients or on a hardware accelerator. In high performance KVS Programmable NICs with FPGA now witness large-scale deployment in datacenters. People are now turning to domain-specific architectures (DSAs) for better KV-Direct moves KV processing from the CPU to the programmable NIC in the server. The NIC accesses host memory via PCIe, a packet switched network with KV-Direct enables remote direct key-value access. The programmable NIC on KVS server is an FPGA reconfigured as a KV processor. KV-Direct extends one-sided RDMA operations to key-value operations. It supports two types of vector operations: Sending a scalar to the NIC on the KV storage is partitioned into two parts: a hash table and dynamically allocated memory. To minimize the number of memory accesses, small KV pairs are stored in Our hardware platform is built on an Intel Stratix V FPGA based programmable NIC (\u00a72.3) Our KV processor is implemented in 11K lines We evaluate KV-Direct in a testbed of eight servers and one Arista DCS-7060CX-32S switch. Each server equips two There are two free parameters in our hash table design: (1) inline threshold, (2) ratio of hash index in the entire memory space. When hash index ratio KV-Direct is 10x more power efficient than CPU-based systems. It is able to reach the clock frequency bound of 180 Mops under read-intensive workload PCIe has 29% TLP header and padding overhead for 64B DMA operations. The DMA engine may not have enough parallelism to saturate the PCIe With 10 KV-Direct NICson a server, the one billion KV op/s performance is readily achievable with a commodity server. Figure 20 shows that K The goal of KV-Direct is to leverage existing hardware in data centers to offload an important workload (KV access) We use programmable NICs, which A large body of distributed KVS are based on CPU. KV-Direct comes with a new hash table and memory management mechanism specially designed for FPGA to minimize KV-Direct is another exercise in leveraging reconfigurable hardware to accelerate an important workload. It is able to obtain superior performance by carefully co-designing hardware and We would like to thank Kun Tan, Ningyi Xu, Ming Wu, Jiansong Zhang and Anuj Kalia for all technical discussions and valuable comments. We\ufffd", "simplified_prediction": "KV-Direct leverages programmable NIC to extend RDMA primitives . This allows remote direct key-value access to the main host memory . Compared with CPU based K We develop several novel techniques that want to hide the latency of the PCIe connection between the NIC and the host memory . Combined , these mechanisms allow a single NIC K In-memory key-value store ( KVS ) to create a system used in many data centers . KVS such as Memcached became popular as Historically , KVS such as Memcached gained popularity as an object caching system for web services . The workload shifts from object cache to generic data structure store implies several high-performance KVS systems fall into three categories : on the CPU of KVS server and on a hardware accelerator . In high performance KVS Programmable NICs with FPGA now see large-scale deployment of the time . People now turning the architecture for better KV-Direct moves KV processing from the CPU to the programmable NIC in the server . The NIC accesses host memory via PCIe , a network with KV-Direct which allows remote direct key-value access to access . The programmable NIC on KVS server is an FPGA computer made by KV processor . It extends one-sided RDMA operations to key-value operations . It supports two different types of vector operations : Sending a scalar to the NIC on the KV storage is partitioned into two parts : a hash table . To minimize the number of memory accesses , small KV pairs are stored in Our hardware platform is built on an Intel Stratix V FPGA based programmable NIC ( ^ 2.3 ) Our KV processor . This is done in 11K lines We evaluate KV-Direct in a testbed of eight servers and one Arista . Each server has two free parameters in our table design : ( 1 ) inline threshold ( 2 ) ratio of hash index in the entire memory space ( 2 ) . When there is more power efficient than CPU-based systems , KV-Direct is 10x more power efficient . It is able to reach the clock frequency bound of 180 Mops under read-intensive workload PCIe has 29 % TLP header for 64B DMA operations in the clock area . The DMA engine may not have enough parallelism to saturate the PCIe With 10 KV-Direct NICson a server , the one billion KV op / s performance is readily being made . K The goal of KV-Direct is to leverage existing hardware in data centers to offload an important workload We use programmable NICs , which A large body of distributed KVS are based on CPU . KV-Direct comes with a new hash table and memory management mechanism specially designed for FPGA to minimize KV-Direct is another exercise in hardware to accelerate an important workload . It is able to get better performance by carefully co-designing hardware and We would like to thank Kun Tan , Ningyi Xu , Ming Wu , Jiansong Zhang and Anuj Kalia for all technical discussions and valuable comments . We !"}
{"ground_truth": "This paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. Only a finite set of a priori known policies are considered. Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle. A statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. Evidence  Anomaly detection was explored by recording three trajectories corresponding to two bikes and a bus. The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn. System was able to detect these trajectories as anomalous (Not within the set of known policies)  Evaluated in simulated driving environment  Notes  Bayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed. Then, the likelihood of all available policies for the target car given the distribution over the car\u2019s potential policies at the current timestep can be computed (sounds like HMM). The CHAMP algorithm infers the maximum a posteriori set of times at which changepoints between policies have occurred, yielding a set of segments. Given a segment from time s to t and a policy pi, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)  Viterbi path is found for the most likely sequence of latent policies  For decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car. For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon. This yields a set of simulated trajectories. The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen. This repeats continuously in a receding horizon manner. Reward function  distance to the goal at the end of the evaluation horizon  minimum distance to obstacles to evaluate safety  lane choice bias to add a preference for the right lane  maximum yaw rate and longitudinal jerk to measure passenger comfort", "prediction": "An autonomous car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. This paper presents an integrated behavioral inference and Decision-making for autonomous driving is hard due to uncertainty on the continuous state of nearby vehicles. We present an integrated behavioral anticipation and decision- Despite the probabilistic nature of the anticipation problem, some approaches in the literature assume no uncertainty on the future states of other participants. Such an The POMDP model provides a mathematically rigorous formulation of the decision making problem in dynamic, uncertain scenarios. Finding an optimal solution to most P We first formulate the problem of decision making in dynamic, uncertain environments as a multiagent POMDP. We then show how we exploit autonomous driving Let V denote the set of vehicles interacting in a local neighborhood of our vehicle. At time t, a vehicle v can take an action avt We make the following approximations to sample from the likely interactions of traffic agents. At any given time, both our vehicle and other vehicles are In this section, we describe how we infer the probability of the policies executed by other cars and their parameters. Our behavioral anticipation method is based on To segment a target car\u2019s history of observed states, we adopt the recently proposed CHAMP algorithm by Niekum et al. In contrast with other anticipation approaches in the literature, here we compute the likelihood of each latent policy by leveraging changepoint detection on the history of observed The time-series segmentation obtained via changepoint detection allows us to perform online detection of anomalous behavior not modeled by our policies. Anomal Algorithm 1 implements the formulation and approximations given in \u00a7III by leveraging the anticipation scheme from \u00a7IV. The algorithm begins by drawing a There are many possible design choices for engineering the set of available policies in our approach, which we wish to explore in future work. However, in A lower-fidelity simulation can capture the necessary interactions between vehicles to make reasonable choices. In practice, we use a simplified simulation model for each The reward function for evaluating the outcome of a rollout \u03a8 involving all vehicles is a weighted combination of metrics. The construction of a reward function based To evaluate our behavioral anticipation method and our multipolicy sampling strategy, we use traffic-tracking data collected using our autonomous vehicle platform. We evaluate our To collect the traffic-tracking dataset we use in this work, we have used our autonomous vehicle platform. The vehicle uses prior maps of the area For our system, we are interested in correctly identifying the behavior of target vehicles by associating it to the most likely policy according to the observations. We now qualitatively explore the performance of our anomaly detection test. We recorded three additional trajectories corresponding to two bikes and a bus. The bikes To show that our approach makes decision-making tractable, we assess the sampling performance in terms of the likelihood of the samples using the recorded intersection We tested the full decision-making algorithm with behavioral prediction in a simulated environment with a multi-lane highway scenario involving two nearby cars. This simulation By explicitly modeling reasonable behaviors of both our vehicle and other vehicles as policies, we make informed high-level behavioral decisions that account for the consequences of This work was supported in part by a grant from Ford Motor Company. The authors are sincerely grateful to Patrick Carmody for his help in collecting the", "simplified_prediction": "An autonomous car must evaluate the consequences of its potential actions by expecting the uncertain intentions of other traffic participants . This paper presents a part of behavioral inference and Decision-making for autonomous driving is hard due to uncertainty on the continuous state of nearby vehicles . We present an integrated behavioral anticipation and decision-spite the nature of the anticipation problem , some approaches in the literature think that there is no uncertainty on the future states of other participants . The POMDP model provides a mathematical formulation of the decision making problem in dynamic , uncertain scenarios . Finding an optimal solution to most P We first formulate the problem of decision making the uncertain environment as a problem for POMDP . We then show how we use autonomous driving Let V , which means the set of vehicles interacting in a local neighborhood of our vehicle . At time t , a vehicle v can take an action avt We make the following kinds of traffic agents to sample from the same vehicle . At any time , both our vehicle and other vehicles are : In this section , we describe how we infer the probability of the policies executed by other cars and their parameters . Our behavioral method is based on To segment a target car romantics history of observed states , we adopt the recently proposed CHAMP algorithm by Niekum et al . In contrast with other thought approaches in the literature , here we compute the likelihood of each latent policy on the history of observed The time-series segmentation obtained by changepoint detection allows us to perform online detection of anomalous behavior not modeled by our policies . Anomal Algorithm 1 implements the formulation and approximations given in } by using the anticipation scheme from } ( } ) . The algorithm begins by drawing a There are many possible design choices for engineering the set of available policies in our approach , which we want to explore in future work . However , in A lower-fidelity simulation can capture the necessary interactions between vehicles to make reasonable choices . In practice , we use a simplified simulation model for each The reward function for evaluating the outcome of a rollout  name involving all vehicles is the same as metrics . The construction of a reward function based to evaluate our behavioral anticipation method and our strategy . This way we use traffic-tracking data collected using our autonomous vehicle platform . We evaluate our To collect the traffic-tracking dataset we use in this work , we have used our computer platform . The vehicle uses prior maps of the area For our system , we are interested in correctly identifying the behavior of target vehicles by using it to the most likely policy according to the observations . We now explore the performance of our anomaly detection test . We recorded three more trajectories at one time , two bikes and a bus . The bikes To show that our approach makes decision-making tractable , we look at the sampling performance in terms of the likelihood of the samples using the full decision-making algorithm with a simulated environment with a highway scenario involving two nearby cars . This simulation By explicitly modeling behaviors of both our vehicle and other vehicles as policies , we make sure that account for the consequences of this work was supported in part by a grant from Ford Motor Company . The authors have been given to Patrick Carmody for his help in collecting the author ."}
{"ground_truth": "`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript. The notes below were made based on v1.`  This paper considers the problem of Maximum Inner Product Search (MIPS). In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$. Recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation). In this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs. Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm). Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree. They propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS. The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS. In order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query. Their experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods.", "prediction": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. We propose to train a spherical kmeans, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS) This simple approach yields The Maximum Inner Product Search (MIPS) problem has recently received increased attention, as it arises naturally in many large scale tasks. MIPS is related to nearest neighbor search (NNS), and to maximum similarity search. It is considered a harder problem because the inner product neither satisfies the triangular inequality as usually do. There are two common types of solution for MIPS in the literature: tree-based methods and hashingbased methods. Tree- based methods are data dependent (i.e. first trained to adapt to the specific data set) While hash-based method are mostly data independent. We follow the previous work by Shrivastava and Li (2015) for reducing the MIPS problem to the MCSS problem by ingeniously rescaling the vectors and adding new components. We then apply two mappings P and Q, one on the data points and another on the query vector. To find the one vector In this section we will evaluate the proposed algorithm for approximate MIPS. We analyze the following characteristics: speedup, compared to the exact full linear search, of retrieving top-K items with largest inner product, and robustness of retrieved results to noise in the query. We have used 2 collaborative filtering datasets and 1 word embedding dataset. Given the user-item matrix Z, we follow the pureSVD procedure described in (Cremonesi et al., 2010) to generate user and movie vectors. We consider 60,000 randomly selected users as queries. We consider the following baselines to compare with. PCA-Tree (Bachrach et al., 2014) is the state-of-the-art tree-based method. SRP-Hash: This is the signed random projection hashing method for MIPS proposed in Shrivastava and Li (2015) K-means and PCA-Tree algorithms were tested on Movielens-10M and Netflix datasets. Hashing-based methods perform better with lower speedups. But their performance decrease rapidly after 10x speedup. In this experiment, we consider a word embedding retrieval task. We take 2,000 random word embeddings from the database and corrupt them random Gaussian noise. We vary the scale of the noise from 0 to 0.4 and plot the performance. We can see that k-means always performs better than other In this paper, we have proposed a new and efficient way of solving approximate K-MIPS based on a simple clustering strategy. We regard the simplicity of this approach as one of its strengths. Empirical results on three real-world datasets show that this simple approach clearly outperforms the other families of techniques. The authors would like to thank the developers of Theano (Bergstra et al., 2010) for developing such a powerful tool. Samsung, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR.", "simplified_prediction": "Maximum Inner Product Search ( MIPS ) is an important task that has many things in recommendation systems and classification with a large number of classes , including a large number of classes . We propose to train a spherical kmeans , after having reduced the MIPS problem to a Maximum Cosine Similarity Search ( MCSS ) This simple approach has become naturally in many large scale tasks . This problem has recently received increased attention , as it arises naturally in many large scale tasks . MI is related to nearest neighbor search and to maximum similarity search . It is considered a harder problem because the inner product neither satisfies the problem as usually do . There are two common types of solution for MIPS in the literature . There are two common types of solution for MIPS . Table methods are data dependent ( i . e . first trained to adapt to the specific data set . While the method is mostly independent . We follow the work by Shrivastava and Li ( 2015 ) for reducing the MIPS problem to the MCSS problem by making the vectors and adding new parts to the problem . We then apply two mappings P and Q , one on the data points and another group of people . To find the one vector In this section we will evaluate the algorithm for using the MIPS algorithm . We looked at the following characteristics : speedup , compared to the exact full linear search , of retrieving top-K items with largest inner product , and robustness of retrieved results to noise . We have used 2 collaborating datasets and 1 word embedding dataset . Given the user-item matrix Z , we follow the pureSVD procedure ( Cremonesi et al . , 2010 ) to generate user and movie vectors . We consider 60,000 randomly selected users as bad . We consider these baselines to be compared with . PCA-Tree ( Bachrach et al . , 2014 ) is the state tree-based method of tree . K-means and PCA-Tree algorithms were proposed on Movielens-10M and Netflix dataets . SRP-Hash : This is the signed random projection hashing method for MIPS proposed in Shrivastava and Li . Hashing methods perform better with a lower speedup . But their performance decrease quickly after 10x speedup . In this experiment , the word '' embedding '' comes from task . We take as random word embeddings from the database and corrupt them randomly noise . We are different from the noise of 0 to 0.4 and plot the performance . We can see that k-means always performs better than other In this paper . We have proposed a new and efficient way of solving K-MIPS based on a simple clustering strategy . We regard the simplicity of this approach as one of its strength . On three real-world datasets show that this simple approach is clearly related to the other families of techniques . The authors would like to get the developers of Theano ( Bergstra et al . , 2010 ) for developing things such as good tool . Samsung , NSERC , Calcul Quebec , Compute Canada , Canada Research Chairs and Canada ."}
{"ground_truth": "Proposes a novel, end-to-end architecture for generating short email responses. Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses. . Challenges in deploying Smart Reply in a user-facing product  Responses must always be of high quality. Ensured by constructing a target response set to select responses from. The likelihood of choosing the responses must be maximised. Ensured by normalising the responses and enforcing diversity. The system should not add latency to emails. Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline. Computation time is further reduced by finding approximate best result instead of the best result. Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system. Architecture  Preprocess Email  Perform actions like language detection, tokenization, sentence segmentation etc on the input email. Triggering Model  A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses. Data  Training set of pairs (o, y) where o is the incoming message and y is a boolean variable to indicate if the message had a response. Features  Unigrams, bigrams from the messages. Signals like - is the recipient in the contact list of the sender. Response Selection  LSTM network to predict the approximate best response for an incoming message o  Network  Sequence to Sequence Learning. Reads the input message (token by token) and encode a vector representation. Compute softmax to get the probability of first output token given the input token sequence. Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token. During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach. Response Set Generation  Generate a set of high-quality responses that also capture the variability in the intent of the response. Canonicalize the email response by extracting the semantic structure using a dependency parser. Partition all response messages into \"semantic\" clusters. These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses. Semantic Intent Clustering  Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used. Graph Construction  Manually define a few clusters with a small number of example responses for each cluster. Construct a graph with frequent response messages (including the labelled nodes) as response nodes (VR). For each response node, extract a set of feature nodes (VF) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node. Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph. After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence. For validation, extract the top k members of each cluster and validate the quality with help of human evaluators. Suggestion Diversity  Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses. If the top two responses contain at least one positive (negative) response and none of the top three responses is negative (positive), the third response is replaced with a negative (positive) one. This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set. Strengths  The system is already in production and assists with around 10% of all mobile responses. This comment has been minimized. Sign in to view  Copy link  Quote reply  vegetakarthhik commented  Nov 26, 2018  hey do you have python implementation?", "prediction": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment A machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavor. Given the current availability of powerful hardware and large amounts of machine-readable data, the times should be ripe Rather than attempting to formally characterize intelligence, we propose here a set of desiderata we believe to be crucial for a machine to be able to autonomously make itself helpful to humans. The guiding principles we implicitly considered Intelligent machines must be able to communicate with us. We propose language as the general interface to the machine. We are agnostic about the nature of the machine\u2019s internal representations. We believe it is uncontroversial that a machine supposed to be helping us in a variety of scenarios, many unforeseen by its developers, should be endowed with the capability of learning. A machine that does not learn cannot adapt In this section, we describe a simulated environment designed to teach the basics of linguistic interaction to an intelligent machine. The simulated ecosystem should be seen as a \u201ckindergarten\u2019 providing basic education to intelligent We propose a dynamic ecosystem akin to that of a computer game. The Learner (the system to be trained) is an actor in this ecosystem. The Teacher assigns tasks and rewards the Learner for desirable behaviour. We show how the Teacher guides the Learner from these basic skills to being able to solve relatively sophisticated Environment navigation problems by exploiting interactive communication. The tasks we describe are incrementally structured, starting with learning to issue Environment An intelligent machine schooled in our ecosystem could later make itself useful in the real world. We consider a scenario in which the machine works as an assistant to Alice, an elderly person living alone. The range of tasks In this section, we will outline some of our ideas about how to build intelligent machines that would benefit from the learning environment we described. While we do not have a concrete proposal yet about how exactly such machines should be There are many types of behavior that we collectively call learning, and it is useful to discuss some of them first. To master basic communication skills, the machine will have to understand the concept of positive and negative reward. Long-term memory should be able to store facts and algorithms corresponding to learned skills, making them accessible on demand. Even the ability to learn should be seen as a set of skills that are stored in the memory. We are convinced that such model should be unrestricted, that is, able to represent any pattern in the data. We are not interested in building the intelligent machine around the concept of the Turing machine; we just aim to We share Turing\u2019s goal of developing a child machine capable of independent communication through natural language. We also diverge with respect to the imitation game, since the purpose of our intelligent machine is not to fool human We defined basic desiderata for an intelligent machine, stressing learning and communication as its fundamental abilities. We proposed a simulated environment that requires the intelligent machine to acquire new facts and skills through communication. In this environment, We thank the Facebook AI Research team for stimulating discussions. An early version of this proposal has been discussed in several research groups since 2013 under the name Incremental learning of algorithms.", "simplified_prediction": "The development of intelligent machines is one of the biggest challenges in computer science . In this paper , we propose some important properties of machines should have , focusing on communication , and learning . We talk about a simple environment A machine capable of performing complex tasks without needing programming would be useful in almost any human endeavor . Given the current availability of powerful hardware and large amounts of desiderata we believe to be crucial for a machine to be able to make itself helpful to humans . This is because a set of desiderata we believe to be able to make itself helpful to humans . The guiding principles we think that Intelligent machines must be able to communicate with us . We propose language as the general interface to the computer . We are about the nature of the machine 's internal representations . We believe it is uncontroversial that a machine supposed to be helping using a variety of scenarios , many unforeseen by its developers should get better . A machine that does not learn cannot adapt In this section , we describe a environment designed to teach the basics of linguistic interaction to an intelligent machine . The ecosystem should be seen as a person who does not have basic education to intelligent We propose a dynamic ecosystem to that of a computer game . The system of Learner ( the system to be trained ) is an actor . The Teacher assigns tasks and rewards the behavior of the person . We show how the Teacher guides the Learner from these basic skills to being able to solve Environment navigation problems by using interactive communication . The tasks we describe are very good , starting with learning to issue Environment An intelligent machine schooled in our ecosystem could later make itself useful in the real world . We consider a scenario in which the machine works as an assistant to Alice , a person living alone . In this section , we will outline some of our ideas about how to build intelligent machines that would help people learn how to learn the learning environment we described . While we do not have a concrete proposal yet about how exactly such machines should be There are many types of behavior that we call learning , and it is useful to talk about some of them first . To master basic communication skills , the machine will understand the concept of positive and negative reward . Long-term memory should be able to store facts and algorithms where they learned skills , making them easy to use . Even the ability to learn should be seen as a set of skills that are stored in memory . We think that such model should be unrestricted , that is , able to represent any pattern in the data . We are not interested in building the intelligent machine around the concept of the Turing machine . We just want to go to We 's goal of developing a child capable of independent communication through natural language . We also talk about the imitation game , since the purpose of our intelligent machine is not to fool human We defined basic desiderata for a intelligent machine , stressing learning and communication as its fundamental abilities . We proposed a environment that requires the intelligent machine to get new facts and skills through communication . In this environment , the AI Research team for making a new discussion . An early version of this proposal has been talked about in several research groups since 2013 , under the name '' Incremental learning '' ."}
{"ground_truth": "They suggest a new method to train GANs. They start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions. Each new block of convolutions is slowly blended in, instead of being added from one batch to the next. Combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new CelebA-HQ dataset (CelebA in high resolution). They also suggest a new scoring method based on the approximated Wasserstein distance between real and generated image patches. According to that score, their progressive training method improves results significantly. What  They suggest a new, progressive training method for GANs. The method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images. They also introduce two new normalization techniques. They also suggest a new method to estimate/score the quality of the generated images. They introduce CelebA-HQ, a variation of CelebA containing high resolution images. How  Progressive growing/training  They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN). Visualization:  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images. Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator. They don't switch to the added convolutions instantly/suddenly, but give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B, where A are the features after just upscaling, B are the features after upscaling AND the convolutions and alpha is the overlay factor, which is gradually increased over time. This is done for both the generator and the discriminator and at all resolutions. Visualization:  Note that all layers are always trained (after they were added to the models). Training for the earlier layers does not stop. Training in this way focuses most of the computation on the earlier resolutions. It also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time. Minibatch Standard Deviation  They try to improve diversity by adding a method very similar to minibatch discrimination. They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers). They do this per example in each minibatch, resulting in B*H*W*C standard deviations. (B = batch size, H = height, W = width, C = channels/filters)  They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output. This adds a channel with one constant value to each example in the minibatch. The value is the same for all examples. Equalized Learning Rate  They use Adam for their training. Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight). They argue that this has the downside of equalizing all weight's stepsizes. But some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\"). As a result, the learning rate will be too small for some weights and too large for others. To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1). Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer. (TODO exact formula for c?) (This looks an aweful lot like weight normalization .) Using simpler weight initialization equalizes the dynamic range of parameters. Doing the normalization then fixes problems related to the simpler weight initialization. Pixelwise Feature Vector Normalization in the Generator  They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control. They fix this by normalizing feature vectors in the generator, similar to local response normalization. They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):  Scoring Images  They suggest a new method to score images generated by the generator. They perform the following steps:  Sample 16384 images from the generator and the dataset. Build a Laplacian Pyramid of each image. It begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution. Each level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band). Sample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level. Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches. Normalize each patch with respect to the computed means and standard deviations. Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real). The result is one value. Lower values are better. CelebA-HQ  They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces. They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images. They use an adversarially-trained superresolution model to upscale the images. They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation. They rescale the images to 1024x1024 using bilinear sampling and box filters. They manually select the 30k best looking images. Other stuff  They use Adam for training (alpha=0.001, beta1=0, beta2=0.99). They use the WGAN-WP method for training, but LSGAN also works. They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions. They also add regularization loss on the discriminator, punishing outputs that are very far away from 0. Their model for CelebA-HQ training is similar to a standard DCGAN model. The generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling. They start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator. They use leaky ReLUs in the generator and discriminator. They remove batch normalization everywhere. Results  Scores  Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:  So progressive growing (b) significantly improves results. Same -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g). Minibatch discrimination worsened the results. Using small batch sizes also worsened the results. In (d) they \"adjusted the hyperparameters\" (??) and removed batch normalization. They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models. They achieve an Inception Score of 8.80 on CIFAR-10. Images look improved. CelebA-HQ example results:  LSUN dining room, horse, kitchen, churches:", "prediction": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively. This both Generative methods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use. Currently the most prominent approaches are Our primary contribution is a training methodology for GANs. We start with low-resolution images, and then progressively increase the resolution by adding layers Salimans et al. (2016) suggest \u201cminibatch discrimination\u2019 as a solution. We simplify this approach drastically while also GANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage We deviate from the current trend of careful weight initialization. We instead use a trivial N (0, 1) initialization and then explicitly scale the To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the In order to compare the results of one GAN to another, one needs to investigate a large number of images. We noticed that existing methods such In this section we discuss a set of experiments that we conducted to evaluate the quality of our results. We will distinguish between the network structure (e We use sliced Wasserstein distance (SWD) and multi-scale structural similarity (MSSSIM) to evaluate the importance our individual contributions Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput. We observe that the progressive variant offers two main To meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset. To this end, we created a high Figure 6 shows a purely visual comparison between our solution and earlier results in LSUN BEDROOM. Figure 7 gives selected examples from seven very The best inception scores for CIFAR10 (10 categories of 32 \u00d7 32 RGB images) we are aware of are 7.90 for un While the quality of our results is generally high compared to earlier work on GANs, there is a long way to true photorealism. We would like to thank Mikael Honkavaara, Tero Kuosmanen, and Timi Hietanen for the compute Table 2 shows network architectures of the full-resolution generator and discriminator that we use with the CELEBA-HQ dataset. Both networks consist In this section we describe the process we used to create the high-quality version of the CELEBA dataset, consisting of 30000 images in Figure 9 shows non-curated images generated in the unsupervised setting. Table 3 compares against prior art in terms of inception scores. We Metz et al. (2016) describe a setup where a generator synthesizes MNIST digits simultaneously to 3 color channels. The digits are classified Figure 10 shows the nearest neighbors found for our generated images. Figure 11 gives additional generated examples from CELEBA-HQ. We enabled mirror aug Figures 12\u201317 show representative images generated for all 30 LSUN categories. A separate network was trained for each category using identical parameters. All Figure 18 shows larger collections of images corresponding to the non-converged setups in Table 1. The training time was intentionally limited to make the", "simplified_prediction": "We describe a new training methodology for the adversarial network . The key idea is to grow both the generator 's and discriminator . It is used to produce novel samples from high-dimensional data distributions , such as images , are finding widespread use . The most prominent approaches are Our primary contribution is a training method for GANs . We start with low images , and then progressively increase the resolution by adding layers of Salimans et al . 2016 ) suggests discrimination at discrimination as a solution . GANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between the two networks , as a result of unhealthy competition . Most if not all earlier solutions do not have to be careful from the current trend of careful weight . In order to compare the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition , we normalize the In order to compare the results of one GAN to another , one needs to investigate a large number of images . This means that one needs to investigate a large number of images . We noticed that existing methods such as In this section we discuss a set of experiments that we want to understand the quality of our results . We will distinguish between the network structure ( e We use Wasserstein distance ( SWD ) and multi-scale structural similarity ( MSSSIM ) to evaluate the importance our individual contributions Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput . We see that the progressive variant offers two main To meaningfully demonstrate our results at high output resolutions , we need a small amount of high-quality dataset . To the end , we created a high Figure 6 shows a visual comparison between our solution and earlier results in LSUN BEDROOM . Figure 7 gives selected examples from seven very The best inception scores for CIFAR10 ( 10 categories of 32 \u00d7 32 RGB images ) we are aware of are 7.90 for un While the quality of our results is generally high compared to earlier work on GANs , there is a long way to true photorealism . We would like to thank Mikael Honkavaara , Tero Kuosmanen , and Timi Hietanen for the compute Table 2 shows network architectures of computer programs and we use with the CELEBA-HQ dataset . Both networks consist In this section we describe the process we used to create the high-quality version of the CELEBA dataset . They are made of 30000 images in Figure 9 shows that are not made . Table 3 compares against prior art in terms of music . Wetz et al . 2016 ) describe a setup where people use MNIST digits at the same time as 3 color channels . The digits are sometimes called Figure 10 shows the nearest neighbors found for our pictures . Figure 11 gives examples of example from CELEBA-HQ . We enabled mirror aug Figures 12 people show pictures that were made for all 30 LSUN categories . A separate network was trained for each category using the parameters . All Figure 18 shows larger collections of images that were made to the non-converged setup . The training time was limited to a little bit ."}
{"ground_truth": "The paper presents the task of abductive NLP (pronounced as alpha NLP) where the model needs to perform abductive reasoning. Abductive reasoning is the inference to the most plausible explanation. Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse. A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided. The dataset models the task as multiple-choice questions to make the evaluation process easy. Task Setup  Given a pair of observations O1 and O2 and two hypothesis h1 and h2, the task is to select the most plausible hypothesis. In general, P(h | O1, O2) is propotional to P(h |O1)P(O2|h, O1). Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the \u201cfully connected\u201d assumption would jointly model both the observations and the hypothesis. Dataset  Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis. Observation pairs are created using the ROCStories dataset which is a collection of short, manually crafted stories of 5 sentences. The average word length for both the content and the hypothesis is between 8 to 9. To collect plausible hypothesis, the crowd workers were asked to fill in a plausible \u201cin-between\u201d sentence in natural language. Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words. Adversarial filtering approach from Zellers et al. is used with BERT as the adversary. A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration. Key Observations  Human performance: 91.4%  Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%  Entailment NLI baseline: 59%. This highlights the additional complexity of abductive NLI as compared to entailment NLI. BERT: 68.9%  GPT: 63.1%  Numerical and spatial knowledge-based data points are particularly hard. The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible", "prediction": "Abductive reasoning is inference to the most plausible explanation. We present the first study that investigates the viability of language-based abductive reasoning. On Abductive NLI, the best Abductive reasoning is inference to the most plausible explanation for incomplete observations. Study of abductive reasoning in narrative text has very rarely appeared in the NLP literature. Most previous work on abduct Abductive Natural Language Inference is a problem of multiple choice problems. Each instance in ART is defined as follows: O1: The observation at time t1. O2: A distinct feature of the \u03b1NLI task is that it requires jointly considering all available observations and their commonsense implications, to identify the correct hypothesis. We formulate a set of probabilistic ART is the first large-scale benchmark dataset for studying abductive reasoning in narrative texts. It consists of 20K narrative contexts with over 200K explanatory hypotheses. We crowdsourced the plausible and We now present our evaluation of finetuned state-of-the-art pre-trained language models on the ART dataset. Since \u03b1NLI is framed as a binary classification problem, Crowdsourcing tasks are complex and require creative writing. BERT (Devlin et al., 2018) and GPT (Radford, 2018) have recently been shown to achieve state There is enough scope for considerably scaling up the dataset based on ROCStories. The learning curve in Figure 5 shows that the performance of the best model plateaus after 10,000 instances We train GPT2 conditioned on the tokens of the two observations O1 and O2. ATOMIC (Sap et al., 2019) is a repository of inferential if- Since abduction is fundamentally concerned with plausible chains of cause-and-effect, our work draws inspiration from previous works that deal with narratives. Rather than learning prototypical scripts or narrative chains, we We present the first study that investigates the viability of language-based abductive reasoning. We create and introduce a new challenge dataset, ART, which consists of 20,000 commonsense narratives. We thank the anonymous reviewers for their insightful feedback. This research was supported in part by NSF (IIS-1524371), the National Science Foundation Graduate Research Fellowship under Grant No. Crowdsourcing was used to assess human performance. Participants were asked to write a probable middle sentence that explains why the second observation should follow after the first one. They were then asked to The warmup proportion was set to 0.2, and cross-entropy was used for computing the loss. The best performance was obtained with a batch size of 4, learning rate of The SVM classifier is trained on simple features like word length, overlap and sentiment features to select one of the two hypothesis choices. The bag-of-words baseline computes the average We use BERT (Devlin et al., 2018) as the adversary and introduce a temperature parameter that controls the maximum number of instances that can be modified in each iteration of AF. In ATOMIC (Sap et al., 2019) represents commonsense knowledge as a graph with events are nodes and the following nine relations as edges. Table 8 describes the format of input to each variation of the generative model evaluated.", "simplified_prediction": "The reason is inference to the most plausible explanation . We present the first study that looks at language-based abductive reasoning . The best known reasoning is inference to the most plausible explanation for incomplete observations , for incomplete observations . Study of abductive reasoning in story has very rarely appeared in the NLP literature . Most previous work on abduct Abductive Natural Language Inference is a problem of many problems . Each example in ART is defined as follows : O1 : The observation at time t1 . O2 : A distinct feature of the  entranceNLI task is that it requires all available observations and all of their common idea , to identify the correct hypothesis . We is the first large-scale benchmark dataset for studying abductive reasoning in narrative texts , for example the first reason . It was made up of 20K narrative contexts with over 200K explanatory . Wesourced the plausible and We now present our evaluation of state-of-the-trained language models on the ART dataset ( ART dataset ) . Because of this , there is a binary classification problem , Crowdsourcing tasks are complex and require creative writing . BERT ( Devlin et al . , 2018 ) and GPT ( Radford , 2018 ) have recently been shown to achieve state There is enough scope for a lot of things to find the dataset based on ROCStories . The learning curve in Figure 5 shows that the performance of the best model plateaus after 10,000 instances , the tokens of the two were shown on O1 and O2 . ATOMIC ( Sap et al . , 2019 ) is a collection of inferential if- Since abduction is usually concerned with plausible chains of cause-and-effect , our work draws inspiration from previous works that deal with narratives . We We present the first study that looks at language-based abductive reasoning . We present the first study that looks at the study of language . We create and introduce a new challenge dataset , ART , which has 20,000 common stories . Wek the person who has reviewed for their feedback . This research was supported by NSF ( IIS-1524371 ) , the National Science Foundation Graduate Research Fellowship under Grant No . He was used to assess human performance . Participants were asked to write a probable middle sentence that explains why the second observation should follow after the first one . They were then asked to The warmup proportion was set to 0.2 . The cross-entropy was used for computing the loss . The best performance was given with a batch size of 4 , learning rate of The SVM classifier is trained on simple features like word length , overlap and sentiment features to select one of the two hypothesis choices . The bag-of-words baseline computes the average We use BERT ( Devlin et al . , 2018 ) as a temperature parameter that controls the maximum number of instances that can be changed in each part of AF . It represents commonsense knowledge as a graph with events are nodes and the following nine relations as edges . Table 8 describes the format of input to each variation of the generative model ."}
{"ground_truth": "In this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error. Succinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by \u201cconvexifying\u201d them. They note that their method doesn\u2019t work with max pooling or very deep CNNs with lots of bells and whistles. This is a thought-provoking paper. I like how the authors pursued a theoretically interesting question, even though there isn\u2019t much practical use yet for this. I don\u2019t have personal experience writing theory papers, but I imagine that this is a good(?) representation of how they often go in ML. The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results.", "prediction": "We describe the class of convexified convolutional neural networks (CCNNs) CCNNs capture the parameter sharing of convolved neural networks in a convex manner. Convolutional neural networks (CNNs) have proven successful across many tasks in machine learning and artificial intelligence. The standard approach to training CNNs is based on solving a non In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem. At a high level, a two-layer CNN1 is a particular type of function that maps an input vector x to an output vector y. This mapping is formed in the following Average pooling and multiple channels are also an integral part of CNNs. We describe a relaxation of the class Fcnn that allows us to obtain a convex formulation of the We now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function In Section 3, we describe iterative algorithms that can be used to solve this form of convex program in the more general setting of nonlinear activation functions. We propose to minimize For nonlinear activation functions \u03c3, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS) Such filters are parametrized by a The algorithm for learning a two-layer CCNN is summarized in Algorithm 1. It is a formalization of the steps described in Section 3.2. In order to solve In this section, we upper bound the generalization error of Algorithm 1. We focus on the binary classification case where the output dimension is d2 = 1. The learning of In this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to In this section, we compare the CCNN approach with other methods. The results are reported on the MNIST dataset and its variations for digit recognition, and on the CIFAR We train twolayer and three-layer models respectively. Each convolutional layer is constructed on 5 \u00d7 5 patches with unit stride, followed by 2\u00d7 2 average pooling In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset. We train CNN and CCNN models with two With the empirical success of deep neural networks, there has been an increasing interest in theoretical understanding. Bengio et al. showed how to formulate neural network training as a convex optimization In this paper, we have shown how convex optimization can be used to efficiently optimize CNNs. For the two-layer CCNN, we proved that its generalization error conver In this appendix, we describe the properties of the two types of kernels. We prove that the associated reproducing kernel Hilbert Spaces (RKHS) of these kernels contain filters. The filter is parametrized by an infinite-dimensional vector wj . Our next step is to reduce the original ERM problem to a finite-dimensional one. In order Theory of Rademacher complexity plays an important role in empirical process theory. We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties", "simplified_prediction": "We describe the class of convexified neural networks CCNNs capture the parameter sharing of neural networks in a convex manner on the parameter . These networks ( CNNs ) have proven successful across many tasks in machine learning and artificial intelligence for a long time . The standard approach to training CNNs is based on solving a non In this section . There are many different kinds of neural networks to be learned and describe the nonconvex optimization problem . A two-layer CNN1 is a particular type of function that maps an input vector x/O. This is done to an output vector y . This is made up of the following Average pooling and multiple channels are also an important part of CNN . We describe relaxing the class of Fcnn that allows us to get a convex formulation of the We now turn to the development of the class of CNN . We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function In Section 3 , we describe iterative algorithms that can be used to solve this program in the more general setting of the algorithm . We propose to minimize For nonlinear activation functions , we relax the class of CNN filters to a reproducing kernel Hilbert space ( RKHS ) Such filters are used by a The algorithm for learning a two-layer in Algorithm 1 . It is a formalization of the steps described in Section 3.2 . In order to solve In this section , we upper bound the error of Algorithm 1 . We focus on the binary classification case where the output size is about 1 . The learning of In this section , we describe a way of learning CNNs with more layers . The idea is to estimate the parameters of the layers that are incrementally from bottom to In this section , we compare the difference with other methods . The results are reported on the MNIST dataset , and on the CIFAR We train twolayer and three-layer models respectively . Each layer is made on 5 \u00d7 5 patches with unit stride . This is followed by 2\u00d7 2 average pooling In order to test the capability of CCNN in complex classification tasks , we report its performance on the CIFAR-10 dataset . We CNN and LondonNN models with two with the success of deep neural networks . There has been an increasing interest in theoretical understanding . Bengio et al . In this paper , we have shown how convex optimization can be used to efficiently optimize CNNs , and that is , we have shown how convex optimization can be used . We proved that its generalization error conver In this appendix , we describe the properties of the two types of kernels , like this appendix . We prove that there is associated reproducing kernel called Hilbert Spaces ( RKHS ) of these kernels . The filter is parametrized by an infinite vector wj with a vector . Our next step is to reduce the original ERM problem to a certain problem . In orderory of Rademacher complexity plays an important role in process theory . We mean the reader to Bartlett and Mendelson [ 4 ] for an introduction to the theoretical properties ."}
{"ground_truth": "An empirical analysis of anonymity in Zcash Kappos et al., USENIX Security\u201918  As we\u2019ve seen before, in practice Bitcoin offers little in the way of anonymity . Zcash on the other hand was carefully designed with privacy in mind. It offers strong theoretical guarantees concerning privacy. So in theory users of Zcash can remain anonymous. In practice though it depends on the way those users interact with Zcash. Today\u2019s paper choice, \u2018An empirical analysis of anonymity in Zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the study. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. The analysis also provides some interesting insights into who is using Zcash and for what as well. Founders and miners combined account for around 66% of the value drawn from the shielded pool. The code for the analysis is available online at  [url]", "prediction": "Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded Since the introduction of Bitcoin in 2008, cryptocurrencies have become increasingly popular. Despite the growing number of legitimate users there are We consider as related all work that has focused on the anonymity of cryptocurrencies, either by building solutions to achieve stronger anonymity Zcash (ZEC) is an alternative cryptocurrency developed as a (code) fork of Bitcoin. In Bitcoin, In this section we describe four types of participants who interact in the Zcash network. Founders took part in the initial We used the zcashd client to download the Zcash blockchain, and loaded a database representation of it into Apache Across all blocks, there were 2,242,847 transactions. The vast majority of transactions are public (i Across all transactions, there have been 1,740,378 distinct t-addresses used. Of these, 8 As discussed in Section 4, a large proportion of the activity on Zcash does not use the shielded pool. This Zcash is a direct fork of Bitcoin and the standard client has the same behavior. In terms of false positives, Given that Zcash is still relatively new, there are not many different types of services that accept Zcash. We Running Heuristic 1 resulted in 560,319 clusters, of which 97,539 contained more than a single address Four out of the top five clusters belong to popular exchanges. Top five clusters accounted for 11.21% of all Although mining pools and founders account for a large proportion of the activity in Zcash, many re-use the same We identified three large organizations that accept Zcash donations: the Internet Archive, torservers.net, and This section explores interactions with the pool at its endpoints, meaning the deposits into (t-to-z) After comparing the list of founder addresses against the outputs of all coingen transactions, we found that 14 of them had The Zcash protocol specifies that all newly generated coins are required to be put into the shielded pool before they can be We gathered 19 t-addresses associated with Zcash mining pools. Figure 10 plots the value of their deposits into Mining pool payouts in Zcash are similar to how many of them are in Bitcoin [27, 18]. Once the miners and founders have been identified, we can assume the remaining transactions belong to more general entities. In this Z-to-z transactions form a crucial part of the anonymity core of Zcash. Our analysis identified 6, The Shadow Brokers (TSB) are a hacker collective that has been active since the summer of 2016. T In May 2017, TSB announced that they would be accepting Zcash for their monthly dump service. To identify potential Before the first TSB blog post in May, we found only a single matching transaction. After the blog post, This paper has provided the first in-depth exploration of Zcash, with a particular focus on its anonymity guarantees. We would like to thank Lustro, the maintainer of the Zchain explorer, for answering specific questions we asked", "simplified_prediction": "Zcash is often said to be the one with the strongest anonymity guarantees , due to its basis in well-regarded Zcash is often touted as the one with the strongest anonymity guarantees , due to its basis in well-regarded since the introduction of Bitcoin in 2008 . Despite the growing number of legitimate users there are We consider as related all work that has focused on the anonymity of cryptocurrencies , either by building solutions to achieve stronger anonymity , Zcash ( ZEC ) is an alternative cryptocurrency developed as a code . In this section we describe four types of people who interact with the Zcash network with the four types . We used the client to download the Zcash blockchain , and loaded a database representation of it into the Apache Across all blocks . There were also 2,242,847 transactions . The vast majority of transactions are public ( i Across all transactions ) , there have been 1,740,378 different types of transactions . Of these , 8 As talked about in Section 4 , a large part of the activity on Zcash does not use the shield . This Zcash is a direct part of Bitcoin and the standard client has the same behavior . In terms of false positives , Given that Zcash is still relatively new , there are not many different types of services that accept Zcash . This meant that Heuristic 1 resulted in 560,319 clusters , of which had more than a single address Four out of the top five clusters belong to popular exchanges . Top five clusters accounted for 11.21 % of all Although mining pools and founders account for a large proportion of the activity in Zcash , many re-use the same We identified three large organizations that accept Zcash donations : the Internet Archive , torservers ; and This section explore interactions interactions with the pool at its endpoints , meaning the deposits into the t-to-z ( t-to-z ) After comparing the list of the list of the pool . Figure 10 plots the value of their deposits into Mining pool payouts . These are similar to how many of them are in Bitcoin ( 27 , 18 ) . Once the miners and founders have been identified , we can take part in the remaining transactions belong to more general places . In this Z-to-z transactions form a very important part of the core of Zcash . Our analysis identified 6 , The Shadow Brokers ( TSB ) are a hacker which has been active since the summer of 2016 . In May 2017 , TSB announced that they would be accepting Zcash for their first month . Before the first TSB blog post in May , we found only a single matching transaction . After the blog post , This paper has provided the first people to talk about Zcash , with a particular focus on its guarantees . We would like to thank Lustro , the idea of the Zchain explorer , for answering specific questions we asked ."}
{"ground_truth": "The paper looks at the problem of learning structured exploration policies for training RL agents. Structured Exploration  Consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters. To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence. Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes. The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution. Consider a distibution over the tasks p(T). At the start of any episode of the ith task, a latent variable zi is sampled from the distribution N(\u03bci, \u03c3i) where \u03bci and \u03c3i are the learned parameters of the distribution and are referred to as the variation parameters. Once sampled, the same zi is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution \u03c0\u03b8(a|s, zi). The intuition is that the latent variable zi would encode the notion of a task or goal that does not change arbitrarily during the episode. Model Agnostic Exploration with Structured Noise  The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks. A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the policy-parameters and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy. General meta-RL approaches have two limitations when it comes to \u201clearning to explore\u201d:  Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively. Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model. Idea behind MAESN is to meta-train policy-parameters so that they learn to use the task-specific latent variables for exploration and can quickly adapt to a new task. An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration. For every iteration of meta-training, an \u201cinner\u201d gradient update is performed on the variational parameters and the post-inner-update parameters are used to perform the meta-update. The authors report that performing the \u201cinner\u201d gradient update on the policy-parameters does not help the overall learning objective and that the step size for each parameter had to be meta-learned. The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case). After training, the variational parameters for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the variational parameters. Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training. To be fair to the baselines, reward shaping is used while training baselines as well. Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time). Experiments  Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion. Each task distribution has 100 meta-training tasks. In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions. In the Locomotion task distributions, the different tasks correspond to the different goal positions. The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy. \u2022 In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer.", "prediction": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator Trajectoryranked Reward EXtrapolation (T-REX) uses ranked demonstrations to extrapolate a user's The goal of our work is to achieve improvements over a suboptimal demonstrator in high-dimensional reinforcement learning tasks Inverse reinforcement learning (IRL) seeks to find a reward function that models the intention of the demonstrator. Torabi et al. (2018a) propose a state-of-the-art model-based approach to Very little work has tried to learn good policies from highly suboptimal demonstrations. Grollman & Billard ( Tucker et al. (2018) tested state-of-the-art IRL methods on the Atari domain We model the environment as a Markov decision process (MDP) consisting of a set of states S, actions Trajectory-ranked Reward EXtrapolation (T-REX) is an algorithm for using ranked subopt We first evaluated our proposed method on three robotic locomotion tasks using the Mujoco simulator. In all three tasks To generate demonstrations, we trained a Proximal Policy Optimization (PPO) agent with the ground-truth We trained the reward network using 5,000 random pairs of partial trajectories of length 50. To prevent overfitting Learned Policy Performance We measured the performance of the policy learned by T-REX by measuring the forward distance traveled We next evaluated T-REX on eight Atari games shown in Table 1. To obtain a variety of subopt We used four convolutional layers with sizes 7x7, 5x5, 3x3, and 3 T-REX outperformed both BCO and GAIL in 7 out of 8 games. GAIL was unable The above results used synthetic demonstrations generated from an RL agent. We also tested T-REX when given ground- All experiments described thus far have had access to ground-truth rankings. To explore the effects of noisy rankings we first T-REX is able to infer a meaningful reward function even when noisy, time-based rankings are provided. T-REX is the first IRL algorithm that is able to significantly outperform the demonstrator without additional external This work has taken place in the Personal AutonomousRobotics Lab (PeARL) at The University of Texas Code as well as supplemental videos are available at ICML2019-TREX. Table 1 shows the full results for the MuJoCo experiments. The T-REX (time-ordered) To build the inverse transition models used by BCO we used 20,000 steps of a random policy to collect transitions We used the OpenAI Baselines implementation of PPO with default hyperparameters. We used 9 parallel workers when In this section, we examine the ability of prior work on active preference learning to exceed the performance of the demonstrator We used the Atari Grand Challenge data set (Kurin et al., 2017) to collect actual human demonstrations for We generated attention maps for the learned rewards for the Atari domains. We used the method proposed by Greydanus et", "simplified_prediction": "A critical flaw of existing inverse reinforcement learning ( IRL ) methods is their inability to significantly outperform the demonstrator Trajectoryranked Reward EXtrapolation ( T-REX ) uses rank demonstrations to extrapolate a user 's The goal of our work is to achieve improvements over a suboptimal demonstrator in high-dimensional learning tasks Inverse learning ( IRL ) to find the function of a reward models . Torabi et al . 2018a ) proposed a state-of-the-based approach to Very little work has tried to learn good policies from being able to do good . Grollman & Billard ( Tucker et al . ( 2018 ) tested state-of-the-art IRL methods on the Atari domain We model the environment as a Markov decision process ( MDP ) consisting of a set of states S , actions Trajectory-ranked Reward EXtrapolation ( T-REX ) is an algorithm for using the Mujoor simulation processor computer using the computers using the Mujoor . In all three tasks To generate demonstrations , we trained a Proximal Policy Optimization ( PPO ) agent with the ground-truth We trained the reward network using 5,000 random pairs of length 50 in length . To stop Learned Policy Performance We measured the performance of the policy learned by T-REX by measuring the forward distance traveled We next evaluated T-REX on eight Atari games shown in Table 1 . We used four layers with different sizes 7x7 , 5x5 , 3x3 , and 3 T-REX outperformed both BCO and GAIL in 7 out of 8 games . The RL agent was unable to see the results used synthetic demonstrations generated from an RL agent . When given ground- All experiments described thus far have had access to ground-truth rankings , this is called T-REX . To explore the effects of noisy rankings we first T-REX is able to get a reward function even when noisy , some of them are provided . T-REX is the first IRL algorithm that is able to make importantly outperform the demonstrator without additional external This work has taken place in the Personal AutonomousRobotics Lab ( PeARL ) at The University of Texas Code as well as other videos that are available at ICML2019 - TREX . Table 1 shows the full results for the experiments . To build the transition models used by BCO we used 20,000 steps of a random policy to collect transitions We used the OpenAI Baselines implementation of PPO with default hyperparameters . We used 9 parallel workers when In this section , we look at the ability of prior work on active preference learning to get more attention than the performance of the demonstrator We used the Atari Grand Challenge data set ( Kurin et al . , 2017 ) to collect actual human demonstrations for We made attention for the learned rewards for the Atari domains . We used the method used by people"}
{"ground_truth": "Build a supervised reading comprehension data set using news corpus. Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task. Reading Comprehension  Estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query. Dataset Generation  Use online newspapers (CNN and DailyMail) and their matching summaries. Parse summaries and bullet points into Cloze style questions. Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder. Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers. The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence. Models  Baseline Models  Majority Baseline  Picks the most frequently observed entity in the context document. Exclusive Majority  Picks the most frequently observed entity in the context document which is not observed in the query. Symbolic Matching Models  Frame-Semantic Parsing  Parse the sentence to find predicates to answer questions like \"who did what to whom\". Extracting entity-predicate triples (e1,V, e2) from query q and context document d  Resolve queries using rules like exact match, matching entity etc. Word Distance Benchmark  Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity. Sum the distance of every word in q to their nearest aligned word in d  Neural Network Models  Deep LSTM Reader  Test the ability of Deep LSTM encoders to handle significantly longer sequences. Feed the document query pair as a single large document, one word at a time. Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output. Attentive Reader  Employ attention model to overcome the bottleneck of fixed width hidden vector. Encode the document and the query using separate bidirectional single layer LSTM. Query encoding is obtained by concatenating the final forward and backwards outputs. Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs). The weights can be interpreted as the degree to which the network attends to a particular token in the document. Model completed by defining a non-linear combination of document and query embedding. Impatient Reader  As an add-on to the attentive reader, the model can re-read the document as each query token is read. Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding. Result  Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling. Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query. Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure. Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document. The paper also includes heat maps over the context documents to visualise the attention mechanism. This comment has been minimized. Sign in to view  Copy link  Quote reply  yauhen-info commented  Apr 28, 2017  Thank you for sharing a good piece of work. Let me also ask if you had found a link to an implementation of the Attentive and Impatient Readers?", "prediction": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen. Until now large scale training and test datasets have been missing. Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen. Until now large scale training and test datasets have been missing. Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars or information extraction methods. Supervised machine learning approaches have largely been absent from this space due to the lack of large scale training datasets. We have collected two new corpora of roughly a The reading comprehension task naturally lends itself to a formulation as a supervised learning problem. We seek to estimate the conditional probability p(a|c, q), where c is a context document, q a query relating to that document, and a the answer to that query. Note that the focus of this paper is to provide a corpus for evaluating a model\u2019s ability to read and comprehend a single document, not world knowledge or co-occurrence. To understand that distinction consider for instance the following Cloze form queries (created from headlines So far we have motivated the need for better datasets and tasks to evaluate the capabilities of machine reading models. We proceed by describing a number of baselines, benchmarks and new models to evaluate against this paradigm. The majority baseline (maximum frequency) picks the entity most frequently observed Traditionally, a pipeline of NLP models has been used for attempting question answering, that is models that make heavy use of linguistic annotation, structured world knowledge and semantic parsing. We develop a benchmark thatmakes use of frame-semantic annotations which we obtained by parsing Neural networks have successfully been applied to a range of tasks in NLP. This includes classification tasks such as sentiment analysis or POS tagging. We propose three neural models for estimating the probability of word type a from document d answering query q. Our hypothesis is that neural models should in principle be well suited for this task. We argued that simple recurrent models such as the LSTM probably have insufficient expressive power for solving tasks that require complex inference. We expect that the attention-based models would therefore outperform the The Attentive and Impatient Readers are able to propagate and integrate semantic information over long distances. The attention mechanism that we have employed is just one instantiation of a very general idea which can be further exploited. There are still many queries requiring complex inference and long range The precise hyperparameters used for the various attentive models are as in Table 6. All models were trained using asynchronous RmsProp [20] with a momentum of 0.9. To understand how the model performance depends on the size of the context, we plot performance versus document lengths in Figures 4 and 5. The first figure (Fig. 4) plots a sliding window of performance across document length, showing that performance of the attentive models degrades slightly We consider examples from the Attentive Reader as well as the Impatient Reader in this appendix. Figures 6 and 9 show examples of queries that require reasonable levels of lexical generalisation and co-reference in order to be answered. Figures 10\u201313 shows how the", "simplified_prediction": "Teaching machines to read natural language documents is very important to read . Machine reading systems can be tested on their ability to answer questions on the contents of documents that they have seen . Until now large scale training and test datasets have been missing . Teaching machines to read natural language documents is very important to read . Machine reading systems can be tested on their ability to answer questions on the contents of documents that they have seen . Until now large scale training and test datasets have been missing . Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars or information extraction methods . Supervised machine learning approaches have not been absent from this space because of the lack of large machine training datasets . We have collected two new '' corpora '' task naturally lends itself to a new task as a supervised learning problem , for example . We seek to estimate the probability p ( a | c , q ) , where c is a context document , q a query relating to that document , and a the answer to that document . Note that the focus of this paper is to provide a corpus for evaluating a model in a document to read and read a single document , not a world knowledge . To understand that difference consider for instance the following Cloze form queries ( created from headlines So far we have motivated the need for better datasets and tasks to understand the features of machine reading models . We proceeded by describing a number of baselines , benchmarks and new models to work against this paradigm . The majority baseline ( maximum frequency ) picks the entity most often seen Traditionally , a pipeline of NLP models has been used for attempting question answering , that is models that make heavy use of linguistic annotation , structured world knowledge and semantic parsing . We develop a benchmark thatmakes use of frame-semantic annotations which we get by using a network of tasks . Neural networks have successfully been applied to the task . This includes tasks such as sentiment analysis or POS tagging . We propose three types of neural models for making the probability of word type a from document . Our hypothesis should have been found in principle and looked at this task . We argued that simple models such as the LSTM probably have insufficient expressive power for solving tasks that need complex inference . We expect that the attention-based models would therefore outperform the The Attentive and Impatient Readers are able to find semantic information over long distances . The attention mechanism that we have used is just one instantiation of a very general idea which can be used more often . There are still many kinds of queries need complex inference and long range The hyperparameters used for the various attentive models are as in Table 6 . All models were trained using asynchronous RmsProp [ 20 ] with a momentum of 0.9 . To understand how the model performance depends on the size of the context , we learn about document lengths in Figures 4 and 5 . The first figure was Fig . The story is about a window of performance across document length . The story shows that performance of the attentive models , which are slightly examples from the Attentive Reader as well as the Impatient Reader . Figures 6 and 9 show examples of queries that need reasonable levels of lexical generalisation and co-reference in order to be answered . Figures 10 clothing shows how the two people"}
{"ground_truth": "The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution. Adversarial Net  Two models - Generative Model(G) and Discriminative Model(D)  Both are multi-layer perceptrons. G takes as input a noise variable z and outputs data sample x(=G(z)). D takes as input a data sample x and predicts whether it came from true data or from G.  G tries to minimise log(1-D(G(z))) while D tries to maximise the probability of correct classification. Think of it as a minimax game between 2 players and the global optimum would be when G generates perfect samples and D can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data). Alternate between k steps of training D and 1 step of training G so that D is maintained near its optimal solution. When starting training, the loss log(1-D(G(z))) would saturate as G would be weak. Instead maximise log(D(G(z)))  The paper contains the theoretical proof for global optimum of the minimax game. Experiments  Datasets  MNIST, Toronto Face Database, CIFAR-10  Generator model uses RELU and sigmoid activations. Discriminator model uses maxout and dropout. Evaluation Metric  Fit Gaussian Parzen window to samples obtained from G and compare log-likelihood. Strengths  Computational advantages  Backprop is sufficient for training with no need for Markov chains or performing inference. A variety of functions can be used in the model. Since G is trained only using the gradients from D, fewer chances of directly copying features from the true data. Can represent sharp (even degenerate) distributions. Weakness  D must be well synchronised with G.  While G may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained. Possible Extensions  Conditional generative models. Inference network to predict z given x. Implement a stochastic extension of the deterministic Multi-Prediction Deep Boltzmann Machines  Using discriminator net or inference net for feature selection. Accelerating training by ensuring better coordination between G and D or by determining better distributions to sample z from during training.", "prediction": "We propose a new framework for estimating generative models via an adversarial process. We simultaneously train two models: a generative model G and a discriminative model D. The training procedure for G is to maximize the probability of D making a mistake. We propose a new framework for estimating generative models via an adversarial process. We simultaneously train two models: a generative model G and a discriminative model D. The training procedure for G is to maximize the probability of D making a mistake. The promise of deep learning is to discover rich, hierarchical models that represent probability distributions over the kinds of data encountered in artificial intelligence applications. So far, the most striking successes in deep learning have involved discriminative models. We propose a new generative model estimation procedure that sidesteps these difficulties. Deep belief networks (DBNs) are hybrid models containing a single undirected layer and several directed layers. In many interesting generative models with several layers of latent variables, it is not even possible to derive a tractable unnormalized probability density. The adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generator\u2019s distribution pg over data x, we define a prior on input noise variables pz(z) We train D to maximize the probability of assigning the correct label to both training examples and samples The generator G implicitly defines a probability distribution pg as the distribution of the samples G(z) obtained when z \u223c pz. We would like Algorithm 1 to converge to a good estimator of pdata, if given enough capacity and training time. We will show in section 4.1 that this minimax game has Proposition 2. IfG andD have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given G, and pg is updated so as to improve the criterion, then pg converges to pdata. We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) and CIFAR-10. The generator nets used a mixture of rectifier linear activations and sigmoid activations, while the discriminator net used maxout [10] activations. Drop This new framework comes with advantages and disadvantages relative to previous modeling frameworks. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. The disadvantages are primarily that there is no explicit representation This framework admits many straightforward extensions: A conditional generative model p(x | c) can be obtained by adding c as input to both G and D. Learned approximate inference can be performed by training an auxiliary network to predict zgiven x. We would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would also like to thank CIFAR, and Canada Research", "simplified_prediction": "We propose a new framework for making generative models via an adversarial process . Wely train two models : a model G and a discriminative model D . The training procedure for G is to look like the probability of D making a mistake . We propose a new framework for making generative models via an adversarial process . Wely train two models : a model G and a discriminative model D . The training procedure for G is to look like the probability of D making a mistake . The promise of deep learning is to discover rich . There are probability distributions over the kinds of data that have been found in artificial intelligence applications . So far , the most striking successes in deep learning have become more popular . We propose a new kind of model that some sidesteps these difficulties . Deep belief networks ( DBNs ) are two types of hybrid models . They contain a single layer and several directed layers . In many interesting models with several layers of latent variables , it is not even possible to get a probability density , for example . The most common modeling framework is most straightforward to apply when the models are both multilayer perceptrons . To learn the romantic generators distribution pg over data x , we define a prior input noise variables pz ( z ) We train D to get the probability of a probability of assigning the correct label to both training examples and samples The generator G implicitly defines a probability distribution pg as the distribution of the samples G. We would like Algorithm 1 to join a good part of pdata , if given enough capacity and training time . We will show in section 4.1 that this game has the same Proposition 2 . IfG andD have enough capacity , and at each step of Algorithm 1 , the discriminator is allowed to reach its optimum given G , and pg is updated so as to improve the criterion . We trained adversarial nets a range of datasets including the Toronto Face Database and the Toronto Face Database . The generators used a mixture of linear activations and sigmoid activations . The discriminator net used maxout [ 10 ] activations . This new framework comes with advantages and disadvantages relative to previous modeling frameworks . The advantages are that Markov chains are never needed . Only backprop is used to get new things , no inference is needed during learning , and a variety of functions can be made into the model . The disadvantages are mostly that there is no explicit representation This framework admits many good things : A conditional generative model p ( x | c ) can be done by adding c as input to both G and D. Learned approximate inference can be done by training an auxiliary network to predict zgiven x. We would like to acknowledge Marcotte , Olivier Delalleau , Kyunghyun Cho , Jason Yosinski , and Kyunghyun for helpful discussions . Yann Dauphin shared his Parzen window evaluation code with us . We would also like to CIFAR and Canada Research ."}
{"ground_truth": "The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies. Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner. The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL). Idea  Instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives). Each model primitive will be good in only a small part of the state space (aka region of specialization). These model primitives can then be used to train a gating mechanism for selecting sub-policies to solve a given task. Since these model primitives are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches. Single Task Learning  A gating controller is trained to choose the sub-policy whose model primitive makes the best prediction. This requires modeling p(Mk | st, at, st+1) where p(Mk) denotes the probability of selecting the kth model primitive. This is hard to compute as the system does not have access to st+1 and at at time t before it has choosen the sub-policy. Properly marginalizing st+1 and at would require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce p(Mk | st). This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout. The paper notes that technique is biased but reports that it still works for the downstream tasks. The gating controller composes the sub-policies as a mixture of Gaussians. For learning, PPO algorithm is used with each model primitives gradient weighted by the probability from the gating controller. Lifelong Learning  Different tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO). Experiments  Domains:  Mujoco ant navigating different mazes. Stacker arm picking up and placing different boxes. Implementation Details:  Gaussian subpolicies  PPO as the baseline  Model primitives are hand-crafted using the true next state provided by the environment simulator. Single Task  Only maze task is considered with the start position (of the ant) and the goal position is fixed. Observation includes distance from the goal. Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task. Lifelong Learning  Maze  10 random Mujoco ant mazes used as the task distribution. MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks). Pick and Place  8 Pick and Place tasks are created with max 3 goal locations. Observation includes the position of the goal. Ablations  Overlapping model primitives can degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust. Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks. The order of tasks (in the 10-Mazz task) does not degrage the performance. Transfering the gating controller leads to negative transfer. Notes  I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.", "prediction": "This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions. This framework performs automatic decomposition of a single source task in a bottom up manner. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness Model Primitive Hierarchical Lifelong Reinforcement Learning. In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13\u201317, 2019. In the lifelong learning setting, we want our agent to solve a series of related tasks drawn from some task distribution rather than a single, isolated task. We draw on the idea of modularity. While learning to perform a complex task, we force the agent to break its We assume the standard reinforcement learning (RL) formulation: an agent interacts with an environment to maximize the expected reward. In a lifelong learning setting, the agent must interact with multiple tasks and successfully solve each of them. The fundamental question in lifelong learning is to determine what knowledge This section outlines the Model Primitive Hierarchical Reinforcement Learning (MPHRL) framework to address the problem of effective piecewise functional decomposition for transfer across a distribution of tasks. The key assumption in MPHRL is access to several diverse world models of the environment dynamics. We use the term model primitives to refer to these approximate world models. The goal of the MPHRL framework is to use these suboptimal predictions to decompose the task space We have shown how MPHRL can decompose a single complex task solution into different functional components. Complex tasks often share structure and can be decomposed into similar sets of subtasks. We transfer the subpolicies to learn target tasks, but not the gating controller or Our experiments aim to answer two questions: (a) can model primitives ensure task decomposition? (b) does such decomposition improve transfer for lifelong learning? We evaluate our approach in two challenging domains: a MuJoCo [29] ant navigating different mazes We focus on two single-task learning experiments where MPHRL learns a number of interpretable subpolicies to solve a single task. Both the L-Maze and D-maze tasks require the ant to learn to walk and reach the green goal within a finite To evaluate MPHRL\u2019s performance in lifelong learning, we generate a family of 10 random mazes for the MuJoCo Ant environment. The agent has a maximum of 3 \u00d7 107 timesteps to reach 80% success rate in each of the 10 tasks. We conduct ablation experiments to answer the following questions. Can MPHRL learn the task decomposition even when the model primitives are quite noisy or when the source task does not cover all \u201ccases\u201d? When does MPHRL fail to decompose the solution? We showed how imperfect world models can be used to decompose a complex task into simpler ones. Our approach does not require access to accurate world models. The recently introduced Neural Processes can potentially be an efficient approach to build upon. We are thankful to Kunal Menda and everyone at SISL for useful comments and suggestions. We are also grateful for the support from Google Cloud in scaling our experiments.", "simplified_prediction": "This paper has a framework for using different world models to decompose complex task solutions in the world . This framework performs automatic decomposition of a single source task at the same time . We perform a series of experiments on high dimensional continuous action control tasks to show the effectiveness Model Primitive Hierarchical Lifelong Reinforcement Learning . Proc . The International Conference on Autonomous Agents and Multiagent Systems ( AAMAS 2019 ) is a sports league in Montreal , Canada . In the lifelong learning setting , we want our agent to solve a series of related tasks drawn from some task distribution rather than a single task . We draw on the idea of modularity . While learning to perform a complex task , we force the agent to break its We assume the standard good learning ( RL ) with an environment to get the expected reward for a long time . The agent must interact with many tasks and successfully solve each of them in a lifelong learning . The most important question in lifelong learning is to determine what knowledge This section outlines the Model Primitive Hierarchical Reinforcement Learning ( MPHRL ) framework to address the problem of effective piecewise for transfer across a distribution of tasks . The key assumption in MPHRL is access to several different world models of the environment . We use the term model primitives to mean these kinds of primitive models . The goal of the MPHRL framework is to use these task predictions to decompose the task space We have shown how the task can decompose a single complex task solution into different functions . Complex tasks often share structure and can be put into the same sets of subtasks . We transfer the way to learn target tasks , but not the gating controller or Our experiments aim to answer two questions : ( a ) can make sure that it is true . ( b ) does such get better for lifelong learning ? We evaluate our approach in two challenging domains : a MuJoCo [ 29 ] ( ant navigating different mazes We focus on two single-task learning experiments where people learn to solve a single task ) . Both the L-Maze and D-maze tasks require the ant to learn to walk and reach the green goal within a evaluate performance in lifelong learning , we make a family of 10 random mazes for the MuJoCo Ant environment . The agent has a maximum of 3 \u00d7 107 timesteps to reach 80 % success rate in each of the 10 tasks . We have ablation experiment to answer the following questions . CanHRL learn the task even when the model primitives are very noisy or when the source task task does not cover all the name is used . When he does not fail to decompose the solution ? We showed how well the world models can be used to make a complex task into simpler ones . Our approach does not require access to new world models . The recently introduced Neural Processes can be an efficient approach to build upon . We do not know about Kunal Menda and everyone at SISL for useful comments and suggestions . We also talk about the support from Google Cloud because of our experiments ."}
{"ground_truth": "Statiscal foundations of virtual democracy Kahng et al., ICML\u201919  This is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently. Consider a food bank that receives donations of food and distributes it to those in need. The goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it. We could hard code a set of rules, but what should they be? And who gets to decide? A democratic solution to this would be to give each of the stakeholders a vote on every decision. In the food bank setting, identified classes of stakeholders include the donors, the recipients, the volunteers (who pick up food from the donor and deliver it to the recipient), and employees. Their votes encode their own preferences and biases, perhaps in a way that even the voters themselves couldn\u2019t neatly codify in a set of explicit rules. It\u2019s not really practical to have an actual vote with all stakeholders participating every time a food donation is made though! One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand. As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues. One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote\u2014 hopefully an informed one\u2014 every time a new dilemma arises. The big idea behind virtual democracy is that we learn the voting preferences of each stakeholder, essentially creating an agent which is able to vote in their place, a virtual voter. Then when we need to make a decision we ask those virtual voters to cast their votes (in the form of a preference ranking). The central question in this paper is this: given a set of preference rankings, how should we combine them to produce an actual decision? The procedure for doing this is known as the voting rule. \u2026 the choice of voting rule can have a major impact on the efficacy of the system. In fact, the question of which voting rule to employ is one of the central questions in computational social choice. It\u2019s one thing to come up with a voting rule that works well when we have the actual true preference rankings of all of the stakeholders. In a virtual democracy setting though, where we have learned approximations to those preference rankings, a highly desirable feature of a voting rule is that it is robust to noise. I.e., we want a voting rule whereby\u2026  \u2026 the output on the true preferences is likely to coincide with the output on noisy estimates thereof. Learning preferences  To learn voter preferences, voters are asked to make a set of pairwise comparisons (about 100) between alternatives. I.e., given this donation, should it be sent to recipient A or recipient B? Each alternative is presented as a set of pre-determined features. In the case of the food bank question voters are given information about the type of donation, and seven additional features such as distance between the donor and recipient, and when the recipient last received a donation. At the end of this process, the training data is used to learn a model of the preferences of the voter. This model is then used to predict the voter\u2019s preference ranking over many hundreds of recipients for a given donation. The Mallows model  To be able to compare the efficacy of various voting rules, we\u2019re going to need a way to compare how good their outputs are. The Kendall tau (KT) distance between two rankings (permutations) of a set is defined as the number of pairs of alternatives on which the rankings disagree. By disagree we mean that given a pair  one ranks  ahead of  , and the other ranks  ahead of  . For example, the KT distance between  and  is 2. The Mallows (1957) model was originally designed for use in situations where there is true ranking of the alternatives, and assigns a probability that a given voter is associated with a given alternative ranking. The probability decreases exponentially with the number of pairs of alternatives on which the true and alternative ranking disagree, i.e., their KT distance. A Mallows model is parameterised by a  parameter  . Our technical approach relies on the observation that the classic Mallows (1957). model is an unusually good fit with our problem. In the problem at hand, instead of a single true ranking, each voter has their own true ranking. When validating a learned  model, the test for accuracy is done using pairwise comparisons, just like in Mallows. Given an observed prediction accuracy  , we can relate this accuracy to an underlying Mallows model through a parameter  , where pairwise comparisons are drawn from within the top  ranked items in the true ranking. (See \u00a73 in the paper). Voting rules and the Borda count  The next piece of the puzzle is the selection of a voting rule to combine rankings and produce a final decision. The main result in the paper concerns the Borda count voting rule. Borda count is a positional scoring rule. Positional scoring rules give a score vector that assigns points to each position in a ranking. E.g. 5 points for being ranked first, 3 points for being ranked second, and so on. The score of an alternative is the sum of its ranking points across all of the voters. The alternative with the biggest score wins (break ties via random selection). The Borda count uses a very straightforward score vector: if there are  alternatives in the ranking, the score vector is defined as  . The heart of the paper is \u00a74, where drawing on the properties of the Mallows model, it\u2019s relationship to the predicted accuracy, and the Borda count rule, the authors show the Borda count is surprisingly robust to noise. I\u2019m going to happily skip over the proofs here and leave you to follow up on those if you\u2019re interested! \u2026 it is intuitive that the separation in Borda scores has to depend on  , but it is encouraging (and, to us, surprising) that his dependence is almost linear\u2026 the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in  . Other rules  So far so good, but what about other voting rules? Are they also robust to noise or is there something special about the Borda count? The main alternative to positional scoring rules are pairwise-majority consistent (PMC) rules, of which there are many examples (e.g., the ranked pairs method). The key result in \u00a75 of the paper is that all rules in this class are not robust to noise. It is instructive to contrast our positive result, Theorem 1, with this negative result. On a very high level, the former result asserts that \u201cif Borda count says that the gaps between alternatives are signi\ufb01cant, then the alternatives will not \ufb02ip under Borda count,\u201d whereas the latter says \u201ceven if a PMC rule says that the gaps between alternatives are very signi\ufb01cant, some alternatives are likely to \ufb02ip under that rule.\u201d  Borda count FTW  So there you have it: if you need to robustly combine noisy rankings of alternatives to make a decision, use the Borda count! Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. Another important feature of the Borda count rule is that the decisions it takes can be easily explained. An explanation consists of two elements: first the average position in the predicted preferences of each of the stakeholder groups, and second the features that were most important in achieving that ranking position (possible since alternatives are presented as vectors of features).", "prediction": "Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people. One of the key questions is which aggregation method \u2014 or voting rule \u2014 to use. We seek voting rules that are robust to prediction errors. Noothigattu et al. are motivated by the challenge of automating ethical decisions. Their approach consists of three steps: first, collect preferences from voters on exam-related dilemmas. Second, learn models of their preferences, which general Mallows (1957) model is an unusually good fit with our problem. In our setting each voter has a (possibly different) true ranking. The voter\u2019s predicted ranking \u03c3i is drawn from a Mallows distribution around \ufffd A number of recent papers have explored the idea of automating ethical decisions via machine learning and social choice. Our work is motivated by the food bank application of the virtual democracy framework, where the number of voters is small and speed is not of the essence We deal with a set of alternatives A such that |A| = m. Preferences over A are represented via a ranking \u03c3. We denote by \u03c3(j) the alternative ranked in position j in \u03c3, where position 1 is the A voting rule (formally known as a social welfare function) is a function f : Ln \u2192 L, which receives a preference profile as input, and returns a \u2018consensus\u2019 ranking of the alternatives. We are especially interested in In the Mallows (1957) model, there is a ground truth ranking \u03c3?, which induces a probability distribution over perceived rankings. The repeated insertion model (Doignon et al., 2004) provides a convenient alternative way of reasoning about the In the virtual democracy framework, we are faced at runtime with a dilemma that induces a set of alternatives A. For each voter i, we obtain a predicted ranking \u03c3i drawn from a Mallows distribution. The Mallows model itself, because it In this section, we rigorously establish the robustness of Borda count to prediction error by showing that it satisfies a formal version of the desired property stated in Section 1. We do this by building on the machinery developed in Section 3, as well Theorem 1 shows that Borda count is robust against noisy perturbations of the preference profile. We show that there exist profiles in which the pairwise majority graph is acyclic and all edge weights are large. With high probability, the In Section 4 we have established that Borda count is robust to prediction error. However, our positive theoretical result, Theorem 1, only provides asymptotic guarantees. For our evaluation metric, we consider the probability of the rule flipping alternatives when Given n voters, m alternatives, a Mallows parameter \u03c6, and a probability p, we generate a true profile \u03c3? = (\u03c3?1 , . . . , \u03c3 ? n) from a mixture of Mallows models. Throughout our experiments, we let n = 100, m = 40, and p = 1. We plot the average probability of flipping the order of alternatives as a function of the difference in average Borda scores. At a high level, error rate decreases Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. Borda counts is also compelling in terms of usability and explainability. In our implemented donor-recipient matching system, clicking on", "simplified_prediction": "Virtual democracy is an approach to making decisions , by learning models of the people who live in each country . One of the key questions is which aggregation method change the way it is used . We seek voting rules that do not have to prediction errors . Noothigattu et al . are motivated by the challenge of making ethical decisions . Their approach consists of three steps : first , collect people from voters on different places . Second , learn models of their preferences , which model is an unusually good fit with our problem , like general Mallows . In our setting each voter has a ( possibly different ) true ranking . It is drawn from a Mallows distribution around A number of recent papers have explored the idea of using ethical decisions via machine learning and social choice , and the idea of ethical decisions . Our work is done by the food bank application of the virtual democracy framework , where the number of voters is small and speed is not of the essence We deal with a set of different people , such as that of A | = m. We say that the alternative ranked in position j in  occasion , where position 1 is the A voting rule ( officially known as a social welfare function ) is a function of the alternatives , which receives a preference profile as input . The A voting rule is also known as a social welfare function . We especially interested in In the Mallows ( 1957 ) model , there is a ground truth ranking  entrance , which makes a probability distribution more important . The repeated insertion model ( Doignon et al . , 2004 ) provides an alternative way of reasoning about the Insaneous democracy , and we do not have an alternative set of alternatives at runtime . For each voter i , we got a predicted the idea that a Mallows distribution could be made . The Mallows model itself , because it In this section , we make the robustness of Borda count to prediction error . This shows that it satisfies a formal version of the desired property stated in Section 1 . We do this by building on the machinery developed in Section 3 , as well Theorem 1 shows that Borda count is robust against the people who talk about it . We show that there are many things in which all the edge weights are large , and all the edges are large . The In Section 4 we have started that Borda count is thought to prediction error because of high probability . However , our positive theoretical result only provides asymptotic guarantees . For our evaluation metric , we consider the probability of the rule that is different when Given n voters , m alternatives , a Mallows parameter grew , and a probability p , we make a true way to do this . (  name ) = ( , ) . Other pages Other pages , name ? n ) comes from a mixture of Mallows . During our experiment , we let n = 100 , m = 40 , and p = 1 . The average probability of the order of alternatives as a function of the difference in average Borda scores . At a high level , error rate decreases Our theoretical and the results identify Borda count as an especially attractive voting rule for virtual democracy . Borda counts is also used in terms of usability and explainability . In our implemented donor-recipient matching system ,"}
{"ground_truth": "RPCValet: NI-driven tail-aware balancing of \u00b5s-scale RPCs Daglis et al., ASPLOS\u201919  Last week we learned about the [increased tail-latency sensitivity of microservices based applications with high RPC fan-outs. Seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager. Today\u2019s paper choice, RPCValet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s). The net result of rapid advancements in the networking world is that inter-tier communications latency will approach the fundamental lower bound of speed-of-light propagation in the foreseeable future. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network. Furthermore, the evaluation shows that \u201cRPCValet leaves no significant room for improvement\u201d when compared against the theoretical ideal (it comes within 3-15%). So what we have here is a glimpse of the limits for low-latency RPCs under load. When it\u2019s no longer physically possible to go meaningfully faster, further application-level performance gains will have to come from eliminating RPCs altogether. RPCValet balances incoming RPC requests among the multiple cores of a server. Consider for example a Redis server maintaining a sorted array in memory\u2026  \u2026 an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few \u00b5s while new translations are installed. While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server. In theory, how fast could we go? Consider a 16-core server handling 16 requests. We could put anywhere from 1 to 16 queues in front of those cores. At one extreme we have a \u201816 x 1\u2019 architecture with 16 queues each with one associated processing unit. At the other extreme is a \u20181 x 16\u2019 architecture with one shared queue serving all 16 processing units. Or we could have e.g. a \u20184 x 4\u2019 with 4 queues each serving 4 units, and so on\u2026  If you model that out with Poisson arrivals and variety of different service time distributions (fixed, uniform, exponential, and generalised extreme value, GEV) what you\u2019ll find is that the \u20181\u00d716\u2019 architecture performs the best, and the \u201816\u00d71\u2019 architecture performs the worst. 1 x 16 significantly outperforms 16 x 1. 16 x 1\u2019s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than 1 x 16 under a tail latency SLO at 10x the mean service time\u2026 The theoretical results suggest that systems should implement a queueing configuration that is as close as possible to a single queue (1 x 16) configuration. In practice those models failed to account for a significant source of overhead: synchronizing across multiple cores on the single queue. When you take synchronisation into account, a dedicated queue per core starts to look like a good idea again. NICs supporting Receive-Side Scaling (RSS) can push messages into each core\u2019s queue, but while RSS can help to achieve load distribution, it can\u2019t truly achieving load balancing. Any resulting load imbalance after applying these [RSS] rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with \u00b5s-scale service times. Introducing RPCValet  RPCValet uses a push-based model, while also taking into account the current loading of the cores. It\u2019s designed for \u201cemerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.\u201d. The key hardware feature is that the network interface has direct access to the server\u2019s memory hierarchy, eliminating round trips over e.g. PCIe. Servers register a part of their DRAM into a partitioned global address space (PGAS), where every server can read and write memory in RDMA fashion. (We\u2019re not given any details on system reconfiguration etc.). An integrated NI can, with proper hardware support, monitor each core\u2019s state and steer RPCs to the least loaded cores. Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g. ~ 1.5 \u00b5s for a 3-hop posted PCIe transaction) would mean that the NI will make delayed\u2014hence sub-optimal, or even wrong\u2014 decisions until the information arrives. With N participants in the system, RPCValet first writes every incoming message into a single PGAS-resident message buffer of NxS slots. Then it notifies the selected core to process the request. Message arrival and memory location are thus decoupled from the assignment of a core for processing. We have the synchronization-free zero-copy behaviour of a partitioned multi-queue architecture, together with the load-balancing flexibility of a single queue system. In the implementation each node maintains a send and a receive buffer. Send buffers contain a valid bit indicating whether they are currently being used, and a pointer to a buffer in local memory containing the payload. Receive buffers on the other hand have slots that are size to accommodate message payloads directly. Overall, the messaging mechanism\u2019s memory footprint is 32 x N x S + (max_msg_size + 64) x N x S bytes. That is, a few tens of MB at most. The implementation uses a simple scheme to estimate core loads. RPCValet simply keeps track of the number of outstanding send requests assigned to each core. Allowing only one inflight request per core corresponds to a true single-system queue behaviour, but introduces a small inefficiency waiting for the notification of completed request processing. A practical compromise is to allow two outstanding requests per core. This results in marginal performance gains over the single request design for ultra-fast RPCs with service times of a few nanoseconds. All IN backends independently handle incoming network packets and access memory directly, but they hand off to a single NI dispatcher (over the on-chip interconnect) that is statically assigned to dispatch requests to cores for servicing. \u2026 for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible (just a few ns). Evaluation  The evaluation is conducted using against three different service implementations: a synthetic service that emulates different service time distributions; the HERD key-value store, and the Masstree data store. An SLO of less than or equal to 10x the mean service time is assumed in each case, and configurations are evaluated in terms of throughput under SLO. Compared to a software implementation, which requires a synchronisation primitive, the hardware implementation delivers 2.3x-2.7x higher throughput under SLO. The following plots show the performance of different queueing arrangements under the three workloads, with the \u20181\u00d716\u2019 arrangement that RPCValet simulates performing the best as expected. ( Enlarge )  Compared to a theoretically optimal 1 x 16 model, RPCValet gets within 3-15% (depending on the service time distribution). \u201cWe attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model.\u201d  At the end of the day though:  RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns. Throughput under tight tail latency goals is improved by up to 1.4x, and tail latency before saturation is reduced by up to 4x.", "prediction": "RPCValet is an NI-driven load-balancing design for architectures with hardware-terminated protocols and integrated NIs. It delivers near-optimal tail latency. RPCValet is an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs. It delivers near-optimal tail latency Tail-tolerant computing is one of the major ongoing challenges in the datacenter space. The growing number of cores on server-grade CPUs exacerbates the challenge of Modern online services are decomposed into deep hierarchies of mutually reliant tiers, which typically interact using RPCs. Fine-grained RPCs exacerbate the tail latency challenge for services with To study the effect of load balancing across cores on tail latency, we conduct a first-order analysis using basic queuing theory. We model a hypothetical 16-core server after a A subtlety not captured by our queuing models is the practical overhead associated with sharing resources. In a manycore CPU, allowing all the cores to pull incoming network messages from a We design RPCValet for emerging architectures featuring fully integrated NIs and hardware-terminated transport protocols. An important class of online services exhibits RPCs with service times that are frequently The NI\u2019s integration on the same piece of silicon as the CPU is the key enabler for handling \u00b5s-scale events. An integrated NI can, with proper Our design goal is to break the tradeoff between the load imbalance inherent in multi-queue systems and the synchronization associated with pulling load from a single queue. We retain the VIA soNUMA enables rapid remote memory access through a lean hardware-terminated protocol and on-chip NI integration. The conventionally monolithic NI is split into two heterogeneous We devise a lightweight implementation of native messaging as a required building block for dynamic load-balancing decisions at the NI. A key difficulty to overcome is support for multi-packet RPCValet is a single-queue queuing system. It uses a single NI dispatcher to handle message dispatch to all available cores. The dispatch frequencies are modest enough for a We model a single tiled 16-core chip implementing soNUMA with a Manycore NI. The modeled chip is part of a 200-node cluster, with remote nodes Fig. 7a shows the performance of HERD with each of the three evaluatedNI-driven load-balancing configurations. 1 \u00d7 16 consistently delivers the best performance, thanks to Fig. 8 compares the performance of RPCValet to a software implementation. Software requires a synchronization primitive (in our case, an MCS lock) for cores to atomically pull Our results in \u00a76.1 qualitatively meet the expectations set by the queuing analysis presented in \u00a72.2. We now quantitatively compare the obtained results to the ones RPCValet improves tail latency by minimizing the effect of queuing. Queuing is only one of many sources of tail latency, which lie in all layers of the server\ufffd RPCValet is anNI-driven dynamic load-balancing mechanism for\u00b5s-scale RPCs. It behaves like a singlequeue system, without incurring the We thank Edouard Bugnion, James Larus, Dmitrii Ustiugov, Virendra Marathe, Dionisios Pnevmatik", "simplified_prediction": "PCValet is an NI-driven load-balancing design for architectures with hardware-terminated protocols and operating systems . It delivers near the tail latency . PCValet is an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols as well . It delivers near-optimal tail latency Tail-tolerant computing is one of the most common challenges in space . The growing number of cores on server-grade CPUs exacerbates the challenge of Modern online services are allowed into deep hierarchies , which usually interact using RPCs . It shows the tail latency challenge for services with To study the effect of load balancing across cores on tail latency . This is because we have a first-order analysis using basic queuing theory . We have a hypothetical 16 - core server after a A subtlety not captured by our model . This is because it is practically done with sharing resources . In a manycore CPU , allowing all the cores to pull incoming network messages from a We design RPCValet for new architectures . This allows NIs and hardware-terminated transport protocols . An important class of online services shows RPCs with service times that are often The NI Dreams integration on the same piece of silicon as the CPU is the key enabler for handling \u03bcs-scale events . An integrated NI can , with proper Our design goal is to break the tradeoff between the load load in the multi-queue systems and the synchronization associated with pulling load from a single load . We kept the VIA soNUMA allows rapid remote memory access through a hardware-terminated protocol and on-chip NI integration . The most common NI is split into two heterogeneous We devise a lightweight implementation of native messaging as a block for dynamic load-balancing decisions at the NI . A key difficulty to overcome is support for multi-packet RPCValet , which is a single-queue system . It uses a single NI dispatcher to send messages to all of the core . The dispatch frequencies are modest enough for a We model that is a single tiled 16 - core chip that uses soNUMA with a manycore NI . The chip is part of a 200 \u00e2 '' node cluster , with remote nodes . 7a shows the performance of HERD with each of the main load-balancing configurations . 1 \u00d7 16 often delivers the best performance , thanks to Fig . This is compared to the performance of RPCValet . Software requires a synchronization primitive ( in our case , an MCS lock ) for cores to pull Our results in which it is expected to meet the expectations set by the queuing analysis presented . We now compare the obtained results to the ones RPCValet improves tail latency by using the effect of queuing . Queuing is only one of many sources of tail latency , which lie in all layers of the server RPCValet is a dynamic load-balancing mechanism for people using the RPC . It is like a singlequeue system , without making the We thank Edouard Bugnion , James Larus , Dmitrii Ustiugov , Virendra Marathe , and Pnevmatik ."}
{"ground_truth": "Mining High-Speed Data Streams \u2013 Domingos & Hulten 2000  This paper won a \u2018test of time\u2019 award at KDD\u201915 as an \u2018outstanding paper from a past KDD Conference beyond the last decade that has had an important impact on the data mining community.\u2019  Here\u2019s what the test-of-time committee have to say about it:  This paper proposes a decision tree learner for data streams, the Hoeffding Tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. This work constitutes a significant step in developing methodology suitable for modern \u2018big data\u2019 challenges and has initiated a lot of follow-up research. The Hoeffding Tree algorithm has been covered in various textbooks and is available in several public domain tools, including the WEKA Data Mining platform. The goal is to create a knowledge discovery system that can cope with large volumes of data (perhaps an unbounded stream) without needing to fit everything in memory (40MB was the allotted amount used in their evaluation tests \u2013 remember this was 2000). Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists. However, the available algorithms of this type have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm. Based on a statistical result known as the Hoeffding bound, the authors show how to create Hoeffding (decision) trees and build a Very Fast Decision Tree (VFDT) system based on them. A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node). In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces. In a classification problem, a set of N training examples of the form (x\u20d7,y) is given, where y is a discrete class label and x\u20d7 is a vector of d attributes. From these examples we need to produce a model y = f(x\u20d7) that will predict the class of future examples x\u20d7 with high accuracy. Decision tree learners create models in the form of decision trees, where each node contains a test on an attribute, each branch corresponds to a possible outcome of the test, and each leaf contains a class prediction. To learn a decision tree you recursively replace leaves by test nodes, starting at the root. Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets. This learner should require each example to be read at most once, and only a small constant time to process it. This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost. In Hoeffding trees, in order to find the best attribute to test at a given node, only a small subset of the training examples that pass through that node are used. The key of course, is to determine how small that subset can be, and what guarantees we can give concerning it. Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively. We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound). Given a real-valued random variable r with range R (e.g. 0-1 for a probability), and n independent observations of the variable, we can compute the mean of those observations, r\u0304. The Hoeffding bound tells us that with probability 1 \u2013 \u03b4, the true mean of the variable is at least r\u0304 \u2013 \u03b5, where:  The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations. The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same \u03b4 and \u03b5). If G(Xi) is the heuristic used to choose test attributes, then we want to ensure with high probability the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples. Suppose that we\u2019ve seen n examples so far, and the best attribute predicted by G is Xa and the second best is Xb. Call the difference between the observed heuristic values of Xa and Xb \u0394G\u0304  Now, given a desired \u03b4, the Hoeffding bound tells us that Xa is the correct choice with probability 1 \u2013 \u03b4 if n examples have been seen at this node and \u0394G\u0304 > \u03b52. Thus a node needs to accumulate examples from the streamuntil \u03b5 becomes smaller than \u2206G. (Notice that \u03b5 is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves. Pseudo-code for a Hoeffding tree algorithm based on this is given in table 1 of the paper. The VFDT system was built using this algorithm, and included a number of additional optimisations:  When two or more attributes have very similar scores, lots of examples may be needed to decide between them with confidence. But if they are very similar, it probably doesn\u2019t matter too much which one we choose, so let\u2019s just pick one after we reach some user-defined threshold and move on\u2026  We don\u2019t need to recompute G after every example since it is unlikely the decision to split will be made at that specific point. So we can micro-batch and accept a minimum number of new examples before recomputing G.  Under memory pressure, VFDT deactivates the least promising leaves in order to make room for new ones. Likewise VFDT can also drop early on attributes that do not look promising. VFDT can be initialised with a tree produced offline by a traditional batch learner. (Trained on a subset of the overall data). VFDT can rescan previously seen examples if desired. This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times. This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once.", "prediction": "VFDT is an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using off-the-shelf hardware. Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications\u2014 data mining ; I.5.2 [Pattern Recognition]: Design Methodology\u2014classifier design and evaluation. Hoeffding trees can be learned in constant time per example, while being nearly identical to the trees a conventional batch learner would produce, given enough examples. VFDT is I/O bound in the sense that it mines examples in less time than it takes The classification problem is generally defined as follows. A set of N training examples of the form (x, y) is given. The goal is to produce from these examples a model y = f(x) that will predict the classes y of future examples x with high We have implemented a decision-tree learning system based on the Hoeffding tree algorithm. VFDT allows the use of either information gain or the Gini index as the attribute evaluation measure. It includes a number of refinements to the algorithm. A system like VFDT is only useful if it is able to learn more accurate trees than a conventional system, given similar computational resources. In this section we test this empirically by comparing VF DT with C4.5 release 8 on a series of synthetic datasets We conducted a series of lesion studies to evaluate the effectiveness of some of the components and parameters of the VFDT system. Figure 6 shows the accuracy of the learners on the (0.25, 0.00, 25209, 12605) data set. We are currently applying VFDT to mining the stream of Web page requests emanating from the whole University of Washington main campus. One purpose this data can be used for is to improve Web caching. The key to this is predicting as accurately as possible which hosts and pages will Previous work on mining large databases using subsampling methods includes the following. VFDT combines the best of both worlds, accessing data sequentially. It can potentially require much less than one scan, as opposed to many. This allows it to scale to larger databases than VFDT may outperform SPRINT/SLIQ even in fully disk-resident datasets. VFDT\u2019s speed and anytime character make it ideal for interactive data mining. We plan to study its application in this context (see [18]). This paper introduced Hoeffding trees, a method for learning online from the high-volume data streams that are increasingly common. VFDT\u2019s application to a high-speed stream of Web log data is under way. This research was partly funded by an NSF CAREER award to the first author. Machine Learning on Very Large Databases. Megainduction: Machine Learning on very large Databases, University of Sydney, Sydney, Australia.", "simplified_prediction": "The system builds decision trees using constant memory and constant time per example ( VFDT ) . VFDT can include tens of thousands of examples per second using hardware . Categories and Subject Descriptors H. 2.8 [ Database Management ] : Database Applications ( I. 5.2 [ Pattern Recognition ] : Design Methodology ) is a computer software for computers . Some trees can be learned in constant time per example , while being nearly identical to the trees a conventional batch learner would produce , given enough examples . I / O bound in the sense that it mines examples in less time than it takes The classification problem is usually defined as follows : A set of N training examples of the form is given . The goal is to produce from these examples a model y = f ( x ) that will predict the classes y of future examples x with high We have started a decision-tree learning system based on the Hoeffding tree algorithm . VFDT allows the use of either information gain or the Gini index to measure how good it is . It includes a number of refinements to the algorithm . A system like VFDT is only useful if it is able to learn more accurate trees than a system , which is similar to computational resources . In this section we test this empirically by comparing VF DT with C4.5 release 8 on a series of synthetic datasets We conducted a series of lesion studies to study the effectiveness of some of the components and parameters of the VFDT system . Figure 6 shows the accuracy of the learners on the ( 0.25 , 0.00 , 25209 , 12605 ) data set . We are currently applying VFDT to mining the stream of Web page requests from the University of Washington 's main campus . One purpose this can be used to make better Web caching . The key to this is predicting as accurately as possible which hosts and pages will be used on mining large databases . This is because of subsampling methods . VFDT combines the best of both worlds and data sequentially . It can need much less than one scan , as opposed to many people . This allows it to larger databases than VFDT may even outperform SPRINT / SLIQ even even in fully disk-resident datasets . VFDT can mean speed and anytime character make it ideal for talking about data mining . We plan to study its application in this context . This paper showed trees , a method for learning online from the high-volume data streams that are more common , and more common . VFDT can be used to make a stream of Web log data is under way . This research was partly given a award by the NSF award to the first author . Machine Learning on Very Large Databases . Megainduction : Machine Learning on very large Databases , University of Sydney , Australia ."}
{"ground_truth": "BEAT: asynchronous BFT made practical Duan et al., CCS\u201918  Reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still. That\u2019s the world of Byzantine fault tolerance (BFT). We\u2019ve looked at Practical BFT (PBFT) and HoneyBadger on previous editions of The Morning Paper. Today\u2019s paper, BEAT, builds on top of HoneyBadger to offer BFT with even better latency and throughput. Asynchronous BFT protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet. The best performing asynchronous BFT protocol, HoneyBadger , still lags behind the partially synchronous PBFT protocol in terms of throughput and latency. BEAT is actually a family of five different asynchronous BFT protocols that start from the HoneyBadger baseline and make improvements targeted at different application scenarios. Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers). The BEAT protocols divide into two groups: those supporting full (general) state-machine replication (SMR), as required e.g. for smart contract use cases (BEAT0, BEAT1, BEAT2); and those that support BFT storage (append-only ledger) use cases only (BEAT3, BEAT4). The following table summarises the BEAT family and the key distinguishing features of each member. ( Enlarge )  There\u2019s a lot of ground to cover here, but I\u2019ll do my best to give you an overview. Alongside the BEAT protocols themselves, the paper also includes two new building blocks: the generalized fingerprinted cross-checksum and an asynchronous verifiable information dispersal (AVID) algorithm. The HoneyBadger baseline  HoneyBadger supports ACS (the asynchronous common subset) meaning that it provides these guarantees:  Validity: if a correct server delivers a set  , then  and  contains the inputs of at least  correct servers. Agreement: if a correct server delivers a set  , then all correct servers deliver  . Totality: if  correct servers submit an input, then all correct servers deliver an output. HoneyBadger uses reliable broadcast (RBC) and asynchronous Byzantine binary agreement (ABA) protocols to achieve its aims. Threshold signatures are used to provide common coins for ABA, and threshold encryption is used to avoid censorship and achieve liveness. In a threshold scheme the partial outputs (e.g. decryption shares) of at least t participants need to be combined in order to recover (decrypt) the intended value. BEAT0: improved security and performance  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption, a direct instantiation of threshold coin-flipping (instead of using threshold signatures), and more flexible and efficient erasure-coding support. BEAT0\u2019s threshold encryption uses the TDH2 scheme by Shoup and , providing 128-bit security under elliptic curve cryptography. This gives stronger security and better performance than the scheme used in HoneyBadger. In place of the zfec erasure coding library used by HoneyBadger, which supports only Reed-Solomon codes and at most 128 servers, BEAT uses the Jerasure library giving access to more efficient erasure coding schemes and lifting the replica restriction. BEAT1: lower latency  Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that (1) most of the latency comes from threshold encryption and threshold signatures, and (2) somewhat surprisingly, when the load is small and there is low contention, erasure-coded reliable broadcast (AVID broadcast) causes significant latency. BEAT1 swaps out the AVID broadcast protocol of BEAT0 for a replication-based reliable broadcast protocol, Bracha\u2019s broadcast . Under small loads BEAT1 has lower latency. With small batch sizes BEAT1\u2019s throughput is higher than HoneyBadger / BEAT0, but with larger batch sizes throughput is down by 20-30%. BEAT2: causal ordering  BEAT2 builds on BEAT1 and also opportunistically moves the use of threshold encryption to the client side. In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions. Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients. BEAT2 can be combined with anonymous communication networks to achieve full liveness. BEAT2 additionally achieves causal order, which prevents the adversary from inserting derived transactions before the original, causally prior transactions. BEAT3: higher throughput for storage use cases  BEAT3 is the first member of the BEAT family targeted for BFT-storage use cases (as opposed to general SMR). Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded). BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric. Whereas so far we\u2019ve been using a reliable broadcast protocol (AVID), BEAT3 replaces this with a bandwidth-efficient information dispersal scheme called AVID-FP. To disperse a block  , AVID requires bandwidth  , whereas AVID-FP can do it in  . To order transactions of size  , the communication complexity of BEAT0 is  , of BEAT1 and BEAT2 is  , and of BEAT3 is  . AVID-FP is a bandwidth-efficient AVID (asynchronous verifiable information dispersal) protocol using fingerprinted cross-checksum. In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, where  and  \u2026 then it generates the corresponding fingerprinted cross-checksum for B with respect to the erasure coding scheme. Each server verifies the correctness of its fragment with respect to the fingerprint cross-checksum, \u201cand then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol.\u201d  An (n,m) fingerprinted cross-checksum contains a cross-checksum array of n values, and a fingerprint array of m values. The ith entry in the checksum array contains the hash of the ith coded fragment. See section 4 in the paper for details of the fingerprint array usage. BEAT4: partial reads  BEAT4 further reduces read bandwidth using a novel erasure-coded reliable broadcast protocol called AVID-FP-Pyramid. This supports use cases where clients only need to read a fraction of a data block. AVID-FD-Pyramid is based on pyramid codes, which trade space for access efficiency in erasure-coded storage systems (about 10% extra space requirement for a 50% drop in access overhead). Pyramid codes can be efficiently built from any (n, m) systematic and MDS (maximum distance separable) code. See section 4 in the paper for brief details, or Huang et al. for an in-depth treatment. BEAT4 uses a 2-level pyramid scheme which can tolerate one failure in each level, and is able to reduce read bandwidth by 50%. Full details are in section 9 of the paper. Evaluation  The evaluation is conducted on EC2 with up to 92 nodes from ten different regions in five different continents, using a variety of network sizes and batch sizes. In the figures that follow,  represents the network size such that BEAT0,1,2 & 3 require  nodes and BEAT4 requires  nodes. When f=1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2x faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger. When f = 1, BEAT4 is about as fast as HoneyBadger\u2026 As f increases, HoneyBadger is much slower than BEAT4. For throughput, BEAT0 slightly outperforms HoneyBadger. BEAT1 and BEAT2 achieve higher throughput than HoneyBadger with small batch sizes, but have 20-30% lower throughput at larger batch sizes. BEAT3 and BEAT4 outperform all the other protocols consistently. If this write-up has captured your interest, I highly encourage you to go an and read the full paper which contains significantly more detail than I was able to convey here.", "prediction": "We present BEAT, a set of practical Byzantine fault-tolerant (BFT) protocols for completely asynchronous environments. BEAT is flexible, versatile, and extensible, consisting of five asynchronous B CCS CONCEPTS \u2022 Security and privacy \u2192 Systems security; Distributed systems security; Computer systems organization. Reliability; Availability;KEYWORDS Byzantine fault tolerance, BFT, asynchronous BFT State machine replication (SMR) is a fundamental software approach to enabling highly available services. Byzantine fault-tolerant SMR (BFT) has recently regained its prominence. BFT has been regarded State machine replication is a technique to provide a fault-tolerant services using a number of server replicas. In SMR, the servers need to communicate with each other and run an interactive consensus protocol Timing assumptions. Distributed systems can be roughly divided into three categories according to their timing assumption: asynchronous, synchronous, or partially synchronous. We consider Byzantine fault-tolerant state machine replication This section reviews the cryptographic and distributed systems building blocks for BEAT. We review robust labeled threshold cryptosystem (i.e., threshold encryption) where a public key is associated with the system and a HoneyBadgerBFT is the most efficient asynchronous BFT protocol known. It favors throughput over latency by aggressively batching client transactions. It can outperform PBFT when the number of servers exceeds 16 This section describes BEAT0, our baseline protocol that uses a set of generic techniques to improve HoneyBadgerBFT. Instead of using CPA/CCA-secure threshold encryption that does not support This section presents two latency-optimized protocols in BEAT: BEAT1 and BEAT2. Most of latency comes from threshold encryption and threshold signatures. When the load is small and there is low BEAT3 significantly improves all performancemetrics that we know of \u2014 latency, bandwidth, storage overhead, throughput, and scalability. BEAT3 can be used for blockchain applications that need append-only This section presents a general optimization for erasure-coded BEAT instances that significantly reduce read bandwidth. Our technique relies on a novel erasures-coded reliable broadcast protocol, AVID-FP-Pyramid HoneyBadgerBFT is 100% Python, and uses the zfec library to implement the Reed-Soloman code, an MDS erasure code. In BEAT, we instead use We deploy and test our protocols on Amazon EC2 utilizing up to 92 nodes from ten different regions in five different continents. We evaluate the protocols under different network sizes (number of replicas) and contention levels We implemented six new protocols (BEAT instances andHB-Bracha) Whilemany of these protocols use similar components, maintaining, deploying, and comparing different BEAT instances takes tremendous effort. We describe the design and implementation of BEAT, a family of practical asynchronous BFT protocols. BEAT protocols are significantly more efficient than HoneyBadgerBFT. We also develop new distributed system ingredients, The authors are indebted to our shepherd Haibo Chen and the CCS reviewers for their helpful comments. Theorem 9.2. Termination is simple, as in AVID-FP. If a correct server initiates disperse, the server erasures codes the transaction, and sends fragments and the fingerprinted", "simplified_prediction": "We present BEAT , which is a set of practical Byzantine fault-tolerant protocols for completely asynchronous environments . BEAT is made up of five asynchronous BCS CONCEPTS Security and privacy  security ; Distributed systems security ; Computer systems organization ; Computer systems organization are known as BEAT is flexible . Byzantine fault tolerance , BFT , asynchronous BFT State machine replication ( SMR ) is a fundamental software approach to enabling highly available services . Byzantine fault-tolerant SMR ( BFT ) has recently become more well known . BFT is a technique to provide a fault-tolerant services using a number of server replications , using a number of server replication . In SMR , the servers need to communicate with each other and run a consensus protocol called Timing assumptions . Distributed systems can be divided into three categories : asynchronous , synchronous , synchronous , or partially synchronous . We consider Byzantine fault-tolerant state machine changes the cryptographic and distributed systems building blocks for the BEAT system . We review robust labeled threshold cryptosystem ( i . e. , threshold encryption ) where a public key is associated with the system and the most efficient BFT protocol known . It favors throughput over latency by using their client transactions . The PBFT when the number of servers exceeds 16 This section describes BEAT0 , our baseline protocol that uses a set of generic techniques to improve HoneyBadgerBFT . Instead of using CPA / CCA-secure threshold encryption that does not support This section presents two in BEAT1 and BEAT2 . Most of the latency comes from threshold encryption and threshold . When the load is small and there is low BEAT3 significantly improves all performancemetrics that we know of the same name , the bandwidth storage overhead , throughput , and scalability . BEAT3 can be used for blockchain applications that need append-only This section presents a general optimization for erasure-coded BEAT instances that need to reduce read bandwidth . It uses the zfec library to implement the Reed-Soloman code , an MDS erasure code , and uses the zfec library to implement the Reed-Soloman code , an MDS erasure code , an MDS erasure code . In BEAT , we instead use We deploy and test our protocols on Amazon EC2 which use 92 nodes from ten different regions in different continents . We evaluate the protocols under different network sizes and content levels We implemented six new protocols ( BEAT instances andHB-Bracha ) Whilemany of these protocols use similar components , maintaining , deploying , and comparing different BEAT instances takes similar effort . We describe the design and implementation of BEAT , a family of practical protocols like BFT protocols . BEAT protocols are significantly more efficient than HoneyBadgerBFT . We also develop new distributed system ingredients , The authors did not like our shepherd Haibo Chen and the CCS reviewers for their helpful comments . Theorem 9.2 It is simple , as in AVID-FP . If a correct server takes away , the server erases the transaction , and sends pieces and the fingerprinted ."}
{"ground_truth": "Scalable Atomic Visibility with RAMP Transactions \u2013 Bailis et al. 2014  RAMP transactions came up last week as part of the secret sauce in Coordination avoidance in database systems that contributed to a 25x improvement on the TPC-C benchmark. So what exactly are RAMP transactions and why might we need them? As soon as you partition your database across multiple servers, things start to get interesting. We\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026  The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. A lot of implemented systems have chosen to go with the fast-and-furious option resulting in incorrect behaviour for cases where atomic visibility matters. The RAMP (Read Atomic Multiple Partition) transaction models introduced in this paper show that you can have performance and scalability of transactions spanning multiple partitions with atomic visibility. \u2026data stores like Bigtable, Dynamo, and many popular \u201cNoSQL\u201d and even some \u201cNewSQL\u201d stores do not provide transactional guarantees for multi-item operations. The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics. Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described. Under evaluation, the RAMP algorithms did not degrade substantially under contention, and scaled linearly to over 7.1 million operations per second on 100 servers. Bad things that can happen when you don\u2019t have atomic multi-partition isolation  Without atomic isolation foreign key constraints, secondary indexing, and materialized view maintenance can all break! Data models often represent bi-directional relationships as two distinct uni-directional relationships. \u201cFor example, in TAO, a user performing a \u2018like\u2019 action on a Facebook page produces updates to both the LIKES and LIKED_BY associations.\u201d  These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access. Without atomic isolation broken bi-directional relationships, and dangling or incorrect references can surface. With data partitioned across servers by primary key, access by secondary attributes becomes more challenging. There are two dominant strategies for distributed secondary indexing. First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server. This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads. Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data. This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two)  Real-world services tend to use either local secondary indexing (non-scalable but correct), or non-atomic (scalable but incorrect) global indexes. In the latter cases queries involving the secondary attributes can return records that shouldn\u2019t match, and omit ones that should. Without atomic isolation, materialized views can diverge from the base data. For example, a count may become inaccurate. With RAMP transactions, base data and views can be updated atomically. The physical maintenance of a view depends on its specification, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition. For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance). For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter. Scalability Requirements  Consider databases that are partitioned over multiple servers. Each item has a single logical copy stored on one of those partitions, which one can be calculated using the item itself (e.g. primary key). In order to achieve scalability the author\u2019s identify two key properties that must be preserved: synchronization independence, and partition independence. Synchronization independence ensures that one client\u2019s transactions cannot cause another client\u2019s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition). (Also known as transactional availability). Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access. Thus, a partition failure only affects transactions that access items contained on the partition. This also reduces load on servers not directly involved in a transaction\u2019s execution. In the distributed systems literature, partition independence for replicated data is called replica availability or genuine partial replication. A third constraint is that the metadata required to achieve synchronization and partition independence is not too large: \u201cthere are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state.\u201d  The RAMP transaction algorithms  You may be wondering why I keep referring to algorithms (plural). This is because the authors actually define three RAMP variants: RAMP-Fast, RAMP-Small, and RAMP-Hybrid. These trade-off between performance and the amount of metadata that needs to be kept. At a high level, RAMP transactions allow reads and writes to proceed concurrently. This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction\u2019s writes, violating RA (i.e., fractured reads might occur). Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions. To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions. RAMP-Fast stores metadata in the form of write sets (thus the overhead is linear in transaction size), and has one RTT for reads in the best case (two in the worst case). RAMP-Small uses constant size metadata (it only stores the transaction timestamp) but always requires two RTT for reads. RAMP-Hybrid takes the same write set information as RAMP-Fast, but encodes it in a Bloom filter. With no false positives from the filter, Ramp-Hybrid would therefore behave as RAMP-Fast. And with all false positives, it behaves as RAMP-Small. All of the variants require two RTTs/transaction for writes. The two-phase atomic commitment protocol used by RAMP ensures readers never block waiting for writes to arrive. It is known that every atomic commitment protocol may block during failures. Blocked writes instead act as \u201cresource leaks\u201d on partitions: partitions will retain prepared versions indefinitely unless action is taken. To \u201cfree\u201d these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP). CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT\u2026 Compared to alternatives (e.g. replicating clients), we have found CTP to be both lightweight and effective. There is of course much more detail in the full paper, which I encourage you to go on and read. Section 6 on Related Work contains a nice short summary of isolation guarantees in the wild. \u201cIn recent years, many \u2018NoSQL\u2019 designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation\u2026\u201d", "prediction": "Multi-partition, multi-operation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms.  distributed databases increasingly split their data across multiple servers, or partitions. This strategy succeeds in allowing near-unlimited scalability for operations that In this paper, we consider the problem of making transactional updates atomically visible to readers. The classic strategy for providing atomic visibility is Many database schemas contain information about relationships between records in the form of foreign key constraints. With RAMP transactions, applications can bundle relevant In this section, we formalize Read Atomic isolation and, to capture scalability, formulate a pair of strict scalability criteria: synchronization A system provides Read Atomic isolation (RA) if it prevents fractured reads anomalies and also prevents transactions from reading uncommitted, aborted, or RA does not prevent concurrent updates or provide serial access to data items. RA is an incorrect choice for an application that wishes to maintain positive We consider databases that are partitioned, with the set of items in the database spread over multiple servers. Each item has a single logical Given specifications for RA isolation and scalability, we present algorithms for achieving both. We first focus on providing read-only and write-  RAMP-Fast stores metadata in the form of write sets (overhead linear in transaction size) Each write contains a timestamp that uniquely RAMP-Small uses constant-size metadata but always requires two RTT for reads. In RAMP-S, if a transaction RAMP-H strikes a compromise between RAMP-F andRAMP-S write protocols. Instead of storing the entire write set The RAMP algorithms allow readers to safely race writers without requiring either to stall. RAMP-F is optimized for fast reads, RAM  RAMP transactions rely on multi-versioning to allow readers to access versions that have not yet committed and/or have been overwritten RAMP transactions operate in a distributed setting, which poses challenges due to latency, partial failure, and network partitions. Synchronization independence RAMP algorithms also allow several possible optimizations. Faster commit detection. Metadata garbage collection. One-phase writes. We proceed to experimentally demonstrate RAMP transaction scalability as compared to existing transactional and non-transactional mechanisms. RAMP To demonstrate the effect of concurrency control on performance and scalability, we implemented several concurrence control algorithms in a partitioned,  RAMP performance scales well with increased load and incurs little overhead. With few concurrent clients, there are few concurrent updates and therefore few We also evaluated the overhead of blocked writes in our implementation of the Cooperative Termination Protocol. To simulate blocked writes, we artificially dropped a  RAMP-F achieves slightly under 7.1 million operations per second, or 1.79 million transactions per second on a set of Replicated databases offer a broad spectrum of isolation guarantees at varying costs to performance and availability. At the strong end of the isolation spectrum is This paper described how to achieve atomically visible multipartition transactions without incurring the performance and availability penalties of traditional algorithms. By leveraging The paper is based on a paper by D. Agrawal and V. Krishnaswamy from MIT. The authors argue that We formalize criteria for atomic (read) sets of versions in the form of companion sets. Each write in RAMP-F contains", "simplified_prediction": "Multi-partition , multi-operation transactional access is often expensive . It uses coordination-intensive locking , validation , or scheduling mechanisms . This category is for information across multiple servers , or partitions . This means that in this paper , we consider the problem of making transactional updates can be visible to readers . This is why it is known to readers . Many database schemas contain information about relationships between records in the form of foreign key constraints . Many database schemas contain information about relationships . With RAMP transactions , applications can bundle relevant In this section , we formalize Read Atomic isolation and , to capture scalability , formulate a pair of strict scalability criteria : synchronization A system provides Read Atomic isolation ( RA ) if it prevents fractured reads anomalies and also prevents new things from reading uncommitted , aborted , or stop things using it . RA is an incorrect choice for a application that wants to maintain positive We consider databases that are partitioned , with the set of items in the database spread over multiple servers . Each item has a single logical Given specifications for RA isolation and scalability . This is done by using both algorithms . We first focus on providing read-only and write-RAMP-Fast stores metadata in the form of write sets ( overhead linear in transaction size ) Each write contains a timestamp that uses constant-size metadata but always requires two RTT for reads . In RAMP-S , if a RAMP-H strikes a compromise between RAMP-F andRAMP-S write protocols . The entire write set The RAMP algorithms allow readers to safely race writers without needing either to stall or stop them . RAMP-F allows readers to access versions that have not yet committed and / or have been overwritten RAMP transactions operate in a distributed setting , which poses challenges due to latency , partial failure , and network partitions . Synchronization independence RAMP algorithms also allowed several possible algorithms . Faster found it on detection . Metadata garbage collection One-phase writes . We proceed to experimentally demonstrate RAMP transaction scalability as compared to existing RAMPal mechanisms . To demonstrate the effect of concurrency control on performance and scalability , we used several concurrence control algorithms in a partitioned , RAMP performance scales well with increased load and incurs little overhead . With few clients , there are few concurrent updates and therefore few We also evaluated the overhead of blocked writes in our implementation of the Cooperative Termination Protocol . We artificially dropped a RAMP-F achieves slightly under 7.1 million operations per second , or 1.79 million transactions per second on a set of databases offer a broad spectrum of isolation guarantees at varying costs and availability . At the strong end of the isolation spectrum , This paper described how to achieve atomically visible transactions without making the performance and availability of traditional algorithms . The paper is based on a paper by D. Agrawal and V. Krishna from MIT ( MIT ) . The authors argue that the criteria for atomic sets of versions in the form of companion sets are different . Each write in RAMP contains"}
{"ground_truth": "Uncertainty propagation in data processing systems Manousakis et al., SoCC\u201918  When I\u2019m writing an edition of The Morning Paper, I often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day. There are three levels of takeaway from today\u2019s paper choice:  If you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence. If you\u2019re savouring a cortado, then you might also want to dip into the techniques we can use to propagate uncertainty through a computation. If you\u2019re lingering over a latte, then the UP (Uncertainty Propagation) framework additionally shows how to integrate these techniques into a dataflow framework. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing, and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. Are you sure? Uncertainty can arise from a number of different sources including probabilistic modelling, machine learning, approximate computing, imprecise sensor data, and such like. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings. If you have a dataflow system with computation based on a DAG, then uncertainty in upstream data values needs to flow through the computation. For example, consider a simple 2-node DAG where an approximate query is used to produce an approximate count of the number of customers in different age groups (e.g., using BlinkDB ), and then we take a weighted average of those groups. The second node will by default produce a single value, but in reality it should result in a distribution. There may be meaningful parts of that distribution where the outcome would be disadvantageous (for example), but the probability of this is completely lost when reporting a single value. Uncertainty propagation  Our method offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation. Consider a function  , where  is an arbitrary function without side-effects representing the computation at a node in a dataflow,  is a set of random variables representing inputs with uncertainties, and  is a set of random variables representing outputs with uncertainties. Depending on the nature of  , we can use different statistical methods to approximate the mean and variance of each variable in the output. When  is a continuous differentiable function we can use first-order Differential Analysis:  The general strategy is to compute  by approximating  using its first-order Taylor series at the expected value of  . This approximation is accurate if  is roughly linear around the support (in other words, neighborhood) of  \u2026  When there are multiple inputs and multiple outputs, the calculation also needs to take into account the covariances between the outputs. When  is a semi-continuous function we have two possibilities. If the support of each input mostly or entirely falls within a continuous differentiable part of the function then we can use Differential Analysis (DA) as before. If it spans a discontinuity then we have to use Monte Carlo simulation. For example, consider the function  when  , and  otherwise. If each input is greater than  then we can use DA. We use Monte Carlo simulation to approximate  for functions  that do not meet (or the developers do not know whether they meet) the requirements for DA. is evaluated on  randomly drawn samples of the input, and the outputs are used as an approximation of  . To generate accurate samples, one must know the joint density of  and pay the heavy computational cost of any rejection-sampling algorithm. Unfortunately that cost grows exponentially with an increasing size of  and thus we resort to two approximations:  Given input distributions, generate samples accordingly and ignore covariances  In the absence of full distributional information, assume that each input is normally distributed with the same mean and covariance matrix as the unknown distribution. (This approximation works because the mean and variance estimation of Y depends solely on the mean and variance of  ). Uncertainty propagation in dataflows  As stated earlier, in a dataflow graph we need to perform uncertainty propagation at all nodes downstream of uncertain data. For Monte Carlo simulation-based uncertainty propagation (UP-MC) we can just treat a node as a black box, dynamically generate samples from the input set, and compute the mean and variance for each output using empirically derived distributions (or assume normal distributions in the absence of this information). The implementation of Differential Analysis (henceforth called UP-DA) is more challenging. Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the outputs\u2026 input covariances can require additional data flow to be added to the DAG for computing output variances and covariances. If the programmer can provide a partial derivative function, then using this often gives better performance than resorting to numerical differentiation. Observe that we might make a saving early in the dataflow by introducing uncertainty (e.g. by computing an approximate result), but then we have to pay more later for the resulting uncertainty propagation. The evaluation explores this trade-off. UP-MapReduce is an implementation of the above ideas in the in MapReduce. The UP-MapReduce extension includes three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions. The extension also introduce the uncertain type PV (Probabilistic Value) which contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution. The UP-DA Continuous Reducer class for example provides an abstract derivative method that a developer can implement to provide a closed-form derivative function. Uncertainty propagation in practice  We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. Baselines for the evaluation are established by running a large Monte Carlo experiment over a precise version of each application. When input errors are small (e.g. below 3%) then UP-MapReduce estimates means with very low error. The following figure shows the relative errors and execution times for the three variants of UP-MC as compared to the baseline. Enlarge  For six of the applications UP-MapReduce is highly accurate, but when input errors are significant (e.g. eig, svd) its estimated relative errors can deviate noticeably from baseline values. The best performance is obtained when using closed-form (user provided) derivatives. tsocial and latency are both multi-stage approximate workflows. The following chart shows the execution times and maximum relative errors for sampling rates from 0.1% to 100% (precise). For tsocial, a sampling rate of 80% or less is required before the overheads of uncertainty propagation are outweighed by the sampling benefits. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low\u2014 an average of 6% performance degradation\u2014 when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computational resources.", "prediction": "We are seeing an explosion of uncertain data from sensors in IoT, sampling-based approximate computations and machine learning algorithms. In many cases, performing computations on uncertain data Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing, Carlsbad, CA, USA, October 11\u2013 Data is being produced and collected at a tremendous pace. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Approximate computing is an emerging source of approximation uncertainty. In this setting, it may be possible for the user to tradeoff precision against execution time and/or energy In this section we introduce our proposed methods for handling uncertain inputs at a DAG node. We discuss how to (approximately) compute Y = f (X), where f We use first-order Differential Analysis to approximate the first two moments of Y, i.e., mean and variance, for functions f that are continuous and differentiable We can leverage the above approach for semi-continuous functions when the support of each Xi in X falls mostly or entirely within a continuous and differentiable part of the function We use Monte Carlo simulation to approximate Y for functions f that do not meet (or the developers to not know whether they meet) the requirements for DA. We evaluate f We now discuss how to apply the UP techniques introduced in the last section to data processing DAGs. Figure 3 shows a small example DAG, where uncertainty is introduced As a proof of concept, we extend Hadoop MapReduce to include the above UP techniques. We first show how our approach can be applied to the MapRed In MapReduce, each program runs in two phases, Map and Reduce. In the Map phase, a user-written map() function is invoked per each input ( We implement UP-MapReduce as an extension of Apache Hadoop 2.7. The extension comprises three Mapper and three Reducer classes. Developers must choose We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. In this section, we evaluate UP-MapReduce by studying it\u2019s accuracy, performance, and scalability. We begin by exploring the two applications, t We leverage real datasets for the two approximate applications under study. We evaluate tsocial using the Facebook social structure from SNAP social circles and latency using traceroute measurements from i We leverage UP-MapReduce to build two multi-stage approximate workflows (tsocial and latency) Both first sample their initial dataset and produce uncertain intermediate values. UP-MapReduce estimates the means with very low bias, especially when the input relative errors are small. We observe that input uncertainties can be relatively stable, contract, We explore the scalability of UP-MapReduce by running applications 3-11 on a cluster of 512 servers. We choose the following input sizes: linreg ( In this paper, we showed how Differential Analysis can be used to propagate uncertainties through DAG nodes. Our approach falls back to Monte Carlo simulation of nodes otherwise, but This work was partially supported by NSF grant CCF-1319755.", "simplified_prediction": "We seeing an explosion of uncertain data from sensors in IoT . This is called approximate computations and machine learning algorithms . In many cases , performing computations on uncertain data in Data Processing Systems ( see below ) . Proceedings of ACM Symposium on Cloud Computing , Carlsbad , CA , USA , October 11 , 2008 , is being produced and collected at a pace . For many applications , data should be represented as probability distributions or estimated values with exact values rather than exact values . Approximate computing is the source of approximation uncertainty . In this setting , it may be possible for the user to tradeoff precision against execution time and / energy In this section we introduce our proposed methods for handling uncertain things . We discuss how to ( approximately ) compute Y = f ( X ) , where f We use first-order Differential Analysis to approximate the first two moments of Y , i. e. , mean and variance , for functions f that are continuous and differentiable We can put the first two moments of Y/O. This means that the support of each Xi in X falls or mostly within a continuous and the differentiable function of the Monte ( it 'sotic system ) . We now talk about how to apply the UP techniques introduced in the last section to data processing DAGs . Figure 3 shows a small example DAG , where it is introduced As a proof of concept , we can be found to include the above UP techniques . We first show how our approach can be given to the MapReduce , each program runs in two phases , Map and Reduce . In the Map phase , a user-written map ( ) function is used each input ( We implement UP-MapReduce as an extension of Apache Hadoop 2.7 . The extension includes three Mapper and three Reducer classes . Developers must choose We have built a toolbox of common operations and modified ten common data processing applications using UP-MapReduce to process uncertain data . In this section , we evaluate UP-MapReduce by studying it 's accuracy , performance , and scalability . We begin by exploring the two applications , real datasets for the two main applications under study . We evaluate tsocial using the star structure from SNAP social circles and latency using traceroute measurements from i We leverage UP-MapReduce to build two workflows ( tsocial and latency ) Both first sample their first sample and produce intermediate values . UP-MapReduce estimates the means with very low bias , especially when the errors are small . We see that input uncertainties can be relatively stable , contract . We look like the scalability of UP-MapReduce by running applications 3 - 11 on a cluster of 512 servers . We choose the following sizes : linreg ( In this paper , we showed how Differential Analysis can be used to find uncertainties through DAG nodes . Our approach falls back to Monte Carlo simulation of nodes otherwise , but this work was partially supported by NSF ."}
{"ground_truth": "Understanding lifecycle management complexity of datacenter topologies Zhang et al., NSDI\u201919  There has been plenty of interesting research on network topologies for datacenters, with Clos-like tree topologies and Expander based graph topologies both shown to scale using widely deployed hardware. This research tends to focus on performance properties such as throughput and latency, together with resilience to failures. Important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure. The great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible but still very important \u201clifecycle management\u201d implications of topology design. In networking, this translates into how easy it is to physically deploy the network, and how easy it to subsequently expand. They find a way to quantify the associated lifecycle management costs, and then use this to help drive the design of a new class of topologies, called FatClique. \u2026 we show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics. Now, there\u2019s probably only a relatively small subset of The Morning Paper readers involved in designing and deploying datacenter network topologies. So my challenge to you as you read through this paper, is to think about where the hidden complexity and costs are in your own systems. Would you do things differently if these were made more visible? It would be great to see more emphasis for example on things like developer experience (DX) and operational simplicity \u2013 in my experience these kinds of attributes can have an outsize impact on the long-term success of a system. Anyway, let\u2019s get back to cables and switches\u2026  Physically deploying network topologies  When it comes to laying out a network topology for real in a datacenter, you need to think about packaging, placement, and bundling. Packaging is how you group things together, e.g. the arrangement of switches in racks, and placement concerns how these racks are physically placed on the datacenter floor. Placement in turn determines the kinds of cabling you need, and for optical cables the power of the transceivers. Within a rack we might package several connected switches into a single chassis using a backplane. At the other end of the scale, blocks are larger units of co-placement and packaging that combine several racks. With all those connections, it makes things a lot easier to group together multiple fibres all connecting the same two endpoints (racks) into bundles, which contain a fixed number of identical length fibres. Manufacturing bundles is simpler than manufacturing individual fibres, and handling such bundles significantly simplifies operational complexity. Patch panels make bundling easier by providing a convenient aggregation point to create and route bundles. Bundles and fibres are physically routed through the datacenter on cable trays. The trays themselves have capacity constraints of course. Here\u2019s an example of a logical Clos topology and its physical instantiation:  The authors identify three key metrics that together capture much of the deployment complexity in a topology:  The number of switches. More switches equals more packaging complexity. The number of patch panels, which is a function of topological structure and a good proxy for wiring complexity. The number of bundle types. This metric captures the other important part of wiring complexity \u2013 how many distinct bundle types are needed. A bundle type is represented by its capacity (how how many fibres) and its length. These complexity measures are complete. The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which are inputs to the topology design). The number of cables and transceivers can be derived from the number of patch panels. Here\u2019s how Clos and Expander (Jellyfish) representative topologies for the same number of servers stack up against these metrics:  The expander graph topology shows much higher deployment complexity in terms of the number of bundle types. Clos also exposes far fewer ports outside of a rack (it has better port hiding). Expanding existing networks  When you want to expand an existing network first you need to buy all the new gear and lay it out on the datacenter floor, and then you can begin a re-wiring process. This is all going on with live traffic flowing, so expansion is carried out in steps. During each step the capacity of the topology is guaranteed to be at least some percentage of the existing topology capacity. The percentage is sometimes known as the expansion SLO. During a step existing links to be re-wired are drained, then human operators physical rewire links at patch panels. The new links are tested and then undrained (strange word! ), i.e., brought into service. For example, here\u2019s a logical expansion (top row) and its physical realisation:  The most time-consuming part of all this is the physical rewiring. The two metrics that capture expansion complexity are therefore:  The number of expansion steps, and  The average number of rewired links in a patch panel rack. Here\u2019s how Clos and Expander stack up on those metrics for the same networks we saw earlier:  This time the victory goes to Expander (Jellyfish). Jellyfish has a much higher north-to-south capacity ratio. Northbound links exit a block, and southbound links are to/from servers within a block. \u201cFat edges\u201d have more northbound than southbound links, and the extra capacity means you can accomplish more movement in each step. Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio. Enter the FatClique  Inspired by these insights, the authors define a new class of topologies called FatClique, which combine the hierarchical structure of Clos with the edge expansion capabilities of expander graphs. There are three levels in the hierarchy. A clique of switches form a sub-block. Cliques of sub-blocks come together to form blocks. And cliques of blocks come together to from the full FatClique topology. Four key design variables determine the particular instantiation of a FatClique topology: the number of ports in a switch that connect to other servers; the number of ports in a switch that connect to other sub-blocks in a block; the number of switches in a sub-block; and the number of sub-blocks in a block. A synthesis algorithm  takes a set of six input constraints (see \u00a75.1) and determines the values for these four design variables. There is plenty more detail in section 5 of the paper which I don\u2019t have the space to do justice too here. FatClique vs Clos vs Expander  The evaluation compares FatClique to Clos, Xpander, and Jellyfish at different network topology sizes, as shown in the table below. ( Enlarge )  Here\u2019s how they stack up against the complexity metrics:  Number of switches  Number of patch panels  Number of bundle types  and associated cabling costs:  Number of expansion steps  Average number of rewired links  We find that FatClique is the best at most scales by all our complexity metrics. (The one exception is that at small and medium scales, Clos has slightly fewer patch panels). It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we were able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5x longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher. The one thing I couldn\u2019t find in the evaluation is any data to back up the opening claim that FatClique achieves all of this \u201cwhile being performance-equivalent to existing topologies.\u201d  The last word  As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction\u2026", "prediction": "Most recent datacenter topology designs have focused on performance properties. FatClique attempts to understand the complexity of deploying a performance properties such as latency and throughput. Life cycle management complexity attempts to understand the complexity of deploying a topology and expanding it Lifecycle management is the process of building a network, physically deploying it on a data-center floor, and expanding it Data centers are often designed for high throughput, low latency and resilience. Existing data center designs can be broadly classified into the Deployment is the process of realizing a physical topology in a data center space. Deployment complexity can be reduced by careful Packaging of a topology involves careful arrangement of switches into racks. Placement of switches on the datacenter floor can Based on the previous discussion, we identify several metrics that quantify the complexity of the two aspects of datacenter topology deployment The two topology classes are qualitatively different by these metrics. Jellyfish only needs a little over half the switches compared to The second important component of topology lifecycle management is expansion. Datacenters are rarely deployed to maximal capacity in one shot Expansion is carried out in steps, where at each step, the capacity of the topology is guaranteed to be at least Figure 2 shows an example of Clos expansion. To maintain 87.5% capacity at each pod, only one link is We identify two metrics that quantify expansion complexity and use these metrics to identify differences between Clos and Jellyfish. Number of Expansion Jellyfish requires 3 steps, while Clos twice the number of steps. To understand why Jellyfish requires fewer steps, Clos uses fewer bundle types and patch panels. Jellyfish has significantly lower switch counts. FatClique answers this question affirm FatClique combines the hierarchical structure in Clos with the edge expansion in expander graphs to achieve lower lifecycle management complexity FatClique has fat edges, which allows draining more and more links at each step of the expansion. At each step, FatClique achieves low lifecycle management complexity while ensuring full-bisection bandwidth. It ensures high edge expansion, resulting in In this section, we compare three classes of topologies, Clos, expander graphs and FatClique by our complexity We evaluate complexity across three different topology sizes based on the number of servers they support: small, medium, and large. The placement of patch panels is determined both by the structure of the topology and its scale. For small and medium scale Cl In this section, we evaluate our different topologies by our three measures of deployment complexity (\u00a73.2) Number of In this section, we evaluate topologies by our two measures of expansion complexity (\u00a74.3): number of expansion steps We find that FatClique is the best at most scales by all our complexity metrics. It uses 50% fewer switches and Previous topology designs have focused on cost effective, high capacity and low diameter datacenter topologies like [6, 35 Lifecycle management consists of network deployment and expansion. As the management complexity of networks increases, the importance of designing for manage Algorithm can only generate non-modular topologies as shown in Figure 13. Topology is composed of heterogenous building", "simplified_prediction": "Most recent designs have focused on performance properties , for example . FatClique attempts to understand the complexity of making a performance properties such as latency and throughput . Life management complexity attempts to understand the complexity of deploying a topology and expanding it Lifecycle management is the process of building a network , physically using it on a data floor , and expanding it Data centers are often designed for high throughput , low and resilience . Existing data center designs can be classified into the process of realizing a physical topology in a data center space , for example . Deployment complexity can be reduced by careful of a topology involves careful pattern of switches into racks . Placement of switches on the datacenter floor can Based on the previous discussion , we say that the complexity of the two aspects of datacenter topology use The two topology classes are qualitatively different by these metrics . Jellyfish only needs a little over half the switches compared to the second important part of the body in its expansion . Datacenters are rarely used in one shot Expansion is carried out in steps , where at each step , the capacity of the topology is guaranteed to be at least two shows an example of Clos expansion . To maintain 87.5 % capacity at each pod , only one link is We identify two metrics that use these metrics to identify differences between Clos and Jellyfish . There are 3 steps , but Clos twice the number of steps twice the number of steps . To understand why Jellyfish needs a fewer steps , Clos uses fewer types and patch panels . Jellyfish has significantly lower switch counts . FatClique answers this question affirming the hierarchical structure in Clos with the edge expansion in expanding graphs to achieve lower lifecycle management complexity FatClique has fat edges , which allows draining more and more links at each step of the expansion . At each step , FatClique achieves low lifecycle management complexity while making a full-bisection bandwidth . In this section , we compare three classes of topologies , Clos , expander graphs and FatClique by our complexity We evaluate complexity across three different topology sizes based on the number of servers they support : small , medium , and large . The placement of patch panels is determined by the structure of the topology and its scale . For small and medium scale Cl In this section , we evaluate our different topologies by our three measures of deployment complexity : number of expansion steps We find that FatClique is the best at most scales by all our complexity metrics . The number of expansion steps We find that FatClique is the best at most scales by all our complexity metrics . It uses 50 % fewer switches and Previous topology designs have focused on cost effective , high capacity and low diameter datacenter , like [ 6 , 35 Lifecycle management consists of network deployment and expansion . As the management complexity of networks increases , the importance of designing for manage Algorithm can only generate non-modular topologies as shown in 13 . Topology is the study of building ."}
