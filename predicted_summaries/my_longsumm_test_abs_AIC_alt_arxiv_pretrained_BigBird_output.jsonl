{"ground_truth": "We present BEAT, a set of practical Byzantine fault-tolerant (BFT) protocols for completely asynchronous environments. BEAT is flexible, versatile, and extensible, consisting of five asynchronous BFT protocols that are designed to meet different goals (e.g., different performance metrics, different application scenarios). Due to modularity in its design, features of these protocols can be mixed to achieve even more meaningful trade-offs between functionality and performance for various applications. Through a 92-instance, five-continent deployment of BEAT on Amazon EC2, we show that BEAT is efficient: roughly, all our BEAT instances significantly outperform, in terms of both latency and throughput, HoneyBadgerBFT, the most efficient asynchronous BFT known.\nState machine replication (SMR) [64, 81] is a fundamental software approach to enabling highly available services in practical distributed systems and cloud computing platforms (e.g., Googles Chubby [20] and Spanner [29], Apache ZooKeeper [53]). Its Byzantine failure counterpart, Byzantine fault-tolerant SMR (BFT), has recently regained its prominence, as BFT has been regarded as the model for building permissioned blockchains where the distributed ledgers know each others identities but may not trust one another. As an emerging technology transforming businessmodels, there has been a large number of industry implementations of permissioned blockchains, including Hyperledger Fabric [7, 87], Hyperledger Iroha [56], R3 Corda [30], Tendermint [88], and many more. The Hyperledger umbrella [5], for instance, has become a global collaborative open-source project under the Linux Foundation, now with more than 250 members. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CCS 18, October 1519, 2018, Toronto, ON, Canada  2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5693-0/18/10. . . $15.00 Asynchronous BFT protocols [14, 21, 23, 70] are arguably themost appropriate solutions for building high-assurance and intrusiontolerant permissioned blockchains in wide-area (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet. Asynchronous BFT ensures liveness of the protocol without depending on any timing assumptions, which is prudent when the network is controlled by an adversary. In contrast, partially synchronous BFT (e.g., PBFT [27]) guarantees liveness only when the network becomes synchronous (i.e., satisfies timing assumptions). For instance, it was shown in [70] that PBFT would achieve zero throughput against an adversarial asynchronous scheduler. Challenges andopportunities in adopting asynchronous permissioned blockchains.While a recent asynchronous BFT protocol, HoneyBadgerBFT [70], significantly improves prior asynchronous BFT protocols [14, 21, 23, 70], there are still significant pain points and challenges that prevent it from being used in practice. Meanwhile, there are also new opportunities for asynchronous BFT with the rise of blockchains. Performance (latency, throughput) issues. Compared to partially synchronous BFT protocols (e.g., PBFT [27]), HoneyBadgerBFT has significantly higher latency and lower throughput, in part due to its use of expensive threshold cryptography (specifically, threshold encryption [10] and threshold signatures [17]). This is particularly visible in cases where each replica has limited computation power. These limitations are further exacerbated by various engineering issues. For example, HoneyBadgerBFT was evaluated at only 80- bit security and it will be even slower if implemented with nowstandard 128-bit security. Moreover, due to its use of an erasurecoding library zfec [93], HoneyBadgerBFT can only support ReedSoloman codes (for which better alternatives exist) and at most 28 servers. No one-size-fits-all BFT. In partially synchronous environments, onesize-fits-all BFT protocols have been hard to achieve (as has been argued in various works, e.g., [8, 31, 59]). Indeed, a variety of partially synchronous BFT protocols [1, 8, 16, 27, 28, 31, 33, 59] have been proposed to meet different needs. For instance, chain-based BFT protocols, such as Aliph-Chain [8], BChain [33], and Shuttle [90], favor throughput over latency. Q/U [1] achieves fault-scalability that tolerates increasing numbers of faults without significantly decreasing performance. Zyzzyva [59] and Aliph [8] are hybrid protocols that have high performance in failure-free cases. Moreover, a large number of robust BFT protocols [4, 9, 16, 28, 91] aim to provide a trade-off between performance and liveness during attacks that affect the timing behavior of the network. While robustness is natively achieved in asynchronous BFT, we still require different designs and trade-offs for different performance metrics. Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers). Append-only ledger vs. smart contracts. We advocate distinguishing two different classes of blockchain applications: append-only ledgers and on-chain smart contracts. The former corresponds to append-only, linearizable storage systems (hereinafter, BFT storage), and the latter corresponds to general SMR. While they share security requirements (agreement, total order of updates, liveness), general SMR requires each replica to maintain a copy of all service state to support contracts that operate on that state. In contrast, BFT storage may leverage erasure coding to reduce overall storage by allowing servers to keep only fragments. (See Sec. 3 for formal definitions.) Both of the applications are rather popular. Applications such as food safety [92] and exchange of healthcare data [54] are examples of append-only ledgers, while AI blockchain [86] and financial payments [55] fall into the category of requiring smart contracts. Internet of things (IoT) with blockchains may be of either type, depending on the applications: if one just uses blockchains to store and distribute IoT data to avoid the single point of failure that the clouds may have, then we just need the distributed ledger functionality; if one additionally uses blockchains to consume and analyze the data, then we will additionally need smart contracts. BFT storage may be extended to support off-chain smart contracts run among clients (e.g., Hyperledger Fabric [7]). While offchain smart contracts have many benefits (e.g., achieving some level of confidentiality, as argued in [7]), they also have limitations: 1) they are less suited to running complex smart contract applications with power- and computation-restricted clients (e.g., IoT devices); 2) they require communication channels among clients; and 3) they do not support efficient cross-contract state update. Some blockchain systems use BFT for building consensus ordering services (e.g., Hyperledger Fabric). We find that BFT storage may be used to model the consensus ordering service, and a more efficient BFT storage can lead to a more efficient ordering service. When designing BEAT, we aimed to answer the following major question: Can we have asynchronous BFT storage that significantly outperforms asynchronous general SMR? Flexible read. Some applications benefit from flexible reading, i.e., reading just a portion of a data block as needed (instead of the whole block). For example, in a blockchain that stores video, a user may only want to read the first portion of the stored video. This can be challenging when we use erasure-coding as the underlying storage mechanism. BEAT aims to achieve flexible read with significantly reduced bandwidth. BEAT in a nutshell.We design, implement, and evaluate BEAT  a set of practical asynchronous BFT protocols that resolve the above challenges. First, BEAT leverages more secure and efficient cryptography support and more flexible and efficient erasure-coding support. Second, BEAT is flexible, versatile, and extensible; the BEAT family includes asynchronous BFT protocols that are designed to meet different needs. BEATs design is modular, and it can be extended to provide many more meaningful trade-offs among functionality and performance. Third, BEAT is efficient. Roughly, all our BEAT instances significantly outperform, in terms of both latency and throughput, HoneyBadgerBFT. TheBEATprotocols.BEAT includes five BEAT instances (BEAT0 BEAT4). BEAT0, BEAT1, are BEAT2 are general SMR that can support both off-chain and on-chain smart contracts, while BEAT3 and BEAT4 are BFT storage that can support off-chain smart contracts only. We summarize the characteristics of the BEAT protocols in Table 1 as a series of improvements to HoneyBadgerBFT.  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption [85], a direct instantiation of threshold coin-flipping [22] (instead of using threshold signatures [17]), and more flexible and efficient erasure-coding support.  BEAT1 additionally replaces an erasure-coded broadcast (AVID broadcast) [24] used in HoneyBadgerBFT with a replicationbased broadcast (Brachas broadcast [19]). This helps reduce latency when there is low contention and the batch size is small.  BEAT2 opportunisticallymoves the encryption part of the threshold encryption to the client, further reducing latency. BEAT2 does so at the price of achieving a weaker liveness notion, but can be combined with anonymous communication networks to achieve full liveness. Asynchronous BFT with Tor networks has been demonstrated in HoneyBadgerBFT. BEAT2 additionally achieves causal order [21, 35, 79], a rather useful property for many blockchain applications that process transactions in a first come, first served manner, such as stock trading and financial payments.  BEAT3 is a BFT storage system.While HoneyBadgerBFT, BEAT0, BEAT1, and BEAT2 use Byzantine reliable broadcast [19, 24, 67], we find that replacing Byzantine reliable broadcast with a different and more efficient primitive  bandwidth-efficient asynchronous verifiable information dispersal (AVID-FP) [45] (using fingerprinted cross-checksum) suffices to build a BFT storage. The bandwidth consumption in BEAT3 is information-theoretically optimal. To order transactions of size B, the communication complexity of BEAT3 is O(B), while the complexity for HoneyBadger and PBFT is O(nB) (where n is the total number of replicas). This improvement is significant, as it allows running BEAT in bandwidth-restricted environments, allows more aggressive batching, and significantly improves scalability.  BEAT4 further reduces read bandwidth. BEAT4 is particularly useful when it is common that clients frequently read only a fraction of stored transactions. We provide a generic framework to enable this optimization, and BEAT4 is a specific instantiation of the framework. Roughly, BEAT4 reduces the access overhead by 50% with around 10% additional storage overhead. To achieve this, we extend fingerprinted cross-checksums [45] to handle partial read and to the case of pyramid codes [51], and we design a novel erasure-coded asynchronous verifiable information dispersal protocol with reduced read bandwidth (AVID-FP-Pyramid). Both techniques may be of independent interest. To our knowledge, all the erasure-coded systems against arbitrary failures in reliable distributed systems community [6, 25, 32, 41, 46] use conventional MDS (maximum distance separable) codes [69] such as Reed-Solomon codes [78] and they inherit the large bandwidth features of MDS codes. On the other hand, a large number of works aim to reduce the read bandwidth by designing new erasure coding schemes [4244, 5052, 58]. The systems using these codes work in synchronous environments only, and do not achieve any strong consistency goals even in the crash failure model (let alone Byzantine failures). It is our goal to blend these two disjoint communities and offer new insights to both, by designing novel Byzantine reliable broadcast and BFT protocols with reduced bandwidth.  BEATs design is modular, and features of these protocols can be mixed to achieve even more meaningful trade-offs among functionalities, performance metrics, and concrete applications.\nWe describe the design and implementation of BEAT, a family of practical asynchronous BFT protocols that are efficient, flexible, versatile, and extensible. We deploy and evaluate the five BEAT protocols using 92 instances on Amazon EC2, and we show BEAT protocols are significantly more efficient than HoneyBadgerBFT, the most efficient asynchronous BFT known. We also develop new distributed system ingredients, including generalized fingerprinted cross-checksum and new asynchronous verifiable information dispersal, which might be of independent interest.\n", "prediction": "in this paper, we show that there is a trade - off between the speed of replication and the security of the protocols. in the case of a replication algorithm that is based on a public key distribution ( pkd ) function : ( i) there exists a set of cryptographic primitives that are necessary and sufficient for the algorithm to converge to a rate at which no security can be deduced from the data generated by the protocol ; ( ii) the rate of convergence is at most linear over a logarithmic time scale (  speed - up - delay - convergence '' or sd / td) ] the design of this algorithm is not a priori known and does not make any assumptions about the underlying function or the distribution of data on the system to be replicated <n> it is only known in a pre - specified order of magnitude as to how much data will be generated per second ( in units of bit - rate per gb of public - key - distributed - access - tp - cloud - secure - private - bit _ ]) the price to pay for this security is in fact a small penalty compared to the amount of privacy that can not be recovered if the secret is lost or tampered with ( the cost to lose or change the key is much smaller than the total cost of keeping the keys for secret storage and use the public-key - based protocols [7 ], [8],[8]] and [9],10][11]e, [13 13] [14] for instance [4 4][4 [5] in [3] we have been the error of [6[5 [[[e] to achieve [e [10[12] as one [12 [6 [ [ ]. in one  ['[<n> [ 4  for [ 6 [ 8]  to [ 5 [ in our [ e  ][e  in which we support [ one to broadcast [ ( [ which [ as [. [ a [ the [ we [ for  '! [ to support for a  that [ that we are [ with [2].<n> for example [4].''[i [] that'as the first [ x[(e as a further [ and  the ie [... [ se ]]  which  we  with the. to help [ while [ among [ is [ 1'[ who [ when [x [ are the message [ whose [ it  is the one is to which is one of which are to make [ an [ 2  as we also [ of  a. for e. e[."}
{"ground_truth": "To operate reliably in real-world traffic, an autonomous car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. This paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of other agents. Each policy captures a distinct high-level behavior and intention, such as driving along a lane or turning at an intersection. We first employ Bayesian changepoint detection on the observed history of states of nearby cars to estimate the distribution over potential policies that each nearby car might be executing. We then sample policies from these distributions to obtain high-likelihood actions for each participating vehicle. Through closed-loop forward simulation of these samples, we can evaluate the outcomes of the interaction of our vehicle with other participants (e.g., a merging vehicle accelerates and we slow down to make room for it, or the vehicle in front of ours suddenly slows down and we decide to pass it). Based on those samples, our vehicle then executes the policy with the maximum expected reward value. Thus, our system is able to make decisions based on coupled interactions between cars in a tractable manner. This work extends our previous multipolicy system [11] by incorporating behavioral anticipation into decision-making to evaluate sampled potential vehicle interactions. We evaluate our approach using real-world traffic-tracking data from our autonomous vehicle platform, and present decision-making results in simulation involving highway traffic scenarios.\nI. INTRODUCTION Decision-making for autonomous driving is hard due to uncertainty on the continuous state of nearby vehicles and, in particular, due to uncertainty over their discrete potential intentions (such as turning at an intersection or changing lanes). Previous approaches have employed hand-tuned heuristics [28, 29, 41] and numerical optimization [17, 21, 42], but these methods fail to capture the coupled dynamic effects of interacting traffic agents. Partially observable Markov decision process (POMDP) solvers [2, 26, 35] offer a theoreticallygrounded framework to capture these interactions, but have difficulty scaling up to real-world scenarios. In addition, current approaches for anticipating future intentions of other traffic agents [1, 22, 24, 25] either consider only the current state of the target vehicle, ignoring the history of its past actions, or rather require expensive collection of training data. In this paper, we present an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. This approach is made tractable by considering only a finite set of a priori known policies. Each policy is designed to capture a different high-level behavior, such as following a lane, changing lanes, or turning at an intersection. Our system proceeds in a sequence of two interleaved stages of behavioral prediction and decision-making. In the first stage, we estimate the probability distribution over the potential policies other traffic agents may be executing. To this aim, we leverage Bayesian changepoint detection to estimate which policy a given vehicle was executing at each point in its history of actions, and then infer the likelihood of each potential intention of the vehicle. Furthermore, we propose a statistical test based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. Individual policies can therefore adjust their behavior to react to anomalous cars. In the second stage, we use this distribution to sample over permutations of other vehicle policies and the policies available for our car, with forward-simulation of these sampled intentions to evaluate their outcomes via a user-defined reward function. Our vehicle finally executes the policy that maximizes the expected reward given the sampled outcomes. Thus, our system is able to make decisions based on closedloop interactions between cars in a tractable manner. We evaluate our behavioral prediction system using a realworld autonomous vehicle, and present decision-making results in simulation involving highway traffic scenarios. The central contributions of this paper are:  A changepoint-based behavioral prediction approach that leverages the history of actions of a target vehicle to infer the likelihood of its possible future actions and detect anomalous behavior online.  A decision-making algorithm that evaluates the outcomes of modeled interactions between vehicles, being able to account for the effect of its actions on the future reactions of other participants.  An evaluation of the proposed system using both traffic data obtained from a real-world autonomous vehicle and simulated traffic scenarios. This work extends our earlier work [11], where we proposed the strategy of selecting between multiple policies for our car by evaluating them via forward simulation, and demonstrated passing maneuvers using a real-world autonomous vehicle. However, that work did not address anticipation of policies for other cars. In contrast, this paper presents a fully integrated behavioral anticipation and decision-making approach.\nWe introduced a principled framework for integrated behavioral anticipation and decision-making in environments with extensively coupled interactions between agents. By explicitly modeling reasonable behaviors of both our vehicle and other vehicles as policies, we make informed high-level behavioral decisions that account for the consequences of our actions. We presented a behavior analysis and anticipation system based on Bayesian changepoint detection that infers the likelihood of policies of other vehicles. Furthermore, we provided a normality test to detect unexpected behavior of other traffic participants. We have shown that our behavioral anticipation approach can identify the most-likely underlying policies that explain the observed behavior of other cars, and to detect anomalous behavior not modeled by the policies in our system. In future work we will explicitly model unexpected behavior, such as the appearance of a pedestrian or vehicles occluded by large objects. We can also extend the system to scale to larger environments by strategically sampling policies to focus on those outcomes that most affect our choices. Exploring principled methods for reacting to detected anomalous behavior is also an avenue for future work.\n", "prediction": "we present an integrated behavioral inference and decision-making approach that models both our vehicle and nearby vehicles as discrete set of closedloop policies that react to the uncertain intentions of other traffic participants.<n> such an approach is able to make decisions based on coupled interactions between cars in a tractable manner using real-world autonomous vehicle simulation and present decisionmaking results in simulation of highway scenarios involving anomalous behavior due to driving over a sequence of changing lanes or turning at an intersection with the expected reward function via a user-defined policy that a vehicle sampled from the policy of a sampled high-likeliaction policy via car-simulation of these sampled wrong outcomes is also an avenue to extend our system to detect the likelihood of each potential intention of the target vehicle, and then infer its expected value for the action of its potential action in the future ( e.g. driving a lane or swerving to pass it ) using a test-tuned probability distribution over its possible future actions ( such as driving in room or slowing down and making room for it to slow down on the observed direction of an other vehicle policy in our direction ). <n> * keywords : * autonomous vehicles ; behavioral anticipation _ keywords and phrases __ * integrated behavior inference * * jel classification * c12 * [ c13 ] * k13 [ k14 * g13 * j11 * h11 [ j12 [ h15 * f11 j14 [ f12 h13 f14 f15 f16 f17 [ g16 g17 g18 g19 g20 g21 g22 g28 g24 g27 g29 g25 g36 g26 g39 g42 g40 g48 g4 gvii g47 [ vii xx cvii [ [ 42 [ 47c [ 44c 42 42 c 47 [ 41 42 47 47 42 44 [ 49 [ 65c 41 41 47 49 49 47 50 50 49 50 47 48 49 48 48 50 42 49 42 50 48 47 60 60 65 65 [ 54 [ 60 50 53 47 53 49 53 50 60 58 [ 59 [ 58 58 60 49 60 57 [ 62 [ 51 47 54 54 60 54 50 58 50 51 50 57 50 54 53 53 60 62 60 59 60 42 60 47 62 62 47 57 57 60 [ 5 [ 53 [ 57 58 44 44 42 41 49 54 41 60 41 54 47 44 47 58 47 59 a 41 44 a 42 we extend the sample the number number a sample our sample a new we estimate the first we also estimate our new a number the second we are the new our first a first to sample from our analysis to estimate that we estimated the we sample [ a to our second to a second our the third the fourth a  to compare the  the results for our a fourth the probability to [ we compare our our [ the fifth to we we were the [ our we will sample over our number to number that the model the previous our third a third our to that our fourth we to model our results that that to another a we presented the distribution for a [ number for all the analysis of our model that sample sample for [ to  we obtained the most we simulated a the result for other that are a model for we observed the simulation for that will the @ number we used the other to be the simulated from a two we have the the a re to all we use the leading the p a that is the upper probability for sample other we sampled over the two to first for another to other other from [ b to re a our other sample that for any number over other a change the re for  that was the to observed a a p to which we predict the change our car that  for two that were observed to"}
{"ground_truth": "Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks  (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to performdespite their strong performance on the related but more narrowly defined task of entailment NLIpointing to interesting avenues for future research.\nThe brain is an abduction machine, continuously trying to prove abductively that the observables in its environment constitute a coherent situation.  Jerry Hobbs, ACL 2013 Lifetime Achievement Award1 Abductive reasoning is inference to the most plausible explanation for incomplete observations (Peirce, 1965a). Figure 1 illustrates an example. Given the incomplete observations about the world that O1: Jenny cleaned her house and went to work, leaving the window just a crack open. and sometime later O2: When Jenny returned home, she saw her house was a mess., we can hypothesize different potential explanations and reason about which is the most likely. We can readily rule out H3 since it fails to justify the observation O2. While H1 and H2 are both plausible, the most likely explanation based on commonsense is H1 as H2 is somewhat implausible given O1. One crucial observation Peirce makes about abductive reasoning is that abduction is the only logical operation which introduces any new ideas, which contrasts with other types of inference such as entailment, that focuses on inferring only such information that is already provided in the premise. Work done while at AI2 1The full transcript of his award speech is available at doi/full/10.1162/COLI_a_00171 ar X iv :1 90 8. 05 73 9v 2 [ cs .C L ] 1 4 Fe b 20 Abductive reasoning has long been considered to be at the core of understanding narratives (Hobbs et al., 1988), reading between the lines (Norvig, 1987; Charniak & Shimony, 1990), reasoning about everyday situations (Peirce, 1965b; Andersen, 1973), and counterfactual reasoning (Pearl, 2002; Pearl & Mackenzie, 2018). Despite the broad recognition of its importance, however, the study of abductive reasoning in narrative text has very rarely appeared in the NLP literature, in large part because most previous work on abductive reasoning has focused on formal logic, which has proven to be too rigid to generalize to the full complexity of natural language. In this paper, we present the first study to investigate the viability of language-based abductive reasoning. This shift from logic-based to language-based reasoning draws inspirations from a significant body of work on language-based entailment (Bowman et al., 2015; Williams et al., 2018b), language-based logic (Lakoff, 1970; MacCartney & Manning, 2007), and language-based commonsense reasoning (Mostafazadeh et al., 2016; Zellers et al., 2018). In particular, we investigate the use of natural language as the representation medium, and probe deep neural models on language-based abductive reasoning. More concretely, we propose Abductive Natural Language Inference (NLI) and Abductive Natural Language Generation (NLG) as two novel reasoning tasks in narrative contexts.2 We formulate NLI as a multiple-choice task to support easy and reliable automatic evaluation: given a context, the task is to choose the more likely explanation from a given pair of hypotheses choices. We also introduce a new challenge dataset, ART, that consists of 20K narratives accompanied by over 200K explanatory hypothesis.34 We then establish comprehensive baseline performance based on state-of-the-art NLI and language models. The best baseline for NLI based on BERT achieves 68.9% accuracy, with a considerable gap compared to human performance of 91.4%(5.2). The best generative model, based on GPT2, performs well below human performance on the NLG task (5.2). Our analysis leads to insights into the types of reasoning that deep pre-trained language models fail to perform  despite their strong performance on the closely related but different task of entailment NLI  pointing to future research directions.\nWe present the first study that investigates the viability of language-based abductive reasoning. We conceptualize and introduce Abductive Natural Language Inference (NLI)  a novel task focused on abductive reasoning in narrative contexts. The task is formulated as a multiple-choice questionanswering problem. We also introduce Abductive Natural Language Generation (NLG)  a novel task that requires machines to generate plausible hypotheses for given observations. To support these tasks, we create and introduce a new challenge dataset, ART, which consists of 20,000 commonsense narratives accompanied with over 200,000 explanatory hypotheses. In our experiments, we establish comprehensive baseline performance on this new task based on state-of-the-art NLI and language models, which leads to 68.9% accuracy with a considerable gap with human performance (91.4%). The NLG task is significantly harder  while humans can write a valid explanation 96% of times, the best generator models can only achieve 45%. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform  despite their strong performance on the closely related but different task of entailment NLI  pointing to interesting avenues for future research. We hope that ART will serve as a challenging benchmark for future research in languagebased abductive reasoning and the NLI and NLG tasks will encourage representation learning that enables complex reasoning capabilities in AI systems.\n", "prediction": "present the first study that investigates the viability of language-based abductive reasoning is presented.<n> the study consists of two parts : ( i ) a full analysis of the reasoning in the literature, which includes the most relevant literature and the results of extensive experiments ; and ( ii) the development of new challenges and insights into the types of reasoning that are difficult to understand or reason about in human - made narrative contexts and that have not yet been widely recognized and studied _ in a systematic way_. in this paper we present full analyses of both of these parts and their results for a corpus of 200,000 commonsense narrative s. we also present a new challenge for the corpus and an analysis that leads to new insights about the reasons for failure of deep pretrained language models and to the insights of future research that will enable us to create a more comprehensive corpus for research in ctive reasoning for human-made narratives and for future language based reasoning ( see also the companion paper by the same authors and a forthcoming chapter in that article by ardavan et al.)the first part of this study is focused on the wide - ranging recognition that the problems of ( inference in) natural language and human make a compelling case for new research and new capabilities for reasoning about situations in which a plausible explanation for incomplete observations is not available ( and this is an example of what we call the  shift from rigid reasoning to shallow reasoning '' which is the shift of rigid logic ( 1965-b) and 2002 ( 2002b etb t ( b.2a b ( a.b b and b b, b-i b; b ib b -b - b) b_a ] b a b ii ii) in b(b-a. bb ii ii bii <n> b[b_b iiiii iiiiii - i ii. ii - ii iii b iii iiib ( c_i - cii. c - iii  ii_ii b i. iii c iii  c. a c-[a ii c ii and c iii ii [ b [ c b c c ( iii - a  b2ii [ a ii[c ii aii iii."}
{"ground_truth": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical kmeans, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.\nThe Maximum Inner Product Search (MIPS) problem has recently received increased attention, as it arises naturally in many large scale tasks. In recommendation systems (Koenigstein et al., 2012; Bachrach et al., 2014), users and items to be recommended are represented as vectors that are learnt at training time based on the user-item rating matrix. At test time, when the model is deployed for suggesting recommendations, given a user vector, the model will perform a dot product of the user vector with all the item vectors and pick top K items with maximum dot product to recommend. With millions of candidate items to recommend, it is usually not possible to do a full linear search within the available time frame of only few milliseconds. This problem amounts to solving a KMIPS problem. Another common instance where the K-MIPS problem arises is in extreme classification tasks (Vijayanarasimhan et al., 2014), with a huge number of classes. At inference time, predicting the top-K most likely class labels for a given data point can be cast as a K-MIPS problem. Such extreme (probabilistic) classification problems occur often in Natural Language Processing (NLP) tasks where the classes are words in a predetermined vocabulary. For example in neural probabilistic language models (Bengio et al., 2003) the probabilities of a next word given the context of the few previous words is computed, in the last layer of the network, as a multiplication of the last hidden layer representation with a very large matrix (an embedding dictionary) that has as many columns as there are words in the vocabulary. Each such column can be seen as corresponding to the embedding of a vocabulary word in the hidden layer space. Thus an inner product is taken between each of these and the hidden representation, to yield an inner product score for each vocabulary word. Passed through a softmax nonlinearity, these yield the predicted probabilities for all possible words. The ranking of these probability values is unaffected by the softmax layer, so finding the k most probable words is Equal contribution and CIFAR ar X iv :1 50 7. 05 91 0v 3 [ cs .L G ] 3 0 N ov 2 exactly equivalent to finding the ones with the largest inner product scores, i.e. solving a K-MIPS problem. In many cases the retrieved result need not be exact: it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high, and thus highly likely (though not guaranteed) to contain some of the exact K-MIPS vectors. These examples motivate research on approximate K-MIPS algorithms. If we can obtain large speedups over a full linear search without sacrificing too much on precision, it will have a direct impact on such large-scale applications. Formally the K-MIPS problem is stated as follows: given a set X = {x1, . . . , xn} of points and a query vector q, find argmax (K) iX q >xi (1) where the argmax(K) notation corresponds to the set of the indices providing the K maximum values. Such a problem can be solved exactly in linear time by calculating all the q>xi and selecting the K maximum items, but such a method is too costly to be used on large applications where we typically have hundreds of thousands of entries in the set. All the methods discussed in this article are based on the notion of a candidate set, i.e. a subset of the dataset that they return, and on which we will do an exact K-MIPS, making its computation much faster. There is no guarantee that the candidate set contains the target elements, therefore these methods solve approximate K-MIPS. Better algorithms will provide us with candidate sets that are both smaller and have larger intersections with the actual K maximum inner product vectors. MIPS is related to nearest neighbor search (NNS), and to maximum similarity search. But it is considered a harder problem because the inner product neither satisfies the triangular inequality as distances usually do, nor does it satisfy a basic property of similarity functions, namely that the similarity of an entry with itself is at least as large as its similarity with anything else: for a vector x, there is no guarantee that xTx  xT y for all y. Thus we cannot directly apply efficient nearest neighbor search or maximum similarity search algorithms to the MIPS problem. Given a set X = {x1, . . . , xn} of points and a query vector q, the K-NNS problem with Euclidean distance is defined as: argmin (K) iX ||q  xi|| 2 2 = argmax (K) iX q Txi  ||xi||22 2 (2) and the maximum cosine similarity problem (K-MCSS) is defined as: argmax (K) iX qTxi ||q|| ||xi|| = argmax (K) iX qTxi ||xi|| (3) K-NNS and K-MCSS are different problems than K-MIPS, but it is easy to see that all three become equivalent provided all data vectors xi have the same Euclidean norm. Several approaches to MIPS make use of this observation and first transform a MIPS problem into a NNS or MCSS problem. In this paper, we propose and empirically investigate a very simple approach for the approximate K-MIPS problem. It consists in first reducing the problem to an approximate K-MCSS problem (as has been previously done in (Shrivastava and Li, 2015) ) on top of which we perform a spherical k-means clustering. The few clusters whose centers best match the query yield the candidate set. The rest of the paper is organized as follows: In section 2, we review previously proposed approaches for MIPS. Section 3 describes our proposed simple solution k-means MIPS in more details and section 4 discusses ways to further improve the performance by using a hierarchical k-means version. In section 5, we empirically compare our methods to the state-of-the-art in tree-based and hashing-based approaches, on two standard collaborative filtering benchmarks and on a larger word embedding datasets. Section 6 concludes the paper with discussion on future work.\nIn this paper, we have proposed a new and efficient way of solving approximate K-MIPS based on a simple clustering strategy, and showed it can be a good alternative to the more popular LSH or tree-based techniques. We regard the simplicity of this approach as one of its strengths. Empirical results on three real-world datasets show that this simple approach clearly outperforms the other families of techniques. It achieves a larger speedup while maintaining precision, and is more robust to input corruption, an important property for generalization, as query test points are expected to not be exactly equal to training data points. Clustering MIPS generalizes better to related, but unseen data than the hashing approaches we evaluated. In future work, we plan to research ways to adapt on-the-fly the clustering for our approximate KMIPS as its input representation evolves during the learning of a model, leverage efficient K-MIPS to speed up extreme classifier training and improve precision and speedup by combining multiple clusterings. Finally, we mention that, while putting the final touches to this paper, another very recent and different MIPS approach, based on vector quantization, came to our knowledge (Guo et al., 2015). We highlight that the first arXiv post of our work predates their work. Nevertheless, while we did not have time to empirically compare to this approach here, we hope to do so in future work.\n", "prediction": "in this paper, we compare two very simple approaches for solving approximate k-means clustering problem.<n> the first one is to train a spherical kmeans model and the second approach is based on a hierarchical clustering of the set of all possible words to be recommend to a user based upon the similarity of a vector with a training- set that has been used as an input to the model for a long time and we show that this simple approach yields much higher speedups for same precision than state-of- the - art methods as well as more robust retrieval when corrupted noise is present in the training set ( this is a common situation in large scale applications where a large number of candidate words are required and it is necessary to perform a full linear search within only a few milliseconds ) in order to have a direct impact on such large-scale applications ( we have examples on two benchmarks and show it can be a simple way to obtain a speedup of 3 orders of magnitude for the same query in comparison to state - of - the art and more efficient tree-based methods ; we will have to wait and see how this approach will be extended to larger datasets and how it will impact applications in recommendation systems with large numbers of classes : we hope to see this as a starting point for further research on this problem and in other tasks where we can not guarantee the precision and regard to speed-ups while maintaining precision while preserving precision s precision for any precision in a precision on the top precision of any full precision to achieve a very high precision ( for all precision as the most precision over the search for it to search a search in search  search ( the best results for full word embedding of an all the inner layer of words with the full vocabulary ( a small word word - search vocabulary for many words and to find all word and a vocabulary with any word ( in any vocabulary in an embeddings ] and this are obtained by a <n> _ i ( i and all for these all all ov t viii ng th  the whole all to all a two and any all ( vox ( x-([[x [i and these are the two are a one and two ( 2 and an a. for this all these is the vector and ]."}
{"ground_truth": "Most recent datacenter topology designs have focused on performance properties such as latency and throughput. In this paper, we explore a new dimension, life cycle management complexity, which attempts to understand the complexity of deploying a topology and expanding it. By analyzing current practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.\nOver the past decade, there has been a long line of work on designing datacenter topologies [2, 35, 31, 32, 3, 4, 20, 1]. While most have focused on performance properties such as latency and throughput, and on resilience to link and switch failures, datacenter lifecycle management [30, 38] has largely been overlooked. Lifecycle management is the process of building a network, physically deploying it on a data-center floor, and expanding it over several years so that it is available for use by a constantly increasing set of services. With datacenters living on for years, sometimes up to a decade [31, 12], their lifecycle costs can be high. A data center design that is hard to deploy can stall the rollout of services for months; this can be expensive considering the rate at which network demands have historically increased [31, 23]. A design that is hard to expand can leave the network functioning with degraded capacity impacting the large array of services that depend on it. It is therefore desirable to commit to a data-center network design only after getting a sense of its lifecycle management cost and complexity over time. Unfortunately, the costs of the large array of components needed for deployment such as switches, transceivers, cables, racks, patch panels1, and cable trays, are proprietary and change over time, and so are hard to quantify. An alternative approach is to develop complexity measures (as opposed to dollar costs) for lifecycle management, but as far as we know, no prior work has addressed this. In part, this is due to the fact that intuitions about lifecycle management are developed over time and with operations experience, and these lessons are not made available universally. 1A patch panel or a wiring aggregator is a device that simplifies cable re-wiring. Unfortunately, in our experience, this lack of a clear understanding of lifecycle management complexity often results in costly mistakes in the design of datacenters that are discovered during deployment and therefore cannot be rectified. Our paper is a first step towards useful characterizations of lifecycle management complexity. Contributions. To this end, our paper makes three contributions. First, we design several complexity metrics (3 and 4) that can be indicative of lifecycle management costs (i.e., capital expenditure, time and manpower required). These metrics include the number of: switches, patch panels, bundle-types, expansion steps, and links to be re-wired at a patch panel rack during an expansion step. We design these metrics by identifying structural elements of network deployments that make their deployment and expansion challenging. For instance, the number of switches in the topology determines how complex the network is in terms of packaging  laying out switches into homogeneous racks in a space efficient manner. Wiring complexity can be assessed by the number of cable bundles and the patch panels a design requires. As these increase, the complexity of manufacturing and packaging all the different cable bundles efficiently into cable trays, and then routing them from one patch panel to the next can be expected to increase. Finally, because expansion is carried out in steps [38], where the network operates at degraded capacity at each step, the number of expansion steps is a measure of the reduced availability in the network induced by lifecycle management. Wiring patterns also determine the number of links that need to be rewired at a patch panel during each step of expansion, a measure of step complexity [38]. Our second contribution is to use these metrics to compare the lifecycle management costs of two main classes of datacenter topologies recently explored in the research literature (2), Clos [2] and expander graphs [32, 35]. We find that neither class dominates the other: Clos has relatively lower wiring complexity; its symmetric design leads to more uniform bundling (and fewer cable bundle types); but expander graphs at certain scales can have simpler packaging requirements due to their edge expansion property [32]; they end up using much fewer switches than Clos to achieve the same network capacity. Expander graphs also demonstrate better expansion properties because they have fat edges (4) which permit more links to be rewired in each step. Finally we design and synthesize a novel and practical class of topologies called FatClique (5), that has lower overall lifecycle management complexity compared to Clos and expander graphs. We do this by combining favorable design USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 235 elements from these two topology classes. By design, FatClique incorporates 3 levels of hierarchy and uses a clique as a building block while ensuring edge expansion. At every level of its hierarchy, FatClique is designed to have fat edges, for easier expansion, while utilizing much fewer patch panels and therefore inter-rack cabling. Evaluations of these topology classes at three different scales, the largest of which is 16 the size of Jupiter, shows that FatClique is the best at most scales by all our complexity metrics. It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5  longer to expand the topology.\nIn this paper, we have attempted to characterize the complexity of lifecycle management of datacenter topologies, an unexplored but critically important area of research. Lifecycle management consists of network deployment and expansion, and we devise metrics that capture the complexity of each. We use these to compare topology classes explored in the research literature: Clos and expander graphs. We find that each class has low complexity by some metrics, but high by others. However, our evaluation suggests topological features important for low lifecycle complexity: hierarchy, edge expansion and fat edges. We design a family of topologies called FatClique that incorporates these features, and this class has low complexity by all our metrics at large scale. As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction; several future directions remain. Topology oversubscription. In our comparisons, we have only considered topologies with an over-subscription ratio of 1:1. Jupiter [31] permits over-subscription at the edge of the network, but there is anecdotal evidence that providers also over-subscribe at higher levels in Clos topologies. To explore the manageability of over-subscribed topologies it will be necessary to design over-subscription techniques in FatClique, Xpander and Jellyfish in a way in which all topologies can be compared on a equal footing. Topology heterogeneity. In practice, topologies have a long lifetime over which they accrue heterogeneity: new blocks with higher radix switches, patch panels with different port counts etc. These complicate lifecycle management. To evaluate these, we need to develop data-driven models for how heterogeneity accrues in topologies over time and adapt our metrics for lifecycle complexity to accommodate heterogeneity. Other management problems. Our paper focuses on topology lifecycle management, and explicitly does not consider other network management problems like fault isolation or control plane complexity. Designs for manageability must take these into account. USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 247\n", "prediction": "recent design topologies have focused on latency and throughput properties of such as panels and isolation in this paper we explore new life cycle management complexity, which attempts to understand the complexity of deploying topology and expanding it in datacenter management. <n> we show that existing topology classes have low lifecycle management complexities by some metrics and not by others and design new classes of topology that while being performance - driven and easy to deploy have lower complexity by all of our metrics for the same management patterns : we also design them to be rewired at a rate that can be determined by the management pattern of the expansion and the number of expansion steps needed to rewire the network at each patch panel during each expansion step ; this is a first step towards useful characterizations of network deployment and management over the life - cycle of a service provider s datacenters ( e.g. @xmath0 for 50 years ) and over this time we can assess the costs of this management in terms of its complexity and how it can stall the rollout of services by increasing costs over time ( which is desirable but so far not known to us and which results in costly end- to - end mistakes in committing to a datacenter design that is hard for us to commit because we know that we commit to it only after getting a clear understanding of how to manage this complexity ( in our case 16th percentiles for these complexity measures we devise a design for expansion of which we determine the size of complexity over our size and we use to design and those of their size over their number and to the top- and their top and that to those to determine their topology topology in the topology ( we are to re- number number for all top number to increase the design of all to have the to their expansion to all and then to our topology  and they have a subset of that re re to use the most to expand the other to add to we have number by a top elements and all by those that have to  to to make the subset and a number in all the patch panels ( radix to cover and by we to these top types and these to one and ng and one is the others to any top - and so we we do and others by th to an expansion number ( and rad ( for their to they are a to some to do all that are the t and an we  ( to which to and top e - we include the the we design number is to many to ( jacobs and number that and and for a we find the ( ( a and are number - a by and ( 2 to e and our top is by which  for any number at 16 and @ 1 to number  the and some number has a  is an number we will be the rad and many number e is 2 is number 2 by to that by that has to top   in a ( 3 to 2 and while the graph and is  we is ( @ @ 2 - to rank and another to @ and 2 for ] and its number while we introduce to subset  while to rate and e _ and [ 32 to are 2. to generate the elements  2 ( is 1 by an and independent and in which by @ a the @ ( by 2  that -2 to 3 and from the data number. the rate of jac and patch number increases the main top. ( 32 is for number @ 3 for 2 we derive the rest to [ 3 - by by number [ 35 to fe and two to p - for an a rad. a. [32 to se and rate to map to such to find that they rew and graph to is that for many top of number the [ 38 to split the graphs by their"}
{"ground_truth": "Performance of in-memory key-value store (KVS) continues to be of great importance as modern KVS goes beyond the traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation in data centers. Recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most KVS from the network to the CPU. RDMA-capable NIC partly alleviates the problem, but the primitives provided by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling in-network processing. In this paper, we present KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory. We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 s. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP 17, Shanghai, China  2017 ACM. 978-1-4503-5085-3/17/10. . . $15.00 DOI: 10.1145/3132747.3132756 CCS CONCEPTS Information systems Key-value stores; Hardware Hardware-software codesign;\nIn-memory key-value store (KVS) is a key distributed system component in many data centers. KVS enables access to a shared key-value hash table among distributed clients. Historically, KVS such as Memcached [25] gained popularity as an object caching system for web services. Large web service providers such as Amazon [17] and Facebook [3, 57], have deployed distributed key-value stores at scale. More recently, as main-memory based computing becomes a major trend in the data centers [18, 58], KVS starts to go beyond caching and becomes an infrastructure to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases [12], model parameters in machine learning [46], nodes and edges in graph computing [67, 74] and sequencers in distributed synchronization [37]. For most of these applications, the performance of the KVS is the key factor that directly determines the system efficiency. Due to its importance, over the years significant amount of research effort has been invested on improving KVS performance. Earlier key-value systems [17, 25, 57] are built on top of traditional OS abstractions such as OS lock and TCP/IP stack. This puts considerable stress on the performance of *Bojie Li and Zhenyuan Ruan are co-first authors who finish this work during internship at Microsoft Research. the OS, especially the networking stack. The bottleneck is exacerbated by the fact that physical network transport speed has seen huge improvements in the last decade due to heavy bandwidth demand from data center applications. More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down [21, 69], a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. One line of research [36, 37] uses two-sided RDMA to accelerate communication (Figure 1a). KVS built with this approach are bounded by CPU performance of the KVS servers. Another line of research uses one-sided RDMA to bypass remote CPU and shift KV processing workload to clients [18, 55] (Figure 1b). This approach achieves better GET performance but degrades performance for PUT operations due to high communication and synchronization overhead. Due to lack of transactional support, the abstraction provided by RDMA is not a perfect fit for building efficient KVS. In the meantime, another trend is emerging in data center hardware evolution. More and more servers in data centers are now equipped with programmable NICs [10, 27, 64]. At the heart of a programmable NIC is a field-programmable gate array (FPGA) with an embedded NIC chip to connect to the network and a PCIe connector to attach to the server. Programmable NIC is initially designed to enable network virtualization [24, 44]. However, many found that FPGA resources can be used to offload some workloads of CPU and significantly reduce CPU resource usage [14, 30, 52, 60]. Our work takes this general approach. We present KV-Direct, a new in-memory key-value system that takes advantage of programmable NIC in data center. KVDirect, as its name implies, directly fetches data and applies updates in the host memory to serve KV requests, bypassing host CPU (Figure 1c). KV-Direct extends the RDMA primitives from memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops). Compared with one-sided RDMA based systems, KVDirect deals with the consistency and synchronization issues at server-side, thus removes computation overhead in client and reduces network traffic. In addition, to support vectorbased operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages [19] and delegate certain computation to programmable NIC for efficiency. Since the key-value operations are offloaded to the programmable NIC, we focus our design on optimizing the PCIe traffic between the NIC and host memory. KV-Direct adopts a series of optimizations to fully utilize PCIe bandwidth and hide latency. Firstly, we design a new hash table and memory allocator to leverage parallelism available in FPGA and minimize the number of PCIe DMA requests. On average, KV-Direct achieves close to one PCIe DMA per READ operation and two PCIe DMAs per WRITE operation. Secondly, to guarantee consistency among dependent KV operations, KV-Direct includes an out-of-order execution engine to track operation dependencies while maximizing the throughput of independent requests. Thirdly, KV-Direct exploits on-board DRAM buffer available on programmable NIC by implementing a hardware-based load dispatcher and caching component in FPGA to fully utilize on-board DRAM bandwidth and capacity. A single NIC KV-Direct is able to achieve up to 180 M KV operations per second (Ops), equivalent to the throughput of 36 CPU cores [47]. Compared with state-of-art CPU KVS implementations, KV-Direct reduces tail latency to as low as 10 s while achieving a 3x improvement on power efficiency. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a server, we achieve 1.22 billion KV operations per second in a single commodity server, which is more than an order of magnitude improvement over existing systems. KV-Direct supports general atomic operations up to 180 Mops, equal to normal KV operation and significantly outperforms the number reported in state-of-art RDMA-based system: 2.24 Mops [36]. The atomic operation agnostic performance is mainly a result of our out-of-order execution engine that can efficiently track the dependency among KV operations without explicitly stalling the pipeline.\nIn this paper, we describe the design and evaluation of KVDirect, a high performance in-memory key-value store. Following a long history in computer system design, KV-Direct is another exercise in leveraging reconfigurable hardware to accelerate an important workload. KV-Direct is able to obtain superior performance by carefully co-designing hardware and software in order to remove bottlenecks in the system and achieve performance that is close to the physical limits of the underlying hardware. After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction.\n", "prediction": "in this paper, we report on the performance of a programmable key-value store ( psks ).<n> we focus on performance that is agnostic to the number of programmable cards in a server ; we achieve 1.22 billion operations per second ( per t - s)@xmath0 for our programmable pks and achieve almost an order-of- magnitude improvement over existing systems in terms of the power efficiency : we are able to achieve 180-s of performance and a speedup of more than two orders of magnitude over our traditional system [ 37 ] in the field of programming [ 36]of [ 38]and [ 41]a [ 42]an example of [ 47]the trend in computing and information technology continues to increase the amount of data and the speed with which they can be processed [ 44]this trend is driven by the growth of communication and computing [ 57]with the proliferation of multiprocessors and multicores [ 58]we have witnessed the increase of bandwidth in network and network processing [ 67]to support the growing demand of high speed and high performance in distributed data centers [ 45]these years we have seen the rise of distributed systems with the decrease of latency in these systems [ 55]several years ago we reported on a high - performance [ 60]that we achieved in [57]as a result of hardware-based computing ( hardware and software [ 62]in the trend of computing to build on this architecture ( 62 [57)we achieved the high-[67][62 [37]is [ [62[[38] to [37 _ [[36] for [ th[in [ 32! [] with [38 [32] we [ 3 ][[3   37] is [ 43  38 ]. [ a [ 33  for the [ in  to obtain [ ( ['[ with a  a] [ to hide [ for  in one [ 40] <n> [ while [ three [ the  42 0][([32) with  from [ 46  with one ], we also [ @[i [... [ is a to  [ we to we is  we obtain ie [ who [ 03  62  32 [ and [. [ that [ which [ as [ 2 [ from the two [ an [ 30 [ one with we in our [ 20'with several [ 22 [<n> in we we set [ ii [ e[e [ 39 [ * [ g []. with with our to add [ 26  the we com [ including [ several  is the[ [ after [ 5 [ 6 [ along [ 52 [ of ' that we[is to which is to a we with that with to our[g [ by [ 28 [ at [ are [ this [ multiple [ p[to [2][the [ they [ 18 [ it [ together [ 10 2]. with another ["}
{"ground_truth": "Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networksat the level of small network subgraphs remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.\nHigher-order organization of complex networks Austin R. Benson,1 David F. Gleich,2 Jure Leskovec3 1Institute for Computational and Mathematical Engineering, Stanford University 2Department of Computer Science, Purdue University 3Computer Science Department, Stanford University To whom correspondence should be addressed; E-mail: jure@cs.stanford.edu Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networksat the level of small network subgraphs remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns. 1 ar X iv :1 61 2. 08 44 7v 1 [ cs .S I] 2 6 D ec 2 01 6 Networks are a standard representation of data throughout the sciences, and higher-order connectivity patterns are essential to understanding the fundamental structures that control and mediate the behavior of many complex systems (17). The most common higher-order structures are small network subgraphs, which we refer to as network motifs (Figure 1A). Network motifs are considered building blocks for complex networks (1, 8). For example, feedforward loops (Figure 1A M5) have proven fundamental to understanding transcriptional regulation networks (9), triangular motifs (Figure 1A M1M7) are crucial for social networks (4), open bidirectional wedges (Figure 1A M13) are key to structural hubs in the brain (10), and two-hop paths (Figure 1A M8M13) are essential to understanding air traffic patterns (5). While network motifs have been recognized as fundamental units of networks, the higher-order organization of networks at the level of network motifs largely remains an open question. Here we use higher-order network structures to gain new insights into the organization of complex systems. We develop a framework that identifies clusters of network motifs. For each network motif (Figure 1A), a different higher-order clustering may be revealed (Figure 1B), which means that different organizational patterns are exposed depending on the chosen motif. Conceptually, given a network motif M , our framework searches for a cluster of nodes S with two goals. First, the nodes in S should participate in many instances of M . Second, the set S should avoid cutting instances of M , which occurs when only a subset of the nodes from a motif are in the set S (Figure 1B). More precisely, given a motif M , the higher-order clustering framework aims to find a cluster (defined by a set of nodes S) that minimizes the following ratio: M(S) = cutM(S, S)/min(volM(S), volM(S)), (1) where S denotes the remainder of the nodes (the complement of S), cutM(S, S) is the number of instances of motif M with at least one node in S and one in S, and volM(S) is the number of nodes in instances of M that reside in S. Equation 1 is a generalization of the conductance 2 metric in spectral graph theory, one of the most useful graph partitioning scores (11). We refer to M(S) as the motif conductance of S with respect to M . Finding the exact set of nodes S that minimizes the motif conductance is computationally infeasible (12). To approximately minimize Equation 1 and hence identify higher-order clusters, we develop an optimization framework that provably finds near-optimal clusters (Supplementary Materials (13)). We extend the spectral graph clustering methodology, which is based on the eigenvalues and eigenvectors of matrices associated with the graph (11), to account for higher-order structures in networks. The resulting method maintains the properties of traditional spectral graph clustering: computational efficiency, ease of implementation, and mathematical guarantees on the near-optimality of obtained clusters. Specifically, the clusters identified by our higher-order clustering framework satisfy the motif Cheeger inequality (14), which means that our optimization framework finds clusters that are at most a quadratic factor away from optimal. The algorithm (illustrated in Figure 1C) efficiently identifies a cluster of nodes S as follows:  Step 1: Given a network and a motif M of interest, form the motif adjacency matrix WM whose entries (i, j) are the co-occurrence counts of nodes i and j in the motif M : (WM)ij = number of instances of M that contain nodes i and j. (2)  Step 2: Compute the spectral ordering  of the nodes from the normalized motif Laplacian matrix constructed via WM (15).  Step 3: Find the prefix set of  with the smallest motif conductance, formally: S := arg minr M(Sr), where Sr = {1, . . . , r}. For triangular motifs, the algorithm scales to networks with billions of edges and typically only takes several hours to process graphs of such size. On smaller networks with hundreds 3 of thousands of edges, the algorithm can process motifs up to size 9 (13). While the worstcase computational complexity of the algorithm for triangular motifs is (m1.5) , where m is the number of edges in the network, in practice the algorithm is much faster. By analyzing 16 real-world networks where the number of edges m ranges from 159,000 to 2 billion we found the computational complexity to scale as (m1.2). Moreover, the algorithm can easily be parallelized and sampling techniques can be used to further improve performance (16). The framework can be applied to directed, undirected, and weighted networks as well as motifs (13). Moreover, it can also be applied to networks with positive and negative signs on the edges, which are common in social networks (friend vs. foe or trust vs. distrust edges) and metabolic networks (edges signifying activation vs. inhibition) (13). The framework can be used to identify higher-order structure in networks where domain knowledge suggests the motif of interest. In the Supplementary Material (13) we also show that when domain-specific higherorder pattern is not known in advance, the framework can also serve to identify which motifs are important for the modular organization of a given network (13). Such a general framework allows for a study of complex higher-order organizational structures in a number of different networks using individual motifs and sets of motifs. The framework and mathematical theory immediately extend to other spectral methods such as localized algorithms that find clusters around a seed node (17) and algorithms for finding overlapping clusters (18). To find several clusters, one can use embeddings from multiple eigenvectors and k-means clustering (13,19) or apply recursive bi-partitioning (13, 20). The framework can serve to identify higher-order modular organization of networks. We apply the higher-order clustering framework to the C. elegans neuronal network, where the fournode bi-fan motif (Figure 2A) is over-expressed (1). The higher-order clustering framework then reveals the organization of the motif within the C. elegans neuronal network. We find a cluster of 20 neurons in the frontal section with low bi-fan motif conductance (Figure 2B). 4 The cluster shows a way that nictation is controlled. Within the cluster, ring motor neurons (RMEL/V/R), proposed pioneers of the nerve ring (21), propagate information to IL2 neurons, regulators of nictation (22), through the neuron RIH and several inner labial sensory neurons (Figure 2C). Our framework contextualizes the sifnifance of the bi-fan motif in this control mechanism. The framework also provides new insights into network organization beyond the clustering of nodes based only on edges. Results on a transportation reachability network (23) demonstrate how it finds the essential hub interconnection airports (Figure 3). These appear as extrema on the primary spectral direction (Figure 3C) when two-hop motifs (Figure 3A) are used to capture highly connected nodes and non-hubs. (The first spectral coordinate of the normalized motif Laplacian embedding was positively correlated with the airport citys metropolitan population with Pearson correlation 99% confidence interval [0.33, 0.53]). The secondary spectral direction identified the West-East geography in the North American flight network (it was negatively correlated with the airport citys longitude with Pearson correlation 99% confidence interval [-0.66, -0.50]). On the other hand, edge-based methods conflate geography and hub structure. For example, Atlanta, a large hub, is embedded next to Salina, a non-hub, with an edge-based method (Figure 3D). Our higher-order network clustering framework unifies motif analysis and network partitioning two fundamental tools in network scienceand reveals new organizational patterns and modules in complex systems. Prior efforts along these lines do not provide worst-case performance guarantees on the obtained clustering (24), do not reveal which motifs organize the network (25), or rely on expanding the size of the network (26, 27). Theoretical results in the Supplementary Material (13) also explain why classes of hypergraph partitioning methods are more general than previously assumed and how motif-based clustering provides a rigorous framework for the special case of partitioning directed graphs. Finally, the higher-order net- 5 work clustering framework is generally applicable to a wide range of networks types, including directed, undirected, weighted, and signed networks. 6 7 8 9\nAll data is available at our project web site at The web site includes links to datasets used for experiments throughout the supplementary material (7, 56, 5860, 7483). S68 References and Notes 1. R. Milo, et al., Science 298, 824 (2002). 2. S. Mangan, A. Zaslaver, U. Alon, Journal of molecular biology 334, 197 (2003). 3. J. Yang, J. Leskovec, Proceedings of the IEEE 102, 1892 (2014). 4. P. W. Holland, S. Leinhardt, American Journal of Sociology pp. 492513 (1970). 5. M. Rosvall, A. V. Esquivel, A. Lancichinetti, J. D. West, R. Lambiotte, Nature communi- cations 5 (2014). 6. N. Przulj, D. G. Corneil, I. Jurisica, Bioinformatics 20, 3508 (2004). 7. J. Leskovec, K. J. Lang, A. Dasgupta, M. W. Mahoney, Internet Mathematics 6, 29 (2009). 8. O. N. Yaveroglu, et al., Scientific reports 4 (2014). 9. S. Mangan, U. Alon, Proceedings of the National Academy of Sciences 100, 11980 (2003). 10. C. J. Honey, R. Kotter, M. Breakspear, O. Sporns, Proceedings of the National Academy of Sciences 104, 10240 (2007). 11. S. E. Schaeffer, Computer Science Review 1, 27 (2007). 12. Minimizing M(S) is NP-hard, which follows from the NP-hardness of the traditional def- inition of conductance (68). 13. See the Supplementary Material. 14. Formally, when the motif has three nodes, the selected cluster S satisfies M(S)  4  M  1, where M is the smallest motif conductance of any possible node set S. This inequality is proved in the Supplementary Material. S69 15. The normalized motif Laplacian matrix is LM = D1/2(DWM)D1/2, where D is a di- agonal matrix with the row-sums of WM on the diagonal (Dii =  j(WM)ij), and D 1/2 is the same matrix with the inverse square-roots on the diagonal (D1/2ii = 1/  j(WM)ij). The spectral ordering  is the by-value ordering of D1/2z, where z is the eigenvector corresponding to the second smallest eigenvalue of LM , i.e., i is the index of D1/2z with the ith smallest value. 16. C. Seshadhri, A. Pinar, T. G. Kolda, Statistical Analysis and Data Mining: The ASA Data Science Journal 7, 294 (2014). 17. R. Andersen, F. Chung, K. Lang, Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (2006), pp. 475486. 18. J. J. Whang, I. S. Dhillon, D. F. Gleich, SIAM Data Mining (2015). 19. A. Y. Ng, M. I. Jordan, Y. Weiss, Advances in Neural Information Processing Systems 14 (2002), pp. 849856. 20. D. Boley, Data Mining and Knowledge Discovery 2, 325 (1998). 21. D. L. Riddle, T. Blumenthal, B. J. Meyer, et al., eds., C. elegans II (Cold Spring Harbor Laboratory Press, 1997), second edn. 22. H. Lee, et al., Nature neuroscience 15, 107 (2012). 23. B. J. Frey, D. Dueck, Science 315, 972 (2007). 24. B. Serrour, A. Arenas, S. Gomez, Computer Communications 34, 629 (2011). 25. T. Michoel, A. Joshi, B. Nachtergaele, Y. Van de Peer, Molecular BioSystems 7, 2769 (2011). S70 26. A. R. Benson, D. F. Gleich, J. Leskovec, SIAM Data Mining (2015). 27. F. Krzakala, et al., Proceedings of the National Academy of Sciences 110, 20935 (2013). 28. M. Kaiser, C. C. Hilgetag, PLoS Computational Biology 2, e95 (2006). 29. U. Alon, Nature Reviews Genetics 8, 450 (2007). 30. O. Sporns, R. Kotter, PLoS Biology 2, e369 (2004). 31. A. Inokuchi, T. Washio, H. Motoda, Principles of Data Mining and Knowledge Discovery (Springer, 2000), pp. 1323. 32. F. R. Chung, Proceedings of ICCM (Citeseer, 2007), vol. 2, p. 378. 33. J. R. Lee, S. O. Gharan, L. Trevisan, Journal of the ACM 61, 37 (2014). 34. F. Chung, Annals of Combinatorics 9, 1 (2005). 35. D. Boley, G. Ranjan, Z.-L. Zhang, Linear Algebra and its Applications 435, 224 (2011). 36. F. D. Malliaros, M. Vazirgiannis, Physics Reports 533, 95 (2013). 37. G. Karypis, R. Aggarwal, V. Kumar, S. Shekhar, Very Large Scale Integration (VLSI) Sys- tems, IEEE Transactions on 7, 69 (1999). 38. S. Agarwal, K. Branson, S. Belongie, Proceedings of the 23rd International Conference on Machine Learning (ACM, 2006), pp. 1724. 39. D. Zhou, J. Huang, B. Scholkopf, Advances in Neural Information Processing Systems 19 (MIT Press, 2006), pp. 16011608. 40. J. Rodrguez, Linear and Multilinear Algebra 50, 1 (2002). S71 41. L. Trevisan, Lecture notes on expansion, sparsest cut, and spectral graph theory, http:// www.eecs.berkeley.edu/luca/books/expanders.pdf. Accessed June 28, 2015. 42. S. Demeyer, et al., PloS ONE 8, e61183 (2013). 43. M. Houbraken, et al., PLoS ONE 9, e97896 (2014). 44. S. Wernicke, IEEE/ACM Transactions on Computational Biology and Bioinformatics 3, 347 (2006). 45. S. Wernicke, F. Rasche, Bioinformatics 22, 1152 (2006). 46. C. R. Aberger, A. Notzli, K. Olukotun, C. Re, arXiv preprint arXiv:1503.02368 (2015). 47. M. Latapy, Theoretical Computer Science 407, 458 (2008). 48. J. W. Berry, et al., Proceedings of the 5th Conference on Innovations in Theoretical Com- puter Science (ACM, New York, NY, USA, 2014), pp. 225234. 49. D. Marcus, Y. Shavitt, IEEE 30th International Conference on Distributed Computing Sys- tems Workshops (2010), pp. 9298. 50. N. Chiba, T. Nishizeki, SIAM Journal on Computing 14, 210 (1985). 51. T. Schank, D. Wagner, Experimental and Efficient Algorithms (Springer, 2005), pp. 606 609. 52. L. Becchetti, P. Boldi, C. Castillo, A. Gionis, Proceedings of the 14th ACM SIGKDD inter- national conference on Knowledge discovery and data mining (ACM, 2008), pp. 1624. 53. J. Cohen, Computing in Science & Engineering 11, 29 (2009). S72 54. B. N. Parlett, The Symmetric Eigenvalue Problem, vol. 7 (SIAM, 1980). 55. K. J. Maschhoff, D. C. Sorensen, Applied Parallel Computing Industrial Computation and Optimization (Springer, 1996), pp. 478486. 56. J. Leskovec, A. Krevl, SNAP Datasets: Stanford large network dataset collection, http: //snap.stanford.edu/data (2014). 57. P. Boldi, B. Codenotti, M. Santini, S. Vigna, Software: Practice and Experience 34, 711 (2004). 58. P. Boldi, S. Vigna, Proceedings of the 13th International Conference on World Wide Web (ACM, 2004), pp. 595602. 59. P. Boldi, M. Rosa, M. Santini, S. Vigna, Proceedings of the 20th International Conference on World Wide Web (ACM, 2011), pp. 587596. 60. P. Boldi, A. Marino, M. Santini, S. Vigna, Proceedings of the companion publication of the 23rd international conference on World wide web companion (International World Wide Web Conferences Steering Committee, 2014), pp. 227228. 61. A. Azad, A. Buluc, J. R. Gilbert, Proceedings of the IPDPSW, Workshop on Graph Algo- rithm Building Blocks (GABB) (2015), pp. 804811. 62. M. Rosvall, C. T. Bergstrom, Proceedings of the National Academy of Sciences 105, 1118 (2008). 63. V. D. Blondel, J.-L. Guillaume, R. Lambiotte, E. Lefebvre, Journal of statistical mechanics: theory and experiment 2008, P10008 (2008). S73 64. R. E. Ulanowicz, C. Bondavalli, M. S. Egnotovich, Trophic Dynamics in South Florida Ecosystem, FY 97: The Florida Bay Ecosystem, Tech. Rep. CBL 98-123, Chesapeake Biological Laboratory, Solomons, MD (1998). 65. J. Bascompte, C. J. Melian, E. Sala, Proceedings of the National Academy of Sciences of the United States of America 102, 5443 (2005). 66. J. Bascompte, et al., Science 325, 416 (2009). 67. D. B. Stouffer, J. Camacho, W. Jiang, L. A. N. Amaral, Proceedings of the Royal Society of London B: Biological Sciences 274, 1931 (2007). 68. D. Wagner, F. Wagner, Proceedings of the 18th International Symposium on Mathematical Foundations of Computer Science (1993), pp. 744750. 69. C. D. Manning, P. Raghavan, H. Schutze, et al., Introduction to Information Retrieval, vol. 1 (Cambridge university press Cambridge, 2008). 70. R. Dobrin, Q. K. Beg, A.-L. Barabasi, Z. N. Oltvai, BMC bioinformatics 5, 10 (2004). 71. H. Kwak, C. Lee, H. Park, S. Moon, Proceedings of the 19th International Conference on World Wide Web (ACM, 2010), pp. 591600. 72. T. Chakraborty, N. Ganguly, A. Mukherjee, Advances in Social Networks Analysis and Mining (ASONAM), 2014 IEEE/ACM International Conference on (IEEE, 2014), pp. 130 137. 73. J. Leskovec, J. Kleinberg, C. Faloutsos, ACM Transactions on Knowledge Discovery from Data (TKDD) 1, 2 (2007). S74 74. R. West, H. S. Paskov, J. Leskovec, C. Potts, Transactions of the Association for Computa- tional Linguistics 2, 297 (2014). 75. J. Leskovec, J. Kleinberg, C. Faloutsos, Proceedings of the eleventh ACM SIGKDD inter- national conference on Knowledge discovery in data mining (ACM, 2005), pp. 177187. 76. J. Gehrke, P. Ginsparg, J. Kleinberg, ACM SIGKDD Explorations Newsletter 5, 149 (2003). 77. R. Albert, H. Jeong, A.-L. Barabasi, Nature 401, 130 (1999). 78. J. Leskovec, L. A. Adamic, B. A. Huberman, ACM Transactions on the Web (TWEB) 1, 5 (2007). 79. J. Leskovec, D. P. Huttenlocher, J. M. Kleinberg, ICWSM (2010). 80. J. Leskovec, J. J. Mcauley, Advances in neural information processing systems (2012), pp. 539547. 81. L. Takac, M. Zabovsky, International Scientific Conference and International Workshop Present Day Trends of Innovations (2012), pp. 16. 82. L. Backstrom, D. Huttenlocher, J. Kleinberg, X. Lan, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (ACM, 2006), pp. 4454. 83. J. Yang, J. Leskovec, 2012 IEEE 12th International Conference on Data Mining (IEEE, 2012), pp. 745754. S75 Authors would like to thank Rok Sosic for insightful comments. ARB acknowledges the support of a Stanford Graduate Fellowship. DFG acknowledges the support of NSF CCF1149756 and IIS-1422918 and DARPA SIMPLEX. JL acknowledges the support of NSF IIS1149837 and CNS-1010921, NIH BD2K, DARPA XDATA and SIMPLEX, Boeing, Lightspeed, and Volkswagen. S76\n", "prediction": "we develop a framework for clustering of networks based on higher-order connectivity patterns and show that it provides mathematical guarantees on the optimality of clusters and of scales with billions of edges.<n> the framework shows that clusters of a network can be identified in a generalized way by a set of motifs of the network, which is determined by the number of nodes in the motif and their spectral conductance with respect to the graph in which the cluster is embedded ( e.g. in an airport graph ) in order to maximize the probability of finding a cluster of interest at a later time in that graph ( see figure [ fig : network motif ] for a more general cluster structure and more information about the clustering structure of network motifs ; fig.[fig:network motif]for a general network structure])the framework provides a method to find clusters in networks that are not separable and to identify clusters that can not be distinguished from clusters by other methods of analysis ( for an example of clustering in biological networks see [ s.j.w.h.forscher et al.,2010 _ j. bioinformatique materie et modlisation a ( rensselaerchen - institut de recherche en informatiques physiques et biomtrie des hautes symmetries de @xmath0 auteurs de mouvement de la forme et du prsence du graphe de le graphes de graphie du rie et le  to a la prstique du du ( j et de du le - du j - we are of se cours ( 2, we show the first of which we find the e of graph of j ( @ 2 in graph @ @ j <n> @ ( we have to ( p. 2 ( ( the graphs of graphs ( 1 th e - j and the corresponding graph for graph and @  to e  ( to @ we ( and we to j in graphs and graph e ( graph to be the @ to  for graphs  t ( a graph graphs @ e to which ( 3 to 2  in @ 1 ( x ( 4 to show ( in 2 and e and j @ - to p ( 5 ). to obtain the j we extend to all e for all the second graph - graph we also to graphs for the ( which are to one of e graph graph from graph with graphs to graph i graph is to an e we were to we found the  are the matrix ( i - e- ( from the remaining graph j to determine the to solve the bi- and are graph which @ and all to map ( with the most e @.  we identify the results of all ( graphs with graph 2 - ( of @ the analysis of 2 to see graph matrix  @ from graphs are  the we - and a to k - @<n> to make the distribution of number to and  from j for e from e which to number - graphs in e, and one to two graphs from @ which  and graphs we of  of ( 6 to have the se - which one - from 2. to [ 2 we we obtained the i to are e are from  with e with j are ( - the. we see the p - for which were the and also we showed the hyper- to do the [ 1 -! to to. @ are we  - p to m - with @ p and p  which have graph p from a. ( are also from which which - i. and from p for @ = 2 are @ i and 2 for j j from to x - [ j = j with a number for 2 with "}
{"ground_truth": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.\nGenerative methods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord et al., 2016a), image-to-image translation (Zhu et al., 2017; Liu et al., 2017; Wang et al., 2017), and image inpainting (Iizuka et al., 2017). Currently the most prominent approaches are autoregressive models (van den Oord et al., 2016b;c), variational autoencoders (VAE) (Kingma & Welling, 2014), and generative adversarial networks (GAN) (Goodfellow et al., 2014). Currently they all have significant strengths and weaknesses. Autoregressive models  such as PixelCNN  produce sharp images but are slow to evaluate and do not have a latent representation as they directly model the conditional distribution over pixels, potentially limiting their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improving this (Kingma et al., 2016). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress (Salimans et al., 2016; Gulrajani et al., 2017; Berthelot et al., 2017; Kodali et al., 2017). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani & Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016). Typically, a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest  the discriminator is an adaptive loss function that gets discarded once the generator has been trained. There are multiple potential problems with this formulation. When we measure the distance between the training distribution and the generated distribution, the gradients can point to more or less random directions if the distributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky & Bottou, 2017). Originally, Jensen-Shannon divergence was used as a distance metric (Goodfellow et al., 2014), and recently that formulation has been improved (Hjelm et al., 2017) and a number of more stable alternatives have been proposed, including least squares (Mao et al., 2016b), absolute deviation with margin (Zhao et al., 2017), and Wasserstein distance (Arjovsky et al., 2017; Gulrajani ar X iv :1 71 0. 10 19 6v 3 [ cs .N E ] 2 6 Fe b 20 18 et al., 2017). Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improved Wasserstein loss, but also experiment with least-squares loss. The generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apart from training images (Odena et al., 2017), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraints, further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, starting from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section 2. The GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation, but that view has been recently challenged (Odena et al., 2017). The degree of preserved variation is currently receiving attention and various methods have been suggested for measuring it, including inception score (Salimans et al., 2016), multi-scale structural similarity (MS-SSIM) (Odena et al., 2017; Wang et al., 2003), birthday paradox (Arora & Zhang, 2017), and explicit tests for the number of discrete modes discovered (Metz et al., 2016). We will describe our method for encouraging variation in Section 3, and propose a new metric for evaluating the quality and variation in Section 5. Section 4.1 discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layers. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section 4.2). We evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets. We improve the best published inception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CELEBA dataset that allows experimentation with output resolutions up to 1024  1024 pixels. This dataset and our full implementation are available at trained networks can be found at along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at the quality of our results is generally high compared to earlier work on GANs, and the training is stable in large resolutions, there is a long way to true photorealism. Semantic sensibility and understanding dataset-dependent constraints, such as certain objects being straight rather than curved, leaves a lot to be desired. There is also room for improvement in the micro-structure of the images. That said, we feel that convincing realism may now be within reach, especially in CELEBA-HQ.\n", "prediction": "we describe attention for new methodology for training adversarial networks. <n> image denoising is a fundamental problem in computer vision, which has seen rapid progress in the last few decades ( see for example the seminal papers of dena et al ( 2012 ) and jelmstein ( 2014 ; al : arxiv:1412.08396v1 ] ( to be published in _ proceedings of the ieee international conference on modalities in natural and social systems ( lics 2016 )(v30 - v30 and v40 - i40 for the original manuscript and the proofs of principle of results ), see also the paper of denord & schapiro ( 2015  variational autoencoders for low - resolution images '' ( j. acm nato jp 16(2): 759 - 762 ]) ).<n> it has recently been shown that it is possible to achieve a significant improvement over the state - of - the - art by using a network of @xmath0 adversarial discriminators that are trained on a set of high - quality images of arbitrary shape and size (see for instance the article of tanaka & iguchi ( 2016a proc natl acad sci usa 1)(v31 - 1a1a2a3a4 for a more extensive review of this topic and a complete list of papers on the subject see e.g.(4a1,4b1,3c1,2b2 and 3a2,2 t3b1 and 2a3,3 and 4b3 ( a.2 ( iv3 ar4 ( al and iv ( ( van denor etal ( the first ( 2014) ( in 2014 ( doi jun th rb3 alexander & al 2014 and al al 2016 ( we discuss the video ( [ 2. 2 ( 3. ( 2 evan ( 2013 com ( 4. 3 ( 6. 4 ( et a 2014 to the second ( p.<n> a tam ( m et rie ( @ al - a 2016 to a 2012 ( based ( an et et 2012 and ( b ( d (... (<n> al 2012 to al. [ 6  ( c. 6 ( 8  910 ( 2010 ( 2011  to ... ve ( http:// 2014 [ 4! ( rev ( 2008  [ ]. a et [ 3 [ [ 8. 1]) and [ a4 [ 1 ][2 [... [ 9 ], [ 5. 8]) to 4 [ 7. 7 [4 ]] ( 9] to 8] [ 10.... a 4] -2 ( 1] and we refer to 7] for 4]. [])<n> et... to 2 [[[2 2[6 ( 7]. 2]) ( r. 5]... 4[8 [] a 2]. 0] ( e teo et].<n> [ p p[3 [2]([2]]]. 4 to 3[b et p]] 2...... 2] 4]) for 3]. to 6[7 [ 11] in a... 3])]) [ to [ et]<n> ( i. p...<n> 4... et[i -4 -3 2] struct (e et 4 and... p ia ( x-[n ( 5 [<n>... the 4][4 to make a ([a []. 3] 3  in 4(r [ 12] the 3 to 5[5 4][3]  2 to 9[4]((...[e][12]], a]]) in 2][[9 [3]](a...]...."}
{"ground_truth": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.\nConvolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsitythat each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharingthat the same filter is applied to each patch. However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6]. In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7]. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm. In this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying Computer Science Department, Stanford University, Stanford, CA 94305. Email: zhangyuc@cs.stanford.edu. Computer Science Department, Stanford University, Stanford, CA 94305. Email: pliang@cs.stanford.edu. Department of Electrical Engineering and Computer Science and Department of Statistics, University of California Berkeley, Berkeley, CA 94720. Email: wainwrig@eecs.berkeley.edu. ar X iv :1 60 9. 01 00 0v 1 [ cs .L G ] 4 S ep 2 01 two-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Further relaxing the low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs. On the theoretical front, we prove an oracle inequality on generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite dataa quantity to which we refer as the oracle riskplus a model complexity term that decays to zero polynomially in the sample size. Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. We then apply CCNNs to the MNIST handwritten digit dataset as well as four variation datasets [43], and find that it achieves the state-of-the-art performance. On the CIFAR-10 dataset, CCNNs outperform CNNs of the same depths, as well as other baseline methods that do not involve nonconvex optimization. We also demonstrate that building CCNNs on top of existing CNN filters improves the performance of CNNs. The remainder of this paper is organized as follows. We begin in Section 2 by introducing convolutional neural networks, and setting up the empirical risk minimization problem studied in this paper. In Section 3, we describe the algorithm for learning two-layer CCNNs, beginning with the simple case of convexifying CNNs with a linear activation function, then proceeding to convexify CNNs with a nonlinear activation. We show that the generalization error of a CCNN converges to that of the best possible CNN. In Section 4, we describe several extensions to the basic CCNN algorithm, including averaging pooling, multi-channel input processing, and the layer-wise learning of multi-layer CNNs. In Section 5, we report the empirical evaluations of CCNNs. We survey related work in Section 6 and conclude the paper in Section 7. Notation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let A be its nuclear norm, A2 be its spectral norm (i.e., maximal singular value), and AF be its Frobenius norm. We use `2(N) to denote the set of countable dimensional vectors v = (v1, v2, . . . ) such that  `=1 v 2 ` < . For any vectors u, v  `2(N), the inner product u, v :=  `=1 uivi and the `2-norm u2 :=  u, u are well defined.\nIn this paper, we have shown how convex optimization can be used to efficiently optimize CNNs as well as understand them statistically. Our convex relaxation consists of two parts: the nuclear norm relaxation for handling parameter sharing, and the RKHS relaxation for handling non-linearity. For the two-layer CCNN, we proved that its generalization error converges to that of the best possible two-layer CNN. We handled multi-layer CCNNs only heuristically, but observed that adding more layers improves the performance in practice. On real data experiments, we demonstrated that CCNN outperforms the traditional CNN of the same depth, is computationally efficient, and can be combined with the traditional CNN to achieve better performance. A major open problem is to formally study the convex relaxation of deep CNNs.\n", "prediction": "we introduce the class of convexified convolutional neural networks ( cs ), which capture bounded parameter sharing of neural network filters in convex manner.<n> we prove that convexification converges to convex optimization and that the best possible performance can be achieved by this algorithm along with the standard approach of solving a nonconvex optimization problem ( that is : stochastic backpagation in a fully - connected layerwise manner in which the network parameters are represented as low -rank matrices in the reproducing kernel space of the input and output vectors ; the filters are the same as the low-rank matrix for the output vector and the filter s spectral norm is equal to the norm of its input vectors ( which is a nonlinear activation function for which we refer to this problem as a simple oracle riskplus model complexity term in this paper and show that it is polynomially efficient for this model to converge to a solution of any problem in its domain of applicability )(i.e. @xmath0 the number of filters per input vector ] is lower than that of greedy filters and is higher than the optimality of existing methods for learning convexly -connected neural nets <n> _ we demonstrate that our model has two desirable features that make it appealing for use in machine learning and artificial intelligence tasks such as image classification and face recognition ( see [28 ], text berkeley:  wain 00.1 edu@berkeley.edu, 1 epedu.the paper [48], find empirical results for empirical error of empirical performance on empirical data using empirical training depth of two empirical layer-[48 ( i. 1 u=1 u= u1 and two layers in [ 48 t [[[a ]) and [u th [2 [ u[n[i and a layer [n [i ].[u[v [ii ii [a [ii[(i[d[in [([two [ [ 2.[2[the problem [4 [. [ ii [ ( [  the upper layer of [... [ v  [ 1 [] ]]  to [ a [ i ][[iii [ the [ that we show the model [ which [ we have the problem of a two [ b  for two layer  in two  that [ p   ( we find the empirical [ two is the most [ and ...  ii  as [ one [ it  with [ 4 [ @ [2]  and we are [vi [])  is [ as  a! [].  while the results [ n  which  we  it [ to  from the]).  this  @ ). the first [ in our analysis of which @. the @[vi ( ii] and @ @] the layer @ ii. ( the algorithm [, [ c (ii] we is convex @ v.... the analysis [ x ), [viii  - [ 3  by [v [], the function [ iii -]]. the nonlinear [ with a linear]) for [vii  v [ iv vii] for a model @]) to which to @) the] in that @...... a.] to obtain the linear @ ( @v ( v- [ + [x -2 [ * viii]], while we has a convex [) and... @ the]..], we) for all the], which]. [<n> [iii], @ -[e @[it  * [ an [4] ]: 2].]. to...], [ whose [ - @], a) to search of nonlinear @-(v - which] ([]] as that "}
{"ground_truth": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.\nA machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavor, from performing menial jobs for us to helping the advancement of basic and applied research. Given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines. Still, since solving AI seems too complex a task to be pursued all at once, in the last decades the computational community has preferred to focus on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose intelligent machines. In this article, we propose an alternative approach: we first define the general characteristics we think intelligent machines should possess, and then we present a concrete roadmap to develop them in realistic, small steps, that are however incrementally structured in such a way that, jointly, they should lead us close to the ultimate goal of implementing a powerful AI. The article is organized as follows. In Section 2 we specify the two fundamental characteristics that we consider crucial for developing intelligenceat least the sort of intelligence we are interested innamely communication and learning. Our goal is ar X iv :1 51 1. 08 13 0v 2 [ cs .A I] 2 6 Fe b to build a machine that can learn new concepts through communication at a similar rate as a human with similar prior knowledge. That is, if one can easily learn how subtraction works after mastering addition, the intelligent machine, after grasping the concept of addition, should not find it difficult to learn subtraction as well. Since, as we said, achieving the long-term goal of building an intelligent machine equipped with the desired features at once seems too difficult, we need to define intermediate targets that can lead us in the right direction. We specify such targets in terms of simplified but self-contained versions of the final machine we want to develop. At any time during its education, the target machine should act like a stand-alone intelligent system, albeit one that will be initially very limited in what it can do. The bulk of our proposal (Section 3) thus consists in the plan for an interactive learning environment fostering the incremental development of progressively more intelligent behavior. Section 4 briefly discusses some of the algorithmic capabilities we think a machine should possess in order to profitably exploit the learning environment. Finally, Section 5 situates our proposal in the broader context of past and current attempts to develop intelligent machines. As that review should make clear, our plan encompasses many ideas that have already appeared in different research strands. What we believe to be novel in our approach is the way in which we are combining such ideas into a coherent program.\nWe defined basic desiderata for an intelligent machine, stressing learning and communication as its fundamental abilities. Contrary to common practice in current machine learning, where the focus is on modeling single skills in isolation, we believe that all aspects of intelligence should be holistically addressed within a single system. We proposed a simulated environment that requires the intelligent machine to acquire new facts and skills through communication. In this environment, the machine must learn to perform increasingly more ambitious tasks, being naturally induced to develop complex linguistic and reasoning abilities. We also presented some conjectures on the properties of the computational system that the intelligent machine may be based on. These include learning of algorithmic patterns from a few examples without strong supervision, and development of a longterm memory to store both data and learned skills. We tried to put this in contrast with currently accepted paradigms in machine learning, to show that current methods are far from adequate, and we must strive to develop non-incrementally novel techniques. This roadmap constitutes only the beginning of a long journey towards AI, and we hope other researchers will be joining it in pursuing the goals it outlined.\n", "prediction": "development of intelligent machines is one of the biggest unsolved challenges in computer science.<n> this paper we propose some fundamental properties these machines should have focusing on communication and learning in simple environment that could be used to machine-based learning and discuss some conjectures on algorithms that support in order to learn from a single system that is capable of performing complex tasks without requiring laborious programming : the machine should be based on isolation, it should act like a stand- alone intelligent system and should behave as human if it were able to perform subtraction with a high degree of accuracy that it can easily learn if one can grasp a concept of addition as a human prior knowledge is. <n> [ [ section ] ) we first define the general characteristics of an intelligent machine and then we present a concrete roadmap to develop them in realistic small steps that that are such that jointly they should lead us close to the ultimate goal of developing an intelligence that we hope will be very useful in almost any human endeavor from performing menial jobs for us to helping advancement of powerful research and applied research as well as widespread interest in sophisticated machine learning methods that include learning of algorithmic patterns and data subtraction ; these methods are widely used in computational community to solve relatively narrow empirical problems that important applications but do not address the goal for developing general-purpose intelligent devices that should last for at least decades<n> we address our goal as an alternative approach that situates in which we are combining ideas into basic desiderata for a coherent program we combine into a machine in a program that in the program and that the coherent programs that to combine it that which is that  machine _ we should combine in small small to be combined in coherent and to make small and we have to build small s that only combining small in it with the algorithm that by small by a small that with small for the individual and in that and it and the most and only by that have only only to support the algorithmic algorithm and by the learning that also the only and those that not to all that will not by and and also that can be the other algorithm in and together and all to which are to and with and which to do and they are that together to both to have and a and are the algorithms and so to not that all by those to we will also by by all the existing and of a new algorithm is to that those and these and while the and not the the to those those in communication that results and where that does and will to are in all and most that they also to  and  to to by which that as the structure that makes the more that of and one that has to any and some and as that leads to most of those of complex and from the [ and [ the new and is the way that most to a more and any to more to learning by any algorithm to will by most by we also also in learning to [ a most most in [ some of communication to is a while that includes the interaction that a to where the while and communication by is in any of which the complex [ while while also a complex that means that one is not and such as and for communication of any other that helps that [ that was also with that for learning  the data and this is also of learning from communication from which also is by more of that among the environment where a the communication with communication  that these are a way to communicate and even the information of data that<n> and especially the [[(([ii [ which will the basic [ with those [[[1 [ is of [ it is most [ by [ one and algorithm[ [ they will [ in most the first [ algorithm [ to further that plays that ( [ as to structure of more [ t that"}
{"ground_truth": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.\nProgress on the path from shallow bag-of-words information retrieval algorithms to machines capable of reading and understanding documents has been slow. Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2]. Supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets, and the difficulty in structuring statistical models flexible enough to learn to exploit document structure. While obtaining supervised natural language reading comprehension data has proved difficult, some researchers have explored generating synthetic narratives and queries [3, 4]. Such approaches allow the generation of almost unlimited amounts of supervised data and enable researchers to isolate the performance of their algorithms on individual simulated phenomena. Work on such data has shown that neural network based models hold promise for modelling reading comprehension, something that we will build upon here. Historically, however, many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments, as such closed worlds inevitably fail to capture the complexity, richness, and noise of natural language [5]. In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set. We observe that summary and paraphrase sentences, with their associated documents, can be readily converted to contextqueryanswer triples using simple entity detection and anonymisation algorithms. Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites. We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension. These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures [6, 7, 8, 4]. This allows a model to focus on the aspects of a document that it believes will help it answer a question, and also allows us to visualises its inference process. We compare these neural models to a range of baselines and heuristic benchmarks based upon a traditional frame semantic analysis provided by a state-of-the-art natural language processing ar X iv :1 50 6. 03 34 0v 3 [ cs .C L ] 1 9 N ov 2 (NLP) pipeline. Our results indicate that the neural models achieve a higher accuracy, and do so without any specific encoding of the document or query structure.\nThe supervised paradigm for training machine reading and comprehension models provides a promising avenue for making progress on the path to building full natural language understanding systems. We have demonstrated a methodology for obtaining a large number of document-queryanswer triples and shown that recurrent and attention based neural networks provide an effective modelling framework for this task. Our analysis indicates that the Attentive and Impatient Readers are able to propagate and integrate semantic information over long distances. In particular we believe that the incorporation of an attention mechanism is the key contributor to these results. The attention mechanism that we have employed is just one instantiation of a very general idea which can be further exploited. However, the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not scale linearly with the data set size. There are still many queries requiring complex inference and long range reference resolution that our models are not yet able to answer. As such our data provides a scalable challenge that should support NLP research into the future. Further, significantly bigger training data sets can be acquired using the techniques we have described, undoubtedly allowing us to train more expressive and accurate models. 7Note that these examples were chosen as they were short, the average CNN validation document contained 763 tokens and 27 entities, thus most instances were significantly harder to answer than these examples.\n", "prediction": "this summary addresses recent advances on the comprehension of sentences in natural language.<n> we introduce a novel approach to data set developments : two new benchmarks were collected from news stories with queries associated with websites and we demonstrate the efficacy of our new corpora by building models for reading models that incorporate recent mechanisms for obtaining large amounts of comprehension data and allowing us to develop a class of attention based neural networks that learn to read real language structure with minimal knowledge of the embedding of semantic information over long distances and particular we employ an attention mechanism that will allow a model to focus on a document without any specific encoding and so allow its inference process to be compared to a traditional frame of inference based upon a 34 range of baselines and heuristic analysis provided by 34-out of a set of million sentences with an associated document that can be readily converted into tripleanswers using simple entity detection and queryisation algorithms using a recurrent mechanism for extracting context information from the document s text, thus indicate that this approach provides a methodology for making progress on supervised reading systems for machine comprehension and machine reading comprehension that have the ability to answer questions posed on large scale documents and that has the potential to address the long standing challenge of obtaining a large amount of real reading data with a small number of features and a low rate of outliers [ 3 ] and two benchmarks that demonstrate that these models are able to achieve high accuracy on reading queries and comprehension by introducing a state-of-(v ) process [ 4 ; 4 [ 5 [ 6 (4 2,3,4] [ 7 _ 4_[5 [ 1 ]. [ 2 ][3!<n> -2 -3 -4 -6 <n> 3_ 2_ 3. -5 [4 4 4 2 2. 2 [ [<n> 1] 2 1_ 1 2 3 2 4 3 3 4 5  4 + [ 0_  2 + 2 5 2 0 -0  3 + 3 1 1 4 = 0 + 0 0 1 + 1 3 - 2 - 4 1 0 2 = 3 0 - 3 5 + + 4 - 1 [  5 5 - 0 3 = 1 = 2<n> 0 = 4 0 4 6 [ + we will [ 8 "}
{"ground_truth": "Modern online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, s-scale RPCs, aggravating the need for faster inter-server communication. In reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level protocols (e.g., InfiniBand/RDMA) and new architectures with fully integrated Network Interfaces (NIs). Such architectures offer a unique opportunity for a new NI-driven approach to balancing RPCs among the cores of manycore server CPUs, yielding major tail latency improvements for s-scale RPCs. We introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValets RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4, and reduces tail latency before saturation by up to 4 for RPCs with s-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system. ACM Reference Format: Alexandros Daglis, Mark Sutherland, and Babak Falsafi. 2019. RPCValet: NI-Driven Tail-Aware Balancing of s-Scale RPCs. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS 19), April 1317, 2019, Providence, RI, USA.ACM, New York, NY, USA, 14 pages. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS 19, April 1317, 2019, Providence, RI, USA  2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00 datacenters deliver a breadth of online services to millions of daily users. In addition to their huge scale, online services come with stringent Service Level Objectives (SLOs) to guarantee responsiveness. Often expressed in terms of tail latency, SLOs target the latency of the slowest requests, and thus bound the slowest interaction a user may have with the service. Tail-tolerant computing is one of the major ongoing challenges in the datacenter space, as long-tail events are rare and rooted in convoluted hardware-software interactions. A key contributor to the well-known \"Tail at Scale\" challenge [15] is the deployment of online services software stacks in numerous communicating tiers, where the interactions between a services tiers take the form of Remote Procedure Calls (RPCs). Large-scale software is often built in this fashion to ensure modularity, portability, and development velocity [26]. Not only does each incoming request result in a wide fan-out of inter-tier RPCs [10, 23], each one lies directly on the critical path between the user and the online service [6, 16, 29, 50]. The amalgam of the tail latency problem with the trend towards ephemeral and fungible software tiers has created a challenge to preserve the benefits of multi-tiered software while making it tail tolerant. To lower communication overheads and tighten tail latency, there has been an intensive evolution effort in datacenter-scale networking hardware and software, away from traditional POSIX sockets and TCP/IP and towards lean userlevel protocols such as InfiniBand/RDMA [21] or dataplanes such as IX and ZygOS [7, 47]. Coupling protocol innovations with state-of-the-art hardware architectures such as Firebox [4], Scale-Out NUMA [43] or Mellanoxs BlueField Smart-NIC [37], which offer tight coupling of the network interface (NI) with compute logic, promises even lower communication latency. The net result of rapid advancements in the networking world is that inter-tier communication latency will approach the fundamental lower bound of speedof-light propagation in the foreseeable future [20, 50]. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network. The growing number of cores on server-grade CPUs [36, 38] exacerbates the challenge of distributing incoming RPCs to handler cores. Any delay or load imbalance caused by  This work was done while the author was at EPFL. this initial stage of the RPC processing pipeline directly impacts tail latency and thus overall service quality. Modern NIC mechanisms such as Receive-Side Scaling (RSS) [42] and Flow Direction [24] offer load distribution and connection affinity, respectively. However, the key issue with these mechanisms, which apply static rules to split incoming traffic into multiple receive queues, is that they do not truly achieve load balancing across the servers cores. Any resulting load imbalance after applying these rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with s-scale service times [47, 53]. In this paper, we propose RPCValet, a co-designed hardware and software system to achieve dynamic load balancing across CPU cores, based on the key insight that on-chip NIs offer the ability to monitor per-core load in real time and steer RPCs to lightly loaded cores. The enabler for this style of dynamic load balancing is tight CPU-NI integration, which allows fine-grained, nanosecond-scale communication between the two, unlike conventional PCIe-attached NIs. To demonstrate the benefits of our design, we first classify existing load-distribution mechanisms from both the hardware and software worlds as representative of different queuing models, and show how none of them is able to reach the performance of the theoretical best case. We then design a minimalistic set of hardware and protocol extensions to Scale-Out NUMA (soNUMA) [43], an architecture with on-chip integrated NIs, to show that a carefully architected system can indeed approach the best queuing models performance, significantly outperforming prior load-balancing mechanisms. To summarize, our contributions include:  RPCValet, an NI-driven dynamic load-balancing design that outperforms existing hardware mechanisms for load distribution, and approaches the theoretical maximum performance predicted by queuing models.  Hardware and protocol extensions to soNUMA for native messaging support, a required feature for efficient RPC handling. We find that, in contrast to prior judgment [43], native messaging support is not disruptive to the key premise of NI hardware simplicity, which such architectures leverage to enable on-chip NI integration.  An RPCValet implementation on soNUMA that delivers near-ideal RPC throughput under strict SLOs, attaining within 316% of the theoretically optimal queuing model. For s-scale RPCs, RPCValet outperforms software-based and RSS-like hardware-driven load distribution by 2.3 2.7 and 2976%, respectively. The paper is organized as follows: 2 outlines the performance differences between multi- and single-queue systems, highlighting the challenges in balancing incoming RPCs with short service times among cores. 3 presents RPCValets design principles, followed by an implementation using soNUMA as a base architecture in 4. We detail our methodology in 5 and evaluate RPCValet in 6. Finally, we discuss related work in 7 and conclude in 8.\nWe introduced RPCValet, anNI-driven dynamic load-balancing mechanism fors-scale RPCs. RPCValet behaves like a singlequeue system, without incurring the synchronization overheads typically associated with single-queue implementations. RPCValet performs within 315% of the ideal singlequeue system and significantly outperforms current RPC load-balancing approaches.\n", "prediction": "this paper presents a solution to the online queueing problem, which is to find a way to balance the workload of a large number of servers without incurring excessive overheads.<n> we show that : a ) the problem of balancing the load of many servers is fundamentally different from the problems ofbalancing the loads of single servers ; b ] the solution of this problem is not unique and can not be obtained by a predefined solution for a given problem set and target load s characteristics <n> a special case of our solution is that of minimizing the total delay of the entire system ( i.e. without any degradation in performance of any single server and no degradation of its support for the workloads of all the servers in the system)@xmath0 for any arbitrary target workload and any input / output protocol 1,inmathcal trrange2ensuremath-textifpperoversetioplusbccc(sigma+b)-1$]a solution that does not incur any hardware overhead and that achieves near - optimal performance under any set of input and output protocols that are designed to support multiple incoming cores and multiple outgoing cores ( e.g. hardware- and software-based solutions for single-queues in [43-]the key enablers to achieve performance within 315% of service times ( [22-[43],[22,23]), so that we introduce the key to @x[42 [[23]we introduce a key [42[[27[21[s[32[47[4[3[52[53[n[a[b[t[30[ 43 ]) _[][26] [ 43] to obtain [ 42[28[40[i[we[29] and [s [27 [ [] ]. [ 23[in [ 22[ [t [... [30 [32 [ 26] in a [ 41 [ 28] that [ 27] we to [ 33] a. [ th [ to introduce [ 47  to... [to [n [ 32[se  ][[re [ that to ...... to we ], [ 20 [ a] with the [ with [.] for [ p[r [ we... we we [ '[e [ in our [ 21  that results to a new []. to that the results [ 30  [<n> [ pj[m [se to further [ and we will  we extend [ 52  in  a to which [ the  into the first [ software [ which results from [ j. to add [ results that is the software that will [ @[... that in which we also [, [ 2  the]... in that that a results with a we are to determine that was [ 3! [ while [ ste  with  from com [ into [ 5 [ who to our results for our analysis of [ one [ from a result that yields that further  results levi  for  which to assist [ it to all [ so [ further to another [ for that our to an [ - [ e  found that which ). [ is deb [ ( ie [ multiple [ as [ salesperson [ye [ not to p  while  who ]] to do that p. we find that all to compare [ solver  and  is [ analysis to others to other [ algorithm to solve [ x- that has  it  by [ simulation [ result  how [ another jacob  others  observed that it results the other to new that also "}
{"ground_truth": "Databases can provide scalability by partitioning data across several servers. However, multi-partition, multi-operation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many realworld systems avoid mechanisms that provide useful semantics for multi-partition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation modelRead Atomic (RA) isolationthat matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transactions updates are observed by other transactions. We present algorithms for Read Atomic MultiPartition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via synchronization independence), and minimized communication between servers (via partition independence). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multi-versioning and by allowing clients to independently resolve non-atomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overheadeven under high contentionand scale linearly to 100 servers.\nFaced with growing amounts of data and unprecedented query volume, distributed databases increasingly split their data across multiple servers, or partitions, such that no one partition contains an entire copy of the database [7,13,18,19,22,29,43]. This strategy succeeds in allowing near-unlimited scalability for operations that access single partitions. However, operations that access multiple partitions must communicate across serversoften synchronously in order to provide correct behavior. Designing systems and algorithms that tolerate these communication delays is a difficult task but is key to maintaining scalability [17, 28, 29, 35]. In this work, we address a largely underserved class of applications requiring multi-partition, atomically visible1 transactional access: cases where all or none of each transactions effects should be visible. The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that 1Our use of atomic (specifically, Read Atomic isolation) concerns all-or-nothing visibility of updates (i.e., the ACID isolation effects of ACID atomicity; Section 3). This differs from uses of atomicity to denote serializability [8] or linearizability [4]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD14, June 2227, 2014, Snowbird, UT, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2376-5/14/06 ...$15.00. are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. Many of the largest modern, real-world systems opt for protocols that guarantee fast and scalable operation but provide fewif anytransactional semantics for operations on arbitrary sets of data items [11, 13, 15, 22, 26, 38, 44]. This results in incorrect behavior for use cases that require atomic visibility, including secondary indexing, foreign key constraint enforcement, and materialized view maintenance (Section 2). In contrast, many traditional transactional mechanisms correctly ensure atomicity of updates [8, 17, 43]. However, these algorithmssuch as two-phase locking and variants of optimistic concurrency controlare often coordination-intensive, slow, and, under failure, unavailable in a distributed environment [5, 18, 28, 35]. This dichotomy between scalability and atomic visibility has been described as a fact of life in the big cruel world of huge systems [25]. The proliferation of non-transactional multi-item operations is symptomatic of a widespread fear of synchronization at scale [9]. Our contribution in this paper is to demonstrate that atomically visible transactions on partitioned databases are not at odds with scalability. Specifically, we provide high-performance implementations of a new, non-serializable isolation model called Read Atomic (RA) isolation. RA ensures that all or none of each transactions updates are visible to others and that each transaction reads from an atomic snapshot of database state (Section 3)this is useful in the applications we target. We subsequently develop three new, scalable algorithms for achieving RA isolation that we collectively title Read Atomic Multi-Partition (RAMP) transactions (Section 4). RAMP transactions guarantee scalability and outperform existing atomic algorithms because they satisfy two key scalability constraints. First, RAMP transactions guarantee synchronization independence: one clients transactions cannot cause another clients transactions to stall or fail. Second, RAMP transactions guarantee partition independence: clients never need to contact partitions that their transactions do not directly reference. Together, these properties ensure guaranteed completion, limited coordination across partitions, and horizontal scalability for multi-partition access. RAMP transactions are scalable because they appropriately control the visibility of updates without inhibiting concurrency. Rather than force concurrent reads and writes to stall, RAMP transactions allow reads to race writes: RAMP transactions can autonomously detect the presence of non-atomic (partial) reads and, if necessary, repair them via a second round of communication with servers. To accomplish this, RAMP writers attach metadata to each write and use limited multi-versioning to prevent readers from stalling. The three algorithms we present offer a trade-off between the size of this metadata and performance. RAMP-Small transactions require constant space (a timestamp per write) and two round trip time delays (RTTs) for reads and writes. RAMP-Fast transactions require metadata size that is linear in the number of writes in the transaction but only require one RTT for reads in the common case and two in the worst case. RAMP-Hybrid transactions employ Bloom filters [10] to provide an intermediate solution. Traditional techniques like locking couple atomic visibility and mutual exclusion; RAMP transactions provide the benefits of the former without incurring the scalability, availability, or latency penalties of the latter. In addition to providing a theoretical analysis and proofs of correctness, we demonstrate that RAMP transactions deliver in practice. Our RAMP implementation achieves linear scalability to over 7 million operations per second on a 100 server cluster (at overhead below 5% for a workload of 95% reads). Moreover, across a range of workload configurations, RAMP transactions incur limited overhead compared to other techniques and achieve higher performance than existing approaches to atomic visibility (Section 5). While the literature contains an abundance of isolation models [2, 5], we believe that the large number of modern applications requiring RA isolation and the excellent scalability of RAMP transactions justify the addition of yet another model. RA isolation is too weak for some applications, but, for the many that it can serve, RAMP transactions offer substantial benefits.\nThis paper described how to achieve atomically visible multipartition transactions without incurring the performance and availability penalties of traditional algorithms. We first identified a new isolation levelRead Atomic isolationthat provides atomic visibility and matches the requirements of a large class of real-world applications. We subsequently achieved RA isolation via scalable, contention-agnostic RAMP transactions. In contrast with techniques that use inconsistent but fast updates, RAMP transactions provide correct semantics for applications requiring secondary indexing, foreign key constraints, and materialized view maintenance while maintaining scalability and performance. By leveraging multi-versioning with a variable but small (and, in two of three algorithms, constant) amount of metadata per write, RAMP transactions allow clients to detect and assemble atomic sets of versions in one to two rounds of communication with servers (depending on the RAMP implementation). The choice of synchronization and partition independent algorithms allowed us to achieve near-baseline performance across a variety of workload configurations and scale linearly to 100 servers. While RAMP transactions are not appropriate for all applications, the many for which they are well suited will benefit measurably. Acknowledgments The authors would like to thank Peter Alvaro, Giselle Cheung, Neil Conway, Aaron Davidson, Mike Franklin, Aurojit Panda, Nuno Preguia, Edward Ribeiro, Shivaram Venkataraman, and the SIGMOD reviewers for their insightful feedback. This research is supported by NSF CISE Expeditions award CCF1139158 and DARPA XData Award FA8750-12-2-0331, the National Science Foundation Graduate Research Fellowship (grant DGE-1106400), and gifts from Amazon Web Services, Google, SAP, Apple, Inc., Cisco, Clearstory Data, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, General Electric, Hortonworks, Huawei, Intel, Microsoft, NetApp, NTT Multimedia Communications Laboratories, Oracle, Samsung, Splunk, VMware, WANdisco and Yahoo!.\n", "prediction": "we identify a new isolation model for atomic updates that provide high - performance implementations for linear partitioning of databases across multiple servers. <n> our work is motivated by the growing volume of data in distributed databases, the increasing number of partitions for which we have access and the growth of communication protocols across servers that allow us to communicate with each other in order to achieve correct semantics for multipartition operations : this leads to incorrect behavior for applications requiring multiversioning operations on arbitrary sets of transactions ; we present algorithms for achieving high performance for these applications without incurring too much overhead for high contention ( in contrast to many existing algorithms that incur too many overheads per round for replication and maintenance ) and without performing any synchronization ( unlike many other algorithms which enforce synchronization but are slow and unavailable in a distributed environment between scalability and communication mechanisms [ 2,3,4,5 ] and [ 6,8,10 ], 5,6,11 ][ 7,8,12][13,14][15,16,17 [18,1922,21 [21,24,25 [24]-cyclic algebra [ 24,26,27,28,30 ]. [25,29,37!<n> [49,50,6?<n> * 50 * ]) s in the real world are not in agreement with the behavior of many real-world applications including secondary indexing and key enforcement [1,1,2,3,3,4,5,6,7... [9,10,12... we provide a linear version of [1,2,...... for [13 [12,14 14 [15 [14, 15 -15, 5,15...[16 [16...16 -16, 16 _16[15 -14, -20 -22 [12 [17... -18, [19 [-15 -19 -23 -21 -4 -5 -6 -2... 15, 2 -3 -26... - [[[12 rie [ [ 15 [ 16 [ 15, ... in [ 5 t  [... while [ 2... ( [ 20 [ 6... 2 [ 10  [ [ * [ 12  for ie ly  to [ 8  in rb  while  that [. [ 9 [ 22 [ (... the [ @ [ 4 [ while we also [ for all [ 3 [ we [ and we guarantee [ that we do [ with [ in all the... that is [ as [ a [ all  the other [ which [ one [ multiple [ several [ including [ two [ the  with all while while the others [ to the analysis [].  as we perform [ even [ it [ is the update [ - while all other... and all... to all we  and  all that was the database ). [ was [ etc  ( we we was for the replication [ j  from [ whose []  which is one ( @ @... with one and @[... which ( com [ many [ p  @  was one for a while that ( the number for several  is all to one  a  by [ who was all all ( for any other and replication of the updates and a replication for multiple updates for one that does [)  of all one to a number [ 7 ), [ 1  - for other  when we performed [2]  until the search [ results  it was to which was in our [ se  * * @ ( which one - to  we were one]. while one while ( *... all in which to @. * while @ a and * and while and...]. and to ( while to we found [ any [).  replication  also  even  among [ some [ none  multiple"}
{"ground_truth": "We are seeing an explosion of uncertain datai.e., data that is more properly represented by probability distributions or estimated values with error bounds rather than exact valuesfrom sensors in IoT, sampling-based approximate computations and machine learning algorithms. In many cases, performing computations on uncertain data as if it were exact leads to incorrect results. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. This paper proposes and evaluates an approach for tackling this challenge in DAG-based data processing systems. We present a framework for uncertainty propagation (UP) that allows developers to modify precise implementations of DAG nodes to process uncertain inputs with modest effort. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. For example, a social network trend analysis application that combines data sampling with UP can reduce execution time by 2.3x when the user can tolerate a maximum relative error of 5% in the final answer. These results demonstrate that our UP framework presents a compelling approach for handling uncertain data in DAG processing.\nData is being produced and collected at a tremendous pace. The need to process this vast amount of data has led to the design and deployment of data processing systems such as MapReduce, Spark and Scope [7, 32, 37]. These frameworks typically allow data processing applications to be expressed as directed acyclic graphs (DAGs) of side-effect free computation nodes, with data flowing through the edges for processing. The frameworks then run applications on clusters of servers, transparently handling issues such as task scheduling, data movement, and fault tolerance. At the same time, there is an urgent need for processing an exploding body of data with uncertainties [4]. For example, data collected using sensors are always estimates that have uncertainties the differences between the estimated and true valuesdue to sensor inaccuracies. Data uncertainties also arise in many other contexts, including probabilistic modeling [10], machine learning [26], approximate storage [29], and the use of sampling-based approximation that produce estimated outputs with error bounds [2, 11]. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings [4]. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. Thus, in this paper, we propose and evaluate a general framework that significantly eases this challenging task. Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains. Our framework is based on techniques that allow programmers to modify precise implementations of DAG computation nodes to handle uncertain inputs with modest effort. Uncertainties can then be propagated locally across each node of the DAG from the point where they are first introduced to the final outputs of the computation. More specifically, we use Differential Analysis (DA) [3] to propagate uncertainties through DAG nodes that are continuous and differentiable functions. For semi-continuous functions, we propagate uncertainties through a combination of DA and Monte Carlo simulation, where our framework automatically selects the appropriate method based on the input distributions and the locations of function discontinuities. For all other function types, we use Monte Carlo simulation. As an example of how a developer uses our framework, suppose a company needs to run a revenue prediction model implemented by a two-node logical DAG 1 shown in Figure 1(a). The first node approximates the number of customers belonging to different age groups in a database using BlinkDB [2]. The second node then computes the revenue as a weighted average, with the (uncertain) weights representing the predicted revenue per customer in a given age group. While the approximation can significantly reduce the execution time of the first node, it produces estimates with uncertainties (error bounds), rather than precise values. A developer can use our proposed framework to handle these uncertainties in the second node by providing the derivatives for the weighted average, which are essentially just the weights, with very few code changes to the precise version. This small amount of additional work will allow the answer to be computed as a distribution rather than an exact value that gives a misleading impression of precision. In particular, a precise answer, e.g., red line in Figure 1(c), may predict high revenue leading to profit while ignoring the left side of the distribution in Figure 1(c), which indicates a significant possibility of low revenue leading to an overall loss. Ignoring this possibility can be dangerous if the company is risk-averse. We implement the proposed framework in UP-MapReduce, an extension of the Hadoop MapReduce, to handle uncertainty propagation (UP). UP-MapReduce allows programmers to develop applications with UP in much the same way as their precise counterparts. Added efforts come in the form of selecting the appropriate uncertain Mapper and Reducer classes provided by UP-MapReduce and respecting some required constraints on code structures (Section 5). Developers can optionally provide closed-form derivatives for DAG nodes that implement UP with DA to enhance performance. We then leverage UP-MapReduce to build a toolbox of operations (e.g., sum, multiply, logarithm) on uncertain data and modify ten applications, including AI/ML, image processing, trend analysis, 1As explained in Section 4, small logical DAGs will often map to extremely large execution DAGs with thousands of execution nodes running on large server clusters when processing large data sets. and model construction applications, to process uncertain data. Our experience shows that UP-MapReduce is easy to use. Running two of these applications on real data sets demonstrates the tremendous potential for combining sampling-based approximation (early in the DAG) with UP to reduce execution time while properly propagating the introduced uncertainties to the final outputs. This propagation allows users to intelligently trade off accuracy for execution time. For example, in one application, execution time can be reduced by 2.3x if the user can tolerate errors of up to 5%. Further, in one of the two applications, the original data set is a sample of network probes and so any computation on this sample necessarily has to deal with uncertainties. UP-MapReduce allows developers to easily tackle these uncertainties. We also perform extensive sensitivity analyses on small to large execution DAGs (ranging up to tens of thousands of nodes), using eight of the applications with synthetic data, which allows us to adjust various input characteristics. Specifically, we explore the impact of UP on the magnitudes of uncertainties (e.g., whether uncertainties become worse after propagation), the accuracy of our UP techniques, overheads of UP, and scalability. Our results show that our UP techniques are highly accurate in most cases. Furthermore, the performance overheads of UP using DA are very low  average of 6% performance degradation  when closed-form derivatives are provided. Performance overheads are more significant when using DA with numerical differentiation or Monte Carlo simulation as input size increases, but this performance impact can be reduced by adding computation resources. Recall that these overheads arise from the need to process uncertain data instead of exact values. Finally, our results demonstrate that UP-MapReduce scales well to a cluster with 512 servers. In summary, our contributions include: (1) identifying existing theories appropriate for UP and showing how to apply them to DAG-based data processing frameworks, (2) designing and implementing our proposed UP approach in a MapReduce framework called UP-MapReduce, (3) implementing a suite of data processing applications to explore the accuracy, performance, and scalability of UP-MapReduce, and (4) showing that our approach is highly effective in many scenarios, allowing applications to efficiently account for data uncertainties.\nIn this paper, we proposed an approach for propagating data uncertainties through DAG computations. Specifically, we showed how Differential Analysis can be used to propagate uncertainties through DAG nodes implementing continuous (and semi-continuous under certain conditions) and differentiable functions. Our approach falls back to Monte Carlo simulation of nodes otherwise, but uses statistical bounds to minimize overheads while achieving a target error bounds. Our approach also allows the inter-mixing of Differential Analysis and Monte Carlo simulation for different nodes within a DAG, offering flexibility in the operations supported and minimizing performance overheads We have shown how our UP approach can be applied to general DAG frameworks. We have also implemented it in the UPMapReduce system. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low  an average of 6% performance degradation  when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computing resources. Our scalability results show that UP-MapReduce scales well to a cluster with 512 servers. Finally, using two workflows that couple approximation with UP, we show that significant reductions in execution time can be achieved with approximation, despite the need for UP which propagates estimated uncertainties to the final output.\n", "prediction": "we present a framework for handling uncertainties in data processing systems.<n> our framework is based on the following assumptions : ( 1 ) uncertainties can be represented as probability distributions, ( 2 ] uncertainties are modeled as approximations to the exact values of a data set ; ( 3) exact data sets have to be approximated as many times as possible with an approximation that is as close to exact as we can possibly provide _ a priori __ ( in our case that means that the approximation used is the best one possible- with a probability of error between 1 and @xmath0 that depends on how the data is described in a way that does not compromise the accuracy of approximation- we show that our approximation converges in probability to 1 as the number of nodes increase- and that this convergence is uniform across all types of data in the same way- ( we also show the rate of convergence varies with the amount of uncertainty- our approach allows us to handle uncertainties with high accuracy- while still providing high performance in most cases- the framework can easily be extended to new scenarios with even higher precision- it offers flexibility in how we handle uncertain data and in what we use to model them- users can choose the way we model their data instead of using exact models ( which means they can not be exactly represented by a model that neglects errors in processing- this way allows users to trade- accuracy for performance- they are provided by adding an error- based approximation to a fraction of the original data while ignoring the error in simulation with numerical derivatives ( the user to use the first derivative with simulation of one to run in an analysis with one of simulation in order to provide an additional derivative in which we have a very accurate and the differential differential analysis of an accurate analysis to one - for the analysis in all simulation and simulation s analysis that simulation while simulation that can run a high - in one and to all analysis and a simulation to  t - a function with all the simulation '' while one with our analysis ( with any function that to [2]. our algorithm that will be used to produce a combination of high and [ 2. [3]. to propagate the combination with two functions that a combined to increase the application that we provide a weighted average in any functions with @ a solution ( to obtain the output with additional to our application ( a rate with that in to an ], which to reduce the function ( that are the average of our functions ( [ 1 [ [ 3. <n> [ 4 to @[[a function [ 6 ]. to determine the contribution of @ [ 5  to which with  the algorithm with [ a contribution with which the [ which [ ]) with 6 and ( our to add a to ( while [[] to calculate the functions to make the processing of all [ ( @ 1] that [([2] with to perform the results with functions [ and we are to process with analysis for a number with their analysis by the fraction with 4 and an average [ the separation of functions in [ that @ ( ][[4] to that that [] and our contribution ( for all to 5. the second to complete the distribution with each function  in that with 5- [ to their simulation for our simulation ( as a [].  - that ( and ]]."}
{"ground_truth": "Many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. Mining these continuous data streams brings unique opportunities, but also new challenges. This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using off-the-shelf hardware. It uses Hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. We study VFDTs properties and demonstrate its utility through an extensive set of experiments on synthetic data. We apply VFDT to mining the continuous stream of Web access data from the whole University of Washington main campus.\nKnowledge discovery systems are constrained by three main limited resources: time, memory and sample size. In traditional applications of machine learning and statistics, sample size tends to be the dominant limitation: the computational resources for a massive search are available, but carrying out such a search over the small samples available (typically less than 10,000 examples) often leads to overfitting or data dredging (e.g., [22, 16]). Thus overfitting avoidance becomes the main concern, and only a fraction of the available computational power is used [3]. In contrast, in many (if not most) present-day data mining applications, the bottleneck is time and memory, not examples. The latter are typically in over-supply, in the sense that it is impossible with current KDD systems to make use of all of them within the available computational resources. As a result, most of the available examples go unused, and underfitting may result: enough data to model very complex phenomena is available, but inappropriately simple models are produced because we are unable to take full advantage of the data. Thus the development of highly efficient algorithms becomes a priority. Currently, the most efficient algorithms available (e.g., [17]) concentrate on making it possible to mine databases that do not fit in main memory by only requiring sequential scans of the disk. But even these algorithms have only been tested on up to a few million examples. In many applications this is less than a days worth of data. For example, every day retail chains record millions of transactions, telecommunications companies connect millions of calls, large banks process millions of ATM and credit card operations, and popular Web sites log millions of hits. As the expansion of the Internet continues and ubiquitous computing becomes a reality, we can expect that such data volumes will become the rule rather than the exception. Current data mining systems are not equipped to cope with them. When new examples arrive at a higher rate than they can be mined, the quantity of unused data grows without bounds as time progresses. Even simply preserving the examples for future use can be a problem when they need to be sent to tertiary storage, are easily lost or corrupted, or become unusable when the relevant contextual information is no longer available. When the source of examples is an open-ended data stream, the notion of mining a database of fixed size itself becomes questionable. Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists. However, the available algorithms of this type (e.g., [20]) have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm. This paper proposes Hoeffding trees, a decision-tree learning method that overcomes this trade-off. Hoeffding trees can be learned in constant time per example (more precisely, in time that is worst-case proportional to the number of attributes), while being nearly identical to the trees a conventional batch learner would produce, given enough examples. The probability that the Hoeffding and conventional tree learners will choose different tests at any given node decreases exponentially with the number of examples. We also describe and evaluate VFDT, a decision-tree learning system based on Hoeffding trees. VFDT is I/O bound in the sense that it mines examples in less time than it takes to input them from disk. It does not store any examples (or parts thereof) in main memory, requiring only space proportional to the size of the tree and associated sufficient statistics. It can learn by seeing each example only once, and therefore does not require examples from an online stream to ever be stored. It is an anytime algorithm in the sense that a ready-to-use model is available at any time after the first few examples are seen, and its quality increases smoothly with time. The next section introduces Hoeffding trees and studies their properties. We then describe the VFDT system and its empirical evaluation. The paper concludes with a discussion of related and future work.\nThis paper introduced Hoeffding trees, a method for learning online from the high-volume data streams that are increasingly common. Hoeffding trees allow learning in very small constant time per example, and have strong guarantees of high asymptotic similarity to the corresponding batch trees. VFDT is a high-performance data mining system based on Hoeffding trees. Empirical studies show its effectiveness in taking advantage of massive numbers of examples. VFDTs application to a high-speed stream of Web log data is under way.\n", "prediction": "in many applications today, the continuous stream of access to data from the whole machine campus is not sufficient to allow the system to learn by seeing examples only once.<n> this paper introduces a learning method that can be learned in constant time per example using only a fraction of the available computational power ( tens of millions of examples per second of disk space ) : the method is based on decision trees and it is highly sensitive to unfavorable sets of data in potentially very complex phenomena ; it can learn on the same data as data mining ( mining a database of fixed size leads to overfitting or mining data dredging ( e.g. [ 20 ] ). <n> the paper describes studies with first few examples and its empirical evaluation that demonstrates its utility through an extensive set of synthetic data and shows its strong application to high-speed mining of continuous data streams ( more precisely to mining from a whole campus of thousands of students using a very limited amount of computational resources and taking a high - speed with it the main concern over the batch version of mining is to mine and not to lose as valuable information as mining and learning methods ( also known as incremental methods desiderata known data or sequential methods ), which are not available on this batch example set and result in an unfavorable model for the model to be obtained by learning on that sample set in the sense that it will be similar to one obtained on a similar model as it produces an example that is obtained as a ready-touse in a decision-(eg and eg  a. a significant shortcoming over-[22] [ 16] a substantial portion of a prior work (e.[e [ 22 [ [e[[a.a s [ e th[g [ g. g[d [ a paper [ ].a [] is a [[i [g[s ], a study of paper ( [ j. t. e] e[x [.] to the corresponding to [<n> a _e g [ the author [(g] in [ x. j ]) is the first paper is [ d.]. [ to e - e, [ is also the [].  the study [ p. is e [... [ in paper  [ i. the results [ c?<n> [ for a]. is  a to a corresponding is in e]. e is over [ m. in this is of [ and [ @ [ results is an e-e - a] the].<n> e) is].[."}
{"ground_truth": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.\nDue to advantages such as computational speed, precise manipulation, and exact timing, computers and robots are often superior to humans at performing tasks with well-defined goals and objectives. However, it can be difficult, even for experts, to design reward functions and objectives that lead to desired behaviors when designing autonomous agents (Ng et al., 1999; Amodei et al., 2016). When goals or rewards are difficult for a human to specify, inverse reinforcement learn- *Equal contribution 1Department of Computer Science, University of Texas at Austin, USA 2Preferred Networks, Japan. Correspondence to: Daniel S. Brown <dsbrown@cs.utexas.edu>, Wonjoon Goo <wonjoon@cs.utexas.edu>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). Figure 1. T-REX takes a sequence of ranked demonstrations and learns a reward function from these rankings that allows policy improvement over the demonstrator via reinforcement learning. ing (IRL) (Abbeel & Ng, 2004) techniques can be applied to infer the intrinsic reward function of a user from demonstrations. Unfortunately, high-quality demonstrations are difficult to provide for many tasksfor instance, consider a non-expert user attempting to give kinesthetic demonstrations of a household chore to a robot. Even for relative experts, tasks such as high-frequency stock trading or playing complex video games can be difficult to perform optimally. If a demonstrator is suboptimal, but their intentions can be ascertained, then a learning agent ought to be able to exceed the demonstrators performance in principle. However, existing IRL algorithms fail to do this, typically searching for a reward function that makes the demonstrations appear near-optimal (Ramachandran & Amir, 2007; Ziebart et al., 2008; Finn et al., 2016; Henderson et al., 2018). Thus, when the demonstrator is suboptimal, IRL results in suboptimal behavior as well. Imitation learning approaches (Argall et al., 2009) that mimic behavior directly without reward inference, such as behavioral cloning (Torabi et al., 2018a), also suffer from the same shortcoming. To overcome this critical flaw in current imitation learning methods, we propose a novel IRL algorithm, Trajectoryranked Reward EXtrapolation (T-REX)1 that utilizes ranked demonstrations to extrapolate a users underlying intent be- 1Code available at ICML2019-TREX ar X iv :1 90 4. 06 38 7v 5 [ cs .L G ] 9 J ul 2 01 9 yond the best demonstration, even when all demonstrations are highly suboptimal. This, in turn, enables a reinforcement learning agent to exceed the performance of the demonstrator by learning to optimize this extrapolated reward function. Specifically, we use ranked demonstrations to learn a statebased reward function that assigns greater total return to higher-ranked trajectories. Thus, while standard inverse reinforcement learning approaches seek a reward function that justifies the demonstrations, we instead seek a reward function that explains the ranking over demonstrations, allowing for potentially better-than-demonstrator performance. Utilizing ranking in this way has several advantages. First, rather than imitating suboptimal demonstrations, it allows us to identify features that are correlated with rankings, in a manner that can be extrapolated beyond the demonstrations. Although the learned reward function could potentially overfit to the provided rankings, we demonstrate empirically that it extrapolates well, successfully predicting returns of trajectories that are significantly better than any observed demonstration, likely due to the powerful regularizing effect of having many pairwise ranking constraints between trajectories. For example, the degenerate all-zero reward function (the agent always receives a reward of 0) makes any given set of demonstrations appear optimal. However, such a reward function is eliminated from consideration by any pair of (non-equally) ranked demonstrations. Second, when learning features directly from high-dimensional data, this regularizing effect can also help to prevent overfitting to the small fraction of state space visited by the demonstrator. By utilizing a set of suboptimal, but ranked demonstrations, we provide the neural network with diverse data from multiple areas of the state space, allowing an agent to better learn both what to do and what not to do in a variety of situations. We evaluate T-REX on a variety of standard Atari and MuJoCo benchmark tasks. Our experiments show that T-REX can extrapolate well, achieving performance that is often more than twice as high as the best-performing demonstration, as well as outperforming state-of-the-art imitation learning algorithms. We also show that T-REX performs well even in the presence of significant ranking noise, and provide results showing that T-REX can learn good policies simply by observing a novice demonstrator that noisily improves over time.\nIn this paper, we introduced T-REX, a reward learning technique for high-dimensional tasks that can learn to extrapolate intent from suboptimal ranked demonstrations. To the best of our knowledge, this is the first IRL algorithm that is able to significantly outperform the demonstrator without additional external knowledge (e.g. signs of feature contributions to reward) and that scales to high-dimensional Atari games. When combined with deep reinforcement learning, we showed that this approach achieves better-thandemonstrator performance as well as outperforming stateof-the-art behavioral cloning and IRL methods. We also demonstrated that T-REX is robust to modest amounts of ranking noise, and can learn from automatically generated labels, obtained by watching a learner noisily improve at a task over time.\n", "prediction": "this paper presents a solution to the problem of learning a sequence of goals from a set of potentially poor demonstrations.<n> this solution is based on the observation that ranked demonstrations ( that is demonstrations in which the demonstrator s annotations are ranked ) can be used to improve the performance of a demonstrator in a video game, as well as many other tasks ( see e.g. ( arxiv : quant - ph/0404144 ; quant-ph/0402144 for a review of this work ).<n> we show that the learning of annotations for ranked demos is significantly easier than for non-ranked demos because of the following two advantages of our solution _ ( i)_. it allows us to learn reward functions that are potentially better than any state-of- the - art that we provide for demonstrations that do not yield best - performing annotation and to demonstrate that a small fraction of state - of - the art is sufficient to yield better annotation than best-performing annotation ( for instance in the above mentioned tasks ), and it also enables the demonstration of multiple goals and multiple tasks with a high - quality annotation of both the annotated and the un annotated part of each demonstration ( especially for tasks that require a large amount of annotation for the annotations of one of these annotated parts and for any task that does not require an annotated portion of that particular task for its performance to be high-quality ] we also provide an algorithm that improves the algorithm ( in this algorithm to obtain the state(e. in addition to learning ( we demonstrate the ranking algorithm for ranking ( ( this method ( which is also in our algorithm [ 1 1 [1 ( 1 - 1] ( 2(a1 for [2 ( the paper ( [ 2] and [4r [ 4 th [ 5] [ 3] t1 2 ( 4 ( 5[1 ]) and 2 [ ar 2[2 [ 6] in 4 [ 9] to show [ [ 7 [ 8 [ 10 [[[4 [... 5. 5 -4 [ the results 2 2. 4[5 5 [ in [. 2 for 3 2 <n> [ p[n [ b[a [b[#1 [ for 5 to 5 for 4. 3 [ -5 [ 0 [ that also 3. 6 [ by [ to add the 5 in 6 to 6 for 6[in 5 5 - 4 to [ and 5 that [ @ 5 2 to 7   to 9. to prove that 5 by the 2 - 5... 6 in 5 from the 6. [ 11 [ which we ... to 4 in that to... 4 for 2 in 3 to 2...... [ from [ results to 3  the 4... 9 [ we will also 5 the  5 6  in... 2 by 6 - 6... 3 - 3[. 1 [ while the classification of 3 for all the fifth [ 12.  2 the top - 2 that  that was also 2 and also the analysis of 5 of [ ranking of 4 -  from 2 from 6 by 5 and...  for 7 to a 5 as 5 until the number of 6 that will 5]. to all 5 while [ no 2 of no 5)  by a ]. 2]..... the operator to no. ranked to  [ it -3 to any [].  until 9 to which also that that of all to 0. by no no - a to that for which  no  (  a. 0 - to 1 for learning from all that makes the most of which. ranking to compare the other. from no.... the first - which to @ 2) and a number that results that by com "}
{"ground_truth": "Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.\nIn the lifelong learning setting, we want our agent to solve a series of related tasks drawn from some task distribution rather than a single, isolated task. Agents must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning [25, 27, 31] and variousmeta-reinforcement learning settings [7, 8], where the agent jointly trains on multiple task environments. Not only do such nonincremental settings make the problem of discovering common structures between tasks easier, they allow the methods to ignore the problem of catastrophic forgetting [16], which is the inability to solve previous tasks after learning to solve new tasks in a sequential learning setting. Our work takes a step towards solutions for such incremental settings. We draw on the idea of modularity [17]. While learning to perform a complex task, we force the agent to break its solution down into simpler subpolicies instead of learning a single monolithic policy. This decomposition allows our agent to rapidly learn another related task by transferring these subpolicies. We Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 1317, 2019, Montreal, Canada.  2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. hypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agents solution increases if it can capture such shared structure. A key ingredient of our proposal is the idea of world models [10, 12, 14]  transition models that can predict future sensory data given the agents current actions. The world however is complex, and learning models that are consistent enough to plan with is not only hard [24], but planning with such one-step models is suboptimal [11]. We posit that the requirement that these world models be good predictors of the world state is unnecessary, provided we have a multiplicity of such models. We use the termmodel primitives to refer to these suboptimal world models. Since each model primitive is only relatively better at predicting the next states within a certain region of the environment space, we call this area the model primitives region of specialization. Model primitives allow the agent to decompose the task being performed into subtasks according to their regions of specialization and learn a specialized subpolicy for each subtask. The same model primitives are used to learn a gating controller to select, improve, adapt, and sequence the various subpolicies to solve a given task in a manner very similar to a mixture of experts framework [15]. Our framework assumes that at least a subset of model primitives are useful across a range of tasks and environments. This assumption is less restrictive than that of successor representations [3, 5]. Even though successor representations decouple the state transitions from the rewards (representing the task or goals), the transitions learned are policy dependent and can only transfer across tasks with the same environment dynamics. There are alternative approaches to learning hierarchical spatiotemporal decompositions from the rewards seen while interacting with the environment. These approaches include meta-learning algorithms like Meta-learning Shared Hierarchies (MLSH) [8], which require a multiplicity of pretrained subpolicies and joint training on related tasks. Other approaches include the option-critic architecture [1] that allows learning such decompositions in a single task environment. However, this method requires regularization hyperparameters that are tricky to set. As observed by Vezhnevets et al. [30], its learning often collapses to a single subpolicy. Moreover, we posit that capturing the shared structure across task-environments can be more useful in the context of transfer for lifelong learning than reward-based task specific structures. To summarize our contributions:  Given diverse suboptimalworldmodels, we propose amethod to leverage them for task decomposition.  We propose an architecture to jointly train decomposed subpolicies and a gating controller to solve a given task.  We demonstrate the effectiveness of this approach at both single-task and lifelong learning in complex domains with high-dimensional observations and continuous actions. ar X iv :1 90 3. 01 56 7v 1 [ cs .L G ] 4 M ar 2 01 9\nWe showed how imperfect world models can be used to decompose a complex task into simpler ones. We introduced a framework that uses these model primitives to learn piecewise functional decompositions of solutions to complex tasks. The learned decomposed subpolicies can then be used to transfer to a variety of related tasks, reducing the overall sample complexity required to learn complex behaviors. Our experiments showed that such structured decomposition avoids negative transfer and catastrophic interference, a major concern for lifelong learning systems. Our approach does not require access to accurate world models. Neither does it need a well-designed task distribution or the incremental introduction of individual tasks. So long as the set of model primitives are useful across the task distribution, MPHRL is robust to other imperfections. Nevertheless, learning useful and diverse model primitives, subpolicies and task decomposition all simultaneously is left for future work. The recently introduced Neural Processes [9] can potentially be an efficient approach to build upon.\n", "prediction": "we present a framework for decomposition of a single complex task into simpler modular sub policies and simultaneously learning the required decomposition as well as a controller to coordinate them to perform a series of experiments on high dimensional continuous action control tasks.<n> our approach is based on suboptimal world models, which allow decomposition in a way that is similar to a mixture of experts while avoiding the traditional hierarchical reinforcement learning techniques that require the task distribution to hand down top-down in order to learn the decomposition ; we formally define a set of primitives and show that they can be used to decompose a given task in bottom up manner and learn a gating policy that decomposes it in top down manner to allow transfer of the solution across a range of tasks to improve performance on future tasks and that the learned decompositions are piecewise related to each other and allow us to leverage them for transfer for specific tasks in the framework : we demonstrate effectiveness of this approach at both complex single task learning and lifelong learning in complex domains with high-dimensional observations and continuous actions and understand its limitations and importance of different elements in this framework and their impact on decomposition and learning at diverse task domains and highdimensional tasks with low- and low - dimensional observations. <n> = 1 [ section ] [ thm]proposition [ theorem]lemma [ 1]corollary [ 2]definition [ 3]remark [ 4] # 1#2 [ 5]#1 [ 10, # 2 [ 11] to appear in @xmath0 [ 12 s 2 _ # 10 [ 14 -2 [ 9 -4 -3 -5 -6 -16 -20 -7 -18 -15 -14 -19 -21 -0 -22 -27 -10 -17 -13 -25 -26 -24 -29 -23 -30 -60 -40 -50 -49! t ) [ 20  to add the key to the results [ [ 18 [ to [ 19 [ 15 to find the first to solve the number [ a number 2 to  the policy to determine the  ( [ @  in [ 22  [ the [ is the @ @ [ (  is not to make the model to be the set to obtain the ( @ to avoid the the to all [ in  while the other to is to this is a to we are the one to ( the learning to to model is  for the elements to our  that to generate the state to predict the general to do the subsets to other is [ and to are to decomposition to one is that we learned that are not the most to have the others to derive the sub policy is ( ( in all to construct the in primitive and the behavior of  only to select the is is in our model ( to use the. to capture the corresponding to any decomposition is one ( primitive of decomposition into the states to state and we were the primitive  and a policy and are a decomposition from the function to search to an is only in decomposition for a subset of state of one and all the models to further the entire the structure and is decomposition ( a model and  a primitive states of all   as the group and decomposition by the independent to cover the functions to function and it is. the more to individual sub[[. [ while a.. ( is an ]. to which is independent of [ k ], and this to and in one of other [ [. is all is from a one that does the probability of any sub-[([s ]) to. a) to those to it to was the individual to their to these to differentiate the complex and one in any to most the)  are  with the recovery of an the"}
{"ground_truth": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\nThe promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label [14, 22]. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties. 1 In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles. Jean Pouget-Abadie is visiting Universite de Montreal from Ecole Polytechnique. Sherjil Ozair is visiting Universite de Montreal from Indian Institute of Technology Delhi Yoshua Bengio is a CIFAR Senior Fellow. 1All code and hyperparameters available at ar X iv :1 40 6. 26 61 v1 [ st at .M L ] 1 0 Ju n 20 This framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [17] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.\nThis framework admits many straightforward extensions: 1. A conditional generative model p(x | c) can be obtained by adding c as input to both G and D. 2. Learned approximate inference can be performed by training an auxiliary network to predict z given x. This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a fixed generator net after the generator net has finished training. 3. One can approximately model all conditionals p(xS | x 6S) where S is a subset of the indices of x by training a family of conditional models that share parameters. Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM [11]. 4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available. 5. Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training. This paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.\n", "prediction": "this article proposes a new framework for estimating generative models via an adversarial process.<n> specifically, we propose that : 1 ) the adversarial model is the discriminative model that learns to determine whether the data distribution comes from a sample that came from training rather than a family of classifiers that is a subset of the set of parameters that we are interested in ; 2 ] the framework exists with recovering data distributions from arbitrary functions and equal to 1 2 everywhere where the case and the entire system can be trained with backpropagation chains is defined by the same algorithm that has been used for the generation of samples and this algorithm is not necessary for training the system to unroll a network during either training or generation. <n> [ 1][proof]*#1 * @xmath0 _ department of mathematics and computer science + iowa state university + ames, ia 50011 - 1521 + e-mail: abarg@iastate.edu_ + [ 2]department of statistics and operations research + university of kentucky +lexington, ky 40506 - 0055 + gjackson@stat.k.uiowa-tarls1.com  trth1cr-1a1_d2 a. azuelos - ruiz - vzquez + c. chandler + d.a.c.v.23b.4d.54 s.3 0.0c0 2 2.1[4[2 2[d] 2[b][5] 1s2[12t] ][b[1] [ 4][2] c] 4] 3d][c] 5c[3 [4] * 2][4 * 3 -4 * 4 -5 * * 5  5 * 1 * 6   4 *  2 * c * 10 -6 * [ 5 c  *<n> 2 [ 3 * 9  6 * 7  3 2 c 2 3 [ 10 * 8 -3 * 0 * 12 -2 * a * 4 c 5 [ 6 [ 9 *... 2 5 2 4 [  10 c 4 4 5 5 3 3 c [ [ 7 *"}
{"ground_truth": "Among the now numerous alternative cryptocurrencies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcashs transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.\nSince the introduction of Bitcoin in 2008 [34], cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. While the broader positive potential of blockchain (i.e., the public decentralized ledger underlying almost all cryptocurrencies) is still unclear, despite the growing number of legitimate users there are today still many people using these cryptocurrencies for less legitimate purposes. These range from the purchase of drugs or other illicit goods on so-called dark markets such as Dream Market, to the payments from victims in ransomware attacks such as WannaCry, with many other crimes in between. Criminals engaged in these activities may be drawn to Bitcoin due to the relatively low friction of making international payments using only pseudonyms as identifiers, but the public nature of its ledger of transactions raises the question of how much anonymity is actually being achieved. Indeed, a long line of research [37, 38, 12, 27, 40] has by now demonstrated that the use of pseudonymous ad- dresses in Bitcoin does not provide any meaningful level of anonymity. Beyond academic research, companies now provide analysis of the Bitcoin blockchain as a business [19]. This type of analysis was used in several arrests associated with the takedown of Silk Road [20], and to identify the attempts of the WannaCry hackers to move their ransom earnings from Bitcoin into Monero [17]. Perhaps in response to this growing awareness that most cryptocurrencies do not have strong anonymity guarantees, a number of alternative cryptocurrencies or other privacy-enhancing techniques have been deployed with the goal of improving on these guarantees. The most notable cryptocurrencies that fall into this former category are Dash [2] (launched in January 2014), Monero [3] (April 2014), and Zcash [7] (October 2016). At the time of this writing all have a market capitalization of over 1 billion USD [1], although this figure is notoriously volatile, so should be taken with a grain of salt. Even within this category of privacy-enhanced cryptocurrencies, and despite its relative youth, Zcash stands somewhat on its own. From an academic perspective, Zcash is backed by highly regarded research [28, 13], and thus comes with seemingly strong anonymity guarantees. Indeed, the original papers cryptographically prove the security of the main privacy feature of Zcash (known as the shielded pool), in which users can spend shielded coins without revealing which coins they have spent. These strong guarantees have attracted at least some criminal attention to Zcash: the underground marketplace AlphaBay was on the verge of accepting it before their shutdown in July 2017 [11], and the Shadow Brokers hacking group started accepting Zcash in May 2017 (and in fact for their monthly dumps accepted exclusively Zcash in September 2017) [16]. Despite these theoretical privacy guarantees, the deployed version of Zcash does not require all transactions to take place within the shielded pool itself: it also supports so-called transparent transactions, which are essentially the same as transactions in Bitcoin in USENIX Association 27th USENIX Security Symposium 463 that they reveal the pseudonymous addresses of both the senders and recipients, and the amount being sent. It does require, however, that all newly generated coins pass through the shielded pool before being spent further, thus ensuring that all coins have been shielded at least once. This requirement led the Zcash developers to conclude that the anonymity set for users spending shielded coins is in fact all generated coins, and thus that the mixing strategies that other cryptocurrencies use for anonymity provide a rather small [anonymity set] in comparison to Zcash and that Zcash has a distinct advantage in terms of transaction privacy [9]. In this paper, we provide the first in-depth empirical analysis of anonymity in Zcash, in order to examine these claims and more generally provide a longitudinal study of how Zcash has evolved and who its main participants are. We begin in Section 4 by providing a general examination of the Zcash blockchain, from which we observe that the vast majority of Zcash activity is in the transparent part of the blockchain, meaning it does not engage with the shielded pool at all. In Section 5, we explore this aspect of Zcash by adapting the analysis that has already been developed for Bitcoin, and find that exchanges typically dominate this part of the blockchain. We then move in Section 6 to examining interactions with the shielded pool. We find that, unsurprisingly, the main actors doing so are the founders and miners, who are required to put all newly generated coins directly into it. Using newly developed heuristics for attributing transactions to founders and miners, we find that 65.6% of the value withdrawn from the pool can be linked back to deposits made by either founders or miners. We also implement a general heuristic for linking together other types of transactions, and capture an additional 3.5% of the value using this. Our relatively simple heuristics thus reduce the size of the overall anonymity set by 69.1%. In Section 7, we then look at the relatively small percentage of transactions that have taken place within the shielded pool. Here, we find (perhaps unsurprisingly) that relatively little information can be inferred, although we do identify certain patterns that may warrant further investigation. Finally, we perform a small case study of the activities of the Shadow Brokers within Zcash in Section 8, and in Section 9 we conclude. All of our results have been disclosed, at the time of the papers submission, to the creators of Zcash, and discussed extensively with them since. This has resulted in changes to both their public communication about Zcashs anonymity as well as the transactional behavior of the founders. Additionally, all the code for our analysis is available as an open-source repository.1 paper has provided the first in-depth exploration of Zcash, with a particular focus on its anonymity guarantees. To achieve this, we applied both well-known clustering heuristics that have been developed for Bitcoin and attribution heuristics we developed ourselves that take into account Zcashs shielded pool and its unique cast of characters. As with previous empirical analyses of other cryptocurrencies, our study has shown that most users are not taking advantage of the main privacy feature of Zcash at all. Furthermore, the participants who do engage with the shielded pool do so in a way that is identifiable, which has the effect of significantly eroding the anonymity of other users by shrinking the overall anonymity set. Future work Our study was an initial exploration, and thus left many avenues open for further exploration. For example, it may be possible to classify more z-to-z transactions by analyzing the time intervals between the transactions in more detail, or by examining other metadata such as the miners fee or even the size (in bytes) of the transaction. Additionally, the behavior of mining pools could be further identified by a study that actively interacts with them. Suggestions for improvement Our heuristics would have been significantly less effective if the founders interacting with the pool behaved in a less regular fashion. In particular, by always withdrawing the same amount in the same time intervals, it became possible to distinguish founders withdrawing funds from other users. Given that the founders are both highly invested in the currency and knowledgeable about how to use it in a secure fashion, they are in the best place to ensure the anonymity set is large. Ultimately, the only way for Zcash to truly ensure the size of its anonymity set is to require all transactions to take place within the shielded pool, or otherwise significantly expand the usage of it. This may soon be computationally feasible given emerging advances in the underlying cryptographic techniques [6], or even if more mainstream wallet providers like Jaxx roll out support for z- USENIX Association 27th USENIX Security Symposium 475 addresses. More broadly, we view it as an interesting regulatory question whether or not mainstream exchanges would continue to transact with Zcash if it switched to supporting only z-addresses.\n", "prediction": "founders now interact numerous alternative addresses fromcryptocurrencies that are touted as one of the most secure cryptos, which we find that it is possible to use in a secure way without revealing its pool of transactions. <n> the first version of this paper was published in the journal : information and communication systems ii ( also known as the icmes ) in vol.3 of 2014 ; in pages j1 - j2 of that paper we provide an analysis of anonymity in [ ] (a)in contrast to what it says on its web page it does not claim that its transactions are secure and that no one can reveal the identity of any recipient s transactions [ ( b)]integers and nodes are anonymous in this version ( a)we do not make any claims or statements about the security of our pool ( c)there are no guarantees that any transaction is secure ( d) there is no way that a user can distinguish between a transaction and a node is anonymous if and only if the transaction takes place on the pool [ b]''(c )(d) '(e)'[f](g)[g][v] '[vi]v[vii]$]the first iteration of [ 24]was based on [ 19]and [ 20]in the original version [ 18]they were used as a way to prove that all users are protected by [19]this version we conclude that this is the only way we do so that users who are not required to take place in an iteration [ [20 [ 21]we also do [ 22[19[20[21] [22 [21 [24]22] we are based [ 2322[2222 -2223 [23 -23 _21 -19 -24 -20 -21 -2 [2 -26 ][22 - [ 2 -25 -29 -18 -28 -27 -3 [<n> -16 [29  23 [3 [28 [27 -30 [[24 [ 3 -32 [] to [ 26 [ 32 [ we also we found that we take [ to add [ also [ a [ add the number [ 27  to the analysis [ as [ the [ @ [ see [ further to further [ 28 to make that [ 5 to we [ make the new [ that also also to do we were to study [ do the further we further further the we did the second [ 29 to also that further that the other to all to @ @ we have also the @ that to be in all [ in our to that in we to a number of all that that most [ with the others to our [ and [ all the previous [ even to most that as we we  that  the to  in that many [ 6 to more to it to both to any [ which are the research to many to obtain the more [ from the many that were also not to which also it was the in to to this [. [ most to find the additional to other that do that they are also more that so [ so to identify that ( [ for all all we first to put the algorithm that was that more in it also in which all in any further. to even that is [ many in most most the. we start to increase the researchers that have the subset to so we make it  for the group that will also further in many other [ while [ it were all further  [ or [ who do most in so the all as that even further from [ 4 to who [ any to some that for [ 9 to its most. that. in other other in further and the groups that did not.[[(["}
{"ground_truth": "Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. One of the key questions is which aggregation method  or voting rule  to use; we offer a novel statistical viewpoint that provides guidance. Specifically, we seek voting rules that are robust to prediction errors, in that their output on peoples true preferences is likely to coincide with their output on noisy estimates thereof. We prove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwisemajority consistent rules is not. Our empirical results further support, and more precisely measure, the robustness of Borda count.\nOne of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand. As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues. One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote  hopefully an informed one  every time a new dilemma arises. But what if we could predict the preferences of voters  instead of explicitly asking them each time  and then aggregate those predicted preferences to arrive at a decision? This is exactly the idea behind the work of Noothigattu et al. (2018), who are motivated by the challenge of automating ethical decisions. Specifically, their approach consists of three1 steps: first, collect preferences from voters on exam- 1School of Computer Science, Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Anson Kahng <akahng@cs.cmu.edu>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). 1Technically four, see Section 1.2. ple dilemmas; second, learn models of their preferences, which generalize to any (previously unseen) dilemma; and third, at runtime, use those models to predict the voters preferences on the current dilemma, and aggregate the predicted preferences to reach a decision. The idea is that we would ideally like to consult the voters on each decision, but in order to automate those decisions we instead use the models that we have learned as a proxy for the flesh and blood voters. In other words, the models serve as virtual voters, which is why we refer to this paradigm as virtual democracy. Since 2017, we have been building on this approach in a collaboration with a Pittsburgh-based non-profit, 412 Food Rescue, that provides on-demand food donation distribution services. The goal is to design and deploy an algorithm that would automatically make the decisions they most frequently face: given an incoming food donation, which recipient organization (such as a housing authority or food pantry) should receive it? The voters in our implementation are stakeholders: donors, recipients, volunteers (who pick up the food from the donor and deliver it to the recipient), and employees. We have collected roughly 100 pairwise comparisons from each voter, where in each comparison, the voter is provided information about the type of donation, as well as seven relevant features of the two alternatives that are being compared, e.g., the distance between donor and recipient, and when the recipient last received a donation. Using this data, we have learned a model of the preferences of each voter, which allows us to predict the voters preference ranking over hundreds of recipients. And given a predicted ranking for each voter, we map them into a ranking over the alternatives by applying a voting rule. While this implementation sounds simple enough, the choice of voting rule can have a major impact on the efficacy of the system. In fact, the question of which voting rule to employ is one of the central questions in computational social choice (Brandt et al., 2016), and in social choice theory more broadly. A long tradition of impossibility results establishes that there are no perfect voting rules (Arrow, 1951), so the answer, such as it is, is often context-dependent. The central premise of this paper is that, in the context of virtual democracy, certain statistical considerations should guide the choice of voting rule. Indeed, the voting rule inherently operates on noisy predictions of the voters true preferences, yet one might hope that it would still output the same ranking as it would in the real election based on the voters true preferences (after all, this is the ideal that virtual democracy is trying to approximate). Our research question, therefore, is ... which voting rules have the property that their output on the true preferences is likely to coincide with their output on noisy estimates thereof?\nOur theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. However, Borda count is also compelling in terms of usability and explainability. In more detail, in our implemented donor-recipient matching system, clicking on a recommended alternative displays an explanation for why it was ranked highly by Borda count, which consists of two components. First, we show the alternatives average position in the predicted preferences of each of the four stakeholder groups. Note that this information determines the Borda score of the alternative, given the weight of each stakeholder group.3 Second  this is the more novel component  we show specific features in which the recommended alternative stands out. This is interesting because classic social choice theory does not have features for alternatives, and we are able to give this type of explanation precisely because our alternatives are represented as vectors of features (which is crucial for the application of learning-to-rank algorithms). Based on the results presented in this paper, as well as these additional insights, we use Borda count in our implemented virtual-democracy-based system. 3These weights were decided by the stakeholders themselves.\n", "prediction": "virtual democracy is an approach to automating decisions, which consists of learning models of the preferences of individual people and aggregating those at runtime those who make the decision. in this paper<n> we show that the classica voting rule is robust in the sense that we measure the impact of our application on their output on the social theory : given the features in our implementation of 412-based services we provide additional insights on how to design our algorithm as well as further the answer to a basic question in computational social choice ; which is crucial to our design as opposed to the solution to this classic question that is not well represented by the results of this implementation ( which was implemented as a highly recommended component ) in order to display this feature in a way that makes it clear why we have been able to successfully implement this algorithm and why it is useful for us to employ it as we deploy it ( in which we make food donation and housing for voters which are not available to us as food and which were delivered to each recipient as an alternative to what we do as blood and blood for the recipients ). <n> [ [ section ]] [ thm]corollary [ theorem]conjecture [ 1]*theorem * [ 2]lemma [ 3]proposition [ 4]remark [ 5]assumption [ 6]definition [ 7]algorithm [ 8]example [ 9]open problem [ 10]summary [ 11]#1 [ 12 [ 14] # 1#1#1[12 [ 15  # 10 # 2 # 3 # 4 # 11 # 6 # 7 # 8 # 9 # 12 # 13 # 14 # 15 # 5 # 16 # _ #<n> s-1 eq-1 ( -2 -3 -0 -4 -6 -5 -8 -7 -1 # # 0 -14 -12 -15 -9  -20 -18 -10 -16 -1, -24 -13 -21 -11 -22 -32 - -19 -98 -91 -93 -80 -26 -28 -27 -23 -44 -25 -50 -52 -30 -17 -29 -60 -94 [ 13 1 1_1 1 2 2 2,3,4]  ( 1  2 -  the number 1 - 1 [ p1  to "}
