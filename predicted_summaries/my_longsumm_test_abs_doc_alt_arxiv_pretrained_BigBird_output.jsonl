{"ground_truth": "We present BEAT, a set of practical Byzantine fault-tolerant (BFT) protocols for completely asynchronous environments. BEAT is flexible, versatile, and extensible, consisting of five asynchronous BFT protocols that are designed to meet different goals (e.g., different performance metrics, different application scenarios). Due to modularity in its design, features of these protocols can be mixed to achieve even more meaningful trade-offs between functionality and performance for various applications. Through a 92-instance, five-continent deployment of BEAT on Amazon EC2, we show that BEAT is efficient: roughly, all our BEAT instances significantly outperform, in terms of both latency and throughput, HoneyBadgerBFT, the most efficient asynchronous BFT known.\nCCS CONCEPTS  Security and privacy  Systems security; Distributed systems security;  Computer systems organization Reliability; Availability; KEYWORDS Byzantine fault tolerance, BFT, asynchronous BFT, blockchain, robustness, threshold cryptography\nState machine replication (SMR) [64, 81] is a fundamental software approach to enabling highly available services in practical distributed systems and cloud computing platforms (e.g., Googles Chubby [20] and Spanner [29], Apache ZooKeeper [53]). Its Byzantine failure counterpart, Byzantine fault-tolerant SMR (BFT), has recently regained its prominence, as BFT has been regarded as the model for building permissioned blockchains where the distributed ledgers know each others identities but may not trust one another. As an emerging technology transforming businessmodels, there has been a large number of industry implementations of permissioned blockchains, including Hyperledger Fabric [7, 87], Hyperledger Iroha [56], R3 Corda [30], Tendermint [88], and many more. The Hyperledger umbrella [5], for instance, has become a global collaborative open-source project under the Linux Foundation, now with more than 250 members. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CCS 18, October 1519, 2018, Toronto, ON, Canada  2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5693-0/18/10. . . $15.00 Asynchronous BFT protocols [14, 21, 23, 70] are arguably themost appropriate solutions for building high-assurance and intrusiontolerant permissioned blockchains in wide-area (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet. Asynchronous BFT ensures liveness of the protocol without depending on any timing assumptions, which is prudent when the network is controlled by an adversary. In contrast, partially synchronous BFT (e.g., PBFT [27]) guarantees liveness only when the network becomes synchronous (i.e., satisfies timing assumptions). For instance, it was shown in [70] that PBFT would achieve zero throughput against an adversarial asynchronous scheduler. Challenges andopportunities in adopting asynchronous permissioned blockchains.While a recent asynchronous BFT protocol, HoneyBadgerBFT [70], significantly improves prior asynchronous BFT protocols [14, 21, 23, 70], there are still significant pain points and challenges that prevent it from being used in practice. Meanwhile, there are also new opportunities for asynchronous BFT with the rise of blockchains. Performance (latency, throughput) issues. Compared to partially synchronous BFT protocols (e.g., PBFT [27]), HoneyBadgerBFT has significantly higher latency and lower throughput, in part due to its use of expensive threshold cryptography (specifically, threshold encryption [10] and threshold signatures [17]). This is particularly visible in cases where each replica has limited computation power. These limitations are further exacerbated by various engineering issues. For example, HoneyBadgerBFT was evaluated at only 80- bit security and it will be even slower if implemented with nowstandard 128-bit security. Moreover, due to its use of an erasurecoding library zfec [93], HoneyBadgerBFT can only support ReedSoloman codes (for which better alternatives exist) and at most 28 servers. No one-size-fits-all BFT. In partially synchronous environments, onesize-fits-all BFT protocols have been hard to achieve (as has been argued in various works, e.g., [8, 31, 59]). Indeed, a variety of partially synchronous BFT protocols [1, 8, 16, 27, 28, 31, 33, 59] have been proposed to meet different needs. For instance, chain-based BFT protocols, such as Aliph-Chain [8], BChain [33], and Shuttle [90], favor throughput over latency. Q/U [1] achieves fault-scalability that tolerates increasing numbers of faults without significantly decreasing performance. Zyzzyva [59] and Aliph [8] are hybrid protocols that have high performance in failure-free cases. Moreover, a large number of robust BFT protocols [4, 9, 16, 28, 91] aim to provide a trade-off between performance and liveness during attacks that affect the timing behavior of the network. While robustness is natively achieved in asynchronous BFT, we still require different designs and trade-offs for different performance metrics. Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers). Append-only ledger vs. smart contracts. We advocate distinguishing two different classes of blockchain applications: append-only ledgers and on-chain smart contracts. The former corresponds to append-only, linearizable storage systems (hereinafter, BFT storage), and the latter corresponds to general SMR. While they share security requirements (agreement, total order of updates, liveness), general SMR requires each replica to maintain a copy of all service state to support contracts that operate on that state. In contrast, BFT storage may leverage erasure coding to reduce overall storage by allowing servers to keep only fragments. (See Sec. 3 for formal definitions.) Both of the applications are rather popular. Applications such as food safety [92] and exchange of healthcare data [54] are examples of append-only ledgers, while AI blockchain [86] and financial payments [55] fall into the category of requiring smart contracts. Internet of things (IoT) with blockchains may be of either type, depending on the applications: if one just uses blockchains to store and distribute IoT data to avoid the single point of failure that the clouds may have, then we just need the distributed ledger functionality; if one additionally uses blockchains to consume and analyze the data, then we will additionally need smart contracts. BFT storage may be extended to support off-chain smart contracts run among clients (e.g., Hyperledger Fabric [7]). While offchain smart contracts have many benefits (e.g., achieving some level of confidentiality, as argued in [7]), they also have limitations: 1) they are less suited to running complex smart contract applications with power- and computation-restricted clients (e.g., IoT devices); 2) they require communication channels among clients; and 3) they do not support efficient cross-contract state update. Some blockchain systems use BFT for building consensus ordering services (e.g., Hyperledger Fabric). We find that BFT storage may be used to model the consensus ordering service, and a more efficient BFT storage can lead to a more efficient ordering service. When designing BEAT, we aimed to answer the following major question: Can we have asynchronous BFT storage that significantly outperforms asynchronous general SMR? Flexible read. Some applications benefit from flexible reading, i.e., reading just a portion of a data block as needed (instead of the whole block). For example, in a blockchain that stores video, a user may only want to read the first portion of the stored video. This can be challenging when we use erasure-coding as the underlying storage mechanism. BEAT aims to achieve flexible read with significantly reduced bandwidth. BEAT in a nutshell.We design, implement, and evaluate BEAT  a set of practical asynchronous BFT protocols that resolve the above challenges. First, BEAT leverages more secure and efficient cryptography support and more flexible and efficient erasure-coding support. Second, BEAT is flexible, versatile, and extensible; the BEAT family includes asynchronous BFT protocols that are designed to meet different needs. BEATs design is modular, and it can be extended to provide many more meaningful trade-offs among functionality and performance. Third, BEAT is efficient. Roughly, all our BEAT instances significantly outperform, in terms of both latency and throughput, HoneyBadgerBFT. TheBEATprotocols.BEAT includes five BEAT instances (BEAT0 BEAT4). BEAT0, BEAT1, are BEAT2 are general SMR that can support both off-chain and on-chain smart contracts, while BEAT3 and BEAT4 are BFT storage that can support off-chain smart contracts only. We summarize the characteristics of the BEAT protocols in Table 1 as a series of improvements to HoneyBadgerBFT.  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption [85], a direct instantiation of threshold coin-flipping [22] (instead of using threshold signatures [17]), and more flexible and efficient erasure-coding support.  BEAT1 additionally replaces an erasure-coded broadcast (AVID broadcast) [24] used in HoneyBadgerBFT with a replicationbased broadcast (Brachas broadcast [19]). This helps reduce latency when there is low contention and the batch size is small.  BEAT2 opportunisticallymoves the encryption part of the threshold encryption to the client, further reducing latency. BEAT2 does so at the price of achieving a weaker liveness notion, but can be combined with anonymous communication networks to achieve full liveness. Asynchronous BFT with Tor networks has been demonstrated in HoneyBadgerBFT. BEAT2 additionally achieves causal order [21, 35, 79], a rather useful property for many blockchain applications that process transactions in a first come, first served manner, such as stock trading and financial payments.  BEAT3 is a BFT storage system.While HoneyBadgerBFT, BEAT0, BEAT1, and BEAT2 use Byzantine reliable broadcast [19, 24, 67], we find that replacing Byzantine reliable broadcast with a different and more efficient primitive  bandwidth-efficient asynchronous verifiable information dispersal (AVID-FP) [45] (using fingerprinted cross-checksum) suffices to build a BFT storage. The bandwidth consumption in BEAT3 is information-theoretically optimal. To order transactions of size B, the communication complexity of BEAT3 is O(B), while the complexity for HoneyBadger and PBFT is O(nB) (where n is the total number of replicas). This improvement is significant, as it allows running BEAT in bandwidth-restricted environments, allows more aggressive batching, and significantly improves scalability.  BEAT4 further reduces read bandwidth. BEAT4 is particularly useful when it is common that clients frequently read only a fraction of stored transactions. We provide a generic framework to enable this optimization, and BEAT4 is a specific instantiation of the framework. Roughly, BEAT4 reduces the access overhead by 50% with around 10% additional storage overhead. To achieve this, we extend fingerprinted cross-checksums [45] to handle partial read and to the case of pyramid codes [51], and we design a novel erasure-coded asynchronous verifiable information dispersal protocol with reduced read bandwidth (AVID-FP-Pyramid). Both techniques may be of independent interest. To our knowledge, all the erasure-coded systems against arbitrary failures in reliable distributed systems community [6, 25, 32, 41, 46] use conventional MDS (maximum distance separable) codes [69] such as Reed-Solomon codes [78] and they inherit the large bandwidth features of MDS codes. On the other hand, a large number of works aim to reduce the read bandwidth by designing new erasure coding schemes [4244, 5052, 58]. The systems using these codes work in synchronous environments only, and do not achieve any strong consistency goals even in the crash failure model (let alone Byzantine failures). It is our goal to blend these two disjoint communities and offer new insights to both, by designing novel Byzantine reliable broadcast and BFT protocols with reduced bandwidth.  BEATs design is modular, and features of these protocols can be mixed to achieve even more meaningful trade-offs among functionalities, performance metrics, and concrete applications.\nThe (subtle) differences between (BFT) SMRand (BFT) atomic registers. State machine replication [81] is a general technique to provide a fault-tolerant services using a number of server replicas. It can support arbitrary operations, not just read and write. In SMR, the servers need to communicate with each other and run an interactive consensus protocol to keep the servers in the same state. Register specifications were introduced by Lamport in a series of papers [62, 65, 66], with atomic register as the strongest one. The notions of linearizability andwait-freedom for atomic registerswere introduced by Herlihy andWing [48] and Herlihy [47], respectively. Atomic registers can only support reads and writes. Atomic registers can be realized in asynchronous distributed systems with failures. However, state machine replication cannot be achieved in asynchronous environments [38], unless it uses randomization to circumvent this impossibility result. HoneyBadgerBFT and BEAT fall into this category. BFT SMR is suitable for a number of permissioned blockchain applications (e.g., on-chain smart contracts), while atomic registers are more suitable to model data-centric and cloud storage applications. Comparison with erasure-coded Byzantine atomic registers. An active line of research studies erasure-coded Byzantine atomic registers, as erasure coding can be used to provide storage reduction and/or reduce bandwidth. Notable systems include Pasis [41], CT [25], M-PoWerStore [32], Loft [46], and AWE [6]. These systems have rather different properties from BEAT storage (i.e., BEAT3 and BEAT4). Loft has the same communication complexity as BEAT storage, but it only achieves obstruction-freedom, vs. BEATs (randomized) wait-freedom. AWE, Pasis, CT, and M-PoWerStore have larger communication complexity. Additionally, while AWE achieves waitfreedom, it relies on an architecture that separates storage from metadata and therefore may rely on more servers. Erasure-code choice in BEAT4 (Or:Why pyramid codes?). As discussed in Section 1, an ingredient in BEAT is a novel adaptation of fingerprinted cross-checksums [45] to accommodate pyramid codes. Pyramid codes and their derivatives have already been used in practice, although in a very different setting (data centers), and offer a significant performance boost [26, 51, 52]. We leverage them here to reduce bandwidth costs for fragments that contain real data. Its close competitor, Xorbas codes [80], reduces bandwidth cost for both data and redundant fragments, though we do not leverage them here. We also do not choose the (more complex) derivatives of basic pyramid codes such as generalized pyramid codes [51] and local reconstruction codes [52] that offer maximal recoverability and improve the fault tolerance of basic pyramid codes. For our designed protocol, these codes offer even greater recoverability than we need and hence would be overkill. Weaver codes [43], HoVer codes [44], and Stepped Combination codes [42] (belonging to LDPC codes) do not provide the bandwidth savings and flexibility that we need. Another direction of research in code design is to read instead from more fragments (see [50, 58] and references therein), but less data from each. However, the bandwidth savings are only around 20%30%, much less than pyramid codes and its derivatives. In addition, these codes do not fit our setting where we assume a fixed number of servers may behave maliciously and we attempt to mask as many Byzantine servers as possible.\nTiming assumptions.Distributed systems can be roughly divided into three categories according to their timing assumption: asynchronous, synchronous, or partially synchronous. An asynchronous system makes no timing assumptions on message processing or transmission delays. If there is a known bound on message processing delays and transmission delays, then the corresponding system is synchronous. The partial synchrony model [37] lies in-between: messages are guaranteed to be delivered within a time bound, but the bound may be unknown to participants of the system. In protocols for asynchronous systems, neither safety nor liveness can rely on timing assumptions. In contrast, a protocol built for a synchronous or partially synchronous system risks having its safety or liveness properties violated if the synchrony assumption on which it depends is violated. For this reason, protocols built for asynchronous systems are inherently more robust to timing and denial-of-service (DoS) attacks [70, 94]. BFT SMR. We consider asynchronous Byzantine fault-tolerant state machine replication (BFT SMR) protocols, where f out of n replicas can fail arbitrarily (Byzantine failures) and a computationally bounded adversary can coordinate faulty replicas. The replicas collectively implement the abstraction of a keyvalue store. A replica delivers operations, each submitted by some client. All operations must be deterministic functions of the keyvalue store contents. The client should be able to compute a final response to its submitted operation from the responses it receives from replicas. Correctness for a secure BFT SMR protocol is specified as follows.  Agreement: If any correct replica delivers an operationm, then every correct replica deliversm.  Total order: If a correct replica has deliveredm1,m2,    ,ms and another correct replica has deliveredm1,m  2,    ,ms  , then mi =m  i for 1  i  min(s, s ).  Liveness: If an operationm is submitted ton f correct replicas, then all correct replicas will eventually deliverm. The liveness property has been referred to by other names, e.g., fairness in CKPS [21] and SINTRA [23], and censorship resilience in HoneyBadgerBFT [70]. We use them interchangeably. We consider two types of BFT SMR services. BFT storage. A BFT storage service implements only read(key) and write(key, val) operations. The former should return to the client the current value for key in the key-value store, and the latter should update the value of key in the key-value store to val. General SMR. A general SMR servicewhich is our default concern, unless specified otherwisesupports operations that consist of arbitrary deterministic programs, or transactions, that operate on the key-value store. To support operations that are arbitrary transactions, each replica will typically maintain the contents of the key-value store in its entirety. Then, total order and the determinism of transactions ensures that the key-value store contents remain synchronized at correct replicas (assuming they begin in the same state). BFT storage can be implemented in more space-efficient ways, e.g., with each replica storing only an erasure-coded fragment for the value of each key (e.g., [24, 41, 45, 46]). Secure causal BFTprotocols.One of the BEAT instances achieves causality, whichwe briefly recall as follows. Input causality prevents the faulty replicas from creating an operation derived from a correct clients but that is delivered (and so executed) before the operation from which it is derived. The problem of preserving input causality in BFT atomic broadcast protocols was first introduced by Reiter and Birman [79]. The notion was later refined by Cachin et al. [21] and recently generalized by Duan et al. [35].\nThis section reviews the cryptographic and distributed systems building blocks for BEAT. Labeled threshold cryptosystems. We review robust labeled threshold cryptosystem (i.e., threshold encryption) [85] where a public key is associated with the system and a decryption key is shared among all the servers. Syntactically, a (t ,n) threshold encryption scheme ThreshEnc consists of the following algorithms. A probabilistic key generation algorithm TGen takes as input a security parameter l , the number n of total servers, and threshold parameter t , and outputs (pk, vk, sk), where pk is the public key, vk is the verification key, and sk = (sk1,    , skn ) is a list of private keys. A probabilistic encryption algorithm TEnc takes as input a public key pk, a messagem, and a label lb, and outputs a ciphertext c . A probabilistic decryption share generation algorithm ShareDec takes as input a private key ski , a ciphertext c , and a label lb, and outputs a decryption share  . A deterministic share verification algorithm Vrf takes as input the verification key vk, a ciphertext c , a label lb, and a decryption share  , and outputs b  {0, 1}. A deterministic combining algorithm Comb takes as input the verification key vk, a ciphertext c , a label lb, a set of t decryption shares, and outputs a messagem, or  (a distinguished symbol). We require the threshold encryption scheme to be chosen ciphertext attack (CCA) secure against an adversary that controls up to t  1 servers. We also require consistency of decryptions, i.e., no adversary that controls up to t  1 servers can produce a ciphertext and two t-size sets of valid decryption shares (i.e., where Vrf returns b = 1 for each share) such that they yield different plaintexts. For our purpose, we require a labeled threshold encryption scheme [85]; threshold cryptosystems that do not support labels [10, 18] are not suitable. Threshold PRF. We review threshold PRF (e.g., [22]), where a public key is associated with the system and a PRF key is shared among all the servers. A (t ,n) threshold PRF scheme for a function F consists of the following algorithms. A probabilistic key algorithm FGen takes as input a security parameter l , the number n of total servers, and threshold parameter t , and outputs (pk,vk, sk), where pk is the public key, vk is the verification key, and sk = (sk1,    , skn ) is a list of private keys. A PRF share evaluation algorithm Eva takes a PRF input c , pk , and a private key ski , and outputs a PRF share yi . A deterministic share verification algorithm Vrf takes as input the verification key vk, a PRF input c , and a PRF share yi , and outputs b  {0, 1}. A deterministic combining algorithm FCom takes as input the verification key vk, x , and a set of t valid PRF shares, and outputs a PRF value y.1 We require the threshold PRF value to be unpredictable against an adversary that controls up to t  1 servers. We also require the threshold PRF to be robust in the sense the combined PRF value for c is equal to F(c). We can use a direct implementation of threshold PRF [22] or can use build a threshold PRF using threshold signatures [17, 83]. Erasure coding scheme. An (n,m) erasure coding scheme takes as inputm data fragments and outputs n (n  m) same-size coded fragments. This essentially captures the encode algorithm of an erasure code, but we (intentionally) leave the decode algorithm undefined. An (n, m) erasure coding scheme is systematic if the n coded fragments contain the originalm data fragments and  = n m redundant fragments. Let di (i  [1..m]) denote the data fragments, and di (i  [m + 1..n]) denote the redundant fragments. We have di (i  [1..n]) to denote all the coded fragments. 1Our syntactic description of threshold encryption and threshold PRF can be made more general and the algorithms are not necessarily non-interactive. An (n,m) erasure coding scheme is maximum distance separable (MDS) [69] if and only if all the data fragments can be recovered from anym-size subset of coded fragments. A systematic (n,m) erasure coding scheme is linear if each redundant fragment di (i  [m + 1..n]) is a linear combination of the data fragments, i.e., di = m j=1 bi jdj , where bi j s are coding coefficients. Basic pyramid codes.Huang et al. [51] introduced (basic) pyramid codes to slightly trade space for access efficiency in erasure-coded storage systems. For practical parameters, the pyramid codes can reduce the access overhead by 50% with around 10% additional storage overhead, compared to MDS erasure codes. Pyramid codes can be efficiently built from any (n,m) systematic and MDS code that tolerates arbitrary  = n m erasures as defined above. Specifically, we divide them data fragments into L equal-size2 disjoint groups Sl (l  [1..L]), each of which contains m/L data fragments. Next, we keep 1 out of  redundant fragments unchanged. These fragments are global redundant fragments. Then, for each group Sl , we compute 0 =   1 group redundant fragments and add them to each group, where the j-th group redundant fragment, denoted dj,l , is a projection of the j-th redundant block dm+j in the original MDS code onto that group, i.e., dj,l is computed as dm+j except setting all the coding coefficients that do not correspond to Sl to 0. This yields an (m + 0L + 1,m) systematic and non-MDS code, which has m data fragments and 0L + 1 redundant fragments, where each of group Sl has 0 group redundant fragments. It is important to note that new code is also linear. We briefly describe two useful properties of the basic pyramid codes [51]: (1) An (m + 0L + 1,m) pyramid code can tolerate arbitrary  = 0 + 1 erasures; (2) Each equal-size group Sl is an (m/L + 0,m/L) MDS code. To decode a data fragment, first try from the group level. For each group Sl , if the number of the unavailable fragments is less than 0, any unavailable fragments can be recovered. Otherwise, first recover all the unavailable fragments from the group level, and then move to global level. One may needs to compute the fragments in the original MDS code that correspond to group redundant fragments, and then uses the conventional decoding algorithm of an MDS code to recover unavailable fragments. Figure 1 uses an example to describe pyramid codes and shows how they can reduce read bandwidth and system I/O. The example builds a (10, 6) pyramid code from a (9, 6) linear, MDS code. In 2While there is no need to requirem divides L for pyramid codes, in practice one almost always uses equal-size sets. Typically, L can be set to 2. the (9, 6) MDS code, there are 6 data fragments and 3 redundant fragments. The redundant fragments (d7, d8, and d9) are linear combinations of the data fragments. For instance, d7 can be written as d7 = 6 j=1 b7jdj , where b7j s are coding coefficients. We then divide the data fragments equally (setting L = 2), and compute one redundant fragment for each group: d71 =3 j=1 b7jdj and d72 = 6 j=4 b7jdj , respectively. Clearly, we have d71 + d72 = d7. They are local (group) redundant fragments, and d8 and d9 are global redundant fragments. If the fragment d1 is not available, one can just use 3 fragments (d2, d3, and d71) to recover d1. If there is more than one failure in the local group, one would need to use the traditional MDS decoding algorithm to recover the faulty local fragment. One may need to compute the sum of d71 and d72 to recover d7 if necessary. Fingerprinted cross-checksum.Afingerprinted cross-checksum [45] is data structure used by a server to verify that its fragment corresponds to a unique original data block. An (n,m) fingerprinted cross-checksum fpcc consists of an array fpcc.cc[ ] of n values and an array fpcc.fp[ ] ofm values. The first array is a cross-checksum [40, 60] that contains the n hashes of the n coded fragments. The second array holds homomorphic fingerprints of data fragments that preserve the property of linear codes. Let h be a collision-resistant hash function and H be a hash function modeled as a random oracle. A homomorphic fingerprinting function fingerprint takes as input a random key and a data fragment and outputs a small field element. A fragment d is consistent with fpcc for index i  [1..n], if fpcc.cc[i] = h(d) and fingerprint(r ,d) = encode(fpcc.fp[1],    , fpcc.fp[m]), where r = H(fpcc.cc[1],    , fpcc.cc[n]). A central theorem in [45, 46] is that for an (n,m) systematic, MDS, and linear erasure coding scheme, no adversaryA can produce two different sets ofm fragments such that each fragment is consistent with fpcc for its index and they can be decoded into two different data blocks with non-negligible probability. Asynchronous verifiable information dispersal using fingerprinted cross-checksum. In an asynchronous verifiable information dispersal (AVID) protocol [24], a client disperses a blockM to n servers (where at most f of them might be faulty). The clients can later retrieve the full blockM through the servers. The verifiability of the protocol ensures that any two clients retrieve the same block. An (n,m)-asynchronous verifiable information dispersal scheme is a pair of protocols (disperse, retrieve) that satisfy the following with high probability:  Termination: If a correct client initializes disperse(M), then all correct servers will eventually complete dispersal disperse(M).  Agreement: If some correct server completes disperse(M), all correct servers eventually complete disperse(M).  Availability: If f + 1 correct servers complete disperse(M), a correct client can run retrieve( ) to eventually reconstruct some blockM .  Correctness: If f + 1 correct servers complete disperse(M), all correct clients that run retrieve( ) eventually retrieve the same block M . If the client that initiated disperse(M) was correct, thenM  = M . Cachin and Tessaro [24] proposed an erasure-coded AVID, which we call AVID-CT, To broadcast a messageM , the communication complexity of AVID-CT is O(n |M |). AVID-FP [45] is a bandwidth-efficient AVID using fingerprinted cross-checksum. In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, wherem  f + 1 and n = m + 2f . Here f is the maximum number of Byzantine faulty servers that system can tolerate, and n is the total number of servers. Then it generates the corresponding fingerprinted crosschecksum for B with respect to the erasure coding scheme. Next, the client distributes the erasure-coded fragments and the same fingerprinted cross-checksum to the servers. Each server verifies the correctness of the fragment that it receives according to the fingerprinted cross-checksum and then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol. Different from AVID-CT, to disperse a messageM , the communication complexity of AVID-FP is O(|M |). Byzantine reliable broadcast.Byzantine reliable broadcast (RBC), also known as the Byzantine generals problem, was first introduced by Lamport et al. [67]. An asynchronous reliable broadcast protocol satisfies the following properties:  Agreement: If two correct servers deliver two messagesM and M  thenM = M .  Totality: If some correct server delivers a messageM , all correct servers deliverM .  Validity: If a correct sender broadcasts a messageM , all correct servers deliverM . Brachas broadcast [19], one that assumes only authenticated channels, is a well-known implementation of Byzantine reliable broadcast. To broadcast amessageM , its communication complexity is O(n2 |M |). Cachin and Tessaro [24] proposed both an erasurecoded AVID (AVID-CT, mentioned above) and an erasure-coded variant of Brachas broadcast  AVID broadcast, which reduces the cost to O(n |M |) compared to that of Brachas broadcast. Note that we explicitly distinguish among AVID-CT and AVID-FP (both of which are verifiable information dispersal protocols) and AVID broadcast (a RBC protocol).\nThis section provides an overview of HoneyBadgerBFT and related primitives. We begin by introducing asynchronous common subset (ACS). Asynchronous common subset.HoneyBadgerBFT uses ACS [14, 21]. Formally, an ACS protocol satisfies the following properties:  Validity: If a correct server delivers a set V , then |V |  n  f and V contains the inputs of at least n  2f correct servers.  Agreement: If a correct server delivers a set V , then all correct servers deliver V .  Totality: Ifn f correct servers submit an input, then all correct servers deliver an output. ACS can trivially lead to asynchronous BFT: each server can propose a subset of transactions, and deliver the union of the transactions in the agreed-upon vector; sequence numbers can be then assigned to the agreed transactions using any predefined but fixed order. HoneyBadgerBFT in a nutshell. HoneyBadgerBFT essentially follows Ben-Or et al. [14], which uses reliable broadcast (RBC) and asynchronous binary Byzantine agreement (ABA) to achieve ACS. HoneyBadgerBFT cherry-picks a bandwidth-efficient, erasurecoded RBC (AVID broadcast) [24] and the most efficient ABA [72] to realize ACS. Specifically, HoneyBadgerBFT uses Boldyrevas threshold signature [17] to provide common coins for the randomized ABA protocol [72]. HoneyBadgerBFT favors throughput over latency by aggressively batching client transactions. It was shown that HoneyBadgerBFT can outperform PBFT when the number of servers exceeds 16 in terms of throughput in WANs, primarily because HoneyBadgerBFT distributes the network load more evenly than PBFT [27]. As illustrated in Figure 2, the HoneyBadgerBFT protocol is composed of two subprotocols/phases: RBC and ABA. In the RBC phase, each replica first proposes a set of transactions and uses reliable broadcast to disseminate its proposal to all other replicas. In the second phase, n concurrent ABA instances are used to agree on an n-bit vector bi for i  [1..n], where bi indicates that if replica is proposed transactions are included. HoneyBadgerBFT proceeds in epochs. Let B be a batch size of client transactions. In each epoch, each replica will propose B/n transactions. Each epoch will commit (B) transactions. To improve efficiency, HoneyBadgerBFT ensures that each replica proposes mostly disjoint sets of transactions. For this reason, it asks replicas to propose randomly selected transactions. To prevent adversary from censoring some particular transaction by excluding whichever replicas propose it, HoneyBadgerBFT requires replicas to use threshold encryption to encrypt transactions proposed to avoid censorship. HoneyBadgerBFT contains four distributed algorithms: a threshold signature [17] that provides common coins for ABA, an ABA protocol [72] that has expected running time O(1) (completing within O(k) rounds with probability 1 2k ), a bandwidth-efficient reliable broadcast [24], and a threshold encryption [10] to avoid censorship and achieve liveness. Roughly, the reliable broadcast dominates the bandwidth and guides the selection of batch size. The threshold encryption scheme and the threshold signature scheme use expensive cryptographic operations, and they and the ABA dominate the latency of HoneyBadgerBFT. While HoneyBadgerBFT is the most efficient asynchronous BFT protocol known, HoneyBadgerBFT favors throughput over other performance metrics (latency, bandwidth, scalability). For instance, HoneyBadgerBFT has rather high latency, which is particularly visible in local area networks (LANs) [89]. This makes it difficult to work in latency-critical applications. Indeed, it is desirable to have asynchronous BFT protocols that are designed for different goals (different performance metrics, different application scenarios).\nThis section describes BEAT0, our baseline protocol, that uses a set of generic techniques to improve HoneyBadgerBFT. Specifically, BEAT0 incorporates a more secure and efficient threshold encryption, a direct implementation of threshold coin flipping, and more flexible and efficient erasure-coding support. BEAT0 specification. Instead of using CPA/CCA-secure threshold encryption that does not support labels, BEAT0 leverages a CCAsecure, labeled threshold encryption [85] to encrypt transactions while making the ciphertexts uniquely identifiable. BEAT0 proceeds in epochs (i.e., rounds). Let r the current epoch number. Let n be the total number of replicas. Let ThreshEnc = (TGen, TEnc, ShareDec,Vrf,Comb) be a (f +1,n) labeled threshold encryption scheme. Let pk and vk be threshold encryption public key and verification key, respectively. Let ski be the private key for replica i  [1..n]. Let B be the batch size of BEAT0. Each replica i  [1..n] randomly selects a setT of transactions of size B/n. It then computes a labeled threshold encryption ciphertext (lb, c) $ TEncpk(lb,T ) where lb = (r , i). Next, each replica submits the labeled ciphertexts to ACS as input. Each replica i , upon receiving some labeled threshold ciphertexts (r , j , c) from some other replica j, does a sanity check to see if j = j  and if there is already a different triple for the same r and j before proceeding. Namely, each replica i only stores and processes one ciphertext from the same j and the same r , and will discard ciphertexts subsequently received for the same j and r . After getting output from ACS, a replica i can run ShareDec to decrypt the ciphertexts using its secret key ski , and broadcasts its decryption shares. When receiving f + 1 valid shares (that pass the verification of Vrf), a replica can use Comb to combine the transactions. Efficiently instantiating CCA secure labeled threshold encryption.Weobserve that much of the latency inHoneyBadgerBFT is due to usage of pairing-based cryptography, which is much slower than elliptic curve cryptography (cf. [71]). We thus implement our threshold encryption using the TDH2 scheme by Shoup and Gennaro [85] using the P-256 curve which provides standard 128-bit security. TDH2 is secure against chosen-ciphertext attacks, under the Decisional Diffie-Hellman (DDH) assumption in the random oracle model [13]. Jumping ahead, while we use a stronger and functionally more complex cryptographic scheme, our experiments show that doing so actually improves the latency of HoneyBadgerBFT greatly. Directly instantiating common coin protocol. Instead of using a threshold signature to derive the common coins as in HoneyBadgerBFT and other multi-party computation protocols, we choose to directly use threshold coin flipping. Specifically, we use the scheme due to Cachin, Kursawe, and Shoup (CKS) [22] and implement it again using the P-256 curve that provides 128 bits of security. The threshold PRF scheme is proven secure under the Computational Diffie-Hellman (CDH) assumption in the random oracle model. Enabling more efficient and more flexible erasure coding. HoneyBadgerBFT uses an erasure-coding library zfec [93] that supports Reed-Soloman codes only and supports at most 128 servers. We integrate the C erasure coding library Jerasure 2.0 [73] with our BEAT framework. This allows us to remove the restriction that HoneyBadgerBFT can only support at most 128 replicas, use more efficient erasure-coding schemes (e.g., Cauchy Reed-Soloman codes [75]), and flexibly choose between erasure-coding scheme parameters to improve performance. Distributedkey generation.Our threshold encryption and threshold PRF are discrete-log based, and BEAT0 and all subsequent BEAT instances allow efficient distributed key generation [39, 57], which should be run during setup. The implementation of distributed key generation, however, is outside the scope of the present paper.\nThis section presents two latency-optimized protocols in BEAT: BEAT1 and BEAT2. BEAT1. Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that 1) most of latency comes from threshold encryption and threshold signatures, and 2) somewhat surprisingly, when the load is small and there is low contention, erasurecoded reliable broadcast (AVID broadcast) [24] causes significant latency. To test the actual latency overhead incurred by erasure-coded broadcast, we implement a variant of HoneyBadgerBFT, HB-Bracha, which replaces erasure-coded broadcast with a popular, replicationbased reliable broadcast protocol  Brachas broadcast [19]. We find that when the client load is small, HB-Bracha outperforms HoneyBadgerBFT in terms of latency by 20%60%. This motivates us to devise BEAT1. BEAT1 replaces the AVID broadcast protocol in BEAT0 with Brachas broadcast. It turns out that when the load is small, BEAT1 is consistently faster than BEAT0, though the difference by percentage is not as significant as that between HB-Bracha and HoneyBadgerBFT. However, when the load becomes larger, BEAT1 has significantly higher throughput, just as the case betweenHB-Bracha and HoneyBadgerBFT. BEAT2. In BEAT0, our use of CCA-secure, labeled threshold encryption is at the server side, to prevent the adversary from choosing which servers proposals to include. BEAT2 opportunistically moves the use of threshold encryption to the client side, while still using Brachas broadcast as in BEAT1. In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions. Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients. BEAT2 can be combined with anonymous communication networks to achieve full liveness. BEAT2 additionally achieves causal order [21, 35, 79], which prevents the adversary from inserting derived transactions before the original, causally prior transactions. Causal order is a rather useful property for blockchain applications that process client transactions in a first come, first served manner, such as trading services, financial payments, and supply chain management.\nThis section presents BEAT3, an asynchronous BFT storage system. BEAT3 significantly improves all performancemetrics that we know of  latency (compared to HoneyBadgerBFT), bandwidth, storage overhead, throughput, and scalability. Deployment scenarios. Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded). BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric [7, 87]. BEAT3. BEAT3 achieves better performance by using a novel combination of a bandwidth-efficient information dispersal scheme (AVID-FP [45]) and an ABA protocol [72]. In comparison, HoneyBadgerBFT, BEAT0, BEAT1, and BEAT2 use a combination of reliable broadcast and an ABA protocol. AVID-FP has optimal bandwidth consumption which does not depend on the number of replicas. The bandwidth required to disperse a blockM in AVID-FP is only O(|M |), while the bandwidth in AVID broadcast (used in HoneyBadgerBFT) is O(n |M |). Technically speaking, AVID-FP has a much smaller communication complexity than AVID-CT because replicas in AVID-FP agree upon a small constant-size fingerprinted cross-checksum instead of on the block itself (i.e., the bulk data). Our basic idea is to replace AVID broadcast used in HoneyBadgerBFT with an (n,m) AVID-FP protocol, where n = m + 2f and m  f + 1. Accordingly, at the end of the AVID-FP protocol, each replica now stores some fingerprinted cross-checksum and the corresponding erasure-coded fragment. There is, however, a challenge to use the approach. In AVID-FP, a correct replica cannot reconstruct its fragment if it is not provided by the AVID-FP client who proposes some transaction (here, some other replica in our protocol). Namely, as mentioned by Hendricks et al. [45], even with a successful dispersal, only f + 1 correct replicas, instead of all correct replicas, may have the corresponding fragments. However, ABA expects all correct replicas to deliver the transaction during the broadcast/dispersal stage (to correctly proceed). Note that we cannot trivially ask replicas in AVID-FP to reconstruct their individual fragment or reconstruct the whole transaction, which would nullify the bandwidth benefit of using AVID-FP. We observe that AVID-FP actually agrees on the fingerprinted cross-checksum of the transaction. It is good enough for us to proceed to the ABA protocol once each replica delivers the fingerprinted cross-checksum. The consequence for BEAT3 is just as in AVID-FP: at least f + 1 correct replicas have their fragments, and some correct replicas may not have their fragments. This causes no problem, as the data is retrievable using f + 1 =m correct fragments. Each replica just needs to send the client the fingerprinted cross-checksum and its fragment. The client can then reconstruct the transaction. More formally, validity, agreement, and totality of the ACS using AVID-FP follow directly from the properties of asynchronous verifiable information dispersal, just as the case of using reliable broadcast. The only difference is that the ACS using AVID-FP now delivers a fingerprinted cross-checksum. We just need to prove that our ACS is functionally correct. This follows easily from correctness of asynchronous verifiable information dispersal: if a fingerprinted cross-checksum is delivered, then the corresponding data (i.e., transaction) is retrievable, and all clients are able to retrieve the data and the data was previously proposed by some server. Bandwidth comparison. To order transactions of size B, the communication complexity of BEAT1, BEAT2, andHB-Bracha isO(n2B), the complexity of HoneyBadgerBFT and BEAT0 is O(nB), while the communication complexity of BEAT3 is only O(B). This improvement is significant, as it allows running BEAT in bandwidthrestricted environments, allows more aggressive batching, and greatly improves scalability.\nThis section presents a general optimization for erasure-coded BEAT instances that significantly reduce read bandwidth. For many blockchain applications, particularly data-intensive ones, it is common for clients to read only a fraction of the data block. Additionally, for many applications using smart contracts, clients may be interested in seeing the first few key terms of a large contract instead of the lengthy, detailed, and explanatory terms. Our technique relies on a novel erasure-coded reliable broadcast protocol, AVID-FP-Pyramid, that reduces read bandwidth. AVIDFP-Pyramid uses pyramid codes [51]. As reviewed in Sec. 4, a (m+0L+1,m) pyramid code can tolerate arbitrary  = 0+1 erasures. Let n =m+0L+1. We define for a (m+0L+1,m) pyramid code a tailored fingerprinted cross-checksum. Our (m+0L+1,m) fingerprinted cross-checksum fpcc consists of an array fpcc.cc[ ] that holds the hashes of all n coded fragments. The second array fpcc.fp[ ] still containsm values that are fingerprints of the firstm data fragments, and because pyramid codes are linear, all the fingerprints of coded fragments can be derived by thesem fingerprints, just as all the coded fragments can be derived by the originalm fragments. We say a fragment d is consistent with fpcc for index i  [1..n], if fpcc.cc[i] = h(d) and fingerprint(r ,d)= encode (fpcc.fp[1],    , fpcc.fp[m]), where r = H(fpcc.cc[1],    , fpcc.cc[n]). We extend the central theorem used in [45, 46] to the case of pyramid codes and to the case for fragments. We derive the following new lemma. Lemma 9.1. For an (m+0L+1,m) fingerprinted cross-checksum fpcc, any probabilistic adversaryA can produce with negligible probability a target data fragment index (resp., data fragment indexes) and two sets of fragments (that may have different sizes) such that each fragment is consistent with fpcc for its index and they can be decoded into two different data fragments for the target index (resp., different sets of fragments for the target indexes). The target data fragment index(es) may be an index of one of data fragment, indexes of all data fragments, or any number of indexes in between. The two set of fragments that A provides can be of different sizes, and the decoding approaches for two sets may differ (may it be a group level or global level decoding). The proof the lemma is an adaptation to the one due to Hendricks et al. [45, Theorem 3.4]. In proving Theorem 3.4 [45], the key claim is that two different sets ofm fragments for the same fragment indexes and the same consistent fingerprinted cross-checksum imply that at least one fragment from the two sets is different, which is the starting point of their proof. Following the same argument, we can show that the probability that two fragments with the same index are different is bounded by   + q   , where   is the advantage of attacking the hash function, q is the total number random oracle queries, and  is the probability of the collisions in the fingerprinting function. The proof applies to any linear erasure-coding schemes, including pyramid codes. AVID-FP-Pyramid. Now we describe AVID-FP-Pyramid, an asynchronous verifiable information dispersal protocol that compared to AVID-FP, further reduces read bandwidth. Instead of using a conventional MDS erasure code, AVID-FP-Pyramid uses a pyramid code. In an MDS code,m valid fragments can be used to reconstruct the original block. In a pyramid code, we need in generalm+0(L1) valid fragments to reconstruct the block. Therefore, we have to make sure that in our newAVID protocol at leastm+0(L1)+f servers receive consistent fragments, of which f servers might be faulty. Moreover, one needs to make sure thatm +0L +1  m +0(L  1)+ 2f , i.e., f  (0 +1)/2, which ensures that the total number of replicas do not overflow. Given a pyramid code (n,m) where n = m + 0L + 1 that can tolerate arbitrary  = 0 + 1 erasures, we construct AVID-FPPyramid where f  m and f  (0 + 1)/2. Specifically, AVID-FPPyramid consists of a triple of protocols (disperse, retrieve, read) which are described as follows. Dispersal. To disperse a block B, a client applies the (n,m) pyramid code to generate n fragments {di }ni=1 and the fingerprinted crosschecksum fpcc. The server then sends each server i its fragment di and fpcc. Upon receiving a disperse message, a server i verifies that the fragment di is consistent with fpcc. (Concretely, server i checks if fpcc.cc[i] = h(d) and fingerprint(r ,d)= encode (fpcc.fp[1],    , fpcc.fp[m]), where r = H(fpcc.cc[1],    , fpcc.cc[n]).) If this is true, the server stores the fragment and sends an echo message containing fpcc (and only fpcc) to all servers. Upon receivingm + 0(L  1) + f echo messages with matching fingerprinted cross-checksum fpcc, a server sends a ready message containing fpcc to all servers. If receiving f + 1 ready with matching fingerprinted crosschecksum fpcc, and if a server does not yet send a ready message, it sends a ready message to all other servers. Upon receiving 2f + 1 ready messages with matching fpcc, it stores and delivers fpcc. Retrieval. The retrieval protocol is almost the same as that in AVIDFP, with only a parameter difference. To retrieve a block, a client retrieves a fragment and fingerprinted cross-checksum from each server, waiting for matching fingerprinted cross-checksums from f + 1 servers and consistent fragments fromm + 0(L  1) servers. These fragments are then decoded and the resulting block is returned. Read. To read a single fragment di , one could choose one of the following two options. In the first option, which we term as the optimistic mode, a client requests from all servers the fingerprinted cross-checksum and only the target server i for the fragment. If it does not receive the fragment in time (set arbitrarily by the client), it queries the servers at the group level that contains the server i , and all servers in the local group should send their fragments. The client will repeat the procedure from the group level until it receives m + 0(L  1) fragments with matching fpcc and then recovers the fragment. In the second, which we term as the balanced mode, a client directly queries all servers at the group level, expecting the fragments from these group level servers. Definition and security.While we could be more general, we provide a definition for AVID-FP-Pyramid that is specifically tailored for our purpose. An (n,m)-asynchronous verifiable information dispersal scheme is a triple of protocols (disperse, retrieve, read) that satisfy the following with high probability:  Termination: If a correct client initializes disperse(M) then all correct servers will eventually complete dispersal disperse(M).  Agreement: If some correct server completes disperse(M), all correct servers eventually complete disperse(M).  Availability: If f + 1 correct servers complete disperse(M), a correct client can run retrieve( ) to eventually reconstruct some blockM . Additionally, if f + 1 correct servers complete disperse(M), a correct client can run read(i) where i  [1..m] to eventually obtain a fragment di .  Correctness: If f + 1 correct servers complete disperse(M), all correct clients that run retrieve( ) eventually retrieve the same block M . If the client that initiated disperse(M) was correct, then M  = M . Additionally, if f + 1 correct servers complete disperse(M), all correct clients that run read(i) for i  [1..m] eventually obtain the same fragmentd i . If the client that initiated disperse(M) was correct, then d i = di , where di is the i-th data fragment ofM . Theorem 9.2. AVID-FP-Pyramid is an asynchronous verifiable information dispersal protocol as defined above. BEAT-FR. Replacing the AVID-FP protocol in BEAT3 with our AVID-FP-Pyramid protocol, we obtain a new BFT storage protocol  BEAT-FR which has reduced read bandwidth. Corollary 9.3. BEAT-FR is a BFT storage. Instantiating BEAT-FR: BEAT4. BEAT-FR is a generic asynchronous BFT framework that reduces read bandwidth. BEAT4 is an instantiation to BEAT-FR for concrete parameters. In BEAT4, we set L = 2,m is even, and 0 = 1, which allows us to tolerate one failure within the local group, and reduces the read bandwidth by 50%. In BEAT4, we have n =m + 2f + 1,m = f + 1, and n = 3m  1. Note that the number of echo messages which a replica has to wait before it can send ready message in BEAT4 ism + f . Technique applicability. We comment that our technique presented in the section is general. While it is described for the setting of AVID-FP, it can be applied to all erasure-coded asynchronous verifiable information dispersal and erasure-coded reliable broadcast protocols, including AVID-CT [24] and AVID broadcast [24]. Therefore, the technique can be used to improve both erasure-coded BFT storage (BEAT3) and general SMR (BEAT0).\n\nWe utilize the HoneyBadgerBFT prototype as the baseline to implement six asynchronous BFT protocols, including five BEAT protocols (BEAT0 to BEAT4) and HB-Bracha. HB-Bracha is implemented to understand the latency overhead caused by erasure coding. HBBracha replaces the underlying erasure-coded reliable broadcast (AVID broadcast) with Brachas Broadcast [19], with the rest of the components intact. Each of the six protocols involves 6,000 to 8,000 lines of code in Python. The underlying erasure-coding schemes (Reed-Soloman codes and pyramid codes) and fingerprinted cross-checksum, however, are implemented in C. The design and implementation of BEAT is modular, and we have implemented the following building blocks for the protocols. Erasure coding support. HoneyBadgerBFT is 100% Python, and uses the zfec library to implement the Reed-Soloman code, an MDS erasure code. The zfec library, while popular in Python projects, suffers from both efficiency and usability issues: it supports only the traditional Reed-Soloman code implementation and supports only a word size (finite field size, a key tunable parameter in erasure coding for efficiency) of 8. Moreover, due to the usage of an erasure-coding library zfec [93], HoneyBadgerBFT supports at most 28 replicas. In BEAT, we instead use Jerasure 2.0 [73], a C library for erasurecoding, to implement the underlying erasure-coding schemes (including Reed-Soloman codes and pyramid codes). Jerasure 2.0 supports a variety of other coding schemes (including Cauchy ReedSoloman codes [75]), and allows fine-grained parameter tuning. Fingerprinted cross-checksum. We observe that for efficiency reasons one cannot separate the implementation of fingerprinting functions from the underlying erasure-coding support. The only implementation of fingerprinting is due to Hendricks et al. [45, 46]. They implemented their own erasure coding scheme using Rabins information dispersal scheme [77] and the corresponding fingerprinted cross-checksum using Shoups NTL [84]. While their fingerprinted cross-checksum is efficient, the erasure coding scheme is rather slow. In contrast, we use GF-Complete [74], the Jerasures underlying Galois Field library using Intel SSID, to implement the fingerprinted cross-checksum primitive. Erasure coding schemes have three parameters n, m, and w , where n is the number of fragments (also the number of replicas),m is the number of data fragments (where m = f +1 in our protocols), andw is the word size (the index size of the Galois Field GF(2w )). It is required that n +m < 2w and therefore n < 2w . The word sizew is typically set to be between 4 and 16 for efficiency, and indeedw = 32 is the largest value supported by Jerasure. However, for our applications, we need to use larger w = 64 or 128 for the security of fingerprinted cross-checksum. We therefore extend Jerasure to include these largews. The specific fingerprinting function we implemented is the evaluation fingerprinting [82]. Currently, we apply Horners rule to evaluate the polynomial directly, without leveraging faster lookup tables. While the implementation can be further improved, we find that the implementation can already improve all performance metrics significantly. We implement fingerprinted cross-checksum in C, with 3,500 lines of code. Finally, we use Cython [11] to wrap the C code in Python and support functions including Reed-Solomon codes, pyramid codes, matrix generation, coding padding, and fingerprinted cross-checksum. The implementation involves around 1,000 lines of code in Python.3 Threshold cryptography. We use the TDH2 scheme [85] for CCA-secure labeled threshold encryption and the threshold PRF scheme [22] for distributed coin flipping.We implement both schemes using the Charm [2] Python library. We use NIST recommended P-256 curve to implement both schemes to provide standard 128-bit security.\nOverview.We deploy and test our protocols on Amazon EC2 utilizing up to 92 nodes from ten different regions in five different continents. Each node is a general purposed t2.medium type with two virtual CPUs and 4GB memory. We evaluate our protocols in both LAN and WAN settings, where the LAN nodes are selected from the same Amazon EC2 region, and the WAN nodes are uniformly selected from different regions. We evaluate the protocols under different network sizes (number of replicas) and contention levels (batch sizes). For each experiment, we use f to represent the network size, where 3f + 1 nodes are launched in total for BEAT0 to BEAT3, HB-Bracha, and HoneyBadgerBFT (abbreviated as HB in the figures), and 3f + 2 nodes are used for BEAT4. We vary the batch size where nodes propose 1 to 20,000 transactions at a time. Bandwidth. The protocols mentioned above have rather different communication complexity. To order transactions of size B, the communication complexity of BEAT1, BEAT2, and HB-Bracha is O(n2B), the communication complexity of HoneyBadgerBFT and BEAT0 is O(nB), while the communication complexity of BEAT3 is only O(B). The consequence for throughput is significant: with the same bandwidth, BEAT3 and BEAT4 can process an order of magnitude more batched transactions, leading to significantly higher throughput. Our evaluation, however, does not set the bandwidth this way, but rather assumes the bandwidth is ample and assumes all protocols use the same batch size. The readers should be aware that BEAT3 and BEAT4 have much higher throughput if using a larger batch size. Latency. We first evaluate the latency in the LAN setting with f = 1, 2, 5, 10, and 15, respectively. We examine and compare the average latency under no contention where each node proposes a single transaction (with variable size) at a time and no concurrent requests are sent by the clients. In the LAN setting, network latency is relatively small, so the overhead is mainly caused by the protocols themselves. We report our result for f = 1, 2 in Figure 3. When f = 1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2 faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger. When f = 1, BEAT4 is about as fast as 3PyECLib [76] is popular python library for erasure-coding: it has a Python interface but implements C based library, Liberasurecode [61], which allows us to use existing erasure-coding library such as Jerasure[73] and Intel(R) ISA-L. We choose not to use PyECLib, primarily because the underlying Liberasurecode has implemented data structures that are not necessary for our purpose. We therefore (have to) write our own wrapper for Jerasure and fingerprinted cross-checksum using Cython [11]. HoneyBadger. This is primarily because BEAT4 has one more node, and the added overhead for the underlying consensus protocols and threshold cryptography is particularly visible when f is small. As f increases, HoneyBadger is much slower than BEAT4. Meanwhile, the difference between BEAT3 and BEAT4 becomes smaller; when f is 15, we barely notice the difference between them (not shown). The differences among BEAT0, BEAT1, and BEAT2 are rather small when the batch size is 1, but becomes much more visible when the batch size becomes larger. However, the difference between BEAT1 and BEAT2 is not as large as the difference between HoneyBadger and HB-Bracha. Meanwhile, when the batch size exceeds 1,000, BEAT0 becomes faster than BEAT1 (not shown). We further assess the latency breakdown for HoneyBadgerBFT, BEAT0, BEAT1, and HB-Bracha in order to better understand why we have these results. As illustrated in Figure 4, we evaluate the time for encrypting transactions, consensus protocols, and decrypting and combining transactions. We find the encryption and decryption for BEAT0 and BEAT1 are about three times faster than those in HoneyBadger and HB-Bracha. In addition, BEAT0 and BEAT1 use threshold PRF to produce the common coins for the consensus, and the latency of the consensus is also reduced by about 50%. HBBracha also achieves lower latency than HoneyBadgerBFT due to the use of latency-optimized Brachas broadcast. This also explains why BEAT1 has lower latency than BEAT0 when the batch size is small. Throughput.We evaluate the throughput of the protocols under different contention levels.We present the results in the LAN setting in Figure 5(a) and the the result in the WAN setting in Figure 5(b). Both cases set f = 1. We also show latency vs. throughput in Figure 5(c). We first notice that BEAT0 slightly outperforms HoneyBadgerBFT in both settings. This is expected since BEAT0 employs optimized threshold cryptography. This also matches the result for the latency under no contention. In comparison, while BEAT1, BEAT2, and HB-Bracha are latency optimized, they do not outperform HoneyBadgerBFT in terms of throughput. We observe that in both the LAN setting and WAN setting, BEAT1, BEAT2, and HBBracha achieve higher throughput than HoneyBadgerBFT when the batch size is small. However, when batch size is higher than 5000, all the three protocols have 20% to 30% lower throughput than HoneyBadgerBFT. This is mainly because HB-Bracha consumes higher network bandwidth, which causes degradation when the batch size is large. This underscores the wisdom in designing HoneyBadgerBFT. BEAT3 and BEAT4 outperform HoneyBadgerBFT consistently. They also outperform BEAT0, BEAT1, and BEAT2 consistently, though under low contention in the LAN setting, BEAT1 has larger throughput than the other protocols. These results also meet our expectation since BEAT3 and BEAT4 are bandwidth optimized. Again, we stress that we compare the performance of the protocols under the same batch size. BEAT3 and BEAT4 actually use much lower network bandwidth than the other protocols, and so for the same bandwidth budget, BEAT3 and BEAT4 (with more aggressive batching) will achievemuch better throughput compared with other protocols. Scalability.We evaluate the scalability of BEAT0, BEAT3, and HoneyBadger by varying f from 1 to 30. We report our comparison between BEAT3 and HoneyBadger in Figure 5(d) (without BEAT0, for ease of illustration). We observe that the throughput for both protocols is in general higher when the number of replicas is smaller. Peak throughput for BEAT3 is reached in all the cases when the batch size is greater than 15,000. In the most extreme case for our experiment, where f = 30 and batch size is 20,000, the average latency is about 1.5 minutes. As we can see in the figure, BEAT3 outperforms HoneyBadgerBFT in all the cases. However, the difference between BEAT3 and HoneyBadgerBFT becomes smaller as the number of replicas grows. This is in part due to the fact that in large-scale networks, network latency may dominate the overhead of the protocol. BEAT0 has performance between BEAT3 and HoneyBadger, and again when f increases their difference becomes smaller.\nWe implemented six new protocols (BEAT instances andHB-Bracha). Whilemany of these protocols use similar components, maintaining, deploying, and comparing different BEAT instances takes tremendous effort. While one of our goals is to make BEAT modular and extensible, in practice it is still challenging to develop all the variants of the protocols. This is in part because even for the same function (e.g., threshold encryption), different APIs need to maintained. In fact, changing a small function in a BEAT instance may need to touch a large number of related functions accordingly. On the other hand, we find that perhaps surprisingly, it may be easier to develop and deploy asynchronous BFT than partially synchronous BFT, for at least two reasons. First, protocols assuming partial synchrony rely on view change subprotocols, which are very difficult to implement well from our own experience and from the fact that a significant number of academic papers choose not to implement the view change protocols. Second, because of native robustness against timing and liveness attacks for asynchronous BFT, we simply do not need to take further measures to ensure robustness.\nWe describe the design and implementation of BEAT, a family of practical asynchronous BFT protocols that are efficient, flexible, versatile, and extensible. We deploy and evaluate the five BEAT protocols using 92 instances on Amazon EC2, and we show BEAT protocols are significantly more efficient than HoneyBadgerBFT, the most efficient asynchronous BFT known. We also develop new distributed system ingredients, including generalized fingerprinted cross-checksum and new asynchronous verifiable information dispersal, which might be of independent interest.\nThe authors are indebted to our shepherd Haibo Chen and the CCS reviewers for their helpful comments that greatly improve our paper.\nProof of Theorem 9.2. Termination is simple, as in AVID-FP. If a correct server initiates disperse, the server erasures codes the transaction, and sends fragments and the fingerprinted cross-checksum to each server. As the server initiating disperse is correct, at least n f  m+0(L 1)+ f correct servers receive dispersemessages, and send echo messages to all servers. Each server will eventually receivem + 0(L  1) + f echo messages, and then sends a ready message, if it has not done so. Each correct server will eventually receive at lest 2f + 1 ready messages, and will then store the fingerprinted cross-checksum and complete. Agreement follows exactly as in AVID-FP. If some correct server completes disperse(M), then the server must have received 2f + 1 ready messages and at least f + 1 ready messages much have come from correct servers. This means that all correct servers will eventually receive ready messages from these correct servers. As our protocol implements the amplification step as in all other Brachas broadcast like broadcast, all correct servers will send ready messages, and all of them will eventually receive at least 2f + 1 ready messages. Agreement thus follows. We first prove the first part of availability. In our protocol, if a correct server completes disperse, it must have received 2f +1 ready messages, and at least one correct server receivedm +0(L 1)+ f echo messages. Therefore, at least m + 0(L  1) correct servers stored consistent fragments. According to the property of pyramid codes, these fragments can be used to reconstruct the original block. Accordingly, if f + 1 correct servers complete disperse, any client that initiates retrievewill receivem+0(L1) consistent fragments and f + 1 matching fingerprinted cross-checksums. The client can then decode the fragments to generate some block. We now prove the second part of availability. Following an analogous line of the above argument, if a correct server completes disperse, at least m + 0(L  1) correct servers stored consistent fragments. If f + 1 correct servers complete disperse, any client that initiates read(i) for i  [1..m] will receive f + 1 matching fingerprinted cross-checksums. If the fragment i happens to be available or there is less than 0 failures in the local group, the fragment will be available for the client. Otherwise, another round of interaction is needed, and the client will obtainm + 0(L  1) consistent fragments and reconstruct the fragment needed. We now prove correctness. We first claim that if some correct server delivers fpcc1 and some correct server delivers fpcc2, then fpcc1 = fpcc2. The proof is quite standard for a quorum based protocol: if fpcc1 is delivered thenm +0(L 1)+ f servers echoed fpcc1, of which at leastm+0(L 1) is correct. The same applied to fpcc2. As a correct server will only echo once, there are at least 2m+ 20(L  1) + f servers echoed, which is larger than the total server (note that L  2 and 2f  (0 + 1)). This leads to a contradiction. Thus, any block decoded during retrieve or any fragment during read is consistent with the same fpcc. By Theorem 3.4 in [45] and by Lemma 9.1, the probability that clients do not obtain the same block or fragment(s) is negligible.\n", "prediction": "in this paper, we show that there is a fundamental trade - off between the amount of privacy that can be shared by an adversary and the speed with which it can retrieve the data that belongs to the adversary.<n> we prove that this tradeoff leads to significant benefits in terms of both security and speed : in our design ( which is based on fault tolerance ) we are able to achieve significantly more security than is possible with fault - tolerant protocols that only support secure storage ; and we can achieve much faster recovery times for our protocols compared to those that do not support recovery of data from adversaries that belong to a class of classes that we refer to as the  adversary class ''<n> [ 1 ] the problem of sharing secret information in a secure way is of fundamental importance in cryptography and information theory @xcite and has attracted a great deal of attention in the last few years ( see for a review on this subject _<n> http://arxiv.org/abs/cond-mat/0406484 __  <n> ]. in order to guarantee the security of the information shared in an exchange of quantum resources ( e.g. the quantum bits ( qubits ), see ref.@xmath0 for an overview of how to share qubit information efficiently on a quantum computer and refs.[4][e]for an introduction to quantum computing see the introduction of [5 [e5[e4[5 ], see reference [6[6 [4 [8[i[8 [[[n[b[43 s[50[a[f[2[22 ][[42 t[21] in [42e[45[in [43[s [b [50 [n [45] [in[62[r[52] we extend [f [ 42 [ [eb [ [d[v[x[we [] to [. [... [ 46[re[ [ in  [ to! [ '[@e [ 43[] ]) we have to solve [ e[(e  to our [ we also [ one [ based []. [ for [ 41 [ the [ @ [ a [ address [ and [ that [ with [ f[the [ which  we  the other [ as [ye [vi[g  in which we support [ further  for the others  that ]] ...  a  based  which [ ( [ is [ from [i  with the most [<n> the algorithm [ while [ it  is  among [ additionally [ of  from the error [ whose [ they  while the database [ among the...... to which has [([.  of our[is [, [ se  one  has  our  whose  it is not [ who is the].  and our algorithm to be the message [ are [ also to have [], [ but [x  also  was [ this [ - [ make the protocols [ x[se  this th[... the graph [is the storage of a further to... that is that has the entire [forward [ storage and a. we do [ p[vi [se [4]  others [ both [ cryptography [ more  they are not the whole [ error of all [ graph and... and levi[ye  who has a... we to all the software  ( the] and that was the function [ these [ each of these com  will  as we will also the[it is to we is also we has to support the we find that the results [2].... with a]. and is for any other that does not "}
{"ground_truth": "To operate reliably in real-world traffic, an autonomous car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. This paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of other agents. Each policy captures a distinct high-level behavior and intention, such as driving along a lane or turning at an intersection. We first employ Bayesian changepoint detection on the observed history of states of nearby cars to estimate the distribution over potential policies that each nearby car might be executing. We then sample policies from these distributions to obtain high-likelihood actions for each participating vehicle. Through closed-loop forward simulation of these samples, we can evaluate the outcomes of the interaction of our vehicle with other participants (e.g., a merging vehicle accelerates and we slow down to make room for it, or the vehicle in front of ours suddenly slows down and we decide to pass it). Based on those samples, our vehicle then executes the policy with the maximum expected reward value. Thus, our system is able to make decisions based on coupled interactions between cars in a tractable manner. This work extends our previous multipolicy system [11] by incorporating behavioral anticipation into decision-making to evaluate sampled potential vehicle interactions. We evaluate our approach using real-world traffic-tracking data from our autonomous vehicle platform, and present decision-making results in simulation involving highway traffic scenarios.\nI. INTRODUCTION Decision-making for autonomous driving is hard due to uncertainty on the continuous state of nearby vehicles and, in particular, due to uncertainty over their discrete potential intentions (such as turning at an intersection or changing lanes). Previous approaches have employed hand-tuned heuristics [28, 29, 41] and numerical optimization [17, 21, 42], but these methods fail to capture the coupled dynamic effects of interacting traffic agents. Partially observable Markov decision process (POMDP) solvers [2, 26, 35] offer a theoreticallygrounded framework to capture these interactions, but have difficulty scaling up to real-world scenarios. In addition, current approaches for anticipating future intentions of other traffic agents [1, 22, 24, 25] either consider only the current state of the target vehicle, ignoring the history of its past actions, or rather require expensive collection of training data. In this paper, we present an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. This approach is made tractable by considering only a finite set of a priori known policies. Each policy is designed to capture a different high-level behavior, such as following a lane, changing lanes, or turning at an intersection. Our system proceeds in a sequence of two interleaved stages of behavioral prediction and decision-making. In the first stage, we estimate the probability distribution over the potential policies other traffic agents may be executing. To this aim, we leverage Bayesian changepoint detection to estimate which policy a given vehicle was executing at each point in its history of actions, and then infer the likelihood of each potential intention of the vehicle. Furthermore, we propose a statistical test based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. Individual policies can therefore adjust their behavior to react to anomalous cars. In the second stage, we use this distribution to sample over permutations of other vehicle policies and the policies available for our car, with forward-simulation of these sampled intentions to evaluate their outcomes via a user-defined reward function. Our vehicle finally executes the policy that maximizes the expected reward given the sampled outcomes. Thus, our system is able to make decisions based on closedloop interactions between cars in a tractable manner. We evaluate our behavioral prediction system using a realworld autonomous vehicle, and present decision-making results in simulation involving highway traffic scenarios. The central contributions of this paper are:  A changepoint-based behavioral prediction approach that leverages the history of actions of a target vehicle to infer the likelihood of its possible future actions and detect anomalous behavior online.  A decision-making algorithm that evaluates the outcomes of modeled interactions between vehicles, being able to account for the effect of its actions on the future reactions of other participants.  An evaluation of the proposed system using both traffic data obtained from a real-world autonomous vehicle and simulated traffic scenarios. This work extends our earlier work [11], where we proposed the strategy of selecting between multiple policies for our car by evaluating them via forward simulation, and demonstrated passing maneuvers using a real-world autonomous vehicle. However, that work did not address anticipation of policies for other cars. In contrast, this paper presents a fully integrated behavioral anticipation and decision-making approach.\n\nDespite the probabilistic nature of the anticipation problem, some approaches in the literature assume no uncertainty on the future states of other participants [10, 31, 33]. Such an approach could be justified in a scenario where vehicles broadcast their intentions over some communications channel, but it is an unrealistic assumption otherwise. Some approaches assume a dynamic model of the obstacle and propagate its state using standard filtering techniques such as the extended Kalman filter [13, 18]. Despite providing rigorous probabilistic estimates over an obstacles future states, these methods often perform poorly when dealing with nonlinearities in the assumed dynamics model and the multimodalities induced by discrete decisions (e.g. continuing straight, merging, or passing). Some researchers have explored using Gaussian mixture models (GMMs) [14, 22] and contextsensitive models [19, 20] to account for nonlinearities and multiple discrete decisions. However, this approach does not consider the history of previous states of the target object, assigning an equal likelihood to each discrete hypothesis and leading to a conservative estimate. A common anticipation strategy in autonomous driving [7, 16, 21] consists in computing the possible goals of a target vehicle by planning from its standpoint, accounting for its current state. This strategy is similar to our factorization of potential driving behavior into a set of policies, but lacks our closed-loop simulation of vehicle interactions. Recent work uses Gaussian process (GP) regression to learn typical motion patterns for classification and prediction of agent trajectories [24, 25, 40], particularly in autonomous driving [1, 38, 39]. Nonetheless, these methods require collecting training data to reflect all possible motion patterns the system may encounter, which can be time consuming. For instance, a lane change motion pattern learned in urban roads will not be representative of the same maneuver performed at higher speeds on the highway.\nThe first instances of decision making systems for autonomous vehicles capable of handling urban traffic situations stem from the 2007 DARPA Urban Challenge [12]. In that event, participants tackled decision making using a variety of solutions ranging from finite state machines (FSMs) [29] and decision trees [28] to several heuristics [41]. However, these approaches were tailored for very specific and simplified situations and were, even according to their authors, not robust to a varied world [41]. More recent approaches have addressed the decision making problem for autonomous driving through the lens of trajectory optimization [17, 21, 42]. However, these methods do not model the closed-loop interactions between vehicles, failing to reason about their potential outcomes. The POMDP model provides a mathematically rigorous formulation of the decision making problem in dynamic, uncertain scenarios such as autonomous driving. Unfortunately, finding an optimal solution to most POMDPs is intractable [27, 32]. A variety of general [2, 5, 26, 35, 37] and domainspecific [8] POMDP solvers exist in the literature that seek to approximate the solution. Nonetheless, online application of POMDP solvers [6] remains challenging because they often explore unlikely regions of the belief space. The idea of assuming finite sets of policies to speed up planning has appeared before in the POMDP literature [3, 23, 36]. However, these approaches dedicate significant resources to compute their sets of policies, and as a result they are limited to short planning horizons and relatively small state, observation, and action spaces. In contrast, we propose to exploit domain knowledge to design a set of policies that are readily available at planning time.\nWe first formulate the problem of decision making in dynamic, uncertain environments with tightly coupled interactions between multiple agents as a multiagent POMDP. We then show how we exploit autonomous driving domain knowledge to make approximations to the POMDP formulation, thus enabling principled decisions in a tractable manner.\nLet V denote the set of vehicles interacting in a local neighborhood of our vehicle, including our controlled vehicle. At time t, a vehicle v  V can take an action avt  Av to transition from state xvt  X v to xvt+1. In our system, a state xvt is a tuple of the pose, velocity, and acceleration and an action avt is a tuple of controls for steering, throttle, brake, shifter, and directionals. As a notational convenience, let xt include all state variables xvt for all vehicles at time t, and similarly let at  A be the actions of all vehicles. We model the vehicle dynamics with a conditional probability function T (xt, at, xt+1) = p(xt+1|xt, at). Similarly, we model observation uncertainty as Z(xt, zvt ) = p(z v t |xt), where zvt  Zv is the observation made by vehicle v at time t, and zt  Z is the vector of all sensor observations made by all vehicles. In our system, an observation zvt is a tuple including the estimated poses and velocities of nearby vehicles and an occupancy grid of static obstacles. Further, we model uncertainty on the behavior of other agents with the following driver model: D(xt, zvt , a v t ) = p(a v t |xt, zvt ), where avt  A is a latent variable that must be inferred from sensor observations. Our vehicles goal is to find an optimal policy  that maximizes the expected reward over a given decision horizon H , where a policy is a mapping  : X  Zv  Av that yields an action from the current maximum a posteriori (MAP) estimate of the state and an observation:  = argmax  E [ H t=t0  X R(xt)p(xt) dxt ] , (1) where R(xt) is a real-valued reward function R : X  R. The evolution of p(xt) over time is governed by p(xt+1) =  X Z A p(xt+1|xt, at)p(zt|xt) p(at|xt, zt)p(xt) dat dzt dxt. (2) The driver model D(xt, zvt , a v t ) implicitly assumes that the instantaneous actions of each vehicle are independent of each other, since avt is conditioned only on xt and z v t . However, modeled agents can still react to the observed states of nearby vehicles via zvt . That is to say that vehicles do not collaborate with each other, as would be implied by an action avt dependent on at. Thus, the joint density for a single vehicle v can be written as pv(xvt , x v t+1, z v t , a v t ) = p(x v t+1|xvt , avt )p(zvt |xvt ) p(avt |xvt , zvt )p(xvt ), (3) and the independence assumption finally leads to p(xt+1) =  vV  Xv Zv Av pv(xvt , x v t+1, z v t , a v t ) da v t dz v t dx v t . (4) Despite assuming independent vehicle actions, marginalizing over the large state, observation and action spaces in Eq. 4 is too expensive to find an optimal policy online in a timely manner. A possible approximation to speed up the process, commonly used by general POMDP solvers [2, 37] is to solve Eq. 1 by drawing samples from p(xt). However, sampling over the full probability space with random walks will yield a large number of low probability samples (see Fig. 1). This paper presents an approach designed to sample from high likelihood scenarios such that the decision-making process is tractable.\nWe make the following approximations to sample from the likely interactions of traffic agents: 1) At any given time, both our vehicle and other vehicles are executing a policy from a discrete set of policies. 2) We approximate the vehicle dynamics and observation models through deterministic, closed-loop forward simulation of all vehicles with assigned policies. These approximations allow us to evaluate the consequences of our decisions over a limited set of high-level behaviors determined by the available policies (for both our vehicle and other agents), rather than performing the evaluation for every possible control input of every vehicle. Let  be a discrete set of policies, where each policy captures a specific high-level driving behavior. Let each policy    be parameterized by a parameter vector  capturing variations of the given policy. For example, for a lanefollowing policy,  can capture the driving style of the policy by regulating its acceleration profile to be more or less aggressive. We thus reduce the search in Eq. 1 to a limited set of policies. By assuming each vehicle v  V is executing a policy vt   at time t, the driver model for other agents can be now expressed as: D(xt, z v t , a v t ,  v t ) = p(a v t |xt, zvt , vt )p(vt |xt, z0:t), (5) where p(vt |xt, z0:t) is the probability that vehicle v is executing the policy vt (we describe how we infer this probability in IV). Thus, the per-vehicle joint density from Eq. 3 can now be approximated in terms of vt : pv(xvt , x v t+1, z v t , a v t ,  v t ) = p(x v t+1|xvt , avt )p(zvt |xvt ) p(avt |xvt , zvt , vt )p(vt |xt, z0:t)p(xvt ). (6) Finally, since we have full authority over the policy executed by our controlled car q  V , we can separate our vehicle from the other agents in p(xt+1) as follows: p(xt+1)   X q Zq pq(xqt , x q t+1, z q t , a q t ,  q t ) dz q t dx q t  vV |v 6=q    Xv Zv pv(xvt , x v t+1, z v t , a v t ,  v t ) dz v t dx v t  . (7) We have thus far factored out the action space from p(xt+1) by assuming actions are given by the available policies. However, Eq. 7 still requires integration over the state and observation spaces. Our second approximation addresses this issue. Given samples from p(vt |xt, z0:t) that assign a policy to each vehicle, we simulate forward in time the interactions of our vehicle and other vehicles under their assigned policies, and obtain a corresponding sequence of future states and observations. We are thereby able to evaluate the reward function over the entire decision horizon.\nIn this section, we describe how we infer the probability of the policies executed by other cars and their parameters. Our behavioral anticipation method is based on a segmentation of the history of observed states of each vehicle, where each segment is associated with the policy most likely to have generated the observations in the segment. We obtain this segmentation using Bayesian changepoint detection, which infers the points in the history of observations where the underlying policy generating the observations changes. Thereby, we can compute the likelihood of all available policies for the target car given the observations in the most recent segment, capturing the distribution p(vt |xt, z0:t) over the cars potential policies at the current timestep. Further, full history segmentation allows us to detect anomalous behavior that is not explained by the set of policies in our system. The changepoint-detection procedure is illustrated by the simulation in Fig. 2. We next describe the anticipation method for a single vehicle, which we then apply successively to all nearby vehicles.\nTo segment a target cars history of observed states, we adopt the recently proposed CHAMP algorithm by Niekum et al. [30], which builds upon the work of Fearnhead and Liu [15]. Given the set of available policies  and a time series of the observed states of a given vehicle z1:n = (z1, z2, . . . , zn), CHAMP infers the MAP set of times 1, 2, . . . , m, at which changepoints between policies have occurred, yielding m+ 1 segments. Thus, the ith segment consists of observations zi+1:i+1 and has an associated policy i   with parameters i. The changepoint positions are modeled as a Markov chain where the transition probabilites are a function of the time since the last changepoint: p(i+1 = t|i = s) = g(t s), (8) where g() is a pdf over time, and G() denotes its cdf. Given a segment from time s to t and a policy , CHAMP approximates the logarithm of the policy evidence for that segment via the Bayesian information criterion (BIC) [4] as: logL(s, t, )  log p(zs+1:t|, ) 1 2 k log(t s), (9) where k is the number of parameters of policy  and  are estimated parameters for policy . The BIC is a well-known approximation that avoids marginalizing over the policy parameters and provides a principled penalty against complex policies by assuming a Gaussian posterior around the estimated parameters . Thus, only the ability to fit policies to the observed data is required, which can be achieved via a maximum likelihood estimation (MLE) method of choice (we elaborate on this in IV-B). As shown by Fearnhead and Liu [15], the distribution Ct over the position of the first changepoint before time t can be estimated efficiently using standard Bayesian filtering and an online Viterbi algorithm. Defining Pt(j, q) = p(Ct = j, q, Ej , z1:t) (10) PMAPt = p(Changepoint at t, Et, z1:t), (11) where Ej is the event that the MAP choice of changepoints has occurred prior to a given changepoint at time j, results in: Pt(j, q) = (1G(t j  1))L(j, t, q)p(q)PMAPj (12) PMAPt = max j,q [ g(t j) 1G(t j  1) Pt(j, q) ] . (13) At any time, the most likely sequence of latent policies (called the Viterbi path) that results in the sequence of observations can be recovered by finding (j, q) that maximize PMAPt , and then repeating the maximization for PMAPj , successively until time zero is reached. Further details on this changepoint detection method are provided by Niekum et al. [30].\nIn contrast with other anticipation approaches in the literature which consider only the current state of the target vehicle and assign equal likelihood to all its potential intentions [16, 21, 22], here we compute the likelihood of each latent policy by leveraging changepoint detection on the history of observed vehicle states. Consider the (m + 1)th segment (the most recent), obtained via changepoint detection and consisting of observations zm+1:n. The likelihood and parameters of each latent policy    for the target vehicle given the present segment can be computed by solving the following MLE problem:   , L() = argmax  log p(zm+1:n|, ). (14) Specifically, we assume p(zm+1:n|, ) to be a multivariate Gaussian with mean at the trajectory , obtained by simulating forward in time the execution of policy  under parameters  from timestep m + 1: p(zm+1:n|, ) = N (zm+1:n;,, I), (15) where  is a nuisance parameter capturing modeling error and I is a suitable identity matrix (we discuss our forward simulation of policies further in V-B). That is, Eq. 15 essentially measures the deviation of the observed states from those prescribed by the given policy. The policy likelihoods obtained via Eq. 14 capture the probability distribution over the possible policies that the observed vehicle might be executing at the current timestep, which can be represented, using delta functions, as a mixture distribution: p(vt |xt, z0:t) =  || i=1 (i)  L(i), (16) where i is the hypothesis over policy i and  is a normalizing constant. We can therefore compute the approximated posterior of Eq. 7 by sampling from this distribution for each vehicle, obtaining high-likelihood samples from the coupled interactions of traffic agents.\nThe time-series segmentation obtained via changepoint detection allows us to perform online detection of anomalous behavior not modeled by our policies. Inspired by prior work on anomaly detection [9, 25, 34], we first define the properties of anomalous behavior in terms of policy likelihoods, and then compare the observed data against labeled normal patterns in previously-recorded vehicle trajectories. Thus, we define the following two criteria for anomalous behavior: 1) Unlikelihood against available policies. Anomalous behavior is not likely to be explained by any of the available policies, since they are designed to abide by traffic rules and provide a smooth riding experience. Therefore, behaviors like driving in the wrong direction or crossing a solid line on the highway will not be captured by the available policies. We thus measure the average likelihood among all segments in the vehicles history as the global similarity of the observed history to all available policies: S = 1 m+ 1 m+1 i=1 L(i), (17) where i is the policy associated with the ith segment. 2) Ambiguity among policies. A history segmentation that fluctuates frequently among different policies might be a sign of ambiguity on the segmentation. To express this criterion formally, we first construct a histogram capturing the occurrences of each policy in the vehicles segmented history. A histogram with a broad spread indicates frequent fluctuation, whereas one with a single mode is more likely to correspond to normal behavior. We measure this characteristic as the excess kurtosis of the histogram,  = 44  3, where 4 is the fourth moment of the mean and  is the standard deviation. The excess kurtosis satisfies 2 <  < . If  = 0, the histogram resembles a normal distribution, whereas if  < 0, the histogram presents a broader spread. That is, we seek to identify changepoint sequences where there is no dominant policy. Using these criteria, we define the following normality measure given a vehicles MAP choice of changepoints: N = 1 2 [(+ 2)S] . (18) This normality measure on the target cars history can then be compared to that of a set of previously recorded trajectories of other vehicles. We thus define the normality test for the current vehicles history as N < 0.5, where  is the minimum normality measure evaluated on the prior time-series.\nWe now present the policy selection procedure for our car (Algorithm 1), which implements the formulation and approximations given in III by leveraging the anticipation scheme from IV. The algorithm begins by drawing a set of samples s  S from the distribution over policies of other cars via Eq. 16, where each sample assigns a policy v   to each nearby vehicle v, excluding our car. For each policy  available to our car and for each sample s, we roll out forward in time until the decision horizon H all vehicles under the policy assignments (, s) with closed loop simulation to yield a set  of simulated trajectories . We then evaluate the reward r,s for each rollout , and finally select the policy  maximizing the expected reward. The process continuously repeats in a receding horizon manner. Note that policies that are not applicable given the current state x0, such as an intersection handling policy when driving on the highway, are not considered for selection (line 5). We next discuss three key points of our decision-making procedure: the design of the set of available policies, using forward simulation to roll out potential interactions, and the reward function. Algorithm 1: Policy selection procedure. Input:  Current MAP estimate of the state, x0.  Set of available policies .  Policy assignment probabilities (Eq. 16).  Planning horizon H . 1 Draw a set of samples s  S via Eq. 16, where each sample assigns a policy to each nearby vehicle. 2 R   // Rewards for each rollout 3 foreach    do // Policies for our car 4 foreach s  S do // Policies for other cars 5 if APPLICABLE(, x0) then 6 ,s  SIMULATEFORWARD(x0, , s,H) // ,s captures all vehicles 7 R  R{(, s, COMPUTEREWARD(,s))} 8 return   SELECTBEST(R)\nThere are many possible design choices for engineering the set of available policies in our approach, which we wish to explore in future work. However, in this work we use a set of policies that covers many in-lane and intersection driving situations, comprising the following policies: lane-nominal, drive in the current lane and maintain distance to the car directly in front; lane-change-right/lane-change-left, separate policies for a single lane change in each direction; and turnright, turn-left, go-straight, or yield at an intersection.\nWhile it is possible to perform high-fidelity simulation for rolling out sampled policy assignments, a lower-fidelity simulation can capture the necessary interactions between vehicles to make reasonable choices for our vehicle behavior, while providing faster performance. In practice, we use a simplified simulation model for each vehicle that assumes an idealized steering controller. Nonetheless, this simplification still faithfully describes the high-level behavior of the between-vehicle interactions our method reasons about. For vehicles classified as anomalous, we simulate them using a single policy accounting only for their current state and map of the environment, since they are not likely to be modeled by the set of behaviors in our system.\nThe reward function for evaluating the outcome of a rollout  involving all vehicles is a weighted combination of metrics mq()  M, with weights wq that express user importance. The construction of a reward function based on a flexible set of metrics derives from our previous work [11], which we extend here to handle multiple potential policies for other vehicles. In our system, typical metrics include the distance to the goal at the end of the evaluation horizon as a measure of accomplishment, minimum distance to obstacles to evaluate safety, a lane choice bias to add a preference for the right lane, and the maximum yaw rate and longitudinal jerk to measure passenger comfort. For a full policy assignment (, s) with rollout ,s, we compute the rollout reward r,s as the weighted sum r,s = |M| q=1 wqmq( ,s). We normalize each mq(,s) across all rollouts to ensure comparability between metrics. To avoid biasing decisions, we set the weight wq to zero when the range of mq() across all samples is too small to be informative. We finally evaluate each policy reward r for our vehicle as the expected reward over all rollout rewards r,s, computed as r = |S| k=1 r,skp(sk), where p(sk) is the joint probability of the policy assignments in sample sk, computed as a product of the per-vehicle assignment probabilities (Eq. 16). We use expected reward to target better average-case performance, as it is easy to become overly conservative when negotiating traffic if one only accounts for worst-case behavior. By weighting by the probability of each sample, we can avoid overcorrecting for low-probability events.\nTo evaluate our behavioral anticipation method and our multipolicy sampling strategy, we use traffic-tracking data collected using our autonomous vehicle platform. We first introduce the traffic-tracking dataset and the vehicle used to collect it. Next, we use this dataset to evaluate our prediction and anomaly detection method and the performance of our multipolicy sampling strategy. Finally, we evaluate our multipolicy approach performing integrated behavioral analysis and decision-making on highway traffic scenarios using our multivehicle simulation engine.\nTo collect the traffic-tracking dataset we use in this work, we have used our autonomous vehicle platform (shown in Fig. 3), a 2013 Ford Fusion equipped with a sensor suite including four Velodyne HDL-32E 3D LIDAR scanners, an Applanix POSLV 420 inertial navigation system (INS), GPS, and several other sensors. The vehicle uses prior maps of the area it operates on that capture information about the environment such as LIDAR reflectivity and road height, and are used for localization and tracking of other agents. The road network is encoded as a metric-topological map that provides information about the location and connectivity of road segments, and lanes therein. Estimates over the states of other traffic participants are provided by a dynamic object tracker running on the vehicle, which uses LIDAR range measurements. The geometry and location of static obstacles are also inferred onboard using LIDAR measurements. The traffic-tracking dataset consists of 67 dynamic object trajectories recorded in an urban area. Of these 67 trajectories (shown in Fig. 4), 18 correspond to follow the lane maneuvers and 20 to lane change maneuvers, recorded on a divided highway. The remaining 29 trajectories correspond to maneuvers observed at a four-way intersection regulated by stop signs. All trajectories were recorded by the dynamic object tracker onboard the vehicle and extracted from approximately 3.5 h of total tracking data. In all experiments we use a C implementation of our system running on a single 2.8GHz Intel i7 laptop computer.\nFor our system, we are interested in correctly identifying the behavior of target vehicles by associating it to the most likely policy according to the observations. Thus, we evaluate our behavioral analysis method in the context of a classification problem, where we want to map each trajectory to the underlying policy (class) that is generating it at the current timestep. The available policies used in this evaluation are:  = {lane-nominal, lane-change-left, lane-change-right}  {turn-right, turn-left, go-straight, yield}, (19) where the first subset applies to in-lane maneuvers and the second subset applies to intersection maneuvers. For all policies we use a fixed set of parameters tuned empirically to control our autonomous vehicle platform, including maximum longitudinal and lateral accelerations, and allowed distances to nearby cars, among other parameters. To assess each classification as correct or incorrect, we leverage the road network map and compare the final lane where the trajectory actually ends to that predicted by the declared policy. In addition, we assess behavioral prediction performance on subsequences of incremental duration of the input trajectory, measuring classification performance on increasingly longer observation sequences. Fig. 5 shows the accuracy and precision curves for policy classification over the entire dataset. The ambiguity among hypotheses results in poor performance when only an early stage of the trajectories is used, especially under 30% completion. However, we are able to classify the trajectories with over 85% accuracy and precision after only 50% of the trajectory has been completed. Note, however, that the closed-loop nature of our policies allows us to maintain safety at all times regardless of anticipation performance.\nWe now qualitatively explore the performance of our anomaly detection test. We recorded three additional trajectories corresponding to two bikes and a bus. The bikes crossed the intersection from the sidewalk, while the bus made a significantly wide turn. We run the test on these trajectories and on three additional intersection trajectories using the minimum normality value on the intersection portion of the dataset,  = 0.1233. As shown by the results in Fig. 6, our test is able to correctly detect the anomalous behaviors not modeled in our system.\nTo show that our approach makes decision-making tractable, we assess the sampling performance in terms of the likelihood of the samples using the recorded intersection trajectories. We compare our multipolicy sampling strategy to an uninformed sampling strategy such as those used by general decisionmaking algorithms that do not account for domain knowledge to focus sampling (e.g., Silver and Veness [35], Thrun [37]). We take groups of coupled trajectories from the dataset involving from one to four vehicles negotiating the intersection simultaneously. For each vehicle in each group, we compute, via Eq. 15, the likelihood of the most likely policy ML in {turn-right, turn-left, go-straight, yield} according to the corresponding trajectory in the group. We then evaluate the computation time required by each of the two sampling strategies to find a sampled trajectory with a likelihood equal or greater than L(ML). The uninformed strategy generates, for each vehicle involved, a trajectory that either remains static for the duration of the trajectory to yield or crosses the intersection at constant speed. This decision is made at random. If the decision is to cross, the direction of the vehicle is determined via random steering wheel angle rates in a simple car kinematic model. Conversely, the multipolicy sampling strategy consists of randomly selecting policies for each vehicle and obtaining their rollouts. The computation times for each strategy are shown in Table I. Times are computed out of 100 simulations for each case (from one to four cars). Although the time required grows dramatically fast for both strategies due to the combinatorial explosion of vehicle intentions, these results show that our multipolicy sampling strategy is able to find high-likelihood samples orders of magnitude faster than an uninformed sampling strategy. A visualization of a sample simulation of this experiment is shown in Fig. 1.\nWe tested the full decision-making algorithm with behavioral prediction in a simulated environment with a multi-lane highway scenario involving two nearby cars. Fig. 7(a) shows the scenario used for testing at an illustrative point at half way through the scenario. This simulation uses the same policy models we have developed and tested on our real-world test car [11]. Fig. 7(b) shows the policy reward function, in which the chosen policy is the maximum of the available policies. Note that this decision process is instantaneous, which explains the oscillations when policies are near decision surfaces. We prevent the executed policy from oscillating with a simple pre-emption model that ensures we only switch policies when distinct maneuvers (such as lane-changes) are complete. We collected timing information on different operations in the experiment to evaluate runtime performance. The main expense is forward simulation and metric evaluation for each rollout, however, these tasks are easily parallelizable. In the test scenario in which we rollout all sample permutations, the theoretical maximum number of rollouts is 27 given 3 policy options per vehicle, but in practice the maximum number of rollouts was 12, with a mean of 8.6. This smaller number of rollouts is because not all policies are applicable at once. Parallel evaluation performance is bounded by the maximum time for a single rollout, for which the mean worst time was 84ms, and the worst time over the whole experiment was 106ms. Even in the worst case, our real-time decision-making target of 1 Hz is acheiveable.\nWe introduced a principled framework for integrated behavioral anticipation and decision-making in environments with extensively coupled interactions between agents. By explicitly modeling reasonable behaviors of both our vehicle and other vehicles as policies, we make informed high-level behavioral decisions that account for the consequences of our actions. We presented a behavior analysis and anticipation system based on Bayesian changepoint detection that infers the likelihood of policies of other vehicles. Furthermore, we provided a normality test to detect unexpected behavior of other traffic participants. We have shown that our behavioral anticipation approach can identify the most-likely underlying policies that explain the observed behavior of other cars, and to detect anomalous behavior not modeled by the policies in our system. In future work we will explicitly model unexpected behavior, such as the appearance of a pedestrian or vehicles occluded by large objects. We can also extend the system to scale to larger environments by strategically sampling policies to focus on those outcomes that most affect our choices. Exploring principled methods for reacting to detected anomalous behavior is also an avenue for future work.\nThis work was supported in part by a grant from Ford Motor Company via the Ford-UM Alliance under award N015392 and in part by DARPA under award D13AP00059. The authors are sincerely grateful to Patrick Carmody for his help in collecting the traffic-tracking data used in this work and to Ryan Wolcott for his helpful comments.\n", "prediction": "operate reliably in realworld traffic, an autonomous car must evaluate consequences of its potential actions by repeating uncertain intentions of other traffic participants.<n> this paper presents an integrated behavioral anticipation and decision-making approach that models both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of another vehicle that captures the high-level intention of driving along a particular lane or over a permutation of the lanes of a specific intersection ; and that is able to make decisions based on these coupled interactions between cars in a tractable manner : it is not necessary to consider expensive collection of training data or use heuristics to determine which policy is more likely to be executed by the other car than the policy of our car in the presence of uncertainty over the potential of each other s actions in that particular scenario ( e.g. changing the direction of an intersection or turning the lane in which we are going to change a direction to avoid a stop at a given stop sign ) <n> it does not need to know the full history of all other participants in any scenario to estimate the likelihood that each of their actions are anomalous and can be used to compute the expected reward of those actions based only on their observed behavior and not their actual actions as they would in situations where they are expected to execute known closed - loop policies in their respective directions ( i.e. turning of two lanes or changing a lane as we would do the same in our case ( p, v t = p = @xt | p(t @ @  p _ p @ p ( @ z ] @<n> pt p_t z = zt + p@x @ a @_ @ ( z(p = a = g @ x @ v @ = dx  @ + @ b! p p + a p z @ m @p @ g = b = x = ( a z + z p. p+ p) @t  z (  p b @ - p x dz = v = m = + b p v + +  + x + g + ( b z z) p * p - z+ z@ p d = d  - @ [ p g p a v p= p [ z. z_ z b ( x ( g) = [ @ * @ d @ j = * z v ( dt b(x p] p[t ( v) + d) zp   = = j + v. @( z+1 p... p i = k +t xt a b + [  b. b b) * * d ps @... z] x ( p; p to p y  ( + = c = 0 p<n> z * + j  to z - v * b@ z [ a. d + * g ln p-t g.  x. + m  d p is p"}
{"ground_truth": "Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks  (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to performdespite their strong performance on the related but more narrowly defined task of entailment NLIpointing to interesting avenues for future research.\nThe brain is an abduction machine, continuously trying to prove abductively that the observables in its environment constitute a coherent situation.  Jerry Hobbs, ACL 2013 Lifetime Achievement Award1 Abductive reasoning is inference to the most plausible explanation for incomplete observations (Peirce, 1965a). Figure 1 illustrates an example. Given the incomplete observations about the world that O1: Jenny cleaned her house and went to work, leaving the window just a crack open. and sometime later O2: When Jenny returned home, she saw her house was a mess., we can hypothesize different potential explanations and reason about which is the most likely. We can readily rule out H3 since it fails to justify the observation O2. While H1 and H2 are both plausible, the most likely explanation based on commonsense is H1 as H2 is somewhat implausible given O1. One crucial observation Peirce makes about abductive reasoning is that abduction is the only logical operation which introduces any new ideas, which contrasts with other types of inference such as entailment, that focuses on inferring only such information that is already provided in the premise. Work done while at AI2 1The full transcript of his award speech is available at doi/full/10.1162/COLI_a_00171 ar X iv :1 90 8. 05 73 9v 2 [ cs .C L ] 1 4 Fe b 20 Abductive reasoning has long been considered to be at the core of understanding narratives (Hobbs et al., 1988), reading between the lines (Norvig, 1987; Charniak & Shimony, 1990), reasoning about everyday situations (Peirce, 1965b; Andersen, 1973), and counterfactual reasoning (Pearl, 2002; Pearl & Mackenzie, 2018). Despite the broad recognition of its importance, however, the study of abductive reasoning in narrative text has very rarely appeared in the NLP literature, in large part because most previous work on abductive reasoning has focused on formal logic, which has proven to be too rigid to generalize to the full complexity of natural language. In this paper, we present the first study to investigate the viability of language-based abductive reasoning. This shift from logic-based to language-based reasoning draws inspirations from a significant body of work on language-based entailment (Bowman et al., 2015; Williams et al., 2018b), language-based logic (Lakoff, 1970; MacCartney & Manning, 2007), and language-based commonsense reasoning (Mostafazadeh et al., 2016; Zellers et al., 2018). In particular, we investigate the use of natural language as the representation medium, and probe deep neural models on language-based abductive reasoning. More concretely, we propose Abductive Natural Language Inference (NLI) and Abductive Natural Language Generation (NLG) as two novel reasoning tasks in narrative contexts.2 We formulate NLI as a multiple-choice task to support easy and reliable automatic evaluation: given a context, the task is to choose the more likely explanation from a given pair of hypotheses choices. We also introduce a new challenge dataset, ART, that consists of 20K narratives accompanied by over 200K explanatory hypothesis.34 We then establish comprehensive baseline performance based on state-of-the-art NLI and language models. The best baseline for NLI based on BERT achieves 68.9% accuracy, with a considerable gap compared to human performance of 91.4%(5.2). The best generative model, based on GPT2, performs well below human performance on the NLG task (5.2). Our analysis leads to insights into the types of reasoning that deep pre-trained language models fail to perform  despite their strong performance on the closely related but different task of entailment NLI  pointing to future research directions.\nAbductive Natural Language Inference We formulate NLI as multiple choice problems consisting of a pair of observations as context and a pair of hypothesis choices. Each instance in ART is defined as follows:  O1: The observation at time t1. 2NLI and NLG are pronounced as alpha-NLI and alpha-NLG, respectively 3ART: Abductive Reasoning in narrative Text. 4Data available to download at  O2: The observation at time t2 > t1.  h+: A plausible hypothesis that explains the two observations O1 and O2.  h: An implausible (or less plausible) hypothesis for observations O1 and O2. Given the observations and a pair of hypotheses, the NLI task is to select the most plausible explanation (hypothesis). Abductive Natural Language Generation NLG is the task of generating a valid hypothesis h+ given the two observations O1 and O2. Formally, the task requires to maximize P (h+|O1, O2).\n\nA Probabilistic Framework for NLI: A distinct feature of the NLI task is that it requires jointly considering all available observations and their commonsense implications, to identify the correct hypothesis. Formally, the NLI task is to select the hypothesis h that is most probable given the observations. h = arg max hi P (H = hi|O1, O2) (1) Rewriting the objective using Bayes Rule conditioned on O1, we have: P (hi|O1, O2)  P (O2|hi, O1)P (hi|O1) (2) We formulate a set of probabilistic models for NLI that make various independence assumptions on Equation 2  starting from a simple baseline that ignores the observations entirely, and building up to a fully joint model. These models are depicted as Bayesian Networks in Figure 2. Hypothesis Only: Our simplest model makes the strong assumption that the hypothesis is entirely independent of both observations, i.e. (H  O1, O2), in which case we simply aim to maximize the marginal P (H). First (or Second) Observation Only: Our next two models make weaker assumptions: that the hypothesis depends on only one of the first O1 or second O2 observation. Linear Chain: Our next model uses both observations, but considers each observations influence on the hypothesis independently, i.e. it does not combine information across the observations. Formally, the model assumes that the three variables O1, H,O2 form a linear Markov chain, where the second observation is conditionally independent of the first, given the hypothesis (i.e. (O1  O2|H)). Under this assumption, we aim to maximize a somewhat simpler objective than Equation 2: h = arg max hi P (O2|hi)P (hi|O1) where (O1  O2|H) (3) Fully Connected: Finally, our most sophisticated model jointly models all three random variables as in Equation 2, and can in principle combine information across both observations to choose the correct hypothesis. To help illustrate the subtle distinction between how the Linear Chain and Fully Connected models consider both observations, consider the following example. Let observation O1: Carl went to the store desperately searching for flour tortillas for a recipe. and O2: Carl left the store very frustrated.. Then consider two distinct hypotheses, an incorrect h1: The cashier was rude and the correct h2: The store had corn tortillas, but not flour ones.. For this example, a Linear Chain model could arrive at the wrong answer, because it reasons about the observations separatelytaking O1 in isolation, both h1 and h2 seem plausible next events, albeit each a priori unlikely. And for O2 in isolationi.e. in the absence of O1, as for a randomly drawn shopperthe h1 explanation of a rude cashier seems a much more plausible explanation of Carls frustration than are the details of the stores tortilla selection. Combining these two separate factors leads the Linear Chain to select h1 as the more plausible explanation. It is only by reasoning about Carls goal in O1 jointly with his frustration in O2, as in the Fully Connected model, that we arrive at the correct answer h2 as the more plausible explanation. In our experiments, we encode the different independence assumptions in the best performing neural network model. For the hypothesis-only and single observation models, we can enforce the independencies by simply restricting the inputs of the model to only the relevant variables. On the other hand, the Linear Chain model takes all three variables as input, but we restrict the form of the model to enforce the conditional independence. Specifically, we learn a discriminative classifier: PLinear Chain(h|O1, O2)  e(O1,h)+ (h,O2) where  and  are neural networks that produce scalar values.\nGiven h+= {wh1 . . . whl }, O1={wo11 . . . wo1m } and O2={wo21 . . . wo2n } as sequences of tokens, the NLG task can be modeled as P (h+|O1, O2) =  P (whi |wh<i, wo11 . . . wo1m , wo21 . . . wo2n ) Optionally, the model can also be conditioned on background knowledge K. Parameterized models can then be trained to minimize the negative log-likelihood over instances in ART: L =  N i=1 logP (whi |wh<i, wo11 . . . wo1m , wo21 . . . wo2n ,K) (4) 4 ART DATASET: ABDUCTIVE REASONING IN NARRATIVE TEXT ART is the first large-scale benchmark dataset for studying abductive reasoning in narrative texts. It consists of 20K narrative contexts (pairs of observations O1, O2) with over 200K explanatory hypotheses. Table 6 in the Appendix summarizes corpus-level statistics of the ART dataset.5 Figure 4 shows some illustrative examples from ART (dev split). The best model based on BERT fails to correctly predict the first two dev examples. 5We will publicly release the ART dataset upon acceptance. Collecting Observations: The pairs O1, O2 in ART are drawn from the ROCStories dataset (Mostafazadeh et al., 2016). ROCStories is a large collection of short, manually curated fivesentence stories. It was designed to have a clear beginning and ending for each story, which naturally map to the first (O1) and second (O2) observations in ART. Collecting Hypotheses Options: We crowdsourced the plausible and implausible hypotheses options on Amazon Mechanical Turk (AMT) in two separate tasks6: 1. Plausible Hypothesis Options: We presented O1 and O2 as narrative context to crowdworkers who were prompted to fill in What happened in-between? in natural language. The design of the task motivates the use of abductive reasoning to hypothesize likely explanations for the two given observations. 2. Implausible Hypothesis Options: In this task, we presented workers with observationsO1,O2 and one plausible hypothesis option h+  H+ collected from the previous task. Crowdworkers were instructed to make minimal edits (up to 5 words) to a given h+ to create implausible hypothesis variations for each plausible hypothesis. A significant challenge in creating datasets is avoiding annotation artifacts  unintentional patterns in the data that leak information about the target label  that several recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have reported on crowdsourced datasets . To tackle this challenge, we collect multiple plausible and implausible hypotheses for each O1, O2 pair (as described above) and then apply an adversarial filtering algorithm to retain one challenging pair of hypotheses that are hard to distinguish between. We describe our algorithm in detail in Appendix A.5. While our final dataset uses BERT as the adversary, preliminary experiments that used GPT as an adversary resulted in similar drops in performance of all models, including all BERT variants. We compare the results of the two adversaries in Table 1.\nWe now present our evaluation of finetuned state-of-the-art pre-trained language models on the ART dataset, and several other baseline systems for both NLI and NLG. Since NLI is framed as a binary classification problem, we choose accuracy as our primary metric. For NLG, we report performance on automated metrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee & Lavie, 2005) and also report human evaluation results.\n6Both crowdsourcing tasks are complex and require creative writing. Along with the ART dataset, we will publicly release templates and the full set of instructions for all crowdsourcing tasks to facilitate future data collection and research in this direction. Model GPT AFAcc. (%) ART Acc. (%) Random (2-way choice) 50.1 50.4 Majority (from dev set) 50.1 50.8 Infersent (Conneau et al., 2017) 50.9 50.8 ESIM+ELMo (Chen et al., 2017) 58.2 58.8 Finetuning Pre-trained LMs GPT-ft 52.6 (0.9) 63.1 (0.5) BERT-ft [hi Only] 55.9 (0.7) 59.5 (0.2) BERT-ft [O1 Only] 63.9 (0.8) 63.5 (0.7) BERT-ft [O2 Only] 68.1 (0.6) 66.6 (0.2) BERT-ft [Linear Chain] 65.3 (1.4) 68.9 (0.5) BERT-ft [Fully Connected] 72.0 (0.5) 68.6 (0.5) Human Performance - 91.4 Table 1: Performance of baselines and finetuned-LM approaches on the test set of ART. Test accuracy is reported as the mean of five models trained with random seeds, with the standard deviation in parenthesis. Despite strong performance on several other NLP benchmark datasets, the best baseline model based on BERT achieves an accuracy of just 68.9% on ART compared to human performance of 91.4%. The large gap between human performance and that of the best system provides significant scope for development of more sophisticated abductive reasoning models. Our experiments show that introducing the additional independence assumptions described in Section 3.1 over the fully connected model tends to degrade system performance (see Table 1) in general. Human Performance We compute human performance using AMT. Each instance (two observations and two hypothesis choices) is shown to three workers who were prompted to choose the more plausible hypothesis choice.7 We compute majority vote on the labels assigned which leads to a human accuracy of 91.4% on the ART test set. Baselines We include baselines that rely on simple features to verify that ART is not trivially solvable due to noticeable annotation artifacts, observed in several crowdsourced datasets. The accuracies of all simple baselines are close to chance-performance on the task  indicating that the dataset is free of simple annotation artifacts. A model for the related but distinct task of entailment NLI (e.g. SNLI) forms a natural baseline for NLI. We re-train the ESIM+ELMo (Chen et al., 2017; Peters et al., 2018) model as its performance on entailment NLI (88.9%) is close to state-of-the-art models (excluding pre-trained language models). This model only achieves an accuracy of 58.8% highlighting that performing well on ART requires models to go far beyond the linguistic notion of entailment. Pre-trained Language Models BERT (Devlin et al., 2018) and GPT (Radford, 2018) have recently been shown to achieve state-of-the-art results on several NLP benchmarks (Wang et al., 2018). We finetune both BERT-Large and GPT as suggested in previous work and we present each instance in their natural narrative order. BERT-ft (fully connected) is the best performing model achieving 68.9% accuracy, compared to GPTs 63.1%.8 Our AF approach was able to reduce BERT performance from over 88% by 20 points.\nWhile there is enough scope for considerably scaling up the dataset based on ROCStories, the learning curve in Figure 5 shows that the performance of the best model plateaus after 10, 000 instances. In addition, there is still a wide gap (23%) between the performance of the best model and human performance. 7Additional crowdsourcing details in the Appendix A.1 8The input format for the GPT model and BERT variants is described in the Appendix A.4. GPT Adversary Table 1 also includes results of our experiments where GPT was used as the adversary. Notably, in this case, adversarially filtering the dataset brings down GPT performance under 53%. On the other hand, the best BERT model, that encodes the fully connected bayesian network performs significantly better than the BERT model that encodes the linear chain assumptions  72% compared to 65%. Therefore, we use the BERT fully connected model as the adversary in ART. The gap between the linear chain and fully connected BERT models diminishes when BERT is used as an adversary  in spite of being a more powerful model  which indicates that adversarial filtering disproportionately impacts the model used as the adversary. However, the dataset also becomes more difficult for the other models that were not used as adversaries. For example, before any filtering, BERT scores 88% and OpenGPT gets 80%, which is much higher than either model achieves in Table 1 when the other model is used for filtering. This result is a reasonable indicator, albeit not a guarantee, that ART will remain challenging for new models released in the future.\nGenerative Language Models As described in Equation 4, we train GPT2 conditioned on the tokens of the two observations O1 and O2. Both observations are enclosed with field-specific tags. ATOMIC (Sap et al., 2019), a repository of inferential if-then knowledge is a natural source of background commonsense required to reason about narrative contexts in ART. Yet, there is no straightforward way to include such knowledge into a neural model as ATOMICs nodes are not canonicalized and are represented as short phrases of text. Thus, we rely on COMeT  a transformer model trained on ATOMIC that generates nine commonsense inferences of events in natural language.9 Specifically, we experiment with two ways of integrating information from COMeT in GPT2: (i) as textual phrases, and (ii) as embeddings. Figure 3 shows how we integrate COMeT representations. Concretely, after the input tokens are embedded by the word-embedding layer, we append eighteen (corresponding to nine relations for each observation) embeddings to the sequence before passing through the layers of the Transformer architecture. This allows the model to learn each tokens representation while attending to the COMeT embeddings  effectively integrating background commonsense knowledge into a language model.10 Discussion Table 2 reports results on the NLG task. Among automatic metrics, we report BLEU4 (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and BERT-Score (Zhang et al., 2019) (with the bert-base-uncased model). We establish human performance through crowdsourcing on AMT. Crowdworkers are shown pairs of observations and a generated hypothesis and asked to label whether the hypothesis explains the given observations. The last column reports the human evaluation score. The last row reports the score of a held-out human-written hypothesis and serves as a ceiling for model performance. Human-written hypotheses are found to be correct for 96% of instances, while our best generative models, even when enhanced with background commonsense knowledge, only achieve 45%  indicating that the NLG generation task is especially challenging for current state-of-the-art text generators. 9Please see Appendix A.6 for a full list of the nine relations. 10We describe the format of input for each model in Appendix A.7. 6 ANALYSIS 6.1 NLI Category Human Accuracy BERT Accuracy  All (1, 000) 91.4 68.8 22.6 Numerical (44) 88.6 56.8 21.8 Spatial (130) 91.5 65.4 26.1 Emotional (84) 86.9 72.6 14.3 Table 3: BERTs performance and human evaluation on categories for 1,000 instances from the test set, based on commonsense reasoning domains (Numerical, Spatial, Emotional). The number in parenthesis indicates the size of the category. Commonsense reasoning categories We investigate the categories of commonsense-based abductive reasoning that are challenging for current systems and the ones where the best model over-performs. While there have been previous attempts to categorize commonsense knowledge required for entailment (LoBue & Yates, 2011; Clark et al., 2007), crowdsourcing this task at scale with high fidelity and high agreement across annotators remains challenging. Instead, we aim to probe the model with soft categories identified by matching lists of category-specific keywords to the hypothesis choices. Table 3 shows the accuracy of the best model (BERT-ft) across various categories of commonsense knowledge. BERT-ft significantly underperforms on instances involving Numerical (56.8%) and Spatial (65.4%) commonsense. These two categories include reasoning about numerical quantities and the spatial location of agents and objects, and highlight some of the limitations of the language models. In contrast, it significantly overperforms on the Emotional category (72.6%) where the hypotheses exhibit strong textual cues about emotions and sentiments. Implausible transitions A model for an instance of the ART dataset should discard implausible hypotheses in the context of the two given observations. In narrative contexts, there are three main reasons for an implausible hypothesis to be labeled as such: 1. O1 6h: h is unlikely to follow after the first observation O1. 2. h 6O2: h is plausible after O1 but unlikely to precede the second observation O2. 3. Plausible: O1, h, O2 is a coherent narrative and forms a plausible alternative, but it is less plausible than O1, h+, O2. We analyze the prevalence of each of these reasons in ART. We design a crowdsourcing task in which we show the implausible option along with the narrative context O1, O2 and get labels for which transition (O1 6h, h 6O2 or neither) in the narrative chain is broken. Table 4 shows the proportion of each category from a subset of 1, 000 instances from the test set. While h 6O2 accounts for almost half of the implausible transitions in ART, all three categories are substantially present in the dataset. BERT performance on each of these categories indicates that the model finds it particularly hard when the narrative created by the incorrect hypothesis is plausible, but less plausible than the correct hypothesis. On that subset of the test set, the fully connected model performs better than the linear chain model where it is important to consider both observations jointly to arrive at the more likely hypothesis. 6.2 NLG Figure 6 shows some examples of generations from the trained models compared to human-written generations. The example on the left is an example of an instance that only humans could get correct, while for the one on the right, COMeT-Emb+GPT2also generates the correct explanation for the observations. 7 TRANSFER LEARNING FROM ART ART contains a large number of questions for the novel abductive reasoning task. In addition to serving as a benchmark, we investigate if ART can be used as a resource to boost performance on other commonsense tasks. We apply transfer learning by first training a model on ART, and subsequently training on four target datasets  WinoGrande Sakaguchi et al. (2020), WSC Levesque et al. (2011), DPR Rahman & Ng (2012) and HellaSwag Zellers et al. (2019). We show that compared to a model that is only trained on the target dataset, a model that is sequentially trained on ART first and then on the target dataset can perform better. In particular, pre-training on ART consistently improves performance on related datasets when they have relatively few training examples. On the other hand, for target datasets with large amounts of training data, pre-training on ART does not provide a significant improvement.\n\nSince abduction is fundamentally concerned with plausible chains of cause-and-effect, our work draws inspiration from previous works that deal with narratives such as script learning (Schank & Abelson, 1975) and the narrative cloze test (Chambers & Jurafsky, 2009; Jans et al., 2012; Pichotta & Mooney, 2014; Rudinger et al., 2015). Rather than learning prototypical scripts or narrative chains, we instead reason about the most plausible events conditioned on observations. We make use of the ROCStories dataset (Mostafazadeh et al., 2016), which was specifically designed for the narrative cloze task. But, instead of reasoning about plausible event sequences, our task requires reasoning about plausible explanations for narrative omissions. Entailment vs. Abductive Reasoning The formulation of NLI is closely related to entailment NLI, but there are two critical distinctions that make abductive reasoning uniquely challenging. First, abduction requires reasoning about commonsense implications of observations (e.g., if we observe that the grass is wet, a likely hypothesis is that it rained earlier) which go beyond the linguistic notion of entailment (also noted by Josephson (2000)). Second, abduction requires non-monotonic reasoning about a set of commonsense implications collectively, to check the potential contradictions against multiple observations and to compare the level of plausibility of different hypotheses. This makes abductive reasoning distinctly challenging compared to other forms of reasoning such as induction and deduction (Shank, 1998). Perhaps more importantly, abduction is closely related to the kind of reasoning humans perform in everyday situations, where information is incomplete and definite inferences cannot be made. Generative Language Modeling Recent advancements in the development of large-scale pretrained language models (Radford, 2018; Devlin et al., 2018; Radford et al., 2019) have improved the quality and coherence of generated language. Although these models have shown to generate reasonably coherent text when condition on a sequence of text, our experiments highlight the limitations of these models to 1) generate language non-monotonically and 2) adhere to commonsense knowledge. We attempt to overcome these limitations with the incorporation of a generative commonsense model during hypothesis generation. Related Datasets Our new resource ART complements ongoing efforts in building resources for natural language inference (Dagan et al., 2006; MacCartney & Manning, 2009; Bowman et al., 2015; Williams et al., 2018a; Camburu et al., 2018). Existing datasets have mostly focused on textual entailment in a deductive reasoning set-up (Bowman et al., 2015; Williams et al., 2018a) and making inferences about plausible events (Maslan et al., 2015; Zhang et al., 2017). In their typical setting, these datasets require a system to deduce the logically entailed consequences of a given premise. In contrast, the nature of abduction requires the use of commonsense reasoning capabilities, with less focus on lexical entailment. While abductive reasoning has been applied to entailment datasets (Raina et al., 2005), they have been applied in a logical theorem-proving framework as an intermediate step to perform textual entailment  a fundamentally different task than NLI.\nWe present the first study that investigates the viability of language-based abductive reasoning. We conceptualize and introduce Abductive Natural Language Inference (NLI)  a novel task focused on abductive reasoning in narrative contexts. The task is formulated as a multiple-choice questionanswering problem. We also introduce Abductive Natural Language Generation (NLG)  a novel task that requires machines to generate plausible hypotheses for given observations. To support these tasks, we create and introduce a new challenge dataset, ART, which consists of 20,000 commonsense narratives accompanied with over 200,000 explanatory hypotheses. In our experiments, we establish comprehensive baseline performance on this new task based on state-of-the-art NLI and language models, which leads to 68.9% accuracy with a considerable gap with human performance (91.4%). The NLG task is significantly harder  while humans can write a valid explanation 96% of times, the best generator models can only achieve 45%. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform  despite their strong performance on the closely related but different task of entailment NLI  pointing to interesting avenues for future research. We hope that ART will serve as a challenging benchmark for future research in languagebased abductive reasoning and the NLI and NLG tasks will encourage representation learning that enables complex reasoning capabilities in AI systems.\nWe thank the anonymous reviewers for their insightful feedback. This research was supported in part by NSF (IIS-1524371), the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1256082, DARPA CwC through ARO (W911NF15-1- 0543), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI. Computations on beaker.org were supported in part by credits from Google Cloud.\n\nWe describe the crowdsourcing details of our data collection method. Task 1 - Plausible Hypothesis Options In this task, participants were presented an incomplete three-part story, which consisted of the first observation (O1) and the second observation (O2) of the story. They were then asked to complete the story by writing a probable middle sentence that explains why the second observation should follow after the first one. We instructed participants to make sure that the plausible middle sentence (1) is short (fewer than 10 words) and (2) simple as if narrating to a child, (3) avoids introducing any extraneous information, and (4) uses names instead of pronouns (e.g., he/she) wherever possible. All participants were required to meet the following qualification requirements: (1) their location is in the US, (2) HIT approval rate is greater than 95(%), and (3) Number of HITs approved is greater than 5,000. The reward of this task was set to be $0.07 per question ($14/hour in average), and each HIT was assigned to five different workers (i.e., 5-way redundancy). Task 2 - Implausible Hypothesis Options In this task, participants were presented a three-part story, which consisted of the first observation (O1), a middle sentence (h+) collected in Task 1, and the second observation (O2) of the story. They were then asked to rewrite the middle sentence (h+) with minimal changes, so that the story becomes unlikely, implausible or inconsistent (h). We asked participants to add or remove at most four words to h+, while ensuring that the new middle sentence is grammatical. In addition, we asked them to stick to the context in the given story. For example, if the story talks about doctors, they are welcome to talk about health or diagnosis, but not mention aliens. Finally, we also asked workers to verify if the given middle (h+) makes a plausible story, in order to confirm the plausibility of h+collected in Task 1. With respect to this tasks qualification, participants were required to fulfill the following requirements: (1) their location is the US or Canada, (2) HIT approval rate is greater than or equal to 99(%), and (3) number of HITs approved is greater than or equal to 10, 000. Participants were paid $0.1 per question ($14/hour in average), and each HIT was assigned to three different participants (i.e., 3-way redundancy). Task 3 - NLI Human Performance Human performance was evaluated by asking participants to answer the NLI questions. Given a narrative context O1, O2 and two hypotheses, they were asked to choose the more plausible hypothesis. They were also allowed to choose None of the above when neither hypothesis was deemed plausible. We asked each question to seven participants with the following qualification requirements: (1) their location is either in the US, UK, or Canada, (2) HIT approval rate is greater than 98(%), (3) Number of HITs approved is greater than 10, 000. The reward was set to $0.05 per HIT. We took the majority vote among the seven participants for every question to compute human performance. A.2 ART DATA STATISTICS Table 6 shows some statistics of the ART dataset.\nWe fine-tuned the BERT model using a grid search with the following set of hyper-parameters:  batch size: {3, 4, 8}  number of epochs: {3, 4, 10}  learning rate: {1e-5, 2e-5, 3e-5, 5e-5} The warmup proportion was set to 0.2, and cross-entropy was used for computing the loss. The best performance was obtained with a batch size of 4, learning rate of 5e-5, and number of epochs equal to 10. Table 7 describes the input format for GPT and BERT (and its variants).\nThe SVM classifier is trained on simple features like word length, overlap and sentiment features to select one of the two hypothesis choices. The bag-of-words baseline computes the average of GloVe (Pennington et al., 2014) embeddings for words in each sentence to form sentence embeddings. The sentence embeddings in a story (two observations and a hypothesis option) are concatenated and passed through fully-connected layers to produce a score for each hypothesis. The accuracies of both baselines are close to 50% (SVM: 50.6; BOW: 50.5). Specifically, we train an SVM classifier and a bag-of-words model using GLoVE embeddings. Both models achieve accuracies close to 50%. An Infersent (Conneau et al., 2017) baseline that uses sentences embedded by max-pooling over Bi-LSTM token representations achieves only 50.8% accuracy.\nGiven an observation pair and sets of plausible and implausible hypotheses O1, O2,H+,H, our adversarial filtering algorithm selects one plausible and one implausible hypothesis O1, O2, h+, h  such that h+ and h are hard to distinguish between. We make three key improvements over the previously proposed Adversarial Filtering (AF) approach in Zellers et al. (2018). First, Instead of a single positive sample, we exploit a poolH+ of positive samples to choose from (i.e. plausible hypotheses). Second, Instead of machine generated distractors, the pool H of negative samples (i.e. implausible hypotheses) is human-generated. Thus, the distractors share stylistic features of the positive samples as well as that of the context (i.e. observations O1 and O2)  making the negative samples harder to distinguish from positive samples. Finally, We use BERT (Devlin et al., 2018) as the adversary and introduce a temperature parameter that controls the maximum number of instances that can be modified in each iteration of AF. In later iterations, fewer instances get modified resulting in a smoother convergence of the AF algorithm (described in more detail below). Algorithm 1 provides a formal description of our approach. In each iteration i, we train an adversarial model Mi on a random subset Ti of the data and update the validation set Vi to make it more challenging for Mi. For a pair (h+k , h  k ) of plausible and implausible hypotheses for an instance k, we denote  = Mi(h + k , h  k ) the difference in the model evaluation of h + k and h  k . A positive value of  indicates that the model Mi favors the plausible hypothesis h+k over the implausible one h  k . With probability ti, we update instance k that Mi gets correct with a pair (h+, h)  H+k  H  k of hypotheses that reduces the value of , where H+k (resp. H  k ) is the pool of plausible (resp. implausible) hypotheses for instance k . We ran AF for 50 iterations and the temperature ti follows a sigmoid function, parameterized by the iteration number, between ts = 1.0 and te = 0.2. Our final dataset, ART, is generated using BERT as the adversary in Algorithm 1. Algorithm 1: Dual Adversarial Filtering input : dataset D0, plausible & implausible hypothesis sets (H+,H), number of iterations n, initial & final temperatures (ts, te) output: dataset Dn 1 for iteration i : 0..n 1 do 2 ti = te + tste 1+e0.3(i 3n 4 ) 3 Randomly partition Di into (Ti,Vi). 4 Train model Mi on Ti. 5 Si = , the selected hypotheses for Vi. 6 for (h+k , h  k )  Vi do 7 Pick r uniformly at random in [0, 1]. 8 if r > ti or Mi(h + k , h  k ) < 0 then 9 Add (h+k , h  k ) to Si. 10 else 11 Pick (h+, h)  H+k H  k s.t. Mi(h +, h) < Mi(h + k , h  k ) 12 Add (h+, h) to Si. 13 end 14 end 15 Di+1 = Ti  Si 16 end\nATOMIC (Sap et al., 2019) represents commonsense knowledge as a graph with events are nodes and the following nine relations as edges: 1. xIntent: Why does X cause an event? 2. xNeed: What does X need to do before the event? 3. xAttr: How would X be described? 4. xEffect: What effects does the event have on X? 5. xWant: What would X likely want to do after the event? 6. xReaction: How does X feel after the event? 7. oReact: How do others feel after the event? 8. oWant: What would others likely want to do after the event? 9. oEffect: What effects does the event have on others?\nTable 8 describes the format of input to each variation of the generative model evaluated.\n", "prediction": "this paper presents the first study that investigates the viability of language-based inference for abductive narratives.<n> the study consists of two parts : ( i ) a comprehensive study of the best performing model on the most plausible scenario for a narrative ; and ( ii) two new experiments, one on multiple scenarios and the other on conditional reasoning for natural language ( cn)ference in contexts of interest to abductively narrative ( a.k.a.non-commutative algebra <n> n.c.f.)and to ctive inference in context of non - causally connected sets _ ( c.v.n.w.h.e.,_the best model achieving 91.4% accuracy on a dataset of 200 observations of human beings in the presence of a linear adversary that has been pre- and post - trained as a neural model in order to achieve high accuracies on both scenarios ( b.i.t.g.an example of how this model fails to perform well in a scenario where the adversary is a highly connected component of an n- or c- type of set s structure in such a way as to make it appear as if the set is composed of several components of different types of sets that are all closely related to each other and that have a high level of similarity to the observed scenario ( j.j.s.d.the model achieves 68.9% accuracy below the performance of 68.3% accuracy for an observation in which state-[e[a ] that is not used to select a state of state ( e.(e(i for state(a for the state for t a  state in [e in an independent state to choose a [a[i as state ii(b(([[b[n](ii] as the]).[ii(s]) and [b.] in one of [i?(n ]) ], while [(t ]](v [v(d(h) in (b (a) to obtain (ii) with a) that we are a, b, [ b) and b(g) while b-) for [ii (c   ii. b[(j!(@b ii ), while (i)... b_(p.b] ( p, ii, which is the b2(w) as b]... ii. ( iii) .... [ ii] with the] and a] to be a......) we introduce the[s) (... ( [...] while the... [. i....[d) which) is ( v.. ii ii_a] for two) the [[2b]),... ii[g]) [] ]..], which are the results of b; b ( 2, a[] [ ( ( the ii-(the] is to [ [_b...(... in... to..., ( in b@e)) of...; [2[@i] we aim ( @e]. in this is in two, while while we introduced ( which we have the @b [, we make ( we will ( to introduce ( for which [) are ( this are to see the  for all the analysis of that ([in [;... with ( that [ i and we... that  to find ( d.2... for... while  in that to which ((in] of ( g.ii in all ( while... we  - ( two. @)<n> [ e, the ( with [ that... which "}
{"ground_truth": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical kmeans, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.\nThe Maximum Inner Product Search (MIPS) problem has recently received increased attention, as it arises naturally in many large scale tasks. In recommendation systems (Koenigstein et al., 2012; Bachrach et al., 2014), users and items to be recommended are represented as vectors that are learnt at training time based on the user-item rating matrix. At test time, when the model is deployed for suggesting recommendations, given a user vector, the model will perform a dot product of the user vector with all the item vectors and pick top K items with maximum dot product to recommend. With millions of candidate items to recommend, it is usually not possible to do a full linear search within the available time frame of only few milliseconds. This problem amounts to solving a KMIPS problem. Another common instance where the K-MIPS problem arises is in extreme classification tasks (Vijayanarasimhan et al., 2014), with a huge number of classes. At inference time, predicting the top-K most likely class labels for a given data point can be cast as a K-MIPS problem. Such extreme (probabilistic) classification problems occur often in Natural Language Processing (NLP) tasks where the classes are words in a predetermined vocabulary. For example in neural probabilistic language models (Bengio et al., 2003) the probabilities of a next word given the context of the few previous words is computed, in the last layer of the network, as a multiplication of the last hidden layer representation with a very large matrix (an embedding dictionary) that has as many columns as there are words in the vocabulary. Each such column can be seen as corresponding to the embedding of a vocabulary word in the hidden layer space. Thus an inner product is taken between each of these and the hidden representation, to yield an inner product score for each vocabulary word. Passed through a softmax nonlinearity, these yield the predicted probabilities for all possible words. The ranking of these probability values is unaffected by the softmax layer, so finding the k most probable words is Equal contribution and CIFAR ar X iv :1 50 7. 05 91 0v 3 [ cs .L G ] 3 0 N ov 2 exactly equivalent to finding the ones with the largest inner product scores, i.e. solving a K-MIPS problem. In many cases the retrieved result need not be exact: it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high, and thus highly likely (though not guaranteed) to contain some of the exact K-MIPS vectors. These examples motivate research on approximate K-MIPS algorithms. If we can obtain large speedups over a full linear search without sacrificing too much on precision, it will have a direct impact on such large-scale applications. Formally the K-MIPS problem is stated as follows: given a set X = {x1, . . . , xn} of points and a query vector q, find argmax (K) iX q >xi (1) where the argmax(K) notation corresponds to the set of the indices providing the K maximum values. Such a problem can be solved exactly in linear time by calculating all the q>xi and selecting the K maximum items, but such a method is too costly to be used on large applications where we typically have hundreds of thousands of entries in the set. All the methods discussed in this article are based on the notion of a candidate set, i.e. a subset of the dataset that they return, and on which we will do an exact K-MIPS, making its computation much faster. There is no guarantee that the candidate set contains the target elements, therefore these methods solve approximate K-MIPS. Better algorithms will provide us with candidate sets that are both smaller and have larger intersections with the actual K maximum inner product vectors. MIPS is related to nearest neighbor search (NNS), and to maximum similarity search. But it is considered a harder problem because the inner product neither satisfies the triangular inequality as distances usually do, nor does it satisfy a basic property of similarity functions, namely that the similarity of an entry with itself is at least as large as its similarity with anything else: for a vector x, there is no guarantee that xTx  xT y for all y. Thus we cannot directly apply efficient nearest neighbor search or maximum similarity search algorithms to the MIPS problem. Given a set X = {x1, . . . , xn} of points and a query vector q, the K-NNS problem with Euclidean distance is defined as: argmin (K) iX ||q  xi|| 2 2 = argmax (K) iX q Txi  ||xi||22 2 (2) and the maximum cosine similarity problem (K-MCSS) is defined as: argmax (K) iX qTxi ||q|| ||xi|| = argmax (K) iX qTxi ||xi|| (3) K-NNS and K-MCSS are different problems than K-MIPS, but it is easy to see that all three become equivalent provided all data vectors xi have the same Euclidean norm. Several approaches to MIPS make use of this observation and first transform a MIPS problem into a NNS or MCSS problem. In this paper, we propose and empirically investigate a very simple approach for the approximate K-MIPS problem. It consists in first reducing the problem to an approximate K-MCSS problem (as has been previously done in (Shrivastava and Li, 2015) ) on top of which we perform a spherical k-means clustering. The few clusters whose centers best match the query yield the candidate set. The rest of the paper is organized as follows: In section 2, we review previously proposed approaches for MIPS. Section 3 describes our proposed simple solution k-means MIPS in more details and section 4 discusses ways to further improve the performance by using a hierarchical k-means version. In section 5, we empirically compare our methods to the state-of-the-art in tree-based and hashing-based approaches, on two standard collaborative filtering benchmarks and on a larger word embedding datasets. Section 6 concludes the paper with discussion on future work.\nThere are two common types of solution for MIPS in the literature: tree-based methods and hashingbased methods. Tree-based methods are data dependent (i.e. first trained to adapt to the specific data set) while hash-based methods are mostly data independent. Tree-based approaches: The Maximum Inner Product Search problem was first formalized in (Ram and Gray, 2012). Ram and Gray (2012) provided a tree-based solution for the problem. Specifically, they constructed a ball tree with vectors in the database and bounded the maximum inner product with a ball. Their novel analytical upper bound for maximum inner product of a given point with points in a ball made it possible to design a branch and bound algorithm to solve MIPS using the constructed ball tree. Ram and Gray (2012) also proposes a dual-tree based search using cone trees when you have a batch of queries. One issue with this ball-tree based approach (IP-Tree) is that it partitions the set of data points based on the Euclidean distance, while the problem hasnt effectively been converted to NNS. In contrast, PCA-Tree (Bachrach et al., 2014), the current state-of-the-art tree-based approach to MIPS, first converts MIPS to NNS by appending an additional component to the vector that ensures that all vectors are of constant norm. This is followed by PCA and by a balanced kd-tree style tree construction. Hashing based approaches: Shrivastava and Li (2014) is the first work to propose an explicit Asymmetric Locality Sensitive Hashing (ALSH) construction to perform MIPS. They converted MIPS to NNS and used the L2-LSH algorithm (Datar et al., 2004). Subsequently, Shrivastava and Li (2015) proposed another construction to convert MIPS to MCSS and used the Signed Random Projection (SRP) hashing method. Both works were based on the assumption that a symmetricLSH family does not exist for MIPS problem. Later, Neyshabur and Srebro (2015) showed an explicit construction of a symmetric-LSH algorithm for MIPS which had better performance than the previous ALSH algorithms. Finally, Vijayanarasimhan et al. (2014) propose to use Winner-TakeAll hashing to pick top-K classes to consider during training and inference in large classification problems. Hierarchical softmax: A notable approach to address the problem of scaling classifiers to a huge number of classes is the hierarchical softmax (Morin and Bengio, 2005). It is based on prior clustering of the words into a binary, or more generally n-ary tree that serves as a fixed structure for the learning process of the model. The complexity of training is reduced from O(n) to O(log n). Due to its clustering and tree structure, it resembles the MIPS techniques we explore in this paper. However, the approaches differ at a fundamental level. Hierarchical softmax defines the probability of a leaf node as the product of all the probabilities computed by all the intermediate softmaxes on the way to that leaf node. By contrast, an approximate MIPS search imposes no such constraining structure on the probabilistic model, and is better though as efficiently searching for top winners of what amounts to a large ordinary flat softmax. 3 k-MEANS CLUSTERING FOR APPROXIMATE MIPS In this section, we propose a simple k-means clustering based solution for approximate MIPS.\nWe follow the previous work by Shrivastava and Li (2015) for reducing the MIPS problem to the MCSS problem by ingeniously rescaling the vectors and adding new components, making the norms of all the vectors approximately the same. Let X = {x1, . . . , xn} be our dataset. Let U < 1 and m  N be parameters of the algorithm. The first step is to scale all the vectors in our dataset by the same factor such that maxi ||xi||2 = U . We then apply two mappings P and Q, one on the data points and another on the query vector. These two mappings simply concatenate m new components to the vectors making the norms of the data points all roughly the same. The mappings are defined as follows: P (x) = [x, 1/2 ||x||22, 1/2 ||x||42, . . . , 1/2 ||x||2 m 2 ] (4) Q(x) = [x, 0, 0, . . . , 0] (5) As shown in Shrivastava and Li (2015), mapping P brings all the vectors to roughly the same norm: we have ||P (xi)||22 = m/4 + ||xi||2 m+1 2 , with the last term vanishing as m  +, since ||xi||2  U < 1. We thus have the following approximation of MIPS by MCSS for any query vector q, argmax (K) i q >xi ' argmax(K)i Q(q)>P (xi) ||Q(q)||2  ||P (xi)||2 (6) 3.2 MCSS USING SPHERICAL k-MEANS Assuming all data points x1, . . . , xn have been transformed as xj  P (xj) so as to be scaled to a norm of approximately 1, then the spherical k-means1 algorithm (Zhong, 2005) can efficiently be used to do approximate MCSS. Algorithm 1 is a formal specification of the spherical k-means algorithm, where we denote by ci the centroid of cluster i (i  {1, . . . ,K}) and aj the index of the cluster assigned to each point xj . Algorithm 1 Spherical k-means aj  rand(k) while ci or aj changed at previous step do ci   j|aj=i xj ||  j|aj=i xj || aj  argmaxi{1,...,k}x>j ci end while The difference between standard k-means clustering and spherical k-means is that in the spherical variant, the data points are clustered not according to their position in the Euclidian space, but according to their direction. To find the one vector that has maximum cosine similarity to query point q in a dataset clustered by this method, we first find the cluster whose centroid has the best cosine similarity with the query vector  i.e. the i such that q>ci is maximal  and consider all the points belonging to that cluster as the candidate set. We then simply take argmaxj|aj=i q >xj as an approximation for our maximum cosine similarity vector. This method can be extended for finding the k maximum cosine similarity vectors: we compute the cosine similarity between the query and all the vectors of the candidate set and take the k best matches. One issue with constructing a candidate set from a single cluster is that the quality of the set will be poor for points close to the boundaries between clusters. To alleviate this problem, we can increase the size of candidate sets by constructing them instead from the top-p best matching clusters to construct our candidate set. We note that other approximate search methods exploit similar ideas. For example, Bachrach et al. (2014) proposes a so-called neighborhood boosting method for PCA-Tree, by considering the path to each leaf as a binary vector (based on decision to go left or right) and given a target leaf, consider all other leaves which are one hamming distance away. 4 HIERARCHICAL k-MEANS FOR FASTER AND MORE PRECISE SEARCH While using a single-level clustering of the data points might yield a sufficiently fast search procedure for moderately large databases, it can be insufficient for much larger collections. Indeed, if we have n points, by clustering our dataset into  n clusters so that each cluster contains approximately  n points, we reduce the complexity of the search from O(n) to roughly O (  n). If we use the single closest cluster as a candidate set, then the candidate set size is of the order of n. But as mentioned earlier, we will typically want to consider the two or three closest clusters as a candidate set, in order to limit problems arising from the query points close to the boundary between clusters or when doing approximate K-MIPS with K fairly big (for example 100). A consequence of increasing candidate sets this way is that they can quickly grow wastefully big, containing many unwanted items. To restrict the candidate sets to a smaller count of better targeted items, we would need to have smaller clusters, but then the search for the best matching clusters becomes the most expensive part. To address this situation, we propose an approach where we cluster our dataset into many small clusters, and then cluster the small clusters into bigger clusters, and so on any number of times. Our approach is thus a bottom-up clustering approach. 1Note that we use K to refer to the number of top-K items to retrieve in search and k for the number of clusters in k-means. These two quantities are otherwise not the same. For example, we can cluster our datasets in n2/3 first-level, small clusters, and then cluster the centroids of the first-level clusters into n1/3 second-level clusters, making our data structure a twolayer hierarchical clustering. This approach can be generalized to as many levels of clustering as necessary. To search for the small clusters that best match the query point and will constitute a good candidate set, we go down the hierarchy keeping at each level only the p best matching clusters. This process is illustrated in Figure 1. Since at all levels the clusters are of much smaller size, we can take much larger values for p, for example p = 8 or p = 16. Formally, if we have L levels of clustering, let Il be a set of indices for the clusters at level l  {0, . . . , L}. Let c(l)i , i  Il be the centroids of the clusters at level l, with {c (L) i } conveniently defined as being the data points themselves, and let a(l)i  Il1, i  Il be the assignment of the centroids c(l)i to the clusters of layer l 1. The candidate set is found using the method described in Algorithm 2. Our candidate set is the set CL obtained at the end of the algorithm. In our approach, Algorithm 2 Search in hierarchical spherical k-means C0 = I0 for l = 0, . . . , L 1 do Al = argmax (p) iClq >c (l) i Cl+1 = { i|a(l+1)i  Al } end for return CL we do a bottom-up clustering, i.e. we first cluster the dataset into small clusters, then we cluster the small cluster into bigger clusters, and so on until we get to the top level which is only one cluster. Other approaches have been suggested such as in (Mnih and Hinton, 2009), where the method employed is a top-down clustering strategy where at each level the points assigned to the current cluster are divided in smaller clusters. The approach of (Mnih and Hinton, 2009) also addresses the problem that using a single lowest-level cluster as a candidate set is an inaccurate solution by having the data points be in multiple clusters. We use an alternative solution that consists in exploring several branches of the clustering hierarchy in parallel.\nIn this section, we will evaluate the proposed algorithm for approximate MIPS. Specifically, we analyze the following characteristics: speedup, compared to the exact full linear search, of retrieving top-K items with largest inner product, and robustness of retrieved results to noise in the query.\nWe have used 2 collaborative filtering datasets and 1 word embedding dataset, which are descibed below: Movielens-10M: A collaborative filtering dataset with 10,677 movies (items) and 69,888 users. Given the user-item matrix Z, we follow the pureSVD procedure described in (Cremonesi et al., 2010) to generate user and movie vectors. Specifically, we subtracted the average rating of each user from his individual ratings and considered unobserved entries as zeros. Then we compute an SVD approximation of Z with its top 150 singular components, Z 'WRT . Each row in W is used as the vector representation of the user and each row in R is the vector representation of the movie. We construct a database of all 10,677 movies and consider 60,000 randomly selected users as queries. Netflix: Another standard collaborative filtering dataset with 17,770 movies (items) and 480,189 users. We follow the same procedure as described for movielens but construct 300 dimensional vector representations, as is standard in the literature (Neyshabur and Srebro, 2015). We consider 60,000 randomly selected users as queries. Word2vec embeddings: We use the 300-dimensional word2vec embeddings released by Mikolov et al. (2013). We construct a database composed of the first 100,000 word embedding vectors. We consider two types of queries: 2,000 randomly selected word vectors from that database, and 2,000 randomly selected word vectors from the database corrupted with Gaussian noise. This acts as a test bench to evaluate the performance of different algorithms based on the characteristics of the queries.\nWe consider the following baselines to compare with. PCA-Tree: PCA-Tree (Bachrach et al., 2014) is the state-of-the-art tree-based method which was shown to be superior to IP-Tree (Koenigstein et al., 2012). This method first converts MIPS to NNS by appending an additional component to the vectors to make them of constant norm. Then the principal directions are learnt and the data is projected using these principal directions. Finally, a balanced tree is constructed using as splitting criteria at each level the median of component values along the corresponding principal direction. Each level uses a different principal direction, in decreasing order of variance. SRP-Hash: This is the signed random projection hashing method for MIPS proposed in Shrivastava and Li (2015). SRP-Hash converts MIPS to MCSS by vector augmentation. We consider n hash functions and each hash function considers p random projections of the vector to compute the hash. WTA-Hash: Winner Takes All hashing (Vijayanarasimhan et al., 2014) is another hashing-based baseline which also converts MIPS to MCSS by vector augmentation. We consider n hash functions and each hash function does p different random permutations of the vector. Then the prefix constituted by the first k elements of each permuted vector is used to construct the hash for the vector.\nIn these first experiments, we consider the two collaborative filtering tasks and evaluate the speedup provided by the different approximate K-MIPS algorithms (for K  {1, 10, 100}) compared to the exact full search. Note that this section does not include the hierarchical version of k-means in the experiments, as the databases were small enough (less than 20,000) for flat k-means to perform well. Specifically, speedup is defined as speedupA0(A) = Time taken by Algorithm A0 Time taken by Algorithm A (7) where A0 is the exact linear search algorithm that consists in computing the inner product with all training items. Because we want to compare the preformance of algorithms, rather than of specifically optimized implementations, we approximate the time with the number of dot product operations computed by the algorithm2. In other words, our unit of time is the time taken by a dot product. 2For example, k-means algorithm was run using GPU while PCA-Tree was run using CPU. All algorithms return a set of candidates for which we do exact linear seacrh. This induces a number of dot products at least as large as the size of the identified candidate set. In addition to the candidate set size, the following operations count towards the count of dot products: k-means: dot products done with all cluster centroids involved in finding the top-p clusters of the (hierarchical) search. PCA-Tree: dot product done to project the query to the PCA space. Note that if the tree is of depth d, then we need to do d dot products to project the query. SRP-Hash: total number of random projections of the data (each random projection is considered a single dot product). If we have n hashes with p random projections each, then the cost is p  n. WTA-Hash: a full random permutation of the vector involves the same number of query element access operations as a single dot product. However, we consider only k prefixes in the permutations, which means we only need to do a fraction of dot product. While a dot product involves accessing all d components of the vector, each permutation in WTA-Hash only needs to access k elements of the vector. So we consider its cost to be a fraction k/d of the cost of a dot product. Specifically, if we have n hash functions each with p random permutations and consider prefixes of length k, then the total cost would be n  p  k/d where d is the dimension of the vector. Let us call true top-K the actual K elements from the database that have the largest inner products with the query. Let us call retrieved top-K the K elements, among the candidate set retrieved by a specific approximate MIPS, that have the largest inner products with the query. We define precision for K-MIPS as the number of elements in the intersection of true top-K and retrived top-K vectors, divided by K. precision at K = |retrieved top K  true top K| K (8) We varied hyper-parameters of each algorithm (k in k-means, depth in PCA-Tree, number of hash functions in SRP-Hash and WTA-Hash), and computed the precision and speedup in each case. Resulting precision v.s. speedup curves obtained for the Movielens-10M and Netflix datasets are reported in Figure 2. We make the following observations from these results:  Hashing-based methods perform better with lower speedups. But their performance decrease rapidly after 10x speedup.  PCA-Tree performs better than SRP-Hash.  WTA-Hash performs better than PCA-Tree with lower speedups. However, their perfor- mance degrades faster as the speedup increases and PCA-Tree outperforms WTA-Hash with higer speedups.  k-means is a clear winner as the speed up increases. Also, performance of k-means degrades very slowly with increase in speedup as compared to rapid decrease in performance of other algorithms.\nIn this experiment, we consider a word embedding retrieval task. As a first experiment, we consider using a query set of 2,000 embeddings, corresponding to a subset of a large database of pretrained embeddings. Note that while a query is thus present in the database, it is not guaranteed to correspond to the top-1 MIPS result. Also, well be interested in the top-10 and top-100 MIPS performance. Algorithms which perform better in top-10 and top-100 MIPS for queries which already belong to the database preserve the neighborhood of data points better. Figure 3 shows the precision vs. speedup curve for top-1, top-10 and top-100 MIPS. From the results, we can see that data dependent algorithms (k-means and PCA-Tree) better preserve the neighborhood, compared to data independent algorithms (SRP-Hash, WTA-Hash), which is not surprising. However, k-means and hierarchical k-means performs significantly better than PCA-Tree in top-10 and top-100 MIPS suggesting that it is better than PCA-Tree in capturing the neighborhood. One reason might be that k-means has the global view of the vector at every step while PCA-Tree considers one dimension at a time. As the next experiment, we would like to study how different algorithms behave with respect to the noise in the query. For a fair comparison, we chose hyper-parameters for each model such that the speedup is the same (we set it to 30x) for all algorithms. We take 2,000 random word embeddings from the database and corrupt them random Gaussian noise. We vary the scale of the noise from 0 to 0.4 and plot the performance. Figure 4 shows the performance of various algorithms on the top-1, top-10, top-100 MIPS problems, as the noise increases. We can see that k-means always performs better than other algorithms, even with increase in noise. Also, the performance of kmeans remains reasonable, compared to other algorithms. These results suggest that our approach might be particularly appropriate in a scenario where word embeddings are simultaneously being trained, and are thus not fixed. In such a scenario, having a robust MIPS method would allow us to update the MIPS model less frequently.\nIn this paper, we have proposed a new and efficient way of solving approximate K-MIPS based on a simple clustering strategy, and showed it can be a good alternative to the more popular LSH or tree-based techniques. We regard the simplicity of this approach as one of its strengths. Empirical results on three real-world datasets show that this simple approach clearly outperforms the other families of techniques. It achieves a larger speedup while maintaining precision, and is more robust to input corruption, an important property for generalization, as query test points are expected to not be exactly equal to training data points. Clustering MIPS generalizes better to related, but unseen data than the hashing approaches we evaluated. In future work, we plan to research ways to adapt on-the-fly the clustering for our approximate KMIPS as its input representation evolves during the learning of a model, leverage efficient K-MIPS to speed up extreme classifier training and improve precision and speedup by combining multiple clusterings. Finally, we mention that, while putting the final touches to this paper, another very recent and different MIPS approach, based on vector quantization, came to our knowledge (Guo et al., 2015). We highlight that the first arXiv post of our work predates their work. Nevertheless, while we did not have time to empirically compare to this approach here, we hope to do so in future work.\nThe authors would like to thank the developers of Theano (Bergstra et al., 2010) for developing such a powerful tool. We acknowledge the support of the following organizations for research funding and computing support: Samsung, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR.\n", "prediction": "in this paper, we compare two very simple approaches for approximate k-means clustering. the first one is based on tree-based vector representations and the second one on spherical kmeans : we propose to train spherical _ k_means after having reduced distance to each leaf of the dataset ( the set of candidate vectors to be clustered ) and we show that this simple approach yields much higher speedups for same precision than current state-of - the - art methods and also yields more robust retrieval when corrupted noise is present ( this is a common situation in large scale applications where we are asked to perform a full linear search within only a few milliseconds ; this problem is common in many applications such as online search and recommendation and it has been addressed in the literature ( e.g. by kim and lee ( 2012 ] and more recently by the authors in arxiv:1211.4321 ). <n> [ theorem]acknowledgement [ 1]algorithm [ 2]axiom[ 3]claim [ 4]conclusion [ 5]condition [ 6]corollary [ 7]criterion [ 8]definition [ 9]example [ 10]exercise [ 11]lemma [ 12]notation [ 13]problem [ 14]proposition [ 15]remark [ 16]solution [ 17]summary [ 18]assumption [ 19]appendix [ 20] # 1#2#3#4#1#2#1 # 3#1  # 2#1_#1#1 - # # 4#1 ( # 2)[4#1(#1]#1-#1 [ 3 -#1 2 -#2] - 2 s # 5 - 0 - 1 - ( 2, 2) to solve the top - @x[n ( 0] to find the number of all the vector points for all to the best points as a top of a vector for the corresponding to all ( top to a cluster as all as the @ @ ( 1 to top in all for a number in @ - to ( t to x - x and to search for any clusters as  to any vector with a small clusters in any other clusters of any small vector that to  ( ( n and all clusters to even the other vector and any vectors and a subset of clusters with the most clusters ( and top clusters that is the subset as top vector in an individual clusters and only to an inner points in its vector as any and ( i."}
{"ground_truth": "Most recent datacenter topology designs have focused on performance properties such as latency and throughput. In this paper, we explore a new dimension, life cycle management complexity, which attempts to understand the complexity of deploying a topology and expanding it. By analyzing current practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.\nperformance properties such as latency and throughput. In this paper, we explore a new dimension, life cycle management complexity, which attempts to understand the complexity of deploying a topology and expanding it. By analyzing current practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.\nOver the past decade, there has been a long line of work on designing datacenter topologies [2, 35, 31, 32, 3, 4, 20, 1]. While most have focused on performance properties such as latency and throughput, and on resilience to link and switch failures, datacenter lifecycle management [30, 38] has largely been overlooked. Lifecycle management is the process of building a network, physically deploying it on a data-center floor, and expanding it over several years so that it is available for use by a constantly increasing set of services. With datacenters living on for years, sometimes up to a decade [31, 12], their lifecycle costs can be high. A data center design that is hard to deploy can stall the rollout of services for months; this can be expensive considering the rate at which network demands have historically increased [31, 23]. A design that is hard to expand can leave the network functioning with degraded capacity impacting the large array of services that depend on it. It is therefore desirable to commit to a data-center network design only after getting a sense of its lifecycle management cost and complexity over time. Unfortunately, the costs of the large array of components needed for deployment such as switches, transceivers, cables, racks, patch panels1, and cable trays, are proprietary and change over time, and so are hard to quantify. An alternative approach is to develop complexity measures (as opposed to dollar costs) for lifecycle management, but as far as we know, no prior work has addressed this. In part, this is due to the fact that intuitions about lifecycle management are developed over time and with operations experience, and these lessons are not made available universally. 1A patch panel or a wiring aggregator is a device that simplifies cable re-wiring. Unfortunately, in our experience, this lack of a clear understanding of lifecycle management complexity often results in costly mistakes in the design of datacenters that are discovered during deployment and therefore cannot be rectified. Our paper is a first step towards useful characterizations of lifecycle management complexity. Contributions. To this end, our paper makes three contributions. First, we design several complexity metrics (3 and 4) that can be indicative of lifecycle management costs (i.e., capital expenditure, time and manpower required). These metrics include the number of: switches, patch panels, bundle-types, expansion steps, and links to be re-wired at a patch panel rack during an expansion step. We design these metrics by identifying structural elements of network deployments that make their deployment and expansion challenging. For instance, the number of switches in the topology determines how complex the network is in terms of packaging  laying out switches into homogeneous racks in a space efficient manner. Wiring complexity can be assessed by the number of cable bundles and the patch panels a design requires. As these increase, the complexity of manufacturing and packaging all the different cable bundles efficiently into cable trays, and then routing them from one patch panel to the next can be expected to increase. Finally, because expansion is carried out in steps [38], where the network operates at degraded capacity at each step, the number of expansion steps is a measure of the reduced availability in the network induced by lifecycle management. Wiring patterns also determine the number of links that need to be rewired at a patch panel during each step of expansion, a measure of step complexity [38]. Our second contribution is to use these metrics to compare the lifecycle management costs of two main classes of datacenter topologies recently explored in the research literature (2), Clos [2] and expander graphs [32, 35]. We find that neither class dominates the other: Clos has relatively lower wiring complexity; its symmetric design leads to more uniform bundling (and fewer cable bundle types); but expander graphs at certain scales can have simpler packaging requirements due to their edge expansion property [32]; they end up using much fewer switches than Clos to achieve the same network capacity. Expander graphs also demonstrate better expansion properties because they have fat edges (4) which permit more links to be rewired in each step. Finally we design and synthesize a novel and practical class of topologies called FatClique (5), that has lower overall lifecycle management complexity compared to Clos and expander graphs. We do this by combining favorable design USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 235 elements from these two topology classes. By design, FatClique incorporates 3 levels of hierarchy and uses a clique as a building block while ensuring edge expansion. At every level of its hierarchy, FatClique is designed to have fat edges, for easier expansion, while utilizing much fewer patch panels and therefore inter-rack cabling. Evaluations of these topology classes at three different scales, the largest of which is 16 the size of Jupiter, shows that FatClique is the best at most scales by all our complexity metrics. It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5  longer to expand the topology.\nData center topology families. Data centers are often designed for high throughput, low latency and resilience. Existing data center designs can be broadly classified into the following families: (a) Clos-like tree topologies, e.g., Googles Jupiter [31], Facebooks fbfabric [3], Microsofts VL2 [13], F10 [22]; (b) Expander graph based topologies, e.g., Jellyfish [32], Xpander [35]; (c) Direct topologies built from multi-port servers, e.g., BCube [14], DCell [15]. (d) Low diameter, strongly-connected topologies that rely on highradix switches, e.g., Slimfly [4], Dragonfly [20]; (e) Reconfigurable optical topologies like Rotornet and ProjectToR [24, 9, 11, 16, 39]. Of these, Clos and Expander based topologies have been shown to scale using widely deployed merchant silicon. The ecosystem around the hardware used by these two classes, e.g., cabling, cable trays used, rack sizes, is mature and wellunderstood, allowing us to quantify some of the operational complexity of these topologies. Direct multi-port server topologies and some reconfigurable optical topologies [24, 11, 16, 39] rely on newer hardware technologies that are not mainstream yet. It is hard to quantify the operational costs of these classes without making significant assumptions about such hardware. Low diameter topologies like Slimfly [4] and Dragonfly [20], can be built with hardware that is available today, but they require strongly connected groups of switches. Their incremental expansion comes at high cost and complexity; high-radix switches either need to be deployed well in advance, or every switch in the topology needs to be upgraded during expansion, to preserve low diameter. To avoid estimating operational complexity of topologies that rely on new hardware, or on topologies that unacceptably constrain expansion, we focus on the Clos and Expander families. Clos. A logical Clos topology with N servers can be constructed using switches with radix k connected in n = log k 2 (N2 ) layers based on a canonical recursive algorithm in [36]2. Fattree [2] and Jupiter [31] are special cases of Clos topology with 3 and 5 layers respectively. Clos construction naturally allows switches to be packaged together to form a chassis [31]. Since there are no known generic Clos packaging algorithm that can help design such a chassis, for a Clos of any scale, we designed one to help our study of its operational complexity. We present this algorithm in A.1. Expander graphs. Jellyfish and Xpander benefit from the high edge expansion property of expander graph to use a near optimal number of switches, while achieving the same bisection bandwidth as Clos based topologies [35]. Xpander splits N servers among switches by attaching s servers to each switch. With a k port switch, the remaining ports p= k s are connected to other switches that are organized in p blocks called metanodes. Metanodes are a group of switches, containing l =N/(s  (p+1)) switches, which increase as topology scale N increases. There are no connections between the switches of a metanode. Jellyfish is a degree bounded random graph (see [32] for more details). Takeaway. A topology with high edge expansion [35] can achieve a target capacity with fewer switches, leading to lower overall cost.\nDeployment is the process of realizing a physical topology in a data center space (e.g., a building), from a given logical topology. Deployment complexity can be reduced by careful packaging, placement and bundling strategies [31, 20, 1].\nPackaging of a topology involves careful arrangement of switches into racks, while placement involves arranging these racks into rows on the data center floor. The spatial arrangement of the topology determines the type of cables needed between switches. For instance, if two connected switches are within the same rack, they can use short-range cheaper copper cables, while connections between racks require more expensive optical cables. Optical cable costs are determined by two factors: the cost of transceivers and the length of cables (3.2). Placement of switches on the datacenter floor can also determine costs: connecting two switches placed at two ends of the data center building might require long range cables and high-end transceivers. Chassis, racks, and blocks. Packaging connected switches into a single chassis using a backplane completely removes the need for physical connecting cables. At scale, the cost and complexity savings from using a chassis-backplane can be significant. One or more chassis that are interconnected can be packed into racks such that: (a) racks are as homogeneous as possible, i.e., a topology makes use of only a few types of racks to simplify manufacturing and (b) racks are packed as 2This equation for n can be used to build a Clos with 1:1 oversubscription. For a Clos with an over-subscription x:y we would need n = log k 2 ( yN/x2 ) layers. 236 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association densely as possible to reduce space wastage. Some topologies define larger units of co-placement and packaging called blocks, which consist of groups of racks. Examples of blocks include pods in Fattree. External cabling from racks within a block are routed to wiring aggregators (i.e., patch panels [25]) to be routed to other blocks. For blocks to result in lower deployment complexity, three properties must be met: (a) the ports on the patch panel that it connects to are not wasted, when the topology is built out to full scale, (b) wiring out of the block should be as uniform as possible, and (c) racks in a block must be placed close to each other to reduce the length and complexity of wiring. Bundling and cable trays. When multiple fibers from the same set of physically adjoint (or neighboring) racks are destined to another set of neighboring racks, these fibers can be bundled together. A fiber bundle is a fixed number of identical-length fibers between two clusters of switches or racks. Manufacturing bundles is simpler than manufacturing individual fibers, and handling such bundles significantly simplifies operation complexity. Cable bundling reduces capex and opex by around 40% in Jupiter [31]. Patch panels facilitate bundling since the patch panel represents a convenient aggregation point to create and route bundles from the set of fibers destined to the same patch panel (or the same set of physically proximate patch panels). Figure 1 shows a Clos topology instance (left) and its physical realization using patch panels (right). Each aggregation block in the Clos network connects with one link to each spine block. The figure on the right shows how these links are routed physically. Bundles with two fibers each from two aggregations are routed to two (lower) patch panels. At each patch panel, these fibers are rebundled, by grouping fibers that go to the same spine in new bundles, and routed to two other (upper) patch panels that connect to spines. The bundles from the upper patch panels are then routed to the spines. Figure 1 assumes that patch panels are used as follows: bundles are connected to both the front and back ports on patch panels. For example, bundles from the aggregation layer connect to front ports on patch panels and bundles from spines connect to the back ports of patch panels. This enables bundle aggregation and rebundling and simplifies topology expansion.3 Bundles and fibers are routed through the datacenter on cable trays. The cables that aggregate at a patch panel rack must be routed overhead by using over-row and cross-row trays [26]. Trays have capacity constraints [34], which can constrain rack placement, block sizes, and patch panel placement. Today, trays can support at most a few thousand fibers [34]. 3[38]s usage of patch panels is slightly different. All bundles are connected to front ports of patch panels and links are established using jumper cables between the back ports of patch panels. For patch panels of a given port count, both approaches require the same number of patch panels. Our approach enables bundling closer to the aggregation and spine layers; [38] does not describe how bundling is accomplished in their design.\nBased on the previous discussion, we identify several metrics that quantify the complexity of the two aspects of datacenter topology deployment: packaging and placement. In the next subsection, we use these metrics to identify differences between Clos and Expander graph topology classes. Number of Switches. The total number of switches in the topology determines the capital expenditure for the topology, but it also determines the packaging complexity (switches need to be packed to chassis and racks) and the placement complexity (racks need to be placed on the datacenter floor). Number of Patch panels. By acting as bundle waypoints, the number of patch panels captures one measure of wiring complexity. The more the number of patch panels, the shorter the cable lengths from switches to the nearest patch panel, but the fewer the bundling opportunities, and vice versa. The number of patch panels needed is a function of topological structure. For instance, in a Clos topology, if an aggregation layer fits into one rack or a neighboring set of racks, a patch panel is not needed between the ToR and the aggregation layer. However, for larger Clos topologies where an aggregation block can span multiple racks, ToR to aggregation links may need to be rebundled through a patch panel. We discuss this in detail in 6.2. Number of Bundle Types. The number of patch panels alone does not capture wiring complexity. The other measure is the number of distinct bundle types. A bundle type is represented by a tuple of (a) the capacity of the number of fibers in the bundle, and (b) the length of the bundle. If a topology requires only a small number of bundle types, its bundling is more homogeneous; manufacturing and procuring such bundles is significantly simpler, and deploying the topology is also simplified since fewer bundling errors are likely with fewer types. These complexity measures are complete. The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 237 are inputs to the topology design). The number of cables and transceivers can be derived from the number of patch panels. In some cases, a metric is related to another metric, but not completely subsumed by it. For example, the number of switches determines rack packaging, which only partially determines the number of transceivers per switch. The other determinant of this quantity is the connectivity in the logical topology (which switch is connected to which other switch). Similarly, the number of patch panels can influence the number of bundle types, but these are also determined by logical connectivity.\nTo understand how the two main classes of topologies compare by these metrics, we apply these to a Clos topology and to a Jellyfish topology that support the same number of servers (131,072) and the same bisection bandwidth. This topology corresponds to twice the size of Jupiter. In 6, we perform a more thorough comparison at larger and smaller scales, and we describe the methodology by which these numbers were generated. Table 1 shows that the two topology classes are qualitatively different by these metrics. Consistent with the finding in [32], Jellyfish only needs a little over half the switches compared to Clos to achieve comparable capacity due to its high edge expansion property. But, by other measures, Clos performs better. It exposes far fewer ports outside the rack (a little over half that of Jellyfish); we say Clos has better port-hiding. A pod in this Clos contains 16 aggregation and 16 edge switches4. The aggregation switches can be can be packed into a single rack, so bundles from edge switches to aggregation switches do not need to be rebundled though patch panels, and we only need two layers of patch panels between aggregation and spine layer. However, in Jellyfish, almost all links are inter-rack links, so it requires more patch panels. Moreover, for Clos, since each pod has the same number of links to each spine, all bundles in Clos have the same capacity (number of fibers). However, the length of bundles can be different, depending on the relative placement of the patch panels between aggregation and spine layers, so Clos has 74 bundle types. However, since Jellyfish is a purely random graph without structure, to enable bundling, we group a fixed amount of neighbor racks as blocks to enable bundling. Since connectivity is random, the number of links between blocks are not uniform, Jellyfish needs almost 20 the number of bundle types. In 6, we show that Xpander also has 4we follow the definition of pod in [2]. qualitatively similar behavior in large scale. Takeaway. Relative to a structured hierarchical class of topologies like Clos, the expander graph topology has inherently higher deployment complexity in terms of the number of bundle types and cannot support port-hiding well.\nThe second important component of topology lifecycle management is expansion. Datacenters are rarely deployed to maximal capacity in one shot; rather, they are gradually expanded as network capacity demands increase.\nIn-place Expansion. At a high-level, expanding a topology involves two conceptual phases: (a) procuring new switches, servers, and cables and laying them on the datacenter floor, and (b) re-wiring (or adding) links between switches in the existing topology and the new switches. Phase (b), the re-wiring phase, can potentially disrupt traffic; as links are re-wired, network capacity can drop, leading to traffic loss. To avoid traffic loss, providers can either take the existing topology offline (migrate services away, for example, to another datacenter), or can carefully schedule link re-wiring while carrying live traffic, but schedule the re-wiring to maintain a desired target capacity. The first choice can impact service availability significantly. So, today, datacenters are expanded while carrying live traffic [30, 12, 31, 38]. To do this, expansion is carried out in steps, where at each step, the capacity of the topology is guaranteed to be at least a percentage p of the capacity of the existing topology. This fraction is sometimes called the expansion SLO. Today, many providers operate at expansion SLOs of 75% [38]; higher SLOs of 85-90% can impact availability budgets less while allowing providers to carry more traffic during expansion. The unit of expansion. Since expansion involves procurement, topologies are usually expanded in discrete units called blocks to simplify the procurement and layout logistics. In a structured topology, there are natural candidates for blocks. For example, in a Clos, a pod can be block, while in an Xpander, the metanode can be a block. During expansion, a block is first fully assembled and placed, and links between switches within a block are connected (as an aside, an Xpander metanode has no such links). During the re-wiring phase, only links between existing blocks and new blocks are re-wired. (This phase does not re-wire links between switches within an existing block). Aside from simplifying logistics, expanding at the granularity of a block preserves structure in structured topologies.\nWhat happens during a step. Figure 2 shows an example of Clos expansion. The upper left figure shows a partiallydeployed logical Clos, in which each spine and aggregation 238 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association block are connected by two links. The upper right is the target fully-deployed Clos, where each spine and aggregation block are connected by a single link. During expansion, we need to redistribute half of existing links (dashed) to the newly added spines without violating wiring and capacity constraints. Suppose we want to maintain 87.5% of the capacity of the topology (i.e., the expansion SLO is 0.875), this expansion will require 4 steps in total, where each patch panel is involved in 2 of these steps. In Figure 2, we only show the rewiring process on the second existing patch panels. To maintain 87.5% capacity at each pod, only one link is allowed to be drained. In the first step, the red link from the first existing aggregation block and the green link from the second existing aggregation block are rewired to the first new spine block. In the second step, the orange links from the first existing aggregation block and the purple link from the second existing aggregation block are rewired to the first new spine block. A similar process happens in the first patch panel. In practice, each step of expansion involves four sub-steps. In the first sub-step, the existing links that are to be re-wired are drained. Draining a link involves programming switches at each end of the link to disable the corresponding ports, and may also require reprogramming other switches or ports to route traffic around the disabled link. Second, one or more human operators physically rewire the links at a patch panel (explained in more detail below). Third, the newly wired links are tested for bit errors by sending test traffic through them. Finally, the new links are undrained. By far the most time consuming part of each step is the second sub-step, which requires human involvement. This sub-step is also the most important from an availability perspective; the longer this sub-step takes, the longer the datacenter operates at reduced capacity, which can impact availability targets [12]. The role of patch panels in re-wiring. The lower figure in Figure 2 depicts the physical realization of the (logical) rewiring shown in the upper figure. (For simplicity, the figure only shows the re-wiring of links on one patch panel to a new pod). Fibers and bundles originate and terminate at patch panels, so re-wiring requires reconnecting input and output ports at each patch panel. One important constraint in this process is that re-wiring cannot remove fibers that are already part of an existing bundle. Patch panels help localize rewiring and reuse existing cable bundling during expansions. Figure 3 shows, in more detail the rewiring process at a single patch panel. The leftmost figure shows the original wiring with connections (a, A), (b, B), (c, C), (d, D). To enable expansion, a topology is always deployed such that some ports at the patch panel are reserved for expansion steps. In the figure, we use these reserved ports to connect new fibers e, f , E and F (Phase 1). To get to a target wiring in the expanded network with connections (a, A), (b, B), (e, C), (f , D), (c, E), (d, F ), the following steps are taken: (1) Traffic is drained from (c, C), (d, D), (2) Connections (c, C), (d, D) are rewired, with c being connected to E, d being connected to F and so on, and (3) The new links are undrained, allowing traffic to use new capacity.\nWe identify two metrics that quantify expansion complexity and use these metrics to identify differences between Clos and Jellyfish in the next subsection. Number of Expansion Steps. As mentioned each expansion step requires a series of substeps which cannot be parallelized. Therefore the number of expansion steps determines the total time for expansion. Average number of rewired links in a patch panel rack per step. With patch panels, manual rewiring dominates the time taken within each expansion step. Within steps, it is possible to parallelize rewiring across racks of patch panels. With such parallelization, the time taken to rewire a single patch panel rack will dominate the time taken for each expansion step.\nTable 2 shows the value of these measures for a medium-sized Clos and a comparable Jellyfish topology, when the expansion USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 239 SLO is 90%. (6 has more extensive comparisons for these metrics, and also describes the methodology more carefully). In this setting, the number of links rewired per patch panel can be a factor of two less than Clos. Moreover, Jellyfish requires 3 steps, while Clos twice the number of steps. To understand why Jellyfish requires fewer steps, we define a metric called the north-to-south capacity ratio for a block. This is the ratio of the aggregate capacity of all northbound links exiting a block to the aggregate capacity of all southbound links to/from the servers within the block. Figure 4 illustrates this ratio: a thin edge (left), has an equal number of southbound and northbound links while a fat edge (right), has more northbound links than southbound links. A Clos topology has a thin edge, i.e., this ratio is 1, since the block is a pod. Now, consider an expansion SLO of 75%. This means that the southbound aggregate capacity must be at least 75%. That implies that, for Clos, at most 25% of the links can be rewired in a single step. However, Jellyfish has a much higher ratio of 3, i.e., it has a fat edge. This means that many more links can be rewired in a single step in Jellyfish than in Clos. This property of Jellyfish is required for reducing the number of expansion steps. Takeaway. Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio.\nOur discussions in 3 and 4, together with preliminary results presented in those sections (6 has more extensive results) suggest the following qualitative comparison between Clos and the expander graph families with respect to lifecycle management costs (Table 3):  Clos uses fewer bundle types and patch panels.  Jellyfish has significantly lower switch counts, uses fewer expansion steps, and touches fewer links per patch panel during an expansion step. In all of these comparisons, we compare topologies with the same number of servers and the same bisection bandwidth. The question we ask in this paper is: Is there a family of topologies which are comparable to, or dominate, both Clos and expander graphs by all our lifecycle management metrics? In this section, we present the design of the FatClique class of topologies and validate in 6 that FatClique answers this question affirmatively.\nFatClique (Figure 5) combines the hierarchical structure in Clos with the edge expansion in expander graphs to achieve lower lifecycle management complexity. FatClique has three levels of hierarchy: individual sub-block (top left), interconnected into a block (top right), which are in turn interconnected to form FatClique (bottom). The interconnection used at every level in the hierarchy is a clique, similar to Dragonfly [20]. Additionally, each level in the hierarchy is designed to have a fat edge (a north-south capacity ratio greater than 1). The cliques enable high edge expansion, while hierarchy enables lower wiring complexity than random-graph based expanders [32, 35]. FatClique is a class of topologies. To obtain an instance of this class, a topology designer specifies two input parameters: N , the number of servers, and k the chip radix. A synthesis algorithm takes these as inputs, and attempts to instantiate four design variables that completely determine the FatClique instance Table 4. These four design variables are:  s, the number of ports in a switch that connect to servers  pc, the number of ports in each switch that connect to other sub-blocks inside a block  Sc, the number of switches in a sub-block  Sb, the number of sub-blocks in a block The synthesis algorithm searches for the best combination of values for design variables, guided by six constraints, C1 through C6, described below. The algorithm also defines auxiliary variables for convenience; these can be derived from the design variables (Table 4). We define these variables in the narrative below. Sub-block connectivity. In FatClique, the sub-block forms the lowest level of the hierarchy, and contains switches and servers. All sub-blocks have the same structure. Servers are distributed uniformly among all switches of the topology, such that each sub-block has the same number of servers attached. However, because this number of servers may not be an exact multiple of the number of switches, we distribute the remainder across the switches, so that some switches may be connected to one more server than others. The alternative would have been to truncate or round up the number of servers per sub-block to be divisible by the number of switches in 240 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association the sub-block, which could lead to overprovisioning or underprovisioning. Within a sub-block, every switch has a link to every other switch within its sub-block, to form a clique (or complete graph). To ensure a fat edge at the sub-block level, each switch must connect to more switches than servers, captured by the constraint C1 : s < rs, where r is the switch radix and s is the number of ports on a switch connected to servers. Block-level connectivity. The next level in the hierarchy is the block. Each sub-block is connected to other sub-blocks within a block using a clique (Figure 5, top-left). In this clique, each sub-block may have multiple links to another sub-block; these inter-sub-block links are evenly distributed among all switches in the sub-block such that every pair of switches from different sub-block has at most one link. Ensuring a fat edge at this level requires that a sub-block has more intersub-block and inter-block links egressing from the sub-block than the number of servers it connects to. Because sub-blocks contain switches which are homogeneous5, this constraint is ensured if the sum of (a) the number ports on each switch connected to other sub-block (pc) and (b) those connected to other blocks (pb, an auxiliary variable in Table 4, see also Figure 6) exceeds the number of servers connected to the switch (captured by C2 : pc +pb > s). Inter-block connectivity. The top of the hierarchy is the overall network, in which each block is connected to every 5They are nearly homogeneous, since a switch may differ from another by one in the number of servers connected other block, resulting in a clique. The inter-block links are evenly distributed among all sub-blocks, and, within a subblock, evenly among all switches. To ensure a fat edge at this level, the number of inter-block links at each switch should be larger than the number of servers it connects to, captured by C3 : pb > s. Note that C3 subsumes (is a stronger constraint than) C2. Moreover, the constraint that blocks are connected in a clique imposes a constraint on the block radix (Rb, a derived variable). The block radix is the total number of links in a block destined to other blocks. Rb should be large enough to reach all other blocks (captured by C4 :Rb Nb1) such that the whole topology is a clique. Incorporating rack space constraints. Beyond connectivity constraints, we need to consider packaging constraints in sub-block design. Ideally, we need to ensure that a sub-block fits completely into one or more racks with no wasted rack space. For example, if we use 58RU racks, and each switch is to be connected to 8 1RU servers, we can accommodate 6 switches per sub-block, leaving 58 (6 8 + 6) = 4U in the rack for power supply and other equipment. In contrast, choosing 8 switches per sub-block would be a bad choice because it would need 88+8 = 72U rack space, overflowing into a second rack that would have 44RU un-utilized. We model this packaging fragmentation as a soft constraint: our synthesis algorithm generates multiple candidate assignments to the design variables that satisfy our constraints, and of these, we pick the alternative that has the lowest wasted rack space. Ensuring edge expansion. At each level of the hierarchy, edge expansion is ensured by using a clique. This is necessary for high edge expansion, but not sufficient, since it does not guarantee that every switch connects to as many other switches across the network as possible. One way to ensure this diversity is to make sure that each pair of switches is connected by at most one link. The constraints discussed so far do not ensure this. For instance, consider Figure 6, in which Lcc (another auxiliary variable in Table 4) is the number of links from one sub-block to another. If this number is greater than the number of switches Sc in the sub-block, then, some pair of switches might have more than one link to each other. Thus, C5 : Lcc  Sc is a condition to ensure that each pair of switches must be connected by a single link. Our topology synthesis algorithm generates assignments to design variables, and a topology generator then assigns links to ensure this property (5.2). Incorporating patch panel constraints. The size of the block is also limited by the number of ports in a single patch panel rack (denoted by PP Rackports). It is desirable to ensure that the inter-block links egressing each block connect to at most 12 the ports in a patch panel rack, so that the rest of the patch panel ports are available for external connections into the block (captured by C6 :Rb  12 PP Rackports). USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 241 5.2 FatClique Synthesis Algorithm Generating candidate assignments. The FatClique synthesis algorithm attempts to assign values to the design variables, subject to constraints C1 to C6. The algorithm enumerates all possible combinations of value assignments for these variables, and filters out each assignment that fails to satisfy all the constraints. For each remaining assignment, it generates the topology specified by the design variable, and determines if the topology satisfies a required capacity Cap, which is an input to the algorithm. Each assignment that fails the capacity test is also filtered out, leaving a candidate set of assignments. These steps are described in A.6. FatClique placement. For each assignment in this candidate set, the synthesis algorithm generates a topology placement. Because FatCliques design is regular, its topology placement algorithm is conceptually simple. A sub-block may span one or more racks, and these racks are placed adjacent to each other. All sub-blocks within a block are arranged in a rectangular fashion on the datacenter floor. For example, if a block has 25 racks, it is arranged in a 55 pattern of racks. Blocks are then arranged in a similar grid-like fashion. Selecting best candidate. For each placement, the synthesizer computes the cabling cost of the resulting placement (using [7]), and picks the candidate with the lowest cost. This step is not shown in Algorithm 3. This approach implicitly filters out candidates whose sub-block cannot be efficiently packed into racks (5.1).\nRe-wiring during expansion. Consider a small FatClique topology, shown top left in Figure 7, that has 3 blocks and Lbb = 5, i.e., five inter-block links. To expand it to a clique with six blocks, we would need to rewire the topology to have Lbb = 2 (top right in Figure 7). This means we need to redistribute more than half (6 out of 10) of existing links (red) at each block to new blocks without violating wiring and capacity constraints. The expansion process with patch panels is shown in the bottom of Figure 7. Similar to the procedure for Clos described in 4.1, all new blocks (shown in orange) are first deployed and interconnected and links from the new blocks are routed to reserved ports on patch panels associated with existing blocks (shown in blue), before re-wiring begins. For FatClique, rewiring one existing link requires releasing one patch panel port so that a new link can be added. Since links are already parts of existing bundles and routed through cable trays, we can not rewire them directly, e.g., by rerouting it from one patch panel to another. For example, link 1 (lower half of Figure 7) is originally connected blocks 1 and 3 by connecting ports a and b on the patch panel. Suppose we want to remove that link, and add two links, one from block 1 to block 5 (labeled 3), and another from block 3 to block 5 (labeled 4). The part of the original link (labeled 1) between the two patch panels is already bundled, so we cannot physically reroute it from block 3 to block 5. Instead, we effect re-wiring by releasing port a, connecting link 3 to port a, connecting link 1 to port c. Logically, this is equivalent to connecting ports a and d and b and c on the patch panel shown in lower half of Figure 7. This preserves bundling, while permitting expansion. If the original topology has Nb blocks, by comparing the old and target topology, the total number of rewired links is computed by Nb(Nb 1)(LbbLbb)/2. For this example, the total number of links to be rewired is 9. Iterative Expansion Plan Generation. By design, FatClique has fat edges, which allows draining more and more links at each step of the expansion, as network capacity increases. At each step, we drain links across all blocks uniformly, so that each block loses the same aggregate capacity. However the relationship between overall network capacity, and the number of links drained at every block in FatClique is unclear, because traffic needs to be sent over non-shortest paths to fully utilize the fabric. Therefore, we use an iterative approach to expansion planning, where, at each step, we search for the maximal ratio of links to be drained that still preserves expansion SLO. (A.4 discusses the algorithm in more detail). Our evaluation 6 shows that the number of expansion steps computed by this algorithm is much smaller than that for expanding symmetric Clos.\nAchieving low complexity. By construction, FatClique achieves low lifecycle management complexity (Table 3), while ensuring full-bisection bandwidth. It ensures high edge expansion, resulting in fewer switches. By packaging clique connections into a sub-block, it exports fewer external ports, an idea we call port hiding. By employing hierarchy and a regular (non-random) structure, it permits bundling and re- 242 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association quires fewer patch panels. By ensuring fat edges at each level of the hierarchy, it enables fewer re-wired links per patch panel, and fewer expansion steps. We quantify these in 6. Scalability. Since Xpander and Jellyfish do not incorporate hierarchy, they can be scaled to arbitrarily large sizes. However, because Clos and FatClique are hierarchical, they can only scale to a fixed size for a given chip radix. Table 5 shows the maximum scale of each topology as a function of switch radix k. FatClique scales to the same order of magnitude as a 5-layer Clos. As shown in 6, both of them can scale to 64 times bisection bandwidth of Jupiter. FatClique and Dragonfly. FatClique is inspired by Dragonfly [20] and they are both hierarchical topologies that use cliques as building blocks, but differ in several respects. First, for a given switch radix, FatClique can scale to larger topologies than Dragonfly because it incorporates one additional layer of hierarchy. Second, the Dragonfly class of topologies is defined by many more degrees of freedom than FatClique, so instantiating an instance of Dragonfly can require an expensive search [33]. In contrast, FatCliques constraints enable more efficient search for candidate topologies. Finally, since Dragonfly does not explicitly incorporate constraints for expansion, a given instance of Dragonfly may not end up with fat edges. Routing and Load Balancing on FatClique. Unlike for Clos, ECMP-based forwarding cannot be used achieve high utilization in more recently proposed topologies [20, 35, 32, 19]. FatClique belongs to this latter class, for which a combination of ECMP and Valiant Load Balancing [37] has been shown to achieve performance comparable to Clos [19].\nIn this section, we compare three classes of topologies, Clos, expander graphs and FatClique by our complexity metrics.\nTopology scales. Because the lifecycle complexity of topology classes can be a function of topology scale, we evaluate complexity across three different topology sizes based on the number of servers they support: small, medium, and large. Small topologies support as many servers as a 3-layer clos topology. Medium topologies support as many servers as 4-layer Clos. Large topologies support as many servers as 5-layer Clos topologies6. All our experiments in this section are based on comparing topologies at the same scale. At each scale, we generate one topology for each of Clos, Xpander, Jellyfish, and FatClique. The characteristics of these topologies are listed in Table 6. All these topologies use 32-port switching chips, the most common switch radix available today for all port capacities [5]. To compare topologies 6To achieve low wiring complexity, a full 5-layer Clos topology would require patch panel racks with four times as many ports as available today, so we restrict ourselves to the largest Clos that can be constructed with todays patch panel capacities fairly, we need to equalize them first. Specifically, at a given scale, each topology has approximately the same bisection bandwidth, computed (following prior work [32, 35]) using METIS [18]. All topologies at the same scale support roughly the same number of servers; small, medium and large scale topologies achieve, respectively, 14 , 4, and 16 times capacity of Jupiter. (In A.8, we also compare these topologies using two other metrics). Table 6 also shows the scale of individual building blocks of these topologies in terms of number of switches. For Clos, we use the algorithm in A.1 to design building blocks (chassis) and then use them to compose Clos. One interesting aspect of this table is that, at the 3 scales we consider, a FatCliques sub-block and block designs are identical, suggesting lower manufacturing and assembly complexity. We plan to explore this dimension in future work. For each topology we compute the metrics listed in Table 3: the number of switches, the number of bundle types, the number of patch panels, the average number of re-wired links at a patch panel during each expansion step, and the number of expansion steps. To compute these, we need component parameters, and placement and expansion algorithms for each topology class. Component Parameters. In keeping with [4, 40], we use optical links for all inter-rack links. We use 96 port 1RU patch panels [10] in our analysis. A 58RU [28] rack with patch panels can aggregate 2  96  58 = 11,136 fibers. We call this rack a patch-panel rack. Most datacenter settings, such as rack dimensions, aisle dimensions, cable routing and distance between cable trays follow practices in [26]. We list all parameters used in our paper in A.7. Placement Algorithms. For Clos, following Facebooks fbfabric [3], spine blocks are placed at the center of the datacenter, which might take multiple rows of racks, and pods are placed at two sides of spine blocks. Each pod is organized into a rectangular area with aggregation blocks placed in the middle to reduce the cable length from ToR to aggregation. FatCliques placement algorithm is discussed in 5.2. For Xpander, we use the placement algorithm proposed in [19]. We follow the practice that all switches in a metanode are placed closed to each other. However, instead of placing a metanode into a row of racks, we place a metanode into a rectangular area of racks, which reduces cable lengths when metanodes are large. For Jellyfish, we design a random search algorithm to aggressively reduce the cable length (A.2). Expansion Algorithms. For Clos, as shown in [38], it is fairly complex to compute the optimal number of rewired links for asymmetric Clos during expansion. However, when the original and target topologies are both symmetric, this number is easy to compute. For this case, we design an optimal algorithm (A.5) which rewires the maximum number of links at each step and therefore uses the smallest number of steps to finish expansion. For FatClique, we use the algorithm discussed in 5.3. For Xpander and Jellyfish, we design an USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 243 expansion algorithm based on the intuition from [35, 32] that, to expand a topology by n ports requires breaking n2 existing links. Finally, we have found that for all topologies, the number of expansion steps at a given SLO is scale invariant: it does not depend on the size of the original topology as long as the expansion ratio (target-topology-size-to-originaltopology-size ratio) is fixed (A.3). Presenting results. In order to bring out the relative merits of topologies, and trends of how cost and complexity increase with scale, we present values for metrics we measure for all topologies and scales in the same graph. In most cases, we present the absolute values of these metrics; in some cases though, because our three topologies span a large size range, for some metrics the results across topologies are so far apart that we are unable to do so without loss of information. In these cases, we normalize our results by the most expensive, or complex topology.\nThe placement of patch panels is determined both by the structure of the topology and its scale. Between edge and aggregation layers in Clos. For small and medium scale Clos, no patch panels are needed between edge and aggregation layers. Each pod at these scales contains 16 aggregation switches, which can be packed into a single rack (we call this an aggregation-rack). Given that a pod at this scale is small, all links from the edge can connect to this rack. Since all links connect to one physical location, bundles form naturally. In this case, each bundle from edge racks contains 316 fibers7. Therefore, no patch panels are needed between edge and aggregation layers. However, a large Clos needs one layer of patch panels between edge and aggregation layers since a pod at this scale is large. An aggregation block consists of 16 middle blocks8, each with 32 switches. The aggregation block by itself occupies a single rack. Based on the logical connectivity, links from any edge need to connect to all middle blocks. Without using patch panels, each bundle could at most contain 316/16 = 3 fibers. In our design, we use patch panels to aggregate local bundles from edges first and then rebundle them on patch panels to form new high capacity bundles from patch panels to aggregation racks. Based on the patch panel 7In our setting, each rack with 58RU can accommodate at most 3 switches and 48 associated servers. The total number of links out of this rack is 316. 8We follow the terminology in [31]. A middle block is a sub-block in an aggregation block. rack capacity constraint, two patch panel racks are enough to form high capacity bundles from edge to aggregation layers. Specifically, in our design 128 edge switches and 8 aggregation racks connect to a single patch panel. In this design, each edge-side bundle contains 48 fibers and each aggregation-side bundle contains 128 fibers. Between aggregation and spine layers. The topology between aggregation and spine layer in Clos is much larger than that inside a pod. For this reason, to form high capacity bundles, two layers of patch panels are needed. As shown in Figure 1, one layer of patch panels is placed near spine blocks at the center of the data center floor. Each patch panel rack aggregates local bundles from four spine racks in medium and large scale topologies. Similarly, another layer of patch panels are placed near aggregation rack, permitting long bundles between those patch panels. In expanders and FatClique. For Jellyfish, Xpander and FatClique, patch panels are deployed at the server block side and long bundles form between those patch panels. In FatClique, each block requires one patch panel rack (5.3). In a large Xpander, since a metanode is too big (Table 6), it is not possible to use one patch panel rack to aggregate all links from a metanode. Therefore, we divide a metanode into homogeneous sections, called sub-metanodes, such that links from a sub-metanode can be aggregated at one patch panel rack. For Jellyfish, we partition the topology into groups, each of which contains the same number of switches as in a block in FatClique, so each group needs one patch panel rack.\nIn this section, we evaluate our different topologies by our three measures of deployment complexity (3.2). Number of Switches. Figure 8 shows how the different topologies compare in terms of number of switches used at various topology scales. Figure 8(a) shows the total number of 244 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association switches for the small topologies, Figure 8(b) for the medium, and Figure 8(c) for the large. The y-axes increase in scale by about an order of magnitude from left to right. FatClique has 20% fewer switches than Clos for a small topology, and 50% fewer for the large. The results for Jellyfish and Xpander are similar, consistent with findings in [35, 32]. This benefit comes from the edge expansion property of the non-Clos topologies we consider. This implies that Clos topologies, at large scale, may require nearly twice the capital expenditures for switches, racks, and space as the other topologies. Number of Patch panels. Figure 9 shows the number of patch panels at different scales. As before, across these graphs, the y-axis scale increases approximately by one order of magnitude from left to right. At small and medium scales, Clos relies on patch panels mainly for connections between aggregation and spine blocks. Of all topologies at these scales, Clos uses the fewest number of patch panels: FatClique uses about 11% more patch panels, and Jellyfish and Xpander use almost 44-50% more. Xpander and Jellyfish rely on patch panels for all northbound links, and therefore in general, as scale increases, the number of patch panels in these networks grows (as seen by the increase in the y-axis scale from left to right). At large scale, however, Clos needs many more patch panels, comparable to Xpander and Jellyfish. At this scale, Clos aggregation blocks span multiple racks, and patch panels are also needed for connections between ToRs and aggregation blocks. Here, FatCliques careful packaging strategy becomes more evident, as it needs nearly 25% fewer patch panels than Clos. The majority of patch panels used in FatClique at all scales comes from inter-block links (which increase with scale). For this metric, Clos and FatClique are comparable at small and medium scales, but FatClique dominates at large scale. Number of Bundle Types. Table 7 shows the number of bundle types used by different topologies at different scales. A bundle type (3.1) is characterized by (a) the number of fibers in the bundle, and (b) the length of the bundle. The number of bundle types is a measure of wiring complexity. In this table, if bundles differ by more than 1m in length, they are designated as separate bundle types. Table 7 shows that Clos and FatClique use the fewest number of bundle types; this is due to the hierarchical structure of the topology, where links between different elements in the hierarchy can be bundled. As the topology size increases, the number of bundle types also increases in these topologies, by a factor of about 40 for Clos to 20 for FatClique when going from small to large topologies. On the other hand, Xpander and Jellyfish use an order of magnitude more bundle types compared to Clos and FatClique at medium and large scales, but use a comparable number for small scale topologies. Even at the small scale, Jellyfish uses many more bundle types because it uses a random connectivity pattern. At small scales Xpander metanodes use a single patch panel rack and bundles from all metanodes are uniform. With larger scales, Xpander metanodes become too big to connect to a single patch panel rack. We have to divide a metanode into several homogeneous sub-metanodes such that all links from sub-metanodes connect to a patch panel rack. However, because of the randomness in connectivity, this subdivision cannot ensure uniformity of bundles egressing sub-metanode patch panel racks, so we find that Xpander has a large number of bundle types in medium and large topologies. Thus, by this metric, Clos and FatClique have the lowest complexity across all three scales, while Xpander and Jellyfish have an order of magnitude more complexity. Moreover, across all metrics FatClique has lowest deployment complexity, especially at large scales. Case Study: Quantifying cabling costs. While not all aspects of lifecycle management complexity can be translated to actual dollar costs, it is possible to estimate one aspect, namely the cost of cables. Cabling cost includes the cost of transceivers and cables, and is reported to be the dominant component of overall datacenter network cost [31, 20]. We can estimate costs because our placement algorithms generate cable or bundle lengths, the topology packaging determines the number of transceivers, and estimates of cable and transceiver costs as a function of cable length are publicly available [7]. Figure 10 quantifies the cabling cost of all topologies, USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 245 across different scales. Clos has higher cabling costs at small and medium scales compared to expander graphs, although the relative difference decreases at medium scale. At large scales, the reverse is true. Clos is around 12% cheaper than Xpander in terms of cabling cost since Xpander does not support port-hiding at all and uses more long inter-rack cables. Thus, given that cabling cost is the dominant component of overall cost, it is unclear whether the tradeoff Xpander and Jellyfish makes in terms of number of switches and cabling design pays off in terms of capital expenditure, especially at large scale. We find that FatClique has the lowest cabling cost of the topologies we study with a cabling cost 23-36% less than Clos. This result came as a surprise to us, because intuitively topologies that require all-to-all clique like connections might use longer length cables (and therefore more expensive transceivers). However on deeper examination, we found that Clos uses a larger number of cables (especially inter-rack cables) compared to other topologies since it has a relatively higher number of switches (Figure 8) to achieve the same bisection bandwidth. Thus, more switches leads to more racks and datacenter floor area, which stretches the cable length. All those factors together explain why Clos cabling costs are higher than FatCliques. Thus, from an equipment capital expenditure perspective, at large scale a FatClique can be at least 23% cheaper than a Clos, because it has at least 23% fewer switches, 33% fewer patch panel racks, and 23% lower cabling costs than Clos.\nIn this section, we evaluate topologies by our two measures of expansion complexity (4.3): number of expansion steps required, and number of rewired-links per patch panel rack per step. Since the number of steps is scale-invariant (6.1), we only present the results from expanding medium size topologies for both metrics9. When evaluating Clos, we study the expansion of symmetric Clos topologies; generic Clos expansion is studied in [38]. As discussed in 6.1, for symmetric Clos, we have developed an algorithm with optimal number of rewiring steps. Number of expansion steps. Figure 11 shows the number of steps (y-axis) required to expand topologies to twice their existing size (expansion ratio = 2) at different expansion SLOs (x-axis). We find that at 75% SLO, all topologies require the same number of expansion steps. But the number 9We have verified that the relative trend in the number of re-wired links per patch panel holds for small and large topologies of steps required to expand Clos with tighter SLOs steeply increases. This is because the number of links that can be rewired per aggregation block in Clos per step, is limited (due to north-to-south capacity ratio 4.3) by the SLO. The tighter the SLO, fewer the number of links rewired per aggregation block per step, and larger the number of steps required to complete expansion. FatClique, Xpander and Jellyfish require fewer and comparable number of expansion steps due to their fat edge property, allowing many more links to be rewired per block per step. Their curves largely overlap (with FatClique taking one more step as SLO increases beyond 95%) . Number of rewired links per patch panel rack per step. This metric is an indication of the time it takes to finish an expansion step because, today, rewiring each patch panel requires a human operator [38]. A datacenter operator can reduce re-wiring time by employing staff to rewire each patch panel rack in parallel, in which case, the number of links per patch panel rack per step is a good indicator of the complexity of an expansion step. Figure 12 shows the average of the maximum rewired links per patch panel rack, per step (y-axis), when expanding to twice the topology size size at different SLOs (y-axis). Even though the north-to-south capacity ratio restricts the number of links that can be rewired in Clos per step, the number of rewired links per patch panel rack per step in Clos remains consistently higher than other topologies, until we hit 97.5% SLO. The reason is that the links that need to be rewired in Clos are usually concentrated in few patch panel racks by design. As such, it is harder to parallelize rewiring in Clos, than it is in the other topologies. FatClique has the lowest rewiring step complexity across all topologies.\nWe find that FatClique is the best at most scales by all our complexity metrics. (The one exception is that at small and medium scales, Clos has slightly fewer patch panels). It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5  longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the 246 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association number of links to be rewired at each step per patch panel can be 30-50% higher.\nTopology Design. Previous topology designs have focused on cost effective, high capacity and low diameter datacenter topologies like [6, 35, 32, 4, 20]. Although they achieve good performance and cost properties, the lifecycle management complexity of these topologies have not been investigated either in the original papers or in subsequent work that has compared topologies [26, 27]. In contrast to these, we explore topology designs that have low lifecycle complexity. Recent work has explored datacenter topologies based on free space optics [24, 11, 9, 16, 39] but because we lack operational experience with them at scale, it is harder to design and evaluate lifecycle complexity metrics for them. Topology Expansion. Prior work has discussed several aspects of topology expansion [30, 32, 35, 8, 38]. Condor [30] permits synthesis of Clos-based datacenter topologies with declarative constraints some of which can be used to specify expansion properties. A more recent paper [38] attempts to develop a target topology for expansion, given an existing Clos topology, that would require the least number of link rewiring. REWIRE [8] finds target expansion topologies with highest capacity and smallest latency without preserving topological structure. Jellyfish [32] and Xpander [35] study expansion properties of their topology, but do not consider practical details in re-wiring. Unlike these, our work is examines lifecycle management as a whole, across different topology classes, and develops new performance-equivalent topologies with better lifecycle management properties.\nIn this paper, we have attempted to characterize the complexity of lifecycle management of datacenter topologies, an unexplored but critically important area of research. Lifecycle management consists of network deployment and expansion, and we devise metrics that capture the complexity of each. We use these to compare topology classes explored in the research literature: Clos and expander graphs. We find that each class has low complexity by some metrics, but high by others. However, our evaluation suggests topological features important for low lifecycle complexity: hierarchy, edge expansion and fat edges. We design a family of topologies called FatClique that incorporates these features, and this class has low complexity by all our metrics at large scale. As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction; several future directions remain. Topology oversubscription. In our comparisons, we have only considered topologies with an over-subscription ratio of 1:1. Jupiter [31] permits over-subscription at the edge of the network, but there is anecdotal evidence that providers also over-subscribe at higher levels in Clos topologies. To explore the manageability of over-subscribed topologies it will be necessary to design over-subscription techniques in FatClique, Xpander and Jellyfish in a way in which all topologies can be compared on a equal footing. Topology heterogeneity. In practice, topologies have a long lifetime over which they accrue heterogeneity: new blocks with higher radix switches, patch panels with different port counts etc. These complicate lifecycle management. To evaluate these, we need to develop data-driven models for how heterogeneity accrues in topologies over time and adapt our metrics for lifecycle complexity to accommodate heterogeneity. Other management problems. Our paper focuses on topology lifecycle management, and explicitly does not consider other network management problems like fault isolation or control plane complexity. Designs for manageability must take these into account. USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 247\nA.1 Clos Generation Algorithm For Clos topologies, the canonical recursive algorithm in [36] can only generate non-modular topologies as shown in Figure 13. In practice, as shown in Jupiter [31], the topology is composed of heterogenous building blocks (chassis), which are packed into a single rack and therefore enforce port hiding (the idea that as few ports from a rack are exposed outside the rack). Although Jupiter is modular and supports port hiding, it is single instance of a Clos-like topology with a specific set of parameters. We seek an algorithm that can take any valid set of Clos parameters and produce chassis-based topologies automatically. Besides, it would be desirable for this algorithm to generate all possible feasible topologies satisfying the parameters, so we can select the one that is most compactly packed. Our logical Clos generation algorithm achieves these goals. Specifically, the algorithm uses the following steps: 1. Compute the total number of layers of homogeneous switching chips needed. Namely, given N servers and radix k switches, we use n= log k 2 (N2 ) to compute the number of layers of chips n needed. 2. Determine the total number of layers of chips for edge, aggregation and core layers, which are represented by e, a and s respectively, such that e+a+s= n. 3. Identify blocks for edge, aggregation and core layer. Clos networks rely on every edge being able to reach every spine through exactly one path, by fanning out via as many different aggregation blocks as possible (and vice versa). We find that the resulting interconnection is a derivative of the classical perfect shuffle Omega network ([21], e.g., aggregation blocks in Figure 14 and Figure 15). Therefore, we use Omega networks to build both the edge and aggregation blocks, and to define the connections between edge-aggregation and aggregationspines. The spine block on the other hand needs to be rearrangeably-nonblocking, so it can relay flows from any edge to any other edge with full capacity. Therefore it is built as a smaller Clos topology [6] (e.g., spine blocks in Figure 14). 4. Compose the whole network using edge, aggregation and core blocks. The process to compose the whole topology is to link all these blocks and uses the same procedure as Jupiter[31]. We have verified that topologies generated by our construction algorithm, such as the ones in Figure 14 and Figure 15, are isomorphic to a topology generated using the canonical algorithm in Figure 13. By changing different combinations of e, a and s, we can obtain multiple candidate topologies, as shown in Figure 14 and Figure 15. A.2 Jellyfish Placement Algorithm For Jellyfish, we use a heuristic random search algorithm to place switches and servers. The algorithm works as follows. At each stage of the algorithm, a node can be in one of two states: placed, or un-placed. A placed node is one which has been positioned in a rack. Each step of the algorithm randomly selects an un-placed node. If the selected node has logical neighbor nodes that have already been placed, we place this node at the centroid of the area formed by its placed logical neighbors. If no placed neighbor exists, the algorithm randomly selects a rack to place the node. We have also tried other heuristics like neighbor-first, which tries to place a switchs logical neighbors as close as possible around it. However, this performs worse than our algorithm. A.3 Scale-invariance of Expansion Scale-invariance of Expandability for Symmetric Clos. For a symmetric Clos network, the number of expansion steps is scale-invariant and independent of the degree to which the original topology is partially deployed. Consider a simplified Clos where the original topology has g aggregation blocks. Each aggregation block has p ports for spine-aggregation links, each of which has the unit capacity. Assume the worstcase traffic in which all sources are located in the left half of aggregation blocks and all destinations are in the right half. This network contains g p/2 crossing links between left and right halves. If, during expansion, the network is expected to support a demand of d units capacity per aggregation block, the total demand traversing the cut between the left and right halves in one direction is d g/2. Then, the maximum number of links that can be redistributed in an expansion step is k = g  p/2 d  g/2 = g(p d)/2, which is linear in the number of aggregation blocks (network size). This linearity between k and g implies scale-invariant expandability, e.g., when an aggregation block is doubled to 2g, the maximum number of redistributed links per expansion step becomes 2k. Scale-invariance of Expandability for Jellyfish, Xpander, and FatClique. A random graph consists of s nodes, which is a first-order approximation for Jellyfishs switch, Xpanders metanode and FatCliques block. Each node has p internode ports, so there are s  p/2 inter-node links. We can treat the network as a bipartite graph. We assume the worstcase traffic matrix, where all traffic is sent through one part of the bipartite graph to the other. Suppose an expansion SLO requires each source-destination node pair to support d unit demand. Then the total demands from all sources are d  s/2. The probability of a link being a cross link is 1/2, and the expected number of cross links is s p/4. These cross links are expected to be the bottleneck between the sourcedestinations pairs. Therefore, in the first expansion step, we can redistribute at most k = s p/4d s/2 = s(p/4d/2) links, and the maximum number of redistributed links is linear in the number of nodes (network size), e.g., if the number of 250 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association nodes is doubled to 2s, we can redistribute 2k links in the first step. It is easy to see that, after each expansion step, the number of links added to the bottleneck is also linear with the number of nodes, so the expandability is scale-invariant. A.4 FatClique Expansion Algorithm Algorithm 1 shows the expansion algorithm for FatClique. The input to the algorithm includes original and target topologies T o and Tn, the link break ratio during an expansion step , multipliers  < 1 and  > 1, which are used to adjust  based on network capacity.  specifies the fraction of existing links that must be broken for re-wiring. The output of the algorithm is the expansion plan Plan. Our expansion algorithm is an iterative trial-and-error approach (Line 4). Each iteration tries to find the right amount of links to break while satisfying the aggregate capacity constraint (Line 11) and the edge capacity constraint (Line 6), which guarantees that the north-to-south capacity ratio is always not smaller than 1 during any expansion step. If all constraints are satisfied, we accept this plan and tentatively increase the link break ratio  (Line 16, by multiplying by ) due to capacity increase. Otherwise, the link break ratio  (Line 12) is decreased (by multiplying by  conservatively.) input : T o, Tn, SLO output: Plan 1 Initialize   (0,),  (0,1),  (1,) 2 Find the total set of links to break, L, based on T o and Tn 3 Compute original capacity c0 4 while |L|> 0 do 5 Select a subset of links Lb, from L uniformly across all blocks, where |Lb|= |L|. 6 if Lb does not satisfy edge capacity constraint then 7 =   8 end 9 Delete Lb from T o 10 c = ComputeCapacity(T o) 11 if c < c0 SLO then 12 =   13 add Lb back to T o 14 else 15 T o = AddNewLinks(Lb, T o, Tn) 16 =   17 Plan.add(Lb) 18 end 19 end Algorithm 1: FatClique Expansion Plan Generation A.5 Expansion for Clos Since the motivation of this work is to compare topologies, we only focus on developping optimal expansion solutions for symmetrical Clos. More general algorithms for Clos expansion can be found in [38]. Also, similar to [38], we assume the worst case traffic matrices for Clos, i.e., servers under a pod will send traffic using full capacity to servers in other pods. Target Topology Generation. As mentioned in 4.1, a pod is the unit of expansion in Clos. When we add new pods and associated spines to a Clos topology for expansion, the wiring pattern inside a pod remains unchanged. To make the target topology non-blocking and to ease expansion (i.e., number of to-be-redistributed links on each pod is the same), links from a pod should be distributed across all spines as evenly as possible. Expansion plan generation. Once a target Clos topology is generated, the next step is to redistribute links to convert the original topology into the target topology. By comparing the original and target topology, it is easy to figure out which new links should be routed to which patch panels to satisfy the wiring constraint. In this section, we mainly focus on how to drain links such that the capacity constraint is satisfied and the number of expansion steps is minimized. Insight 1: Maximum rewired links at each pod is bounded. At each expansion step, when links are drained, network capacity drops. At the same time, as expansion proceeds, new devices are added incrementally, the overall network capacity increases gradually during the whole expansion process. In general, during expansion, the incrementally added capacity should be leveraged to speed up the expansion process. Due to the thin edges in Clos, no matter what the overall network capacity is, the maximum number links to be drained at each pod is bounded by the number of links on each pod multiplied by (1SLO). Figure 16 shows an example. The leftmost figure is a folded Clos, where each pod has 16 links (4 trunks). If the SLO is 75%, the maximum number of links to be drained at a single step is 16 (10.75) = 4. For our expansion plan generation algorithm, we try to achieve this bound at each pod at every single step. Insight 2: Drain links at spines uniformly across edges (pods). Given the number of links allowed to be drained at each pod, we need to carefully select which links are to be drained. Figure 16 shows two draining plans. Drain plan 1 will drain links from two spines uniformly across all pods. The residual capacity is 48, satisfying the requirement USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 251 Insight 3: Create physical loops by selecting the right target spines. Ideally, drained links with the same index on a pod on the same original spine should be redistributed to the same spine because the traffic sent from the pod to the target spine has a return path to the pod. Otherwise, the traffic will be dropped. Figure 17 illustrates this insight. The right side of the figure shows the performance of two redistribution plans. The y axis shows the normalized capacity of the network at each expansion step. In the first plan, link 1 is first moved to spine s1 (1-s1),followed by link 3 to the same spine s1 (3-s1) which results in 75% capacity loss, since the two pods are connected by three paths instead of four. Once links 1 and 3 are undrained, s1 connects the two pods by a fourth path, and the normalized capacity is restored to 1. This redistribution step now provides leeway for supporting 25% capacity loss in the next step. In this next step, links 2 and 4 are rewired to connect to s2. During the rewiring, capacity again drops to 75%, with three paths between the pods. On undraining links 2 and 4, the capacity is once again restored to 1. In contrast, redistribution plan 2 violates SLO because it does not focus on restoring capacity by establishing paths via the new spine, as suggested by the insight (links 1 and 3 are moved to different spines). Inspired by these insights, we designed Algorithm 2, which can achieve all our insights simultaneously when both original and target topologies are symmetric. The algorithm is optimal since at every expansion step, it achieves the upper bound of the links that could be drained. Therefore, our algorithm uses smallest steps to expand Clos. The input to the algorithm is the original and new symmetric topology T o and Tn. We use T osp and T n sp to represent the number of links between spine s and pod p in the old and new topology respectively. Initially, T osp = 0, where s  is a new spine. The output of the algorithm is the draining plan, Subplani, for expansion step i. The final expansion plan Plan= {Subplani} and the number of Subplan, |Plan|, is the total expansion step. The algorithm starts by indexing old spines, new spines and links on each pod from left to right respectively (Line 1-2), which are critical for the correctness of the algorithm since the algorithm relies on these indexes to break ties when selecting spines and links to redistribute. Then, based on our Insight 1, Line 3 computes the upper bound on the number of links to be redistributed on each pod, np. We show experimentally that our algorithm can always achieve this upper bound in each individual step as long as T o and Tn are symmetric. Next, the algorithm iterates over all indexed old spines (Line 4) and tries to drain np links uniformly across all pods (Line 5) such that Insight 2 is satisfied. Line 6 compares the number of remaining to-be-redistributed links sp and np and is useful only at the last expansion step. For each pod, the algorithm needs to find spines to redistribute links to (Line 7-14) while satisfying the constraint in Insight 3, i.e., drained links with the same index on a pod on the same original spine are redistributed to the same spine. Due to indexing and symmetric structure of Clos, our algorithm can always satisfy Insight 3. Specifically, when selecting spines, the spine satisfying sp = TnspT o sp > 0 with the smallest index will be considered first (Line 8-Line 10). When selecting links from pod to redistribute, we always select the first na links to redistribute (Line 14). Theorem 1 Algorithm 2 produces the optimal expansion plan for Clos topology. The proof is simple. Since at every expansion step, our algorithm achieves the upper bound of the links that could be drained, our algorithm uses smallest steps to finish the expansion. A.6 FatClique Topology Synthesis Algorithm The topology synthesis algorithm for FatClique is shown in Algorithm 3. Essentially, the algorithm is a search algorithm, and leverages the constraints C1 to C6 in 5.1 to prune the search space. It works as follows. The outermost loop (Line 2) enumerates the number of racks used for a sub-block. Based on the rack space constraints, sub-block size Sc is determined Line 4. Next, the algorithm iterates over the number of subblocks in a block Sb Line 5, whose size is constrained by MaxBlockSize. Inside this loop, we leverage constraints C1 to C6 and derivations in 5.1 to find the feasible set of pc, which is represented by Pc (Line 6). Then we construct FatClique based all design variables Line 8 and compute its capacity Line 9. If the capacity matches the target capacity, 252 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association input : T o, Tn, SLO output: Subplan 1 Index original and new spines from left to right starting from 1 respectively 2 Index links at each pod from left to right starting from 1 3  pod p, np = num_links_per_pod  (1-SLO) // Insight 1 4 foreach Original Spine s do 5 foreach pod p do // Insight 2 6 sp = T ospTnsp, np = min(np, sp) // Insight 2 7 while np > 0 do 8 foreach New Spine s do // Insight 3 9 sp = TnspT o sp 10 if sp > 0 then break 11 12 end 13 na =min(sp,np) 14 Find the first na to-be-distributed links, Lsp 15 np = npna, update(T o) 16 Subplan.add(Lsp) 17 end 18 end 19 end Algorithm 2: Single Step Clos Expansion Plan Generation we add this topology into candidate set (Line 15). If the capacity is larger than required, the algorithm will increase s by 1 which will decrease the number of switches used n= N/s (N is fixed) and therefore reduce the network capacity in next search step (Line 13). If the capacity is smaller than required, the algorithm will decrease s by 1 (Line 11) to increase the number of switches and capacity in next search step. A.7 Parameter Setting The cable price with transceivers used in our evaluation is listed in Table 9. We found that a simple linear model does not fit the data. The data is better approximated by a piecewise linear function: cables shorter than 100 meters are fit using one linear model and cables beyond 100 meters are fit using another linear model. The latter has a larger slope because beyond 100 meters, more advanced and expensive transceivers are necessary. In our experiment, since we only know the discrete price for cables and associated transceivers, we do the following: if the length of the cable is X, we use the exact price; if the length if larger than X, we use the first cable price larger than X. A.8 Other Metrics In our evaluations, we have tried to topologies with qualitatively similar properties 6. In this section, we quantify other properties of these topologies. Edge Expansion and Spectral Gap. Since computing edge expansion is computationally hard, we follow the method in [35] using spectral gap [17] to approximate edge expansion. A larger spectral gap implies larger edge expansion. To fairly compare topologies, we equalize their bisection bandwidth first. As shown before, to achieve the same bisection USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 253 100 200 300 400 500 Small-scale Clos Jellyfish Xpander FatClique 4 50 5 10 15 Nu m be r o f P at hs Path Length Figure 19: Path Diversity for Small-scale Topologies bandwidth, Clos uses many more switches. Also, Clos is not a d-regular graph and do not know of a way to compute the spectral graph for Clos-like topologies. Therefore, we compare the spectral gap only for d-regular graphs, Jellyfish, Xpander and FatClique at different scales (1k-4k nodes). The spectral gap is defined as follows [17]. Let G with node degree d and A(G) denote the d-regular topology and its adjacent matrix. The matrix A(G) has n real eigenvalues which we denote by 1  2     n. Spectral gap SG= d2. In our experiments, chip radix is 32 and each node in those topologies connects to 8 servers, d= 24. The result is shown in Figure 18. First, we observe that spectral gap stays roughly the same under different scales. Also, the spectral gap of FatClique is slightly lower than that of other topologies, which implies that FatClique has slightly smaller edge expansion compared to Jellyfish and Xpander. This is to be expected, since FatClique adds some hierarchical structure to cliques. Path Diversity. We compute the path diversity for different topologies. For Clos, we only calculate the number of shortest paths between two ToR switches from different pods. For other topologies, we compute the number of paths which are no longer than the shortest paths in the same-scale Clos. For example, for small-scale Clos, the shortest path length is 5. We will only calculate paths whose length is no larger than 5 in other topologies. This is a rough metric for path diversity. The results are shown in Figure 19 and Figure 20. We found that Jellyfish, Xpander and FatClique have the same level of path diversity, which is higher than that of Clos. Also, those topologies have shorter paths than Clos. 254 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\n", "prediction": "this paper explores new ways to quantify the complexity of deploying and managing lifecycles in data - centers.<n> we show that existing metrics in the literature have low complexity for lifecycle management but not for topologies, and we devise new metrics that enable us to understand the nature of the topology and the expansion of datacenters that have lower complexity than other metrics for the same amount of management and data center capacity ; we also show these metrics can be used to determine the number of patches in a patch panel and how many patches need to be placed in each patch to achieve a given level of performance : we design new topology classes that are comparable in performance and complexity to existing classes and use them to demonstrate the need for new complexity measures for deployment and management of life - cycles in both physical and logical domains <n> _ index terms_. datacenter network operations management ( qom ) has traditionally focused on latency and throughput properties of physical network access control ( lans)@xcite and virtual private networks ( vpn)/vot / vps physical availability control systems ( raman)-a system of virtual machines that can communicate and process information in an undirected fashion between physical systems @xmath0 and their logical counterparts  applications '' that access the physical system via the virtual machine s logical domain and transmit information to and from the logical system?<n> + [ 2 ] we present a methodology for quantifying the size of a logical vp / rvp physical vpb physical access controls for any logical logical systems and any physical physical information ( [ 3d [ 4 ], we use the v2 [3 [2 and [4 [4] and 2 [ 1] to obtain the 2[2 to derive the [ [2] [[[4[3[v[s [3] [ii [ 5 -2 [5 [6 [ 6] in 2] [ 10, [10 [] and to [  to the top panel [ ( ( 2 to 2 ( to ( 3 t to a [ 32 ]) to  the second panel ( @ 2. [ to we derive [ a panel to make the @ [ * 2 - to one to split the other to @ @ ( the panel in [ @ to to map [ the ]. to two to which to p  ( a top - [ p ( p to * to e  [ we to add the p. to our top to another panel that to b [ b to x  for [ e - we are the 3 to also to that [ x to an [ with the ( x - ( we do to top   in to further to re-[([i to se to 5 to ii  a to have the]  with [ in th to do the x. ( * *  that is to all to is the most to include the two ( e. we have to more to these to are to many to m to r  is  we  will to number to this to j to generate the group ( which is not to whose top [ and also the graph to those to bundle ( that we split to other top and a  are not that that the more top with top into the remaining to some top ( and that has to patch panels and all top of top that  into top we can also we introduce the to its top in which are a number that also that of other other ( with a more that results to their top for all the we were to cover the many top- and its to g to who are all that will be the others to k to most top[g to - which we found to increase the entire top.  2 and p and top the"}
{"ground_truth": "Performance of in-memory key-value store (KVS) continues to be of great importance as modern KVS goes beyond the traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation in data centers. Recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most KVS from the network to the CPU. RDMA-capable NIC partly alleviates the problem, but the primitives provided by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling in-network processing. In this paper, we present KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory. We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 s. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP 17, Shanghai, China  2017 ACM. 978-1-4503-5085-3/17/10. . . $15.00 DOI: 10.1145/3132747.3132756 CCS CONCEPTS Information systems Key-value stores; Hardware Hardware-software codesign;\nWe develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 s. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP 17, Shanghai, China  2017 ACM. 978-1-4503-5085-3/17/10. . . $15.00 DOI: 10.1145/3132747.3132756 CCS CONCEPTS Information systems Key-value stores; Hardware Hardware-software codesign; KEYWORDS Key-Value Store, Programmable Hardware, Performance ACM Reference format: Bojie Li* Zhenyuan Ruan[1] Wencong Xiao Yuanwei Lu and Yongqiang Xiong Andrew Putnam Enhong Chen Lintao Zhang . 2017. KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC. In Proceedings of SOSP 17, Shanghai, China, October 28, 2017, 16 pages. DOI: 10.1145/3132747.3132756\nIn-memory key-value store (KVS) is a key distributed system component in many data centers. KVS enables access to a shared key-value hash table among distributed clients. Historically, KVS such as Memcached [25] gained popularity as an object caching system for web services. Large web service providers such as Amazon [17] and Facebook [3, 57], have deployed distributed key-value stores at scale. More recently, as main-memory based computing becomes a major trend in the data centers [18, 58], KVS starts to go beyond caching and becomes an infrastructure to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases [12], model parameters in machine learning [46], nodes and edges in graph computing [67, 74] and sequencers in distributed synchronization [37]. For most of these applications, the performance of the KVS is the key factor that directly determines the system efficiency. Due to its importance, over the years significant amount of research effort has been invested on improving KVS performance. Earlier key-value systems [17, 25, 57] are built on top of traditional OS abstractions such as OS lock and TCP/IP stack. This puts considerable stress on the performance of *Bojie Li and Zhenyuan Ruan are co-first authors who finish this work during internship at Microsoft Research. the OS, especially the networking stack. The bottleneck is exacerbated by the fact that physical network transport speed has seen huge improvements in the last decade due to heavy bandwidth demand from data center applications. More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down [21, 69], a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. One line of research [36, 37] uses two-sided RDMA to accelerate communication (Figure 1a). KVS built with this approach are bounded by CPU performance of the KVS servers. Another line of research uses one-sided RDMA to bypass remote CPU and shift KV processing workload to clients [18, 55] (Figure 1b). This approach achieves better GET performance but degrades performance for PUT operations due to high communication and synchronization overhead. Due to lack of transactional support, the abstraction provided by RDMA is not a perfect fit for building efficient KVS. In the meantime, another trend is emerging in data center hardware evolution. More and more servers in data centers are now equipped with programmable NICs [10, 27, 64]. At the heart of a programmable NIC is a field-programmable gate array (FPGA) with an embedded NIC chip to connect to the network and a PCIe connector to attach to the server. Programmable NIC is initially designed to enable network virtualization [24, 44]. However, many found that FPGA resources can be used to offload some workloads of CPU and significantly reduce CPU resource usage [14, 30, 52, 60]. Our work takes this general approach. We present KV-Direct, a new in-memory key-value system that takes advantage of programmable NIC in data center. KVDirect, as its name implies, directly fetches data and applies updates in the host memory to serve KV requests, bypassing host CPU (Figure 1c). KV-Direct extends the RDMA primitives from memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops). Compared with one-sided RDMA based systems, KVDirect deals with the consistency and synchronization issues at server-side, thus removes computation overhead in client and reduces network traffic. In addition, to support vectorbased operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages [19] and delegate certain computation to programmable NIC for efficiency. Since the key-value operations are offloaded to the programmable NIC, we focus our design on optimizing the PCIe traffic between the NIC and host memory. KV-Direct adopts a series of optimizations to fully utilize PCIe bandwidth and hide latency. Firstly, we design a new hash table and memory allocator to leverage parallelism available in FPGA and minimize the number of PCIe DMA requests. On average, KV-Direct achieves close to one PCIe DMA per READ operation and two PCIe DMAs per WRITE operation. Secondly, to guarantee consistency among dependent KV operations, KV-Direct includes an out-of-order execution engine to track operation dependencies while maximizing the throughput of independent requests. Thirdly, KV-Direct exploits on-board DRAM buffer available on programmable NIC by implementing a hardware-based load dispatcher and caching component in FPGA to fully utilize on-board DRAM bandwidth and capacity. A single NIC KV-Direct is able to achieve up to 180 M KV operations per second (Ops), equivalent to the throughput of 36 CPU cores [47]. Compared with state-of-art CPU KVS implementations, KV-Direct reduces tail latency to as low as 10 s while achieving a 3x improvement on power efficiency. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a server, we achieve 1.22 billion KV operations per second in a single commodity server, which is more than an order of magnitude improvement over existing systems. KV-Direct supports general atomic operations up to 180 Mops, equal to normal KV operation and significantly outperforms the number reported in state-of-art RDMA-based system: 2.24 Mops [36]. The atomic operation agnostic performance is mainly a result of our out-of-order execution engine that can efficiently track the dependency among KV operations without explicitly stalling the pipeline.\n\nHistorically, KVS such as Memcached [25] gained popularity as an object caching system for web services. In the era of in-memory computation, KVS goes beyond caching and becomes an infrastructure service to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases [12], model parameters in machine learning [46], nodes and edges in graph computing [67, 74] and sequencers in distributed synchronization [37, 45]. The workload shifts from object cache to generic data structure store implies several design goals for KVS. High batch throughput for small KV. In-memory computations typically access small key-value pairs in large batches, e.g., sparse parameters in linear regression [48, 74] or all neighbor nodes in graph traversal [67], therefore a KVS should be able to benefit from batching and pipelining. Predictable low latency. For many data-parallel computation tasks, the latency of an iteration is determined by the slowest operations [59]. Therefore, it is important to control the tail latency of KVS. CPU based implementations often have large fluctuations under heavy load due to scheduling irregularities and inflated buffering. High efficiency under write-intensive workload. For cache workloads, KVS often has much more reads than writes [3], but it is no longer the case for distributed computation workloads such as graph computation [61], parameter servers [46]. These workloads favor hash table structures that can handle both read and write operations efficiently. Fast atomic operations. Atomic operations on several extremely popular keys appear in applications such as centralized schedulers [63], sequencers [37, 45], counters [76] and short-term values in web applications [3]. This requires high throughput on single-key atomics. Support vector-type operations. Machine learning and graph computing workloads [46, 67, 74] often require operating on every element in a vector, e.g., incrementing every element in a vector with a scalar or reducing a vector into the sum of its elements. KVS without vector support requires the client to either issue one KVS operation per element, or retrieve the vector back to the client and perform the operation. Supporting vector data type and operations in KVS can greatly reduce network communication and CPU computation overhead.\nBuilding a high performance KVS is a non-trivial exercise of optimizing various software and hardware components in a computer system. Characterized by where the KV processing takes place, state-of-the-art high-performance KVS systems basically falls into three categories: on the CPU of KVS server (Figure 1a), on KVS clients (Figure 1b) or on a hardware accelerator (Figure 1c). When pushed to the limit, in high performance KVS systems the throughput bottleneck can be attributed to the computation in KV operation and the latency in random memory access. CPU-based KVS needs to spend CPU cycles for key comparison and hash slot computation. Moreover, KVS hash table is orders of magnitude larger than the CPU cache, therefore the memory access latency is dominated by cache miss latency for practical access patterns. By our measurement, a 64-byte random read latency for a contemporary computer is 110 ns. A CPU core can issue several memory access instructions concurrently when they fall in the instruction window, limited by the number of loadstore units in a core (measured to be 34 in our CPU) [26, 28, 75]. In our CPU, we measure a max throughput of 29.3 M random 64B access per second per core. On the other hand, an operation to access 64-byte KV pair typically requires 100 ns computation or 500 instructions, which is too large to fit in the instruction window (measured to be 100200). When interleaved with computation, the performance of a CPU core degrades to only 5.5 M KV operations per second (Mops). An optimization is to batch memory accesses in a KV store by clustering the computation for several operations together before issuing the memory access all at once [47, 56]. This improves the per-core throughput to 7.9 MOps in our CPU, which is still far less than the random 64B throughput of host DRAM. Observing the limited capacity of CPU in KV processing, recent work [18, 55, 70] leverage one-sided RDMA to offload KV processing to clients and effectively using the KVS server as a shared memory pool. Despite the high message rate (8150 Mops [37]) provided by RDMA NICs, it is challenging to find an efficient match between RDMA primitives and key-value operations. For a write (PUT or atomic) operation, multiple network round-trips and multiple memory accesses may be required to query the hash index, handle hash collisions and allocate variable-size memory. RDMA does not support transactions. Clients must synchronize with each other to ensure consistency using RDMA atomics or distributed atomic broadcast [70], both incurring communication overhead and synchronization latency [18, 55]. Therefore, most RDMA-based KVS [18, 36, 55] recommend using onesided RDMA for GET operations only. For PUT operations, they fall back to the server CPU. The throughput of writeintensive workload is still bottlenecked by CPU cores.\nTen years ago, processor frequency scaling slowed down and people turned to multi-core and concurrency [69]. Nowadays, power ceiling implies that multi-core scaling has also met difficulties [22]. People are now turning to domain-specific architectures (DSAs) for better performance. Due to the increasing mismatch of network speed and CPU network processing capacity, programmable NICs with FPGA [10, 24, 27, 44] now witness large-scale deployment in datacenters. As shown in Figure 2, the heart of the programmable NIC we use is an FPGA, with an embedded NIC chip to connect to the network. Programmable NICs typically come with on-board DRAM as packet buffers and runtime memory for NIC firmware [44], but the DRAM is typically not large enough to hold the entire key-value store.\nKV-Direct moves KV processing from the CPU to the programmable NIC in the server (Figure 1c). Same as RDMA, the KV-Direct NIC accesses host memory via PCIe. PCIe is a packet switched network with 500 ns round-trip latency and 7.87 GB/s theoretical bandwidth per Gen3 x8 endpoint. On the latency side, for our programmable NIC, the cached PCIe DMA read latency is 800 ns due to additional processing delay in FPGA. For random non-cached DMA read, there is an additional 250 ns average latency (Figure 3b) due to DRAM access, DRAM refresh and PCIe response reordering in PCIe DMA engine. On the throughput side, each DMA read or write operation needs a PCIe transport-layer packet (TLP) with 26-byte header and padding for 64-bit addressing. For a PCIe Gen3 x8 NIC to access host memory in 64-byte granularity, the theoretical throughput is therefore 5.6 GB/s, or 87 Mops. To saturate PCIe Gen3 x8 with 64-byte DMA requests, 92 concurrent DMA requests are needed considering our latency of 1050 ns. In practice, two factors further limit the concurrency of DMA requests. First, PCIe credit-based flow control constrains the number of in-flight requests for each DMA type. The PCIe root complex in our server advertises 88 TLP posted header credits for DMA write and 84 TLP non-posted header credits for DMA read. Second, DMA read requires assigning a unique PCIe tag to identify DMA responses which may come out of order. The DMA engine in our FPGA only support 64 PCIe tags, further limiting our DMA read concurrency to 64 requests, which renders a throughput of 60 Mops as shown in Figure 3a. On the other hand, with 40 Gbps network and 64-byte KV pairs, the throughput ceiling is 78 Mops with client-side batching. In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET. This boils down to three challenges: Minimize DMA requests per KV operation. Hash table and memory allocation are two major components in KVS that require random memory access. Previous works propose hash tables [9, 18] with close to 1 memory access per GET operation even under high load factors. However, under higher than 50% load factor, these tables need multiple memory accesses per PUT operation on average with large variance. This not only consumes PCIe throughput, but also leads to latency variations for write-intensive workloads. In addition to hash table lookup, dynamic memory allocation is required to store variable-length KVs that cannot be inlined in the hash table. Minimizing hash table lookups per KV operation and memory accesses per memory allocation is essential for matching PCIe and network throughput under write-intensive, small-KV workloads. Hide PCIe latency while maintaining consistency. An efficient KVS on NIC must pipeline KV operations and DMA requests to hide the PCIe latency. However, KV operations may have dependencies. A GET following PUT on a same key needs to return the updated value. This requires tracking KV operations being processed and stall the pipeline on data hazard, or better, design an out-of-order executor to resolve data dependency without explicitly stalling the pipeline. Dispatch load between NIC DRAM and host memory. An obvious idea is to use the DRAM on NIC as a cache for host memory, but in our NIC, the DRAM throughput (12.8 GB/s) is on par with the achievable throughput (13.2 GB/s) of two PCIe Gen3 x8 endpoints. It is more desirable to distribute memory access between DRAM and host memory in order to utilize both of their bandwidths. However, the onboard DRAM is small (4 GiB) compared to the host memory (64 GiB), calling for a hybrid caching and load-dispatching approach. In the following, we will present KV-Direct, a novel FPGAbased key-value store that satisfies all aforementioned goals and describe how we address the challenges.\n\nKV-Direct enables remote direct key-value access. Clients send KV-Direct operations (3.2) to KVS server while the programmable NIC processes the requests and sending back results, bypassing the CPU. The programmable NIC on KVS server is an FPGA reconfigured as a KV processor (3.3). Figure 2 shows the architecture of KV-Direct.\nKV-Direct extends one-sided RDMA operations to key-value operations, as summarized in Table 1. In addition to standard KVS operations as shown in the top part of Table 1, KV-Direct supports two types of vector operations: Sending a scalar to the NIC on the server and the NIC applies the update to each element in the vector; or send a vector to the server and the NIC updates the original vector element-by-element. Furthermore, KV-Direct supports user-defined update functions as a generalization to atomic operations. The update function needs to be pre-registered and compiled to hardware logic before executing. KV operations with user-defined update functions are similar to active messages [19], saving communication and synchronization cost. When a vector operation update, reduce or filter is operated on a key, its value is treated as an array of fixed-bit-width elements. Each function  operates on one element in the vector, a client-specified parameter , and/or an initial value  for reduction. The KV-Direct development toolchain duplicates the  several times to leverage parallelism in FPGA and match computation throughput with PCIe throughput, then compiles it into reconfigurable hardware logic using an high-level synthesis (HLS) tool [2]. The HLS tool automatically extracts data dependencies in the duplicated function and generates a fully pipelined programmable logic. Update operations with user-defined functions are capable of general stream processing on a vector value. For example, a network processing application may interpret the vector as a stream of packets for network functions [44] or a bunch of states for packet transactions [68]. Single-object transaction processing completely in the programmable NIC is also possible, e.g., wrapping around S QUANTITY in TPC-C benchmark [16]. Vector reduce operation supports neighbor weight accumulation in PageRank [61]. Non-zero values in a sparse vector can be fetched with vector filter operation.\nAs shown in Figure 4, the KV processor in FPGA receives packets from the network, decodes vector operations and buffers KV operations in the reservation station (3.3.3). Next, the out-of-order engine (3.3.3) issues independent KV operations from reservation station into the operation decoder. Depending on the operation type, the KV processor looks up the hash table (3.3.1) and executes the corresponding operations. To minimize the number of memory accesses, small KV pairs are stored inline in the hash table, others are stored in dynamically allocated memory from the slab memory allocator (3.3.2). Both the hash index and the slaballocated memory are managed by a unified memory access engine (3.3.4), which accesses the host memory via PCIe DMA and caches a portion of host memory in NIC DRAM. After the KV operation completes, the result is sent back to the out-of-order execution engine (3.3.3) to find and execute matching KV operations in reservation station. As discussed in 2.4, the scarcity of PCIe operation throughput requires the KV processor to be frugal on DMA accesses. For GET operation, at least one memory read is required. For PUT or DELETE operation, one read and one write are minimal for hash tables. Log-based data structures can achieve one write per PUT, but it sacrifices GET performance. KVDirect carefully designs the hash table to achieve close to ideal DMA accesses per lookup and insertion, as well as the memory allocator to achieve < 0.1 amortized DMA operations per dynamic memory allocation. 3.3.1 Hash Table. To store variable-sized KVs, the KV storage is partitioned into two parts. The first part is a hash index (Figure 5), which consists a fixed number of hash buckets. Each hash bucket contains several hash slots and some metadata. The rest of the memory is dynamically allocated, and managed by a slab allocator (3.3.2). A hash index ratio configured at initialization time determines the percentage of the memory allocated for hash index. The choice of hash index ratio will be discussed in 5.1.1. Each hash slot includes a pointer to the KV data in dynamically allocated memory and a secondary hash. Secondary hash is an optimization that enables parallel inline checking. The key is always checked to ensure correctness, at the cost of one additional memory access. Assuming a 64 GiB KV storage in host memory and 32-byte allocation granularity (a trade-off between internal fragmentation and allocation metadata overhead), the pointer requires 31 bits. A secondary hash of 9 bits gives a 1/512 false positive probability. Cumulatively, the hash slot size is 5 bytes. To determine the hash bucket size, we need to trade-off between the number of hash slots per bucket and the DMA throughput. Figure 3a shows that the DMA read throughput below 64B granularity is bound by PCIe latency and parallelism in the DMA engine. A bucket size less than 64B is suboptimal due to increased possibility of hash collision. On the other hand, increasing the bucket size above 64B would decrease hash lookup throughput. So we choose the bucket size to be 64 bytes. KV size is the combined size of key and value. KVs smaller than a threshold are stored inline in the hash index to save the additional memory access to fetch KV data. An inline KV may span multiple hash slots, whose pointer and secondary hash fields are re-purposed for storing KV data. It might not be optimal to inline all KVs that can fit in a bucket. To minimize average access time, assuming that smaller and larger keys are equally likely to be accessed, it is more desirable to inline KVs smaller than an inline threshold. To quantify the portion of used buckets in all buckets, we use memory utilization instead of load factor, because it relates more to the number of KVs that can fit in a fixed amount of memory. As shown in Figure 6, for a certain inline threshold, the average memory access count increases with memory utilization, due to more hash collisions. Higher inline threshold shows a more steep growth curve of memory access count, so an optimal inline threshold can be found to minimize memory accesses under a given memory utilization. As with hash index ratio, the inline threshold can also be configured at initialization time. When all slots in a bucket are filled up, there are several solutions to resolve hash collisions. Cuckoo hashing [62] and hopscotch hashing [29] guarantee constant-time lookup by moving occupied slots during insertion. However, in writeintensive workload, the memory access time under high load factor would experience large fluctuations. Linear probing may suffer from primary clustering, therefore its performance is sensitive to the uniformity of hash function. We choose chaining to resolve hash conflicts, which balances lookup and insertion, while being more robust to hash clustering. 3.3.2 Slab Memory Allocator. Chained hash slots and non-inline KVs need dynamic memory allocation. We choose slab memory allocator [7] to achieve O(1) average memory access per allocation and deallocation. The main slab allocator logic runs on host CPU and communicates with the KV-processor through PCIe. Slab allocator rounds up allocation size to the nearest power of two, called slab size. It maintains a free slab pool for each possible slab size (32, 64, . . . , 512 bytes), and a global allocation bitmap to help to merge small free slabs back to larger slabs. Each free slab pool is an array of slab entries consisting of an address field and a slab type field indicating the size of the slab entry. The free slab pool can be cached on the NIC. The cache syncs with the host memory in batches of slab entries. Amortized by batching, less than 0.07 DMA operation is needed per allocation or deallocation. When a small slab pool is almost empty, larger slabs need to be split. Because the slab type is already included in a slab entry, in slab splitting, slab entries are simply copied from the larger pool to the smaller pool, without the need for computation. Including slab type in the slab entry also saves communication cost because one slab entry may contain multiple slots. On deallocation, the slab allocator needs to check whether the freed slab can be merged with its neighbor, requiring at least one read and write to the allocation bitmap. Inspired by garbage collection, we propose lazy slab merging to merge free slabs in batch when a slab pool is almost empty and no larger slab pools have enough slabs to split. 3.3.3 Out-of-Order Execution Engine. Dependency between two KV operations with the same key in the KV processor will lead to data hazard and pipeline stall. This problem is magnified in single-key atomics where all operations are dependent, thus limiting the atomics throughput. We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context. To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed. However, comparing 256 16-byte keys in parallel would take 40% logic resource of our FPGA. Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key. To simplify hash collision resolution, we regard KV operations with the same hash as dependent, so there may be false positives, but it will never miss a dependency. Operations with the same hash are organized in a chain and examined sequentially. Hash collision would degrade the efficiency of chain examination, so the reservation station contains 1024 hash slots to make hash collision probability below 25%. The reservation station not only holds pending operations, but also caches their latest values for data forwarding. When a KV operation is completed by the main processing pipeline, its result is returned to the client, and the latest value is forwarded to the reservation station. Pending operations in the same hash slot are checked one by one, and operations with matching key are executed immediately and removed from the reservation station. For atomic operations, the computation is performed in a dedicated execution engine. For write operations, the cached value is updated. The execution result is returned to the client directly. After scanning through the chain of dependent operations, if the cached value is updated, a PUT operation is issued to the main processing pipeline for cache write back. This data forwarding and fast execution path enable single-key atomics to be processed one operation per clock cycle (180 Mops), eliminate head-of-line blocking under workload with popular keys, and ensure consistency because no two operations on the same key can be in the main processing pipeline simultaneously. 3.3.4 DRAM Load Dispatcher. To further save the burden on PCIe, we dispatch memory accesses between PCIe and the NIC on-board DRAM. Our NIC DRAM has 4 GiB size and 12.8 GB/s throughput, which is an order of magnitude smaller than the KVS storage on host DRAM (64 GiB) and slightly slower than the PCIe link (14 GB/s). One approach is to put a fixed portion of the KVS in NIC DRAM. However, the NIC DRAM is too small to carry a significant portion of memory accesses. The other approach is to use the NIC DRAM as a cache for host memory, the throughput would degrade due to the limited throughput of our NIC DRAM. We adopt a hybrid solution to use the DRAM as a cache for a fixed portion of the KVS in host memory, as shown in Figure 7. The cache-able part is determined by the hash of memory address, in granularity of 64 bytes. The hash function is selected so that a bucket in hash index and a dynamically allocated slab have an equal probability of being cache-able. The portion of cache-able part in entire host memory is called load dispatch ratio (l). Assume the cache hit probability is h(l). To balance load on PCIe and NIC DRAM, the load dispatch ratio l should be optimized so that: l tputDRAM = (1  l) + l  (1  h(l)) tputPCIe let k be the ratio of NIC memory size and host memory size. Under uniform workload, cache hit probability h(l) = NIC memory sizecache-able corpus size = k l when k  l . Caching under uniform workload is not efficient. Under long-tail workload with Zipf distribution, assume n is the total number of KVs, approximately h(l) = log(NIC memory size)log(cache-able corpus size) = log(kn) log(ln) when k  l . Under long-tail workload, the cache hit probability is as high as 0.7 with 1M cache in 1G corpus. An optimal l can be solved numerically, as discussed in 6.3.1.\nOur hardware platform is built on an Intel Stratix V FPGA based programmable NIC (2.3). The programmable NIC is attached to the server through two PCIe Gen3 x8 links in a bifurcated x16 physical connector, and contains 4 GiB of on-board DRAM with a single DDR3-1600 channel. For development efficiency, we use Intel FPGA SDK for OpenCL [2] to synthesize hardware logic from OpenCL. Our KV processor is implemented in 11K lines of OpenCL code and all kernels are fully pipelined, i.e., the throughput is one operation per clock cycle. With 180 MHz clock frequency, our design can process KV operations at 180 M op/s if not bottlenecked by network, DRAM or PCIe. Below highlights several implementation details. Slab Memory Allocator. As shown in Figure 8, for each slab size, the slab cache on the NIC is synchronized with host DRAM using two double-ended stacks. For the NICside double-ended stack (left side in Figure 8), the left end is popped and pushed by the allocator and deallocator, and the right end is synchronized with the left end of the corresponding host-side stack via DMA. The NIC monitors the size of NIC stack and synchronizes to or from the host stack according to high and low watermarks. Host daemon periodically checks the size of host-side double-ended stack. If it grows above a high watermark, slab merging is triggered; when it drops below a low watermark, slab splitting is triggered. Because each end of a stack is either accessed by the NIC or the host, and the data is accessed prior to moving pointers, race conditions would not occur. DRAM Load Dispatcher. One technical challenge is the storage of metadata in DRAM cache, which requires additional 4 address bits and one dirty flag per 64-byte cache line. Cache valid bit is not needed because all KVS storage is accessed exclusively by the NIC. To store the 5 metadata bits per cache line, extending the cache line to 65 bytes would reduce DRAM performance due to unaligned access; saving the metadata elsewhere will double memory accesses. Instead, we leverage spare bits in ECC DRAM for metadata storage. ECC DRAM typically has 8 ECC bits per 64 bits of data. For Hamming code to correct one bit of error in 64 bits of data, only 7 additional bits are required. The 8th ECC bit is a parity bit for detecting double-bit errors. As we access DRAM in 64-byte granularity and alignment, there are 8 parity bits per 64B data. We increase the parity checking granularity from 64 data bits to 256 data bits, so double-bit errors can still be detected. This allows us to have 6 extra bits which can save our address bits and dirty flag. Vector Operation Decoder. Compared with PCIe, network is a more scarce resource with lower bandwidth (5 GB/s) and higher latency (2 s). An RDMA write packet over Ethernet has 88 bytes of header and padding overhead, while a PCIe TLP packet has only 26 bytes of overhead. This is why previous FPGA-based key-value stores [5, 6] have not saturated the PCIe bandwidth, although their hash table designs are less efficient than KV-Direct. This calls for client-side batching in two aspects: batching multiple KV operations in one packet and supporting vector operations for a more compact representation. Towards this end, we implement a decoder in the KV-engine to unpack multiple KV operations from a single RDMA packet. Observing that many KVs have a same size or repetitive values, the KV format includes two flag bits to allow copying key and value size, or the value of the previous KV in the packet. Fortunately, many significant workloads (e.g. graph traversal, parameter server) can issue KV operations in batches. Looking forward, batching would be unnecessary if higher-bandwidth network is available.\nIn this section, we first take a reductionist perspective to support our design choices with microbenchmarks of key components, then switch to a holistic approach to demonstrate the overall performance of KV-Direct in system benchmark. We evaluate KV-Direct in a testbed of eight servers and one Arista DCS-7060CX-32S switch. Each server equips two 8 core Xeon E5-2650 v2 CPUs with hyper-threading disabled, forming two NUMA nodes connected through QPI Link. Each NUMA node is populated with 8 DIMMs of 8 GiB Samsung DDR3-1333 ECC RAM, resulting a total of 128 GiB of host memory on each server. A programmable NIC [10] is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch. The programmable NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector. The tested server equips SuperMicro X9DRG-QF motherboard and one 120 GB SATA SSD running Archlinux (kernel 4.11.9-1). For system benchmark, we use YCSB workload [15]. For skewed Zipf workload, we choose skewness 0.99 and refer it as long-tail workload.\n5.1.1 Hash Table. There are two free parameters in our hash table design: (1) inline threshold, (2) the ratio of hash index in the entire memory space. As shown in Figure 9a, when hash index ratio grows, more KV pairs can be stored inline, yielding a lower average memory access time. Figure 9b shows the increase of memory accesses as more memory is utilized. As shown in Figure 10, the maximal achievable memory utilization drops under higher hash index ratio, because less memory is available for dynamic allocation. Consequently, aiming to accommodate the entire corpus in a given memory size, the hash index ratio has an upper bound. We choose this upper bound and get a minimal average memory access times, shown as the dashed line in Figure 10. In Figure 11, we plot the number of memory accesses per GET and PUT operation for three possible hash table designs: chaining in KV-Direct, bucket cuckoo hashing in MemC3 [23] and chain-associative hopscotch hashing in FaRM [18]. For KV-Direct, we make the optimal choice of inline threshold and hash index ratio for the given KV size and memory utilization requirement. For cuckoo and hopscotch hashing, we assume that keys are inlined and can be compared in parallel, while the values are stored in dynamically allocated slabs. Since the hash table of MemC3 and FaRM cannot support more than 55% memory utilization for 10B KV size, the three rightmost bars in Figure 11a and Figure 11b only show the performance of KV-Direct. For inline KVs, KV-Direct has close to 1 memory access per GET and close to 2 memory accesses per PUT under non-extreme memory utilizations. GET and PUT for noninline KVs have one additional memory access. Comparing KV-Direct and chained hopscotch hashing under high memory utilization, hopscotch hashing performs better in GET, but significantly worse in PUT. Although KV-Direct cannot guarantee worst case DMA accesses, we strike for a balance between GET and PUT. Cuckoo hashing needs to access up to two hash slots on GET, therefore has more memory accesses than KV-Direct under most memory utilizations. Under high memory utilization, cuckoo hashing incurs large fluctuations in memory access times per PUT. 5.1.2 Slab Memory Allocator. The communication overhead of slab memory allocator comes from the NIC accessing available slab queues in host memory. To sustain the maximal throughput of 180M operations per second, in the worst case, 180M slab slots need to be transferred, consuming 720 MB/s PCIe throughput, i.e., 5% of total PCIe throughput of our NIC. The computation overhead of slab memory allocator comes from slab splitting and merging on host CPU. Fortunately, they are not frequently invoked. For workloads with stable KV size distributions, newly freed slab slots are reused by subsequent allocations, therefore does not trigger splitting and merging. Slab splitting requires moving continuous slab entries from one slab queue to another. When the workload shifts from large KV to small KV, in the worst case the CPU needs to move 90M slab entries per second, which only utilizes 10% of a core because it is simply continuous memory copy. Merging free slab slots to larger slots is rather a timeconsuming task, because it involves filling the allocation bitmap with potentially random offsets and thus requiring random memory accesses. To sort the addresses of free slabs and merge continuous ones, radix sort [66] scales better to multiple cores than simple bitmap. As shown in Figure 12, merging all 4 billion free slab slots in a 16 GiB vector requires 30 seconds on a single core, or only 1.8 seconds on 32 cores using radix sort [66]. Although garbage collecting free slab slots takes seconds, it runs in background without stalling the slab allocator, and practically only triggered when the workload shifts from small KV to large KV. 5.1.3 Out-of-Order Execution Engine. We evaluate the effectiveness of out-of-order execution by comparing the throughput with the simple approach that stalls the pipeline on key conflict, under atomics and long-tail workload. Onesided RDMA and two-sided RDMA [37] throughputs are also shown as baselines. Without this engine, an atomic operation needs to wait for PCIe latency and processing delay in the NIC, during which subsequent atomic operations on the same key cannot be executed. This renders a single-key atomics throughput of 0.94 Mops in Figure 13a, consistent with 2.24 Mops measured from an RDMA NIC [37]. The higher throughput of RDMA NIC can be attributed to its higher clock frequency and lower processing delay. With out-of-order execution, single-key atomic operations in KV-Direct can be processed at peak throughput, i.e., one operation per clock cycle. In MICA [51], single-key atomics throughput cannot scale beyond a single core. Atomic fetch-and-add can be spread to multiple cores in [37], but it relies on the commutativity among the atomics and therefore does not apply to non-commutative atomics such as compare-and-swap. With out-of-order execution, single-key atomics throughput improves by 191x and reaches the clock frequency bound of 180 Mops. When the atomic operations spread uniformly among multiple keys, the throughput of one-sided RDMA, two-sided RDMA and KV-Direct without out-of-order execution grow linearly with the number of keys, but still far from the optimal throughput of KV-Direct. Figure 13b shows the throughput under the long-tail workload. Recall that the pipeline is stalled when a PUT operation finds any in-flight operation with the same key. The long-tail workload has multiple extremely popular keys, so it is likely that two operations with the same popular key arrive closely in time. With higher PUT ratio, it is more likely that at least one of the two operations is a PUT, therefore triggering a pipeline stall. 5.1.4 DRAM Load Dispatcher. Figure 14 shows the throughput improvement of DRAM load dispatch over the baseline of using PCIe only. Under uniform workload, the caching effect of DRAM is negligible because its size is only 6% of host KVS memory. Under long-tail workload, 30% of memory accesses are served by the DRAM cache. Overall, the memory access throughput for 95% and 100% GET achieves the 180 Mops clock frequency bound. However, if DRAM is simply used as a cache, the throughput would be adversely impacted because the DRAM throughput is lower than PCIe throughput. 5.1.5 Vector Operation Decoder. To evaluate the efficiency of vector operations in KV-Direct, Table 2 compares the throughput of atomic vector increment with two alternative approaches: (1) If each element is stored as a unique key, the bottleneck is the network to transfer the KV operations. (2) If the vector is stored as a large opaque value, retrieving the vector to the client also overwhelms the network. Additionally, the two alternatives in Table 2 do not ensure consistency within the vector. Adding synchronization would incur further overhead. KV-Direct client packs KV operations in network packets to mitigate packet header overhead. Figure 15 shows that network batching increases network throughput by up to 4x, while keeping networking latency below 3.5 s.\n5.2.1 Methodology. Before each benchmark, we tune hash index ratio, inline threshold and load dispatch ratio according to the KV size, access pattern and target memory utilization. Then we generate random KV pairs with a given size. The key size in a given inline KV size is irrelevant to the performance of KV-Direct, because the key is padded to the longest possible inline KV size during processing. To test inline case, we use KV size that is a multiple of slot size (when size  50, i.e. 10 slots). To test non-inline case, we use KV size that is a power of two minus 2 bytes (for metadata). As the last step of preparation, we issue PUT operations to insert the KV pairs into an idle KVS until 50% memory utilization. The performance under other memory utilizations can be derived from Figure 11. During benchmark, we use an FPGA-based packet generator [44] in the same ToR to generate batched KV operations, send them to the KV server, receive completions and measure sustainable throughput and latency. The processing delay of the packet generator is pre-calibrated via direct loop-back and removed from latency measurements. Error bars represent the 5th and 95th percentile. 5.2.2 Throughput. Figure 16 shows the throughput of KV-Direct under YCSB uniform and long-tail (skewed Zipf) workload. Three factors may be the bottleneck of KV-Direct: clock frequency, network and PCIe/DRAM. For 5B15B KVs inlined in the hash index, most GETs require one PCIe/DRAM access and PUTs require two PCIe/DRAM accesses. Such tiny KVs are prevalent in many systems. In PageRank, the KV size for an edge is 8B. In sparse logistic regression, the KV size is typically 8B-16B. For sequencers and locks in distributed systems, the KV size is 8B. Under same memory utilization, larger inline KVs have lower throughput, due to a higher probability of hash collision. 62B and larger KVs are not inlined, so they require an additional memory access. Long-tail workload has higher throughput than uniform workload and able to reach the clock frequency bound of 180 Mops under read-intensive workload, or reach the network throughput bound for 62B KV sizes. Under long-tail workload, the out-of-order execution engine merges up to 15% operations on the most popular keys, and the NIC DRAM has 60% cache hit rate under 60% load dispatch ratio, which collectively lead to up to 2x throughput as uniform workload. As shown in Table 3, the throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores. 5.2.3 Power efficiency. When the KV-Direct server is at peak throughput, the system power is 121.4 watts (measured on the wall). Compared with state-of-the-art KVS systems in Table 3, KV-Direct is 3x more power efficient than other systems, being the first general-purpose KVS system to achieve 1 million KV operations per watt on commodity servers. When the KV-Direct NIC is unplugged, an idle server consumes 87.0 watts power, therefore the combined power consumption of programmable NIC, PCIe, host memory and the daemon process on CPU is only 34 watts. The measured power difference is justified since the CPU is almost idle and the server can run other workloads when KV-Direct is operating (we use the same criterion for one-sided RDMA, shown at parentheses of Table 3). In this regard, KV-Direct is 10x more power efficient than CPU-based systems. 5.2.4 Latency. Figure 17 shows the latency of KV-Direct under the peak throughput of YCSB workload. Without network batching, the tail latency ranges from 39 s depending on KV size, operation type and key distribution. PUT has higher latency than GET due to additional memory access. Skewed workload has lower latency than uniform due to more likelihood of being cached in NIC DRAM. Larger KV has higher latency due to additional network and PCIe transmission delay. Network batching adds less than 1 s latency than non-batched operations, but significantly improves throughput, which has been evaluated in Figure 15. 5.2.5 Impact on CPU performance. KV-Direct is designed to bypass the server CPU and uses only a portion of host memory for KV storage. Therefore, the CPU can still run other applications. Our measurements find a minimal impact on other workloads on the server when a single NIC KV-Direct is at peak load. Table 4 quantifies this impact at the peak throughput of KV-Direct. Except for sequential throughput of CPU 0 to access its own NUMA memory (the line marked in bold), the latency and throughput of CPU memory accesses are mostly unaffected. This is because 8 channels of host memory can provide far higher random access throughput than all CPU cores could consume, while the CPU can indeed stress the sequential throughput of the DRAM channels. The impact of the host daemon process is minimal when the distribution of KV sizes is relatively stable, because the garbage collector is invoked only when the number of available slots for different slab sizes are imbalanced.\n\nPCIe has 29% TLP header and padding overhead for 64B DMA operations (2.4) and the DMA engine may not have enough parallelism to saturate the PCIe bandwidth-delay product with small TLPs (4). Larger DMA operations with up to 256-byte TLP payload is supported by the PCIe root complex in our system. In this case, the TLP head and padding overhead is only 9%, and the DMA engine has enough parallelism (64) to saturate the PCIe link with 27 in-flight DMA reads. To batch the DMA operations on PCIe link, we can leverage the CPU to perform scatter-gather (Figure 19). First, the NIC DMAs addresses to a request queue in host memory. The host CPU polls the request queue, performs random memory access, put the data in response queue and writes MMIO doorbell to the NIC. The NIC then fetches the data from response queue via DMA. Figure 18 shows that CPU-based scatter-gather DMA has up to 79% throughput improvement compared to the CPUbypassing approach. In addition to the CPU overhead, the primary drawback of CPU-based scatter-gather is the additional latency. To save MMIOs from the CPU to the NIC, we batch 256 DMA operations per doorbell, which requires 10 s to complete. The overall latency for the NIC to access host memory using CPU-based scatter-gather is 20 s, almost 20x higher than direct DMA.\nIn some cases, it may be desirable to build a dedicated keyvalue store with maximal throughput per server. Through simulation, [47] showed the possibility of achieving a billion KV op/s in a single server with four (currently unavailable) 60-core CPUs. As shown in Table 3, with 10 KV-Direct NICs on a server, the one billion KV op/s performance is readily achievable with a commodity server. The KV-Direct server consumes 357 watts power (measured on the wall) to achieve 1.22 Gop/s GET or 0.61 Gop/s PUT. In order to saturate the 80 PCIe Gen3 lanes of two Xeon E5 CPUs, we replace the motherboard of the benchmarked server (Sec. 5) with a SuperMicro X9DRX+-F motherboard with 10 PCIe Gen3 x8 slots. We use PCIe x16 to x8 converters to connect 10 programmable NICs on each of the slots, and only one PCIe Gen3 x8 link is enabled on each NIC, so the throughput per NIC is lower than Figure 16. Each NIC owns an exclusive memory region in host memory and serves a disjoint partition of keys. Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation. Fortunately, for a small number of partitions (e.g. 10), the load imbalance is not significant [47, 51]. Under YCSB longtail workload, the highest-loaded NIC has 1.5x load of the average, and the added load from extremely popular keys is served by the out-of-order execution engine (Sec. 3.3.3). Figure 20 shows that KV-Direct throughput scales almost linearly with the number of NICs on a server.\n6.3.1 NIC hardware with different capacity. The goal of KV-Direct is to leverage existing hardware in data centers to offload an important workload (KV access), instead of designing a special hardware to achieve maximal KVS performance. We use programmable NICs, which usually contain limited amount of DRAM for buffering and connection-state tracking. Large DRAMs are expensive in both die size and power consumption. Even if future NICs have faster or larger on-board memory, under long-tail workload, our load dispatch design (Sec. 3.3.4) still shows performance gain over the simple design of partitioning the keys uniformly according to NIC and host memory capacity. Table 5 shows the optimal load dispatch ratio for long-tail workload with a corpus of 1 billion keys, under different ratio of NIC DRAM and PCIe throughput and different ratio of NIC and host memory size. If a NIC has faster DRAM, more load will be dispatched to the NIC. A load dispatch ratio of 1 means the NIC memory behaves exactly like a cache of host memory. If a NIC has larger DRAM, a slightly less portion of load will be dispatched to the NIC. As shown in Table 6, even when the size of NIC DRAM is a tiny fraction of host memory, the throughput gain is significant. The hash table and slab allocator design (Sec. 3.3.1) is generally applicable to hash-based storage systems that need to be frugal of random accesses for both lookup and insertion. The out-of-order execution engine (Sec. 3.3.3) can be applied to all kinds of applications in need of latency hiding, and we hope future RDMA NICs to support that for atomics. In 40 Gbps networks, network bandwidth bounds nonbatched KV throughput, so we use client-side batching (Sec.4). With higher network bandwidth, batch size can be reduced, thus reducing latency. In a 200 Gbps network, a KV-Direct NIC could achieve 180 Mop/s without batching. KV-Direct leverages widely-deployed programmable NICs with FPGAs [10, 64]. FlexNIC [40, 41] is another promising architecture for programmable NICs with Reconfigurable Match-action Tables (RMT) [8]. NetCache [35] implements a KV cache in RMT-based programmable switches, showing potential for building KV-Direct in an RMT-based NIC. 6.3.2 Implications for real-world applications. Backof-the-envelope calculations show potential performance gains when KV-Direct is applied in end-to-end applications. In PageRank [61], because each edge traversal can be implemented with one KV operation, KV-Direct supports 1.22G TEPS on a server with 10 programmable NICs. In comparison, GRAM [73] supports 250M TEPS per server, bound by interleaved computation and random memory access. KV-Direct supports user-defined functions and vector operations (Table 1) that can further optimize PageRank by offloading client computation to hardware. Similar arguments hold for parameter server [46]. We expect future work to leverage hardware-accelerated key-value stores to improve distributed application performance.\nAs an important infrastructure, the research and development of distributed key-value store systems have been driven by performance. A large body of distributed KVS are based on CPU. To reduce the computation cost, Masstree [53], MemC3 [23] and libcuckoo [48] optimize locking, caching, hashing and memory allocation algorithms, while KV-Direct comes with a new hash table and memory management mechanism specially designed for FPGA to minimize the PCIe traffic. MICA [51] partitions the hash table to each core thus completely avoids synchronization. This approach, however, introduces core imbalance for skewed workloads. To get rid of the OS kernel overhead, [31, 65] directly poll network packets from NIC and [34, 54] process them with the user space lightweight network stack. Key-value store systems [39, 47, 51, 58, 59] benefit from such optimizations for high performance. As a further step towards this direction, recent works [1, 36, 36, 37, 37] leverage the hardware-based network stack of RDMA NIC, using two-sided RDMA as an RPC mechanism between KVS client and server to further improve per-core throughput and reduce latency. Still, these systems are CPU bound (2.2). Another different approach is to leverage one-sided RDMA. Pilaf [55] and FaRM [18] adopt one-sided RDMA read for GET operation and FaRM achieves throughput that saturates the network. Nessie [70], DrTM [72], DrTM+R [13] and FaSST [38] leverage distributed transactions to implement both GET and PUT with one-sided RDMA. However, the performance of PUT operation suffer from unavoidable synchronization overhead for consistency guarantee, limited by RDMA primitives [37]. Moreover, client-side CPU is involved in KV processing, limiting per-core throughput to 10 Mops on the client side. In contrast, KV-Direct extends the RDMA primitives to key-value operations while guarantees the consistency in server side, leaving the KVS client totally transparent while achieving high throughput and low latency even for PUT operation. As a flexible and customizable hardware, FPGA is now widely deployed in datacenter-scale [10, 64] and greatly improved for programmability [4, 44]. Several early works have explored building KVS on FPGA. But some of them are not practical by limiting the data storage in on-chip (about several MB memory) [50] or on-board DRAM (typically 8GB memory) [11, 32, 33]. [6] focuses on improving system capacity rather than throughput, and adopts SSD as the secondary storage out of on-board DRAM. [11, 50] limit their usage in fixed size key-value pairs, which can only work for the special purpose rather than a general key-value store. [5, 43] uses host DRAM to store the hash table, and [71] uses NIC DRAM as a cache of host DRAM, but they did not optimize for network and PCIe DMA bandwidth, resulting in poor performance. KV-Direct fully utilizes both NIC DRAM and host DRAM, making our FPGA-based key-value store system general and capable of large-scale deployment. Furthermore, our careful hardware and software co-design, together with optimizations for PCIe and networking push the performance to the physical limitation, advancing state-of-art solutions. Secondary index is an important feature to retrieve data by keys other than the primary key in data storage system [20, 42]. SLIK [42] supports multiple secondary keys using a B+ tree algorithm in key-value store system. It would be interesting to explore how to support secondary index to help KV-Direct step towards a general data storage system. SwitchKV [49] leverages content-based routing to route requests to backend nodes based on cached keys, and NetCache [35] takes a further step to cache KV in the switches. Such load balancing and caching will also benefit our system. Eris [45] leverages network sequencers to achieve efficient distributed transactions, which may give a new life to the one-sided RDMA approach with client synchronization.\nIn this paper, we describe the design and evaluation of KVDirect, a high performance in-memory key-value store. Following a long history in computer system design, KV-Direct is another exercise in leveraging reconfigurable hardware to accelerate an important workload. KV-Direct is able to obtain superior performance by carefully co-designing hardware and software in order to remove bottlenecks in the system and achieve performance that is close to the physical limits of the underlying hardware. After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction.\nWe would like to thank Kun Tan, Ningyi Xu, Ming Wu, Jiansong Zhang and Anuj Kalia for all technical discussions and valuable comments. Wed also like to thank the whole Catapult v-team at Microsoft for support on the FPGA platform. We thank our shepherd, Simon Peter, and other anonymous reviewers for their valuable feedback and comments.\n", "prediction": "in this paper, we present a new approach to distributed storage that enables us to achieve high storage efficiency.<n> our approach is based on the following four ideas : ( i ) we introduce the concept of _ distributed memory_. <n> ( ii ] we define the notion of distributed memories to be those that can be accessed by a user of a distributed system ; ( iii) for a given set of partitions of the storage space we assign to each partition in the set the size of its associated block of operations  the distance between the number of accesses to this partition and the value of this value at that particular point '' ( truncation of arithmetic operations is not required ).<n> we prove that the block size associated with each access to the partition is an infimum of an arithmetic function of that partition s block sizes ( which we refer to as arithmetic functions of partition sizes for our convenience and to avoid confusing the reader with a system of linear operations on a partition of arbitrary size and at the same time achieving a high throughput of our system in a short period of time while keeping the memory capacity at an acceptable level of storage ); ( iv )(vi) by exploiting the fact that our storage capacity is limited by the amount of memory available for each logical operation ( e.g. @xmath0 for [67 ], [71 0] ), we achieve a performance of 1.22 billion operations per second and an improvement over existing systems by more than one order of operation per operation for the first operation of one operation in an operation by one ( 1a per operations ( for one and one per ( a state ( one for all one in one to a number ( in [37 ( with one one [ 38 ( b th[4 [[e [4[[n[g[a [g [e[x[in [ [x [ one]) to one ng[22 ]) ie [n [6 [38] to obtain one[43 ]. [(a one of [(n for an [ng [a[an [ a [se [ for several one is a one with [one [ ( g[s [ to [ which [ g [ with the [ that is one by [  to find [ is [ 4 to add [ as one([v [ 1 [ in which to  for multiple [ e  [ @[i to which is the one]. in all [] for  in to make [ye to all to get [ the operation [ and [ x[5 [... [ '[which [ an one] with an a to state [. [ 3 ][[another []. to access [ by which one that [ p[one to an is to keep the @e for more to we find  with multiple to to further to... to @ [ who is that to for two [ of several [ 32] is in multiple multiple[. to reach [ while [ 2  that  the[]  is for which  a  as [ * [ multiple with which with two to our [ it is  from the]. with p  while the com [4]. in our to].. a] and we is]. while a]. the performance [2]."}
{"ground_truth": "Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networksat the level of small network subgraphs remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.\nHigher-order organization of complex networks Austin R. Benson,1 David F. Gleich,2 Jure Leskovec3 1Institute for Computational and Mathematical Engineering, Stanford University 2Department of Computer Science, Purdue University 3Computer Science Department, Stanford University To whom correspondence should be addressed; E-mail: jure@cs.stanford.edu Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networksat the level of small network subgraphs remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns. 1 ar X iv :1 61 2. 08 44 7v 1 [ cs .S I] 2 6 D ec 2 01 6 Networks are a standard representation of data throughout the sciences, and higher-order connectivity patterns are essential to understanding the fundamental structures that control and mediate the behavior of many complex systems (17). The most common higher-order structures are small network subgraphs, which we refer to as network motifs (Figure 1A). Network motifs are considered building blocks for complex networks (1, 8). For example, feedforward loops (Figure 1A M5) have proven fundamental to understanding transcriptional regulation networks (9), triangular motifs (Figure 1A M1M7) are crucial for social networks (4), open bidirectional wedges (Figure 1A M13) are key to structural hubs in the brain (10), and two-hop paths (Figure 1A M8M13) are essential to understanding air traffic patterns (5). While network motifs have been recognized as fundamental units of networks, the higher-order organization of networks at the level of network motifs largely remains an open question. Here we use higher-order network structures to gain new insights into the organization of complex systems. We develop a framework that identifies clusters of network motifs. For each network motif (Figure 1A), a different higher-order clustering may be revealed (Figure 1B), which means that different organizational patterns are exposed depending on the chosen motif. Conceptually, given a network motif M , our framework searches for a cluster of nodes S with two goals. First, the nodes in S should participate in many instances of M . Second, the set S should avoid cutting instances of M , which occurs when only a subset of the nodes from a motif are in the set S (Figure 1B). More precisely, given a motif M , the higher-order clustering framework aims to find a cluster (defined by a set of nodes S) that minimizes the following ratio: M(S) = cutM(S, S)/min(volM(S), volM(S)), (1) where S denotes the remainder of the nodes (the complement of S), cutM(S, S) is the number of instances of motif M with at least one node in S and one in S, and volM(S) is the number of nodes in instances of M that reside in S. Equation 1 is a generalization of the conductance 2 metric in spectral graph theory, one of the most useful graph partitioning scores (11). We refer to M(S) as the motif conductance of S with respect to M . Finding the exact set of nodes S that minimizes the motif conductance is computationally infeasible (12). To approximately minimize Equation 1 and hence identify higher-order clusters, we develop an optimization framework that provably finds near-optimal clusters (Supplementary Materials (13)). We extend the spectral graph clustering methodology, which is based on the eigenvalues and eigenvectors of matrices associated with the graph (11), to account for higher-order structures in networks. The resulting method maintains the properties of traditional spectral graph clustering: computational efficiency, ease of implementation, and mathematical guarantees on the near-optimality of obtained clusters. Specifically, the clusters identified by our higher-order clustering framework satisfy the motif Cheeger inequality (14), which means that our optimization framework finds clusters that are at most a quadratic factor away from optimal. The algorithm (illustrated in Figure 1C) efficiently identifies a cluster of nodes S as follows:  Step 1: Given a network and a motif M of interest, form the motif adjacency matrix WM whose entries (i, j) are the co-occurrence counts of nodes i and j in the motif M : (WM)ij = number of instances of M that contain nodes i and j. (2)  Step 2: Compute the spectral ordering  of the nodes from the normalized motif Laplacian matrix constructed via WM (15).  Step 3: Find the prefix set of  with the smallest motif conductance, formally: S := arg minr M(Sr), where Sr = {1, . . . , r}. For triangular motifs, the algorithm scales to networks with billions of edges and typically only takes several hours to process graphs of such size. On smaller networks with hundreds 3 of thousands of edges, the algorithm can process motifs up to size 9 (13). While the worstcase computational complexity of the algorithm for triangular motifs is (m1.5) , where m is the number of edges in the network, in practice the algorithm is much faster. By analyzing 16 real-world networks where the number of edges m ranges from 159,000 to 2 billion we found the computational complexity to scale as (m1.2). Moreover, the algorithm can easily be parallelized and sampling techniques can be used to further improve performance (16). The framework can be applied to directed, undirected, and weighted networks as well as motifs (13). Moreover, it can also be applied to networks with positive and negative signs on the edges, which are common in social networks (friend vs. foe or trust vs. distrust edges) and metabolic networks (edges signifying activation vs. inhibition) (13). The framework can be used to identify higher-order structure in networks where domain knowledge suggests the motif of interest. In the Supplementary Material (13) we also show that when domain-specific higherorder pattern is not known in advance, the framework can also serve to identify which motifs are important for the modular organization of a given network (13). Such a general framework allows for a study of complex higher-order organizational structures in a number of different networks using individual motifs and sets of motifs. The framework and mathematical theory immediately extend to other spectral methods such as localized algorithms that find clusters around a seed node (17) and algorithms for finding overlapping clusters (18). To find several clusters, one can use embeddings from multiple eigenvectors and k-means clustering (13,19) or apply recursive bi-partitioning (13, 20). The framework can serve to identify higher-order modular organization of networks. We apply the higher-order clustering framework to the C. elegans neuronal network, where the fournode bi-fan motif (Figure 2A) is over-expressed (1). The higher-order clustering framework then reveals the organization of the motif within the C. elegans neuronal network. We find a cluster of 20 neurons in the frontal section with low bi-fan motif conductance (Figure 2B). 4 The cluster shows a way that nictation is controlled. Within the cluster, ring motor neurons (RMEL/V/R), proposed pioneers of the nerve ring (21), propagate information to IL2 neurons, regulators of nictation (22), through the neuron RIH and several inner labial sensory neurons (Figure 2C). Our framework contextualizes the sifnifance of the bi-fan motif in this control mechanism. The framework also provides new insights into network organization beyond the clustering of nodes based only on edges. Results on a transportation reachability network (23) demonstrate how it finds the essential hub interconnection airports (Figure 3). These appear as extrema on the primary spectral direction (Figure 3C) when two-hop motifs (Figure 3A) are used to capture highly connected nodes and non-hubs. (The first spectral coordinate of the normalized motif Laplacian embedding was positively correlated with the airport citys metropolitan population with Pearson correlation 99% confidence interval [0.33, 0.53]). The secondary spectral direction identified the West-East geography in the North American flight network (it was negatively correlated with the airport citys longitude with Pearson correlation 99% confidence interval [-0.66, -0.50]). On the other hand, edge-based methods conflate geography and hub structure. For example, Atlanta, a large hub, is embedded next to Salina, a non-hub, with an edge-based method (Figure 3D). Our higher-order network clustering framework unifies motif analysis and network partitioning two fundamental tools in network scienceand reveals new organizational patterns and modules in complex systems. Prior efforts along these lines do not provide worst-case performance guarantees on the obtained clustering (24), do not reveal which motifs organize the network (25), or rely on expanding the size of the network (26, 27). Theoretical results in the Supplementary Material (13) also explain why classes of hypergraph partitioning methods are more general than previously assumed and how motif-based clustering provides a rigorous framework for the special case of partitioning directed graphs. Finally, the higher-order net- 5 work clustering framework is generally applicable to a wide range of networks types, including directed, undirected, weighted, and signed networks. 6 7 8 9\ntering method We now cover the background and theory for deriving and understanding the method presented in the main text. We will start by reviewing the graph Laplacian and cut and volume measures for sets of vertices in a graph. We then define network motifs in Section S1.2 and generalizes the notions of cut and volume to motifs. Our new theory is presented in Section S1.6 and then we summarize some extensions of the method. Finally, we relate our method to existing methods for directed graph clustering and hypergraph partitioning. S1.1 Review of the graph Laplacian for weighted, undirected graphs Consider a weighted, undirected graphG = (V,E), with |V | = n. Further assume thatG has no isolated nodes. LetW encode the weights, of the graph, i.e., Wij = Wji = weight of edge (i, j). The diagonal degree matrixD is defined asDii = n j=1Wij , and the graph Laplacian is defined as L = D W . We now relate these matrices to the conductance of a set S, (G)(S): (G)(S) = cut(G)(S, S)/min(vol(G)(S), vol(G)(S)), (S3) cut(G)(S, S) =  iS, jS Wij, (S4) vol(G)(S) =  iS Dii (S5) Here, S = V \\S. (Note that conductance is a symmetric measure in S and S, i.e., (G)(S) = (G)(S).) Conceptually, the cut and volume measures are defined as follows: cut(G)(S, S) = weighted sum of weights of edges that are cut (S6) vol(G)(S) = weighted number of edge end points in S (S7) Since we have assumed G has no isolated nodes, vol(G)(S) > 0. If G is disconnected, then for any connected component C, (G)(C) = 0. Thus, we usually consider breaking G into S10 connected components as a pre-processing step for algorithms that try to find low-conductance sets. We now relate the cut metric to a quadratic form on L. Later, we will derive a similar form for a motif cut measure. Note that for any vector y  Rn, yTLy =  (i,j)E wij(yi  yj)2. (S8) Now, define x to be an indicator vector for a set of nodes S i.e., xi = 1 if node i is in S and xi = 0 if node i is in S. Note that if an edge (i, j) is cut, then xi and xj take different values and (xi  xj)2 = 1; otherwise, (xi  xj)2 = 0. Thus, xTLx = cut(G)(S, S). (S9) S1.2 Definition of network motifs We now define network motifs as used in our work. We note that there are alternative definitions in the literature (1). We consider motifs to be a pattern of edges on a small number of nodes (see Figure S4). Formally, we define a motif on k nodes by a tuple (B,A), whereB is a kk binary matrix and A  {1, 2, . . . , k} is a set of anchor nodes. The matrix B encodes the edge pattern between the k nodes, and A labels a relevant subset of nodes for defining motif conductance. In many cases, A is the entire set of nodes. Let A be a selection function that takes the subset of a k-tuple indexed by A, and let set() be the operator that takes an (ordered) tuple to an (unordered) set. Specifically, set((v1, v2, . . . , vk)) = {v1, v2, . . . , vk}. The set of motifs in an unweighted (possibly directed) graph with adjacency matrix A, denoted M(B,A), is defined by M(B,A) = {(set(v), set(A(v))) | v  V k, v1, . . . , vk distinct, Av = B}, (S10) S11 M3 M5M4M1 M2 M8M7M6 M9 M10 M11 M12 M13 Mbifan Medge + e d c b a M2 A B Figure S4: A: Illustration of network motifs used throughout the main text and supplementary material. The motif Medge is used to represent equivalence to undirected the graph. B: Diagram of motif definitions. The motif is defined by a binary matrix B and an anchor set of nodes. The figure shows an anchored version of motif M2 with anchors on the nodes that form the bi-directional edge. There are two instances of the motif in the graph on the right. Note that ({a, b, d}, {a, b}) is not included in the set of motif instances because the induced subgraph on the nodes a, b, and d is not isomorphic to the graph defined by B. where Av is the k  k adjacency matrix on the subgraph induced by the k nodes of the ordered vector v. Figure S4 illustrates these definitions. The set operator is a convenient way to avoid duplicates when defining M(B,A) for motifs exhibiting symmetries. Henceforth, we will just use (v, A(v)) to denote (set(v), set(A(v))) when discussing elements of M(B,A). Furthermore, we call any (v, A(v))  M(B,A) a motif instance. When B and A are arbitrary or clear from context, we will simply denote the motif set by M . We call motifs where A(v) = v simple motifs and motifs where A(v) 6= v anchored motifs. Motif analysis in the literature has mostly analyzed simple motifs (29). However, the anchored motif provides us with a more general framework, and we use an anchored motif for S12 the analysis of the transportation reachability network. Often, a distinction is made between a functional and a structural motif (30) (or a subgraph and an induced subgraph (31)) to distinguish whether a motif specifies simply the existence of a set of edges (functional motif or subgraph) or the existence and non-existence of edges (structural motif or induced subgraph). By the definition in Equation S10, we refer to structural motifs in this work. Note that functional motifs consist of a set of structural motifs. Our clustering framework allows for the simultaneous consideration of several motifs (see Section S1.9), so we have not lost any generality in our definitions. S1.3 Definition of motif conductance Recall that the key definitions for defining conductance are the notions of cut and volume. For an unweighted graph, these are (G)(S, S) = cut(G)(S, S)/min(vol(G)(S), vol(G)(S)), (S11) cut(G)(S, S) = number of edges cut, (S12) vol(G)(S) = number of edge end points in S. (S13) Our conceptual definition of motif conductance simply replaces an edge with a motif instance of type M :  (G) M (S) = cut (G) M (S, S)/min(vol (G) M (S), vol (G) M (S)), (S14) cut(G)M (S, S) = number of motif instances cut, (S15) vol(G)M (S) = number of motif instance end points in S. (S16) We say that a motif instance is cut if there is at least one anchor node in S and at least one S13 anchor node in S. We can formalize this when given a motif set M as in Equation S10: cut(G)M (S, S) =  (v,A(v))M 1( i, j  A(v) | i  S, j  S), (S17) vol(G)M (S) =  (v,A(v))M  iA(v) 1(i  S), (S18) where 1(s) is the truth-value indicator function on s, i.e., 1(s) takes the value 1 if the statement s is true and 0 otherwise. Note that Equation S17 makes explicit use of the anchor set A. The motif cut measure only counts an instance of a motif as cut if the anchor nodes are separated, and the motif volume counts the number of anchored nodes in the set. However, two nodes in an achor set may a part of several motif instances. Specifically, following the definition in Equation S10, there may be many different v with the same A(v), and the nodes in A(v) still get counted proportional to the number of motif instances. S1.4 Definition of the motif adjacency matrix and motif Laplacian Given an unweighted, directed graph and a motif set M , we conceptually define the motif adjacency matrix by (WM)ij = number of motif instances in M where i and j participate in the motif. (S19) Or, formally, (WM)ij =  (v,A(v))M 1({i, j}  A(v)), (S20) for i 6= j. Note that weight is added to (WM)ij only if i and j appear in the anchor set. This is important for the transportation reachability network analyzed in the main text and in Section S6, where weight is added between cities i and j based on the number of intermediary cities that can be traversed between them. Next, we define the motif diagonal degree matrix by (DM)ii = n j=1(WM)ij and the motif Laplacian as LM = DM  WM . Finally, the normalized motif Laplacian is LM = S14 D 1/2 M LMD 1/2 M = I D 1/2 M WMD 1/2 M . The theory in the next section will examine quadratic forms LM and derive the main clustering method that uses an eigenvector of LM . S1.5 Algorithm for finding a single cluster We are now ready to describe the algorithm for finding a single cluster in a graph. The algorithm finds a partition of the nodes into S and S. The motif conductance is symmetric in the sense that (G)M (S) =  (G) M (S), so either set of nodes (S or S) could be interpreted as a cluster. However, in practice, it is common that one set is substantially smaller than the other. We consider this smaller set to represent a module in the network. The algorithm is based on the Fiedler partition (32) of the motif weighted adjacency matrix and is presented below in Algorithm 1.1 Algorithm 1: Motif-based clustering algorithm for finding a single cluster. Input: Directed, unweighted graph G and motif M Output: Motif-based cluster (subset of nodes in G) (WM)ij  number of instances of M that contain nodes i and j. GM  weighted graph induced WM DM  diagonal matrix with (DM)ii =  j(WM)ij z  eigenvector of second smallest eigenvalue for LM = I D1/2M WMD 1/2 M i  to be index of D1/2M z with ith smallest value /* Sweep over all prefixes of  */ S  arg minl (GM )(Sl), where Sl = {1, . . . , l} if |S| < |S| then return S else return S It is often informative to look at all conductance values found from the sweep procedure. We refer to a plot of (GM )(Sl) versus l as a sweep profile plot. In the following subection, we show that when the motif has three nodes, (GM )(Sl) =  (G) M (Sl). In this case, the sweep profile shows how motif conductance varies with the size of the sets in Algorithm 1. In the following subsection, we show that when the motif M has three nodes, the cluster 1An implementation of Algorithm 1 is available in SNAP. See higher-order/. S15 satisfies (G)M (S)  4  , where  is the smallest motif conductance over all sets of nodes. In other words, the cluster is nearly optimal. Later, we extend this algorithm to allow for signed, colored, and weighted motifs and to simultaneously finding multiple clusters. S1.6 Motif Cheeger inequality for network motifs with three nodes We now derive the motif Cheeger inequality for simple three-node motifs, or, in general, motifs with three anchor nodes. The crux of this result is deriving a relationship between the motif conductance function and the weighted motif adjacency matrix, from which the Cheeger inequality is essentially a corollary. For the rest of this section, we will use the following notation. Given an unweighted, directed G and a motif M , the corresponding weighted graph defined by Equation S20 is denoted by GM . The following Lemma relates the motif volume to the volume in the weighted graph. This lemma applies to any anchor setA consisting of at least two nodes. For our main result, we will apply the lemma assuming |A| = 3. However, we will apply the lemma more generally when discussing four node motifs in Section S1.7. Lemma 1. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif on k nodes and |A|  2 anchor nodes. Then for any S  V , vol(G)M (S) = 1 |A|  1 vol(GM )(S) Proof. Consider an instance (v, A(v)) of a motif. Let (u1, . . . , u|A|) = A(v). By Equation S20, (WM)u1,j is incremented by one for j = u2, . . . , u|A|. Since (DM)u1,u1 =  j (WM)u1,j , the motif end point u1 is counted |A|  1 times. S16 xi 2 + xj 2 + xk 2 - xixj - xjxk -xkxi = xi = 1 xj = 1 xk = 1 i j k xi = -1 xj = -1 xk = -1 i j k xi = 1 xj = 1 xk = -1 i j k xi = 1 xj = -1 xk = -1 i j k 0 0 4 4 ii kj l xj = 1 xk = 1 xi = 1 xl = 1 ii kj l xj = -1 xk = -1 xi = -1 xl = -1 ii kj l xj = 1 xk = -1 xi = 1 xl = 1 ii kj l xj = -1 xk = -1 xi = 1 xl = -1 ii kj l xj = 1 xk = -1 xi = 1 xl = -1 0 0 6 6 8 6 - xixj - xixk - xixl - xjxk - xjxl - xkxl =BA Figure S5: Illustrations of the quadratic forms on indicator functions for set assignment. Here, the blue nodes have assignment to set S and the green nodes have assignment to set S. The quadratic function gives the penalty for cutting that motif. A: Illustration of Equation S21. The quadratic form is proportional to the indicator on whether or not the motif is cut. B: Illustration of Equation S22. The quadratic form is equal to zero when all nodes are in the same set. However, the form penalizes 2/2 splits more than 3/1 splits. The following lemma states that the truth value for determining whether three binary variables in {1, 1} are not all equal is a quadratic function of the variables (see Figure S5). Because this function is quadratic, we will be able to relate motif cuts on three nodes to a quadratic form on the motif Laplacian. S17 Lemma 2. Let xi, xj, xk  {1, 1}. Then 4  1(xi, xj, xk not all the same) = x2i + x2j + x2k  xixj  xjxk  xkxi. It will be easier to derive our results with binary indicator variables taking values in {1, 1}. However, in terms of the quadratic form on the Laplacian, we have already seen how indicator vectors taking values in {0, 1} relate to the cut value (Equation S9). The following lemma shows that the {0, 1} and {1, 1} indicator vectors are equivalent, up to a constant, for defining the cut measure in terms of the Laplacian. Lemma 3. Let z  {0, 1}n and define x by xi = 1 if zi = 1 and xi = 1 if zi = 0. Then for any graph Laplacian L = D W , 4zTLz = xTLx. Proof. xTLx =  (i,j)E Wij(xi  xj)2 =  (i,j)E Wij4(zi  zj)2 = 4zTLz. The next lemma contains the essential result that relates motif cuts in the original graph G to weighted edge cuts in GM . In particular, the lemma shows that the motif cut measure is proportional to the cut on the weighted graph defined in Equation S19 when there are three anchor nodes. Lemma 4. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif with |A| = 3. Then for any S  V , cut(G)M (S, S) = 1 2 cut(GM )(S, S) S18 Proof. Let x  {1, 1}n be an indicator vector of the node set S. 4  cut(G)M (S, S) =  (v,{i,j,k})M 4  1(xi, xj, xk not all the same) =  (v,{i,j,k})M ( x2i + x 2 j + x 2 k )  (xixj + xjxk + xkxi) = 1 2 xTDMx 1 2 xTWMx = 1 2 xTLMx = 2  cut(GM )(S, S). The first equality follows from the definition of cut motifs (Equation S17). The second equality follows from Lemma 2. The third equality follows from Lemma 1 and Equation S20. The fourth equality follows from the definition of LM . The fifth equality follows from Lemma 3. We are now ready to prove our main result, namely that motif conductance on the original graph G is equivalent to conductance on the weighted graph GM when there are three anchor nodes. The result is a consequence of the volume and cut relationships provided by Lemmas 1 and 4. Theorem 5. Let G = (V,E) be a directed, unweighted graph and let WM be the weighted adjacency matrix for any motif with |A| = 3. Then for any S  V ,  (G) M (S) =  (GM )(S)\nconductance on the weighted graph defined by Equation S19. Proof. When |A| = 3, the motif cut and motif volume are both equal to half the motif cut and motif volume measures by Lemmas 1 and 4. S19 For any motif with three anchor nodes, conductance on the weighted graph is equal to the motif conductance. Because of this, we can use results from spectral graph theory for weighted graphs (32) and re-interpret the results in terms of motif conductance. In particular, we get the following motif Cheeger inequality. Theorem 6. Motif Cheeger Inequality. Suppose we use Algorithm 1 to find a low-motif conductance set S. Let  = minS  (G) M (S ) be the optimal motif conductance over any set of nodes\n1. (G)M (S)  4   and 2.   2/2 Proof. The result follows from Theorem 5 and the standard Cheeger ineqaulity (32). The first part of the result says that the set of nodes S is within a quadratic factor of optimal. This provides the mathematical guarantees that our procedure finds a good cluster in a graph, if one exists. The second result provides a lower bound on the optimal motif conductance in terms of the eigenvalue. We use this bound in our analysis of a food web (see Section S7.1) to show that certain motifs do not provide good clusters, regardless of the procedure to select S. S1.7 Discussion of motif Cheeger inequality for network motifs with four or more nodes Analogs of the indicator function in Lemma 2 for four or more variables are not quadratic. Subsequently, for motifs with |A| > 3, we no longer get the motif Cheeger inequalities guaranteed by Theorem 6. That being said, solutions found by motif-based partitioning approximate a related value of conductance. We now provide the details. We begin with a lemma that shows a functional form for four binary variables taking values in {1, 1} to not all be equal. We see that it is quartic, not quadratic. S20 Lemma 7. Let xi, xj, xk, xl  {1, 1}. Then the indicator function on all four elements not being equal is 8  1(xi, xj, xk, xl not all the same) (S21) = (7 xixj  xixk  xixl  xjxk  xjxl  xkxl  xixjxkxl) . We almost have a quadratic form, if not for the quartic term xixjxkxl. However, we could use the following related quadratic form: 6 xixj  xixk  xixl  xjxk  xjxl  xkxl =  0 xi, xj, xk, xl are all the same 6 exactly three of xi, xj, xk, xl are the same 8 exactly two of xi, xj, xk, xl are 1. (S22) The quadratic still takes value 0 if all four entries are the same, and takes a non-zero value otherwise. However, the quadratic takes a larger value if exactly two of the entries are 1. Figure S5 illustrates this idea. From this, we can provide an analogous statement to Lemma 4 for motifs with |A| = 4. Lemma 8. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif with |A| = 4. Then for any S  V , cut(G)M (S, S) = 1 3 cut(GM )(S, S)  (v,{i,j,k,l})M 1 3  1(exactly two of i, j, k, l in S) S21 Proof. Let x  {1, 1}n be an indicator vector of the node set S. 6  cut(G)M (S, S) +  (v,{i,j,k,l})M 2  1(exactly two of i, j, k, l in S) =  (v,{i,j,k,l})M 6 xixj  xixk  xixl  xjxk  xjxl  xkxl =  (v,{i,j,k,l})M 3 2 ( x2i + x 2 j + x 2 k + x 2 l )  (xixj + xixk + xixl + xjxk + xjxl + xkxl) = 1 2 xTDMx 1 2 xTWMx = 1 2 xTLMx = 2  cut(GM )(S, S). The first equality follows from Equations S17 and S22. The third equality follows from Lemma 1. The fourth equality follows from the definition ofLM . The fifth equality follows from Lemma 3. With four anchor nodes, the motif cut in G is slightly different than the weighted cut in the weighted graph GM . However, Lemma 1 says that the motif volume in G is still the same as the weighted volume in GM . We use this to derive the following result. Theorem 9. Let G = (V,E) be a directed, unweighted graph and let WM be the weighted adjacency matrix for any motif with |A| = 4. Then for any S  V ,  (G) M (S) =  (GM )(S)  (v,{i,j,k,l})M 1(exactly two of i, j, k, l in S) vol(GM )(S)\nthe exact conductance with an additional penalty for splitting the four anchor nodes into two groups of two. Proof. This follows from Lemmas 1 and 8. S22 To summarize, we still get a Cheeger inequality from the weighted graph, but it is in terms of a penalized version of the motif conductance (G)M (S). However, the penalty makes senseif the group of four nodes is more split (2 and 2 as opposed to 3 and 1), the penalty is larger. When |A| > 4, we can derive similar penalized approximations to (G)M (S). S1.8 Methods for simultaneously finding multiple clusters For clustering a network into k > 2 clusters based on motifs, we could recursively cut the graph using the sweep procedure with some stopping criterion (20). For example, we could continue to cut the largest remaining cluster until the graph is partitioned into some pre-specified number of clusters. We refer to this method as recursive bi-partitioning. In addition, we can use the following method of Ng et al. (19). Algorithm 2: Motif-based clustering algorithm for finding several clusters. Input: Directed, unweighted graph G, motif M , number of clusters k Output: k disjoint motif-based clusters (WM)ij  number of instances of M that contain nodes i and j. DM  diagonal matrix with (DM)ii =  j(WM)ij z1, . . . , zk  eigenvectors of k smallest eigenvalues for LM = I D1/2M WMD 1/2 M Yij  zij/ k j=1 z 2 ij Embed node i into Rk by taking the ith row of the matrix Y Run k-means clustering on the embedded nodes This method does not have the same Cheeger-like guarantee on quality. However, recent theory shows that by replacing k-means with a different clustering algorithm, there is a performance guarantee (33). While this provides motivation, we use k-means for its simplicity and empirical success. S1.9 Extensions of the method for simultaneously analyzing several network motifs All of our results carry through when considering several motifs simultaneously. In particular, suppose we are interested in clustering based on motif sets M1, . . . ,Mq for q different motifs. S23 Further suppose that we want to weight the impact of some motifs more than other motifs. Let WMj be the weighted adjacency matrix for motifMj , j = 1, . . . , q, and let j  0 be the weight of motif Mj , then we can form the weighted adjacency matrix WM = q j=1 jWMj . (S23) Now, the cut and volume measures are simply weighted sums by linearity. Suppose that the Mj all have three anchor nodes and let GM be the weighted graph corresponding to WM . Then cut(GM )(S, S) = q j=1 jcut (G) Mj (S, S), vol(GM )(S) = q j=1 jvol (G) Mj (S), and Theorem 6 applies to a weighted motif conductance equal toq j=1 jcut (G) Mj (S, S) min (q j=1 jvol (G) Mj (S), q j=1 jvol (G) Mj (S) ) . S1.10 Extensions of the method to signed, colored, and weighted motifs Our results easily generalize for signed networks. We only have to generalize Equation S10 by allowing the adjacency matrix B to be signed. Extending the method for motifs where the edges or nodes are colored or labeled is similar. If the edges are colored, then we again just allow the adjacency matrix B to capture this information. If the nodes in the motif are colored, we only count motif instances with the specified pattern. We can also generalize the notions of motif cut and motif volume for weighted motifs, i.e., each motif has an associated nonnegative weight. Let (v,A(v)) be the weight of a motif instance. Our cut and volume metrics are then cut(G)M (S, S) =  (v,A(v))M (v,A(v)) 1( i, j  A(v) | i  S, j  S), vol(G)M (S) =  (v,A(v))M (v,A(v))  iA(v) 1(i  S). S24 Subsequently, we adjust the motif adjacency matrix as follows: (WM)ij =  (v,A(v))M (v,A(v)) 1({i, j}  A(v)) (S24) S1.11 Connections to directed graph partitioning Our framework also provides a way to analyze methods for clustering directed graphs. Existing principled generalizations of undirected graph partitioning to directed graph partitioning proceed from graph circulations (34) or random walks (35) and are difficult to interpret. Our motif-based clustering framework provides a simple, rigorous framework for directed graph partitioning. For example, consider the common heuristic of clustering the symmetrized graph W = A + AT , where A is the (directed) adjacency matrix (36). Following Theorem 5, conductance-minimizing methods for partitioningW are actually trying to minimize a weighted sum of motif-based conductances for the directed edge motif and the bi-directional edge motif: B1 = [ 0 1 0 0 ] , B2 = [ 0 1 1 0 ] , where both motifs are simple (A = {1, 2}). If W1 and W2 are the motif adjacency matrices for B1 and B2, then A + AT = W = W1 + 2W2. This weighting scheme gives a weight of two to bi-directional edges in the original graph and a weight of one to uni-directional edges. An alternative strategy for clustering a directed graph is to simply remove the direction on all edges, treating bi-directional and uni-directional edges the same. The resulting adjacency matrix is equivalent to the motif adjacency matrix for the bi-directional and uni-directional edges (without any relative weighting). Formally, W = W1 + W2. We refer to this motif as Medge (Figure S4), which will later provide a convenient notation when discussing both motifbased clustering and edge-based clustering. S25 S1.12 Connections to hypergraph partitioning Finally, we contextualize our method in the context of existing literature on hypergraph partitioning. The problem of partitioning a graph based on relationships between more than two nodes has been studied in hypergraph partitioning (37), and we can interpret motifs as hyperedges in a graph. In contrast to existing hypergraph partitioning problems, we induce the hyperedges from motifs rather than take the hyperedges as given a priori. The goal with our analysis of the Florida Bay food web, for example, was to find which hyperedge sets (induced by a motif) provide a good clustering of the network (see Section S7.1). In general, our motif-based spectral clustering methodology falls into the area of encoding a hypergraph partitioning problem by a graph partitioning problem (38, 39). With simple motifs on k nodes, the motif Laplacian LM formed from WM (Equation S20) is a special case of the Rodrguez Laplacian (38, 40) for k-regular hypergraphs. The motif Cheeger inequality we proved (Theorem 6) explains why this Laplacian is appropriate for 3-regular hypergraphs. Specifically, it respects the standard cut and volume metrics for graph partitioning.\nWe now analyze the computation of the higher-order clustering method. We first provide a theoretical analysis of the computational complexity, which depends on motif. After, we empirically analyze the time to find clusters for triangular motifs on a variety of real-world networks, ranging in size from a few hundred thousand edges to nearly two billion edges. Finally, we show that we can practically compute the motif adjacency matrix for motifs up to size 9 on a number of real-world networks. S26 S2.1 Analysis of computational complexity We now analyze the computational complexity of the algorithm presented in Theorem 6. Overall, the complexity of the algorithm is governed by the computations of the motif adjacency matrix WM , an eigenvector, and the sweep cut procedure. For simplicity, we assume that we can access edges in a graph in O(1) time and access and modify matrix entries in O(1) time. Let m and n denote the number of edges in the graph. Theoretically, the eigenvector can be computed in O((m + n)(log n)O(1)) time using fast Laplacian solvers (41). For the sweep cut, it takes O(n log n) to sort the indices given the eigenvector using a standard sorting algorithm such is merge sort. Computing motif conductance for each set Sr in the sweep also takes linear term. In pratice, the sweep cut step takes a small fraction of the total running time of the algorithm. For the remainder of the analysis, we consider the more nuanced issue of the time to compute WM . The computational time to formWM is bounded by the time to find all instances of the motif in the graph. Naively, for a motif on k nodes, we can compute WM in (nk) time by checking each k-tuple of nodes. Furthermore, there are cases where there are (nk) motif instances in the graph, e.g., there are (n3) triangles in a complete graph. However, since most real-world networks are sparse, we instead focus on the complexity of algorithms in terms of the number of edges and the maximum degree in the graph. For this case, there are several efficient practical algorithms for real networks with available software (4246). Theoretically, motif counting is efficient. Here we consider four classes of motifs: (1) triangles, (2) wedges (connected, non-triangle three-node motifs), (3) four-node motifs, and (4) k-cliques. Let m be the number of edges in a graph. Latapy analyzed a number of algorithms for listing all triangles in an undirected network, including an algorithm that has computational complexity (m1.5) (47). For a directed graph G, we can use the following algorithm: (1) form a new graph Gundir by removing the direction from all edges in G (2) find all triangles in Gundir, S27 (3) for every triangle in Gundir, check which directed triangle motif it is in G. Since step 1 is linear and we can perform the check in step 3 in O(1) time, the same (m1.5) complexity holds for directed networks. This analysis holds regardless of the structure of the networks. However, additional properties of the network can lead to improved algorithms. For example, in networks with a power law degree sequence with exponent greater than 7/2, Berry et al. provide a randomized algorithm with expected running time (m) (48). In the case of a bounded degree graph, enumerating over all nodes and checking all pairs of neighbors takes time (nd2max), where dmax is the maximum degree in the graph. We note that with triangular motifs, the number of non-zeros in WM is less than the number of non-zeros in the original adjacency matrix. Thus, we do not have to worry about additional storage requirements. Next, we consider wedges (open triangles). We can list all wedges by looking at every pair of neighbors of every node. This algorithm has (nd2max) computational complexity, where n is the number of nodes and dmax is again the maximum degree in the graph (a more precise bound is (  j d 2 j), where dj is the degree of node j.) If the graph is sparse, the motif adjacency matrix will have more non-zeros than the original adjacency matrix, so additional storage is required. Specifically, there is fill-in for all two-hop neighbors, so the motif adjacency matrix has O(  j d 2 j) non-zeros. This is impractical for large real-world networks but manageable for modestly sized networks. Marcus and Shavitt present an algorithm for listing all four-node motifs in an undirected graph in O(m2) time (49). We can employ the same edge direction check as for triangles to extend this result to directed graphs. Chiba and Nishizeki develop an algorithm for finding a representation of all quadrangles (motif on four nodes that contains a four-node cycle as a subgraph) in O(am) time and O(m) space, where a is the arboricity of the graph (50). The arboricity of any connected graph is bounded byO(m1/2), so this algorithm runs in timeO(m3/2). Chiba and Nishizeki present an algorithms for k-clique enumeration that also depends on S28 the arboricity of the graph. Specifically, they provide an algorithm for enumerating all k-cliques inO(kak2m) time, where a is the arboricity of the graph. This algorithm achieves the (m3/2) bound for arbitrary graphs. (We note that the triangle listing sub-case is similar in spirit to the algorithm proposed by Schank and Wagner (51)). For four-node cliques, the algorithm runs in time O(m2) time, which matches the complexity of Marcus and Shavitt (49). We note that we could also employ approximation algorithms to estimate the weights in the motif adjacency matrix (52). Such methods balance computation time and accuracy. Finally, we note that the computation of WM and the computation of the eigenvector are suitable for parallel computation. There are already distributed algorithms for triangle enumeration (53), and the (parallel) eigenvector computation of a sparse matrix is a classical problem in scientific computing (54, 55). S2.2 Experimental results on triangular motifs In this section, we demonstrate that our method scales to real-world networks with billions of edges. We tested the scalability of our method on 16 large directed graphs from a variety of real-world applications. These networks range from a couple hundred thousand to two billion edges and from 10 thousand to over 50 million nodes. Table S1 lists short descriptions of these networks. The wiki-RfA, email-EuAll, cit-HepPh, web-NotreDame, amazon0601, wiki-Talk, ego-Gplus, soc-Pokec, and soc-LiveJournal1 networks were downloaded from the SNAP collection at (56). The uk-2014tpd, uk-2014-host, enwiki-2013, uk-2002, arabic-2005, twitter-2010, and sk-2005 networks were downloaded from the Laboratory for Web Algorithmics collection at unimi.it/datasets.php (5760). Links to all datasets are available on our project website: Recall that Algorithm 1 consists of two major computational components: S29 1. Form the weighted graph WM . 2. Compute the eigenvector z of second smallest eigenvalue of the matrix LM . After computing the eigenvector, we sort the vertices and loop over prefix sets to find the lowest motif conductance set. We consider these final steps as part of the eigenvector computation for our performance experiments. For each network in Table S1, we ran the method for all directed triangular motifs (M1 M7). To compute WM , we used a standard algorithm that meets the O(m3/2) bound (47, 51) with some additional pre-processing based on the motif. Specifically, the algorithm is: 1. Take motif type M and graph G as input. 2. (Pre-processing.) If M is M1 or M5, remove all bi-directional edges in G since these motifs only contain uni-directional edges. If M is M4, remove all uni-directional edges in G as this motif only contains bi-directional edges. 3. Form the undirected graph Gundir by removing the direction of all edges in G. 4. Let du be the degree of node u in Gundir. Order the nodes in Gundir by increasing degree, breaking ties arbitrarily. Denote this ordering by . 5. For every edge undirected edge {u, v} in Gundir, if u < v, add directed edge (u, v) to Gdir; otherwise, add directed edge (v, u) to Gdir. 6. For every node in u in Gdir and every pair of directed edges (u, v) and (u,w), check to see if u, v, and w form motif M in G. If they do, check if the triangle forms motif M in G and update WM accordingly. The algorithm runs in time (m3/2) time in the worst case, and is also known as an effective heuristic for real-world networks (48). After, we find the largest connected component of the S30 graph corresponding to the motif adjacency matrix WM , form the motif normalized Laplacian LM of the largest component, and compute the eigenvector of second smallest eigenvalue of LM . To compute the eigenvector, we use MATLABs eigs routine with tolerance 1e-4 and the smallest algebraic option for the eigenvalue type. Table S2 lists the time to compute WM and the time to compute the eigenvector for each network. We omitted the time to read the graph from disk because this time strongly depends on how the graph is compressed. All experiments ran on a 40-core server with four 2.4 GHz Intel Xeon E7-4870 processors. All computations of WM were in serial and the computations of the eigenvectors were in parallel. Over all networks and all motifs, the longest computation of WM (including pre-processing time) was for M2 on the sk-2005 network and took roughly 52.8 hours. The longest eigenvector computation was for M6 on the sk-2005 network, and took about 1.62 hours. We note that WM only needs to be computed once per network, regardless of the eventual number of clusters that are extracted. Also, the computation ofWM can easily be accelerated by parallel computing (the enumeration of motifs can be done in parallel over nodes, for example) or by more sophisticated algorithms (48). In this work, we perform the computation of WM in serial in order to better understand the scalability. In theory, the triangle enumeration time is O(m1.5). We fit a linear regression of the log of the computation time of the last step of the enumeration algorithm to the regressor log(m) and a constant term: log(time)  a log(m) + b (S25) If the computations truly took cm1.5 for some constant c, then the regression coefficient for log(m) would be 1.5. Because of the pre-processing of the algorithm, the number of edges m depends on the motif. For example, with motifs M1 and M5, we only count the number of uni-directional edges. The pre-processing time, which is linear in the total number of edges, S31 is not included in the time. The regression coefficient for log(m) (a in Equation S25) was found to be smaller 1.5 for each motif (Table S3). The largest regression coefficient was 1.31 for M3 (with 95% confidence interval 1.31  0.19). We also performed a regression over the aggregate times of the motifs, and the regression coefficient was 1.17 (with 95% confidence interval 1.17  0.09). We conclude that on real-world datasets, the algorithm for computing WM performs much better than the worst-case guarantees. Table S1: Summary of networks used in scalability experiments with triangular motifs. The total number of edges is the sum of the number of unidirectional edges and twice the number of bidirectional edges. Name description # nodes # edges total unidir. bidir. wiki-RfA Adminship voting on Wikipedia 10.8K 189K 175K 7.00K email-EuAll Emails in a research institution 265K 419K 310K 54.5K cit-HepPh Citations for papers on arXiv HEP-PH 34.5K 422K 420K 657 web-NotreDame Hyperlinks on nd.edu domain 326K 1.47M 711K 380K amazon0601 Product co-purchasing on Amazon 403K 3.39M 1.50M 944K wiki-Talk Wikipedia users interactions 2.39M 5.02M 4.30M 362K ego-Gplus Circles on Google+ 108K 13.7M 10.8M 1.44M uk-2014-tpd top private domain links on .uk web 1.77M 16.9M 13.7M 1.58M soc-Pokec Pokec friendships 1.63M 30.6M 14.0M 8.32M uk-2014-host Host links on .uk web 4.77M 46.8M 33.7M 6.55M soc-LiveJournal1 LiveJournal friendships 4.85M 68.5M 17.2M 25.6M enwiki-2013 Hyperlinks on English Wikipedia 4.21M 101M 82.6M 9.37M uk-2002 Hyperlinks on .uk web 18.5M 292M 231M 30.5M arabic-2005 Hyperlinks on arabic-language web pages 22.7M 631M 477M 77.3M twitter-2010 Twitter followers 41.7M 1.47B 937M 266M sk-2005 Hyperlinks on .sk web 50.6M 1.93B 1.69B 120M S32 Table S2: Time to compute the motif adjacency matrix WM and the second eigenvector of the motif normalized Laplacian LM in seconds for each directed triangular motif. Motif adjacency matrix WM Second eigenvector of LM Network M1 M2 M3 M4 M5 M6 M7 M1 M2 M3 M4 M5 M6 M7 wiki-RfA 1.19e+00 2.67e+00 1.71e+00 2.06e-02 1.79e+00 2.42e+00 2.35e+00 1.14e-01 2.12e-01 1.22e-01 2.12e-01 2.12e-01 2.94e-01 2.93e-01 email-EuAll 4.74e-01 8.29e-01 6.26e-01 2.46e-01 5.02e-01 5.40e-01 5.41e-01 2.29e-01 1.62e-01 2.43e-01 1.62e-01 1.62e-01 2.35e-01 1.92e-01 cit-HepPh 7.65e+00 3.36e+00 2.73e+00 6.22e+00 8.20e+00 3.29e+00 3.35e+00 2.11e+00 2.10e+00 2.11e+00 2.10e+00 2.10e+00 2.24e+00 2.30e+00 web-NotreDame 9.42e-01 2.39e+01 2.33e+01 2.30e+00 1.17e+00 8.29e+00 8.40e+00 1.86e-01 3.62e-01 5.97e-01 3.62e-01 3.62e-01 9.61e-01 2.06e+00 amazon0601 2.35e+00 8.66e+00 6.91e+00 1.82e+00 2.94e+00 5.47e+00 5.73e+00 1.23e-01 6.96e-01 4.62e+00 6.96e-01 6.96e-01 4.97e+00 4.53e+00 wiki-Talk 1.07e+01 3.00e+01 2.20e+01 3.11e+00 1.35e+01 2.09e+01 2.10e+01 1.28e+00 2.40e+00 2.51e+00 2.40e+00 2.40e+00 2.54e+00 4.52e+00 ego-Gplus 8.55e+02 2.42e+03 1.73e+03 2.08e+01 1.63e+03 2.07e+03 2.17e+03 4.42e+00 1.68e+01 2.11e+01 1.68e+01 1.68e+01 2.57e+01 4.42e+01 uk-2014-tpd 8.10e+01 5.31e+02 4.07e+02 2.56e+01 1.15e+02 3.04e+02 2.85e+02 3.59e+00 9.66e+00 9.92e+00 4.35e+00 9.66e+00 2.10e+01 2.16e+01 soc-Pokec 4.17e+01 1.34e+02 1.21e+02 3.04e+01 4.88e+01 1.00e+02 1.04e+02 1.96e+00 1.75e+01 3.91e+01 1.75e+01 1.75e+01 2.39e+01 2.45e+01 uk-2014-host 9.98e+02 4.68e+03 2.76e+03 8.90e+01 1.32e+03 2.89e+03 2.99e+03 1.81e+01 4.38e+01 6.80e+01 2.04e+01 4.38e+01 8.28e+01 8.73e+01 soc-LiveJournal1 9.08e+01 7.66e+02 6.24e+02 1.24e+02 1.24e+02 4.41e+02 4.49e+02 2.32e+00 2.20e+01 1.06e+02 2.20e+01 2.20e+01 4.49e+01 6.13e+01 enwiki-2013 8.36e+02 9.62e+02 7.09e+02 3.13e+01 9.77e+02 8.19e+02 8.38e+02 2.18e+01 7.58e+01 8.45e+01 7.58e+01 7.58e+01 2.14e+02 1.48e+02 uk-2002 1.47e+03 8.59e+03 5.17e+03 2.45e+02 1.73e+03 4.53e+03 5.29e+03 1.66e+01 8.65e+01 2.52e+02 8.65e+01 8.65e+01 7.87e+02 5.32e+02 arabic-2005 6.51e+03 7.64e+04 6.05e+04 6.08e+03 8.39e+03 3.59e+04 3.69e+04 1.98e+01 1.64e+02 4.80e+02 3.26e+02 1.64e+02 1.95e+03 1.40e+03 twitter-2010 1.21e+04 1.38e+05 1.31e+05 3.33e+04 1.99e+04 8.03e+04 7.65e+04 2.23e+02 1.23e+03 1.95e+03 1.23e+03 1.23e+03 2.22e+03 2.18e+03 sk-2005 5.52e+04 1.63e+05 1.29e+05 1.55e+04 5.23e+04 9.64e+04 8.42e+04 5.73e+01 2.94e+02 7.98e+02 2.94e+02 2.94e+02 5.83e+03 3.81e+03 Table S3: The 95% confidence interval (CI) for the regression coefficient of the regressor log(m) in a linear model for predicting the time to compute WM , based on the computational results for the networks in Table S1. The algorithm runs is guranteed to run in time O(m3/2). Combined refers to the regression coefficient when considering all of the times. Motif M1 M2 M3 M4 M5 M6 M7 Combined 95% CI 1.20 0.19 1.30 0.20 1.31 0.19 0.90 0.31 1.20 0.20 1.27 1.21 1.27 0.21 1.17 0.09 S33 S2.3 Experimental results on k-cliques On smaller graphs, we can compute larger motifs. To illustrate the computation time, we formed the motif adjacency matrix W based on the k-cliques motif for k = 4, . . . , 9. We implemented the k-clique enumeration algorithm by Chiba and Nishizeki with the additional pre-processing of computing the (k  1)-core of the graph. (This pre-processing improves the running time in practice but does not affect the asymptotic complexity.) The motif adjacency matrices for k-cliques are sparser than the adjacency matrix of the original graph. Thus, we do not worry about spatial complexity for these motifs. We ran the algorithm on nine real-world networks, ranging from roughly four thousande nodes and 88 thousand edges to over two million nodes and around five million edges (see Table S4.) Each network contained at least one 9-clique and hence at least one k-clique for k < 9. All networks were downloaded from the SNAP collection at edu/data/ (56). All computations ran on the same server as for the triangular motifs and again there was no parallelism. We terminated computations after two hours. For five of the nine networks, the time to compute WM for the k-clique motif was under two hours for k = 4, . . . , 9 (Table S5). And for each network, the computation finished within two hours for k = 4, 5, 6. The smallest network (in terms of number of nodes and number of edges) was the Facebook ego network, where it took just under two hours to comptue WM for the 6-clique motif and over two hours for the 7-clique motif. This network has around 80,000 edges. On the other hand, for the YouTube network, which contains nearly 3 million edges, we could compute WM for the 9-clique motif in under a minute. We conclude that it is possible to use our frameworks with motifs much larger than the three-node motifs on which we performed many of our experiments. However, the number of edges is not that correlated with the running time to compute WM . This makes sense becuse the Chiba and Nishizeki algorithm complexity is O(ak2m), where a is the arboricity of the graph. S34 Hence, the dependence on the number of edges is always linear. Table S4: Summary of networks used in scalability experiments with k-clique motifs. For each graph, we consider all edges as undirected. Network description # nodes # edges ego-Facebook Facebook friendships 4.04K 88.2K wiki-RfA Adminship voting on Wikipedia 10.8K 182K ca-AstroPh author co-authorship 18.8K 198K email-EuAll Emails in a research institution 265K 364K cit-HepPh paper citations 34.5K 421K soc-Slashdot0811 Slashdot user interactions 77.4K 469K com-DBLP author co-authorship 317K 1.05M com-Youtube User friendships 1.13M 2.99M wiki-Talk Wikipedia users interactions 2.39M 4.66M Table S5: Time to compute WM for k-clique motifs (seconds). Only computations that finished within two hours are listed. Number of nodes in clique (k) Network 4 5 6 7 8 9 ego-Facebook 14 317 6816    wiki-RfA 6 22 63 134 218 286 ca-AstroPh 5 35 285 2164   email-EuAll 1 2 4 5 6 6 cit-HepPh 3 6 11 18 30 36 soc-Slashdot0811 3 12 55 282 1018 2836 com-DBLP 9 129 3234    com-Youtube 12 17 25 33 35 33 wiki-Talk 64 466 2898    S35\ncency matrix For several motifs, the motif adjacency matrix WM (Equation S19) has a simple formula in terms of the adjacency matrix of the original, directed, unweighted graph, G. Let A be the adjacency matrix for G and let U and B be the adjacency matrix of the unidirectional and bidirectional links ofG. Formally, B = AAT and U = AB, where  denotes the Hadamard (entry-wise) product. Table S6 lists the formula of WM for motifs M1, M2, M3, M4, M5, M6, and M7 (see Figure S4) in terms of the matrices U and B. The central computational kernel in these computations is (X  Y )  Z. When X , Y , and Z are sparse, efficient parallel algorithms have been developed and analyzed (61). If the adjacency matrix is sparse, then computing WM for these motifs falls into this framework. Table S6: Matrix-based formulations of the weighted motif adjacency matrix WM (Equation S19) for all triangular three-node simple motifs. P Q denotes the Hadamard (entry-wise) products of matrices P and Q. If A is the adjacency matrix of a directed, unweighted graph G, then B = A  AT and U = AB. Note that in all cases, WM is symmetric. Motif Matrix computations WM = M1 C = (U  U)  UT C + CT M2 C = (B  U)  UT + (U B)  UT + (U  U) B C + CT M3 C = (B B)  U + (B  U) B + (U B) B C + CT M4 C = (B B) B C M5 C = (U  U)  U + (U  UT )  U + (UT  U)  U C + CT M6 C = (U B)  U + (B  UT )  UT + (UT  U) B C M7 C = (U T B)  UT + (B  U)  U + (U  UT ) B C With these matrix formulations, implementing the motif-based spectral partitioning algorithm for modestly sized graphs is straightforward. However, these computations become slower than standard fast triangle enumeration algorithms when the networks are large and sparse. Nevertheless, the matrix formulations provide a simple and elegant computational S36 1 f u n c t i o n [ S , Sbar , c o n d u c t a n c e s ] = M o t i f S p e c t r a l P a r t i t i o n M 6 (A) 2 % S p e c t r a l p a r t i t i o n i n g f o r m o t i f M 6 3 4 B = sp on es (A & A  ) ; % b i d i r e c t i o n a l l i n k s 5 U = A  B ; % u n i d i r e c t i o n a l l i n k s 6 7 % Form m o t i f a d j a c e n c y m a t r i x f o r m o t i f M 6 . 8 % For d i f f e r e n t m o t i f s , r e p l a c e t h i s l i n e wi th a n o t h e r m a t r i x f o r m u l a t i o n . 9 W = (B  U ) . U + (U  B) . U + (U  U) .  B ; 10 11 % Compute e i g e n v e c t o r o f m o t i f n o r m a l i z e d L a p l a c i a n 12 D s q r t = f u l l ( sum (W, 2 ) ) ; 13 D s q r t ( D s q r t = 0 ) = 1 . / s q r t ( D s q r t ( D s q r t = 0 ) ) ; 14 [ I , J , V] = f i n d (W) ; 15 Ln = s p a r s e ( I , J , V . ( D s q r t ( I ) . D s q r t ( J ) ) , s i z e (A, 1 ) , s i z e (A, 2 ) ) ; 16 [ Z , lambdas ] = e i g s ( Ln , 2 ,  s a  ) ; 17 % Matlab  s e i g s i s somet imes o u t o f o r d e r 18 [  , e i g o r d e r ] = s o r t ( d i a g ( lambdas ) ) ; 19 y = D s q r t .  Z ( : , e i g o r d e r ( end ) ) ; 20 21 % L i n e a r t ime sweep p r o c e d u r e 22 [  , o r d e r ] = s o r t ( y ) ; 23 C = W( o r d e r , o r d e r ) ; 24 C sums = f u l l ( sum (C , 2 ) ) ; 25 volumes = cumsum ( C sums ) ; 26 v o l u m e s o t h e r = f u l l ( sum ( sum (W) ) )  ones ( l e n g t h ( o r d e r ) , 1 )  volumes ; 27 c o n d u c t a n c e s = cumsum ( C sums  2  sum ( t r i l (C) , 2 ) ) . / min ( volumes , v o l u m e s o t h e r ) ; 28 [  , s p l i t ] = min ( c o n d u c t a n c e s ) ; 29 S = o r d e r ( 1 : s p l i t ) ; 30 Sbar = o r d e r ( ( s p l i t + 1 ) : end ) ; Figure S6: MATLAB implementation of the motif-based spectral partitioning algorithm for motif M6. For other motifs, line 9 can be replaced with the formulations from Table S6. method for the motif adjacency matrix WM . To demonstrate, Figure S6 provides a complete MATLAB implementation of Algorithm 1 for M6 (Figure S4). The entire algorithm including comments comrpises 28 lines of code. An alternative matrix formulation comes from a motif-node adjacency matrix. LetM(B,A) be a motif set and number the instances of the motif 1, . . . , |M |, so that (vi, A(vi)) is the ith motif. Define the |M | n motif-node adjacency matrix AM by (AM)ij = 1(j  A(vi)). Then S37 (WM)ij = (A T MAM)ij, i 6= j. (S26) This provides a convenient algebraic formulation for defining the weighted motif adjacency matrix. However, in practice, we do not use this formulation for any computations.\nFor our experiments, we compare our spectral motif-based custering to the following methods:  Standard, edge-based spectral clustering, which is a special case of motif-based cluster- ing. In particular, the motifs B1 = [ 0 1 1 0 ] , B2 = [ 0 1 0 0 ] , A = {1, 2} (S27) correspond to removing directionality from a directed graph. We refer to the union of these two motifs as Medge.  Infomap, which is based on the map equation (62). Software for Infomap was down- loaded from We run the algorithm the algorithm for directed links when the network under consideration is directed.  The Louvain method (63). Software for the Louvain method was downloaded from html We use the oriented version of the Louvain method for directed graphs. Infomap and the Louvain method are purely clustering methods in the sense that they take as input the graph and produce as output a set of labels for the nodes in the graph. In contrast to the spectral methods, we do not have control over the number of clusters. Also, only the spectral methods provide embeddings of the nodes into euclidean space, which is useful for S38 visualization. Thus, for our analysis of the transportation reachiability network in Section S6, we only compare spectral methods.\nC. elegans network We now provide more details on the cluster found for the C. elegans network of frontal neurons (28). In this network, the nodes are neurons and the edges are synapses. The network data was downloaded from suppl/celegans131.zip. S5.1 Connected components of the motif adjacency matrices We again first onsider the connected components of the motif adjacency matrices as a preprocessing step. For our analysis, we consider use Mbifan, M8, and Medge (Figure S4). The original network has 131 nodes and 764 edges. The largest connected component of the motif adjacency matrix for motif Mbifan contains 112 nodes. The remaining 19 nodes are isolated and correspond to the neurons AFDL, AIAR, AINR, ASGL/R, ASIL/R, ASJL/R, ASKL/R, AVL, AWAL, AWCR, RID, RMFL, SIADR, and SIBDL/R. The largest connected component of the motif adjacency matrix for motif M8 contains 127 nodes. The remaining 4 nodes are isolated and correspond to the neurons ASJL/R and SIBDL/R. The original network is weakly connected, so the motif adjacency matrix for Medge is connected. S5.2 Comparison of bi-fan motif cluster to clusters found by existing methods We found the motif-based clusters for motifs Mbifan, M8, and Medge by running Algorithm 1 on the largest connected component of the motif adjacency matrix. Sweep profile plots (M(S) as S39 10 0 10 1 10 2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 M o ti f c o n d u c ta n c e Sets Sr Figure S7: Sweep profile plot (M(S) as a function of S from the sweep in Algorithm 1) for Mbifan (green) M8 (dark blue), and Medge (light blue). a function of S from the sweep in Algorithm 1) are shown in Figure S7 and show that the size of the Mbifan returned by Algorithm 1 cluster is smaller than the clusters for M8 and Medge. In fact, the motif-based clusters for M8 and Medge essentially bisect the graph, containing 63 of 127 and 64 of 131 nodes, respectively. Of the 63 nodes in the M8-based cluster, only 2 are in the edge-based cluster, so these partitions give roughly the same information. Next, we compare the clusters found by existing methods to the Mbifan-based cluster found by Algorithm 1. We will show that existing methods do not find the same group of nodes. Let Sbifan be the Mbifan-based cluster, which consists of 20 nodes. The nodes correspond to the following neurons: IL1DL/VL, IL2DL/DR/VL/VR/L/R, OLQDL/R, RIH, RIPL/R, RMEL/R/V, and URADL/DR/VL/VR. The partitions based onM8 andMedge provide two sets of nodes each. For the subsequent analysis, we consider the set with the largest number of overlapping nodes with Sbifan. Call these sets SM8 and Sedge. We also consider the cluster found by Infomap and the Louvain method with the largest overlap with Sbifan. Call these sets SI and SL. To compare the most similar clusters found by other methods to Sbifan, we look at two metrics. First, how many neurons in Sbifan are in a cluster found by existing methods (in other S40 words, the overlap). A cluster consisting of all nodes in the graph would trivially have 100% overlap with Sbifan but loses all precision in the cluster identification. Thus, we also consider the sizes of the clusters. These metrics are summarized as follows: |Sbifan  SM8| = 20, |SM8| = 68 |Sbifan  Sedge| = 20, |Sedge| = 64 |Sbifan  SL| = 13, |SL| = 27 |Sbifan  SI | = 19, |SI | = 114 We see that Sbifan is a subset of SM8 and Sedge and has substantial overlap with SI . However, Sbifan is by far the smallest of all of these sets. We conclude that existing methods do not capture the same information as motif Mbifan. To further investigate the structure found by existing methods, we show the clusters Sedge and SM8 in Figure S8. From the figure, we see that spectral clustering based on edges or motif M8 simply finds a spatially coherent cluster, rather than the control structure formed by the nodes in Sbifan. S41 Figure S8: Illustration of motif-based clusters with true two-dimensional spatial dimensions of the frontal neurons of C. elegans. A: The Mbifan-based cluster consists of the labeled dark blue nodes. B: Partitioning the graph based on motif M8, where the labeled dark blue nodes are the nodes on the side of the partition with largest overlap of the nodes in A. C: Partitioning the graph based on edges, where the labeled dark blue nodes are the nodes on the side of the partition with largest overlap of the nodes in A. Note that the partitions in Figures B and C capture the cluster in Figure A, but also contain many other nodes. Essentially, the partitions in B and C are just capturing spatial information. S42\ntransportation reachability network The nodes in the transportation reachability network are airports in the United States and Canada. There is an edge from city i to city j if the estimated travel time from i to j is less than some threshold (23). The network is not symmetric. The network with estimated travel times was downloaded from and txt. We collected the latitude, longitude, and metropolitan populations of the cities using WolframAlpha and Wikipedia. All of the data is available on our project web page: http: //snap.stanford.edu/higher-order/. S6.1 Methods for spectral embeddings We compared the motif-based spectral embedding of the transportation reachability network to spectral embeddings from other connectivity matrices. For this analysis, we ignore the travel times times and only consider the topology of the network. The two-dimensional spectral embedding for a graph defined by a (weighted) adjacency matrix W  Rnn comes from Algorithm 2: 1. Form the normalized Laplacian L = I D1/2WD1/2, where D is the diagonal degree matrix with Dii =  jWij . 2. Compute the first 3 eigenvectors z1, z2, z3 of smallest eigenvalues for L (z1 has the small- est eigenvalue). 3. Form the normalized matrix Y  Rn3 by Yij = zij/ 3 j=1 z 2 ij . S43 4. Define the primary and secondary spectral coordinates of node i to be Yi2 and Yi3, respec- tively. We consider the following three matrices W . 1. Motif: The sum of the motif adjacency matrix (Equation S20) for three different anchored motifs: B1 = 0 1 11 0 1 1 1 0  , B2 = 0 1 11 0 1 0 1 0  , B3 = 0 1 01 0 1 0 1 0  , A = {1, 3}. (S28) If S is the matrix of bidirectional links in the graph (Sij = 1 if and only ifAij = Aji = 1), then the motif adjacency matrix for these motifs is WM = S2. The resulting embedding is shown in Figure 4C of the main text. 2. Undirected: The adjacency matrix is formed by ignoring edge direction. This is the standard spectral embedding. The resulting embedding is shown in Figure 4D of the main text. 3. Undirected complement: The adjacency matrix is formed by taking the complement of the undirected adjacency matrix. This matrix tends to connect non-hubs to each other. The networks represented by each adjacency matrices are all connected. S6.2 Comparison of motif-based embedding to other embeddings We computed 99% confidence intervals for the Pearson correlation of the primary spectral coordinate with the metropolitan population of the city using the Pearson correlation coefficient. Table S7 lists the confidence intervals. (Since eigenvectors are only unique up to sign, the confidence intervals are symmetric about 0. We list the interval with the largest positive end point S44 Table S7: Summary of Pearson correlations for spectral embeddings of the transportation reachability network. We list the 99% confidence interval for the Pearson correlation coefficient. Primary spectral coordinate Secondary spectral coordinate and metropolitan population and longitude Embedding 99% confidence interval 99% confidence interval Motif 0.43  0.09 0.59  0.08 Undirected 0.11  0.12 0.39  0.11 Undirected complement 0.31  0.11 0.10  0.12 under this permutation to be consistent across embeddings.) The motif-based primary spectral coordinate has the strongest correlation with the city populations. We repeated the computations for the correlation between the secondary spectral coordinate and the longitude of the city. Again, the motif-based clustering has the strongest correlation. Furthermore, the lower end of the confidence interval for the motif-based embedding was above the higher end of the confidence interval for the other three embeddings. Finally, in order to visualize these relationships, we computed Loess regressions of city metropolitan population and longitude against the primary and secondary spectral coordinates for each of the embeddings (Figure S9). The sign of the eigenvector used in each regression was chosen to match correlation shown in Figures 3C and 3D in the main text (primary spectral coordinate positively correlated with population and secondary spectral coordinate negatively correlated with longitude). The Loess regressions visualize the stronger correlation of the motifbased spectral coordinates with the metropolitan popuatlion and longitude. We conclude that the embedding provided by the motif adjacency matrix more strongly captures the hub nature of airports and West-East geography of the network. To gain further insight into the relationship of the primary spectral coordinates relationship with the hub airports, we visualize the adjacency matrix in Figure S10, where the nodes are ordered by the spectral ordering. We see a clear relationship between the spectral ordering and the connectivity. S45 Figure S9: Loess regressions of city metropolitan population against the primary spectral coordinate (top) and longitude against secondary spectral coordinate (bottom) for the motif (left), undirected (middle), and undirected complement (right) adjacency matrices. Figure S10: Visualization of transportation reachability network. Nodes are ordered by the spectral ordering provided by the motif adjacency matrix. A black dot means no edge exists in the network. For the edges in the network, lighter colors mean longer estimated travel times. S46\nWe next use motif-based clustering to analyze several additional networks. Our main goal is to show that motif-based clusters find markedly different structures in many real-world networks compared to edge-based clusters. For the case of a transcription regulation network of yeast, we also show that motif-based clustering more accurately finds known functional modules compared to existing methods. On the English Wikipedia article network and the Twitter network, we identify motifs that find anomalous clusters. On the Stanford web graph and in collaboration networks, we use motifs that have previously been studied in the literature and see how they reveal organizational structure in the networks. S7.1 Motif M6 in the Florida Bay food web We now apply the higher-order clustering framework on the Florida Bay ecosystem food web (64). The dataset was downloaded from data/bio/foodweb/Florida.paj. In this network, the nodes are compartments (roughly, organisms and species) and the edges represent directed carbon exchange (in many cases, this means that species j eats species i). Motifs model energy flow patterns between several species. S7.1.1 Identifying higher-order modular organization In this case study, we use the framework to identify higher-order modular organization of networks. We focus on three motifs: M5 corresponds to a hierarchical flow of energy where species i and j are energy sources (prey) for species k, and i is also an energy source for j; M6 models two species that prey on each other and then compete to feed on a common third species; and M8 describes a single species serving as an energy source for two non-interacting species. Motif M5 is considered a building block for food webs (65, 66), and the prevalence of motif M6 is predicted by a certain niche model (67). S47 The framework reveals that low motif conductance (high-quality) clusters only exist for motif M6 (motif conductance 0.12), whereas clusters based on motifs M5 or M8 have high motif conductance (see Figure S11). In fact, the motif Cheeger inequality (Theorem 6) guarantees that clustering based on motif M5 or M8 will always have larger motif conductance that clustering based on M6. The inequality says that the motif conductance for any cluster in a connected motif adjacency matrix is at least half of the second smallest eigenvalue of the motif-normalized Laplacian. However, finding the cluster with optimal conductance is still computationally infeasible in general (68). The lower bounds using the largest connected component of the motif adjacency matrix for motifs M5, M6, and M8 were 0.2195, 0.0335, and 0.2191, and the clusters found by the Algorithm 1 had motif conductances of 0.4414, 0.1200, and 0.4145. Thus, the cluster S found by the algorithm for M6 has smaller motif M6-conductance (0.12) than any possible clusters motif-M5 or motif-M8 conductance. To state this formally, let C be the cluster found by the algorithm for motif M6 and let HM be the largest connected component of motif adjacency matrix for motif M . Then M6(HM6 , C)  min { min S M5(HM5 , S), min S M8(HM8 , S) } . (S29) This means that, in terms of motif conductance, any cluster based on motifs M5 or M8 is worse than the cluser found by the algorithm in Theorem 6 for motif M6. We note that the same conclusions hold for edge-based clustering. For motif Medge, the lower bound on conductance was 0.2194 and the cluster found by the algorithm had conductance 0.4083. S7.1.2 Analysis of higher-order modular organization Subsequently, we used motif M6 to cluster the food web, revealing four clusters (Figure S11). Three represent well-known aquatic layers: (i) the pelagic system; (ii) the benthic predators of eels, toadfish, and crabs; (iii) the sea-floor ecosystem of macroinvertebrates. The fourth S48 cluster identifies microfauna supported by particulate organic carbon in water and free bacteria. Table S9 lists the nodes in each cluster. We also measured how well the motif-based clusters correlate to known ground truth system subgroup classifications of the nodes (64). These classes are microbial, zooplankton, and sediment organism microfauna; detritus; pelagic, demersal, and benthic fishes; demseral, seagrass, and algae producers; and macroinvertebrates (Table S9).2 We also consider a set of labels which does not include the subclassification for microfauna and producers. In this case, the labels are microfauna; detritus; pelagic, demersal, and benthic fishes; producers; and macroinvertebrates. To quantify how well the clusters found by motif-based clustering reflect the ground truth labels, we used several standard evaluation criteria: adjusted rand index, F1 score, normalized mutual information, and purity (69). We compared these results to the clusters of several methods using the same evaluation criteria. In total, we evaluated six methods: 1. Motif-based clustering with the embedding + k-means algorithm (Algorithm 2) with 500 iterations of k-means. 2. Motif-based clustering with recursive bi-partitioning (repeated application of Algorithm 1 on the largest remaining compoennt). The process continues to cut the largest cluster until there are 4 total. 3. Edge-based clustering with the embedding + k-means algorithm, again with 500 iterations of k-means. 4. Edge-based clustering with recursive bi-partitioning with the same partitioning process. 5. The Infomap algorithm. 2The classifications are also available on our project web page: higher-order/. S49 6. The Louvain method. For the first four algorithms, we control the number of clusters, which we set to 4. For the last two algorithms, we cannot control the number of clusters. However, both methods found 4 clusters. Table S10 shows that the motif-based clustering by embedding + k-means had the best performance for each classification criterion on both classifications. We conclude that the organization of compartments in the Florida Bay foodweb are better described motif M6 than by edges. S7.1.3 Connected components of the motif adjacency matrices Finally, we discuss the discuss the preprocessing step of our method, where we compute computed connected components of the motif adjacency matrices. The original network has 128 nodes and 2106 edges. The largest connected component of the motif adjacency matrix for motif M5 contains 127 of the 128 nodes. The node corresponding to the compartment of roots is the only node not in the largest connected component. The two largest connected components of the motif adjacency matrix for motif M6 contain 12 and 50 nodes. The remaining 66 nodes are isolated. Table S8 lists the nodes in each component. We note that the group of 12 nodes corresponds to the green cluster in Figure S11. The motif adjacency matrix for M8 is connected. The original network is weakly connected, so the motif adjacency matrix for Medge is also connected. S50 Figure S11: Higher-order organization of the Florida Bay food web. A: Sweep profile plot ((G)M (S) as a function of S from the sweep in Algorithm 1) for different motifs on the Florida Bay ecosystem food web (64). A priori it is not clear whether the network is organized based on a given motif. For example, motifs M5 (green) and M8 (blue) do not reveal any higher-order organization (motif conductance has high values). However, the downward spikes of the red curve show that M6 reveals rich higher-order modular structure (7). Ecologically, motif M6 corresponds to two species mutually feeding on each other and also preying on a common third species. B: Clustering of the food web based on motif M6. (For illustration, edges not participating in at least one instance of the motif are omitted.) The clustering reveals three known aquatic layers: pelagic fishes (yellow), benthic fishes and crabs (red), and sea-floor macroinvertebrates (blue) as well as a cluster of microfauna and detritus (green). Our framework identifies these modules with higher accuracy (61%) than existing methods (4853%). C: A higher-order cluster (yellow nodes in (B)) shows how motif M6 occurs in the pelagic layer. The needlefish and other pelagic fishes eat each other while several other fishes are prey for these two species. D: Another higher-order cluster (green nodes in (B)) shows how motif M6 occurs between microorganisms. Here, several microfauna decompose into Particulate Organic Carbon in the water (water POC) but also consume water POC. Free bacteria serves as an energy source for both the microfauna and water POC. S51 Table S8: Connected components of the Florida Bay foodweb motif adjacency matrix for motif M6. There are 50 nodes in component 1, 12 nodes in component 2, and 66 isolated nodes. Two largest components Isolated nodes Compartment (node) Component index Compartment (node) Benthic Phytoplankton 1 Barracuda Thalassia 1 2 m Spherical Phytoplankt Halodule 1 Synedococcus Syringodium 1 Oscillatoria Drift Algae 1 Small Diatoms (<20 m) Epiphytes 1 Big Diatoms (>20 m) Predatory Gastropods 1 Dinoflagellates Detritivorous Polychaetes 1 Other Phytoplankton Predatory Polychaetes 1 Roots Suspension Feeding Polych 1 Coral Macrobenthos 1 Epiphytic Gastropods Benthic Crustaceans 1 Thor Floridanus Detritivorous Amphipods 1 Lobster Herbivorous Amphipods 1 Stone Crab Isopods 1 Sharks Herbivorous Shrimp 1 Rays Predatory Shrimp 1 Tarpon Pink Shrimp 1 Bonefish Benthic Flagellates 1 Other Killifish Benthic Ciliates 1 Snook Meiofauna 1 Sailfin Molly Other Cnidaridae 1 Hawksbill Turtle Silverside 1 Dolphin Echinoderma 1 Other Horsefish Bivalves 1 Gulf Pipefish Detritivorous Gastropods 1 Dwarf Seahorse Detritivorous Crabs 1 Grouper Omnivorous Crabs 1 Jacks Predatory Crabs 1 Pompano Callinectes sapidus (blue crab) 1 Other Snapper Mullet 1 Gray Snapper Blennies 1 Mojarra Code Goby 1 Grunt Clown Goby 1 Porgy Flatfish 1 Pinfish Sardines 1 Scianids Anchovy 1 Spotted Seatrout Bay Anchovy 1 Red Drum Lizardfish 1 Spadefish Catfish 1 Parrotfish Eels 1 Mackerel Toadfish 1 Filefishes Brotalus 1 Puffer Halfbeaks 1 Loon Needlefish 1 Greeb Goldspotted killifish 1 Pelican Rainwater killifish 1 Comorant Other Pelagic Fishes 1 Big Herons and Egrets Other Demersal Fishes 1 Small Herons and Egrets Benthic Particulate Organic Carbon (Benthic POC) 1 Ibis Free Bacteria 2 Roseate Spoonbill Water Flagellates 2 Herbivorous Ducks Water Cilitaes 2 Omnivorous Ducks Acartia Tonsa 2 Predatory Ducks Oithona nana 2 Raptors Paracalanus 2 Gruiformes Other Copepoda 2 Small Shorebirds Meroplankton 2 Gulls and Terns Other Zooplankton 2 Kingfisher Sponges 2 Crocodiles Water Particulate Organic Carbon (Water POC) 2 Loggerhead Turtle Input 2 Green Turtle Manatee Dissolved Organic Carbon (DOC) Output Respiration S52 Table S9: Ecological classification of nodes in the Florida Bay foodweb. Colors correspond to the colors in the clustering of Figure S11. Compartment (node) Classification 1 Classification 2 Assignment Free Bacteria Microbial microfauna Microfauna Green Water Flagellates Microbial microfauna Microfauna Green Water Cilitaes Microbial microfauna Microfauna Green Acartia Tonsa Zooplankton microfauna Microfauna Green Oithona nana Zooplankton microfauna Microfauna Green Paracalanus Zooplankton microfauna Microfauna Green Other Copepoda Zooplankton microfauna Microfauna Green Meroplankton Zooplankton microfauna Microfauna Green Other Zooplankton Zooplankton microfauna Microfauna Green Sponges Macroinvertebrates Macroinvertebrates Green Water POC Detritus Detritus Green Input Detritus Detritus Green Sardines Pelagic Fishes Pelagic Fishes Yellow Anchovy Pelagic Fishes Pelagic Fishes Yellow Bay Anchovy Pelagic Fishes Pelagic Fishes Yellow Halfbeaks Pelagic Fishes Pelagic Fishes Yellow Needlefish Pelagic Fishes Pelagic Fishes Yellow Goldspotted killifish Fishes Demersal Fishes Demersal Yellow Rainwater killifish Fishes Demersal Fishes Demersal Yellow Silverside Pelagic Fishes Pelagic Fishes Yellow Other Pelagic Fishes Pelagic Fishes Pelagic Fishes Yellow Detritivorous Crabs Macroinvertebrates Macroinvertebrates Red Predatory Crabs Macroinvertebrates Macroinvertebrates Red Callinectus sapidus Macroinvertebrates Macroinvertebrates Red Lizardfish Benthic Fishes Benthic Fishes Red Eels Fishes Demersal Fishes Demersal Red Code Goby Benthic Fishes Benthic Fishes Red Clown Goby Benthic Fishes Benthic Fishes Red Herbivorous Shrimp Macroinvertebrates Macroinvertebrates Red Benthic Phytoplankton Producer Demersal Producer Blue Thalassia Producer Seagrass Producer Blue Halodule Producer Seagrass Producer Blue Syringodium Producer Seagrass Producer Blue Drift Algae Producer Algae Producer Blue Epiphytes Producer Algae Producer Blue Benthic Flagellates Sediment Organism microfauna Microfauna Blue Benthic Ciliates Sediment Organism microfauna Microfauna Blue Meiofauna Sediment Organism microfauna Microfauna Blue Other Cnidaridae Macroinvertebrates Macroinvertebrates Blue Echinoderma Macroinvertebrates Macroinvertebrates Blue Bivalves Macroinvertebrates Macroinvertebrates Blue Detritivorous Gastropods Macroinvertebrates Macroinvertebrates Blue Predatory Gastropods Macroinvertebrates Macroinvertebrates Blue Detritivorous Polychaetes Macroinvertebrates Macroinvertebrates Blue Predatory Polychaetes Macroinvertebrates Macroinvertebrates Blue Suspension Feeding Polych Macroinvertebrates Macroinvertebrates Blue Macrobenthos Macroinvertebrates Macroinvertebrates Blue Benthic Crustaceans Macroinvertebrates Macroinvertebrates Blue Detritivorous Amphipods Macroinvertebrates Macroinvertebrates Blue Herbivorous Amphipods Macroinvertebrates Macroinvertebrates Blue Isopods Macroinvertebrates Macroinvertebrates Blue Predatory Shrimp Macroinvertebrates Macroinvertebrates Blue Pink Shrimp Macroinvertebrates Macroinvertebrates Blue Omnivorous Crabs Macroinvertebrates Macroinvertebrates Blue Catfish Benthic Fishes Benthic Fishes Blue Mullet Pelagic Fishes Pelagic Fishes Blue Benthic POC Detritus Detritus Blue Toadfish Benthic Fishes Benthic Fishes Blue Brotalus Fishes Demersal Fishes Demersal Blue Blennies Benthic Fishes Benthic Fishes Blue Flatfish Benthic Fishes Benthic Fishes Blue Other Demersal Fishes Fishes Demersal Fishes Demersal Blue S53 Table S10: Comparison of motif-based algorithms against other methods in finding ground truth structure in the Florida Bay food web (64). Performance for identifying the two classifications provided in Table S9 was evaluated based on Adjusted Rand Index (ARI), F1 score, Normalized Mutual Information (NMI), and Purity. In all cases, the motif-based methods have the best performance. Evaluation Motif embedding Motif recursive Edge embedding Edge recursive Infomap Louvain + k-means bi-partitioning + k-means bi-partitioning C la ss ifi ca tio n 1 ARI 0.3005 0.2156 0.1564 0.1226 0.1423 0.2207 F1 0.4437 0.3853 0.3180 0.2888 0.3100 0.4068 NMI 0.5040 0.4468 0.4112 0.3879 0.4035 0.4220 Purity 0.5645 0.5323 0.4032 0.4194 0.4194 0.5323 C la ss ifi ca tio n 2 ARI 0.3265 0.2356 0.1814 0.1190 0.1592 0.2207 F1 0.4802 0.4214 0.3550 0.3035 0.3416 0.4068 NMI 0.4822 0.4185 0.3533 0.3034 0.3471 0.4220 Purity 0.6129 0.5806 0.4839 0.4355 0.4677 0.5323 S7.2 Coherent feedforward loops in the S. cerevisiae transcriptional regulation network In this network, each node is an operon (a group of genes in a mRNA molecule), and a directed edge from operon i to operon j means that i is regulated by a transcriptional factor encoded by j (29). Edges are directed and signed. A positive sign represents activation and a negative sign represents repression. The network data was downloaded from ac.il/mcb/UriAlon/sites/mcb.UriAlon/files/uploads/NMpaper/yeastdata. mat and uploads/DownloadableData/list_of_ffls.pdf. For this case study, we examine the coherent feedforward loop motif (see Figure S12), which act as sign-sensitive delay elements in transcriptional regulation networks (2, 9). Formally, the feedforward loop is represented by the following signed motifs B1 = 0 + +0 0 + 0 0 0  , B2 = 0  0 0 + 0 0 0  , B3 = 0 + 0 0  0 0 0  , B4 = 0  +0 0  0 0 0  . (S30) S54 Table S11: Connected components of size greater than one for the motif adjacency matrix in the S. cerevisiae network for the coherent feedforward loop. Size operons 18 ALPHA1, CLN1, CLN2, GAL11, HO, MCM1, MFALPHA1, PHO5, SIN3, SPT16, STA1, STA2, STE3, STE6, SWI1, SWI4/SWI6, TUP1, SNF2/SWI1 9 HXT11, HXT9, IPT1, PDR1, PDR3, PDR5, SNQ2, YOR1, YRR1 9 GCN4, ILV1, ILV2, ILV5, LEU3, LEU4, MET16, MET17, MET4 6 CHO1, CHO2, INO2, INO2/INO4, OPI3, UME6 6 DAL80, DAL80/GZF3, GAP1, GAT1, GLN1, GLN3 5 CYC1, GAL1, GAL4, MIG1, HAP2/3/4/5 3 ADH2, CCR4, SPT6 3 CDC19, RAP1, REB1 3 DIT1, IME1, RIM101 These motifs have the same edge pattern and only differ in sign. All of the motifs are simple (A = {1, 2, 3}). For our analysis, we consider all coherent feedforward loops that are subgraphs on the induced subgraph of any three nodes. However, there is only one instance where the coherent feedforward loop itself is a subgraph but not an induced subgraph on three nodes. Specifically, the induced subgraph by DAL80, GAT1, and GLN3 contains a bi-directional edge between DAL80 and GAT1, unidirectional edges from DAL80 and GAT1 to GLN3. S7.2.1 Connected components of the adjacency matrices Again, we analyze the component structure of the motif adjacency matrix as a pre-processing step. The original network consists of 690 nodes and 1082 edges, and its largest weakly connected component consists of 664 nodes and 1066 edges. Every coherent feedforward loop in the network resides in the largest weakly connected component, so we subsequently consider this sub-network in the following analysis. Of the 664 nodes in the network, only 62 participate in a coherent feedforward loop. Forming the motif adjacency matrix results in nine connected components, of sizes 18, 9, 9, 6, 6, 5, 3, 3, and 3. The operons for the connected components consisting of more than one node is listed in Table S11. S55 S7.2.2 Comparison against existing methods We note that, although the original network is connected, the motif adjacency matrix corresponds to a disconnected graph. This already reveals much of the structure in the network (Figure S12). Indeed, this shattering of the graph into components for the feedforward loop has previously been observed in transcriptional regulation networks (70). We additionally used Algorithm 1 to partition the largest connected component of the motif adjacency matrix (consisting of 18 nodes). This revealed the cluster {CLN2, CLN1, SWI4/SWI6, SPT16, HO}, which contains three coherent feedforward loops (Figure S12). All three instances of the motif correspond to the function cell cycle and mating type switch. The motifs in this cluster are the only feedforward loops for which the function is described in Reference (9). Using the same procedure on the undirected version of the induced subgraph of the 18 nodes (i.e., using motif Medge) results in the cluster {CLN1, CLN2, SPT16, SWI4/SWI6 }. This cluster breaks the coherent feedforward loop formed by HO, SWI4/SWI6, and SPT16. We also evaluated our method based on the classification of motif functionality (9).3 In total, there are 12 different functionalities and 29 instances of labeled coherent feedforward loops. We considered the motif-based clustering of the graph to be the connected components of the motif adjacency matrix with the additional partition of the largest connected component. To form an edge-based clustering, we used the embedding + k-means algorithm on the undirected graph (i.e., motif Medge) with k = 12 clusters. We also clustered the graph using Infomap and the Louvain method. Table S12 summarizes the results. We see that the motif-based clustering coherently labels all 29 motifs in the sense that the three nodes in every instance of a labeled motif is placed in the same cluster. The edge-based spectral, Infomap, and Louvain clustering coherently labeled 25, 23, and 23 motifs, respectively. 3The functionalities may be downloaded from our project web page: higher-order/. S56 We measured the accuracy of each clustering method as the rand index (69) on the coherently labeled motifs, multiplied by the fraction of coherently labeled motifs. The motif-based clustering had the highest accuracy. We conclude that motif-based clustering provides an advantage over edge-based clustering methods in identifying functionalities of coherent feedforward loops in the the S. cerevisiae transcriptional regulation network. S57 Figure S12: Higher-order organization of the S. cerevisiae transcriptional regulation network. A: The four higher-order structures used by our higher-order clustering method, which can model signed motifs. These are coherent feedfoward loop motifs, which act as sign-sensitive delay elements in transcriptional regulation networks (2). The edge signs refer to activation (positive) or repression (negative). B: Six higher-order clusters revealed by the motifs in (A). Clusters show functional modules consisting of several motifs (coherent feedforward loops), which were previously studied individually (9). The higher-order clustering framework identifies the functional modules with higher accuracy (97%) than existing methods (6882%). CD: Two higher-order clusters from (B). In these clusters, all edges have positive sign. The functionality of the motifs in the modules correspond to drug resistance (C) or cell cycle and mating type match (D). The clustering suggests that coherent feedforward loops function together as a single processing unit rather than as independent elements. S58 Table S12: Classification of coherent feedforward loop motifs by several clustering methods. In a given motif instance, we say that it is coherently labeled if the nodes comprising the motif are in the same cluster. If a motif is not coherently labeled, a -1 is listed. The accuracy is the rand index on the labels and motif functionality on coherently labeled motifs, multiplied by the fraction of coherently labeled motifs. Motif nodes Function Class label Motif-based Edge-based Infomap Louvain GAL11 ALPHA1 MFALPHA1 pheromone response 1 1 -1 -1 GCN4 MET4 MET16 Metionine biosynthesis 2 2 1 -1 GCN4 MET4 MET17 Metionine biosynthesis 2 2 1 -1 GCN4 LEU3 ILV1 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 ILV2 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 ILV5 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 LEU4 Leucine and branched amino acid biosynthesis 2 2 1 1 GLN3 GAT1 GAP1 Nitrogen utilization 3 3 1 2 GLN3 GAT1 DAL80 Nitrogen utilization 3 3 1 2 GLN3 GAT1 DAL80/GZF3 Glutamate synthetase 3 3 1 2 GLN3 GAT1 GLN1 Glutamate synthetase 3 3 1 2 MIG1 HAP2/3/4/5 CYC1 formation of apocytochromes 4 4 -1 -1 MIG1 GAL4 GAL1 Galactokinase 4 -1 -1 -1 PDR1 YRR1 SNQ2 Drug resistance 5 5 2 3 PDR1 YRR1 YOR1 Drug resistance 5 5 2 3 PDR1 PDR3 HXT11 Drug resistance 5 5 2 3 PDR1 PDR3 HXT9 Drug resistance 5 5 2 3 PDR1 PDR3 PDR5 Drug resistance 5 5 2 3 PDR1 PDR3 IPT1 Drug resistance 5 5 2 3 PDR1 PDR3 SNQ2 Drug resistance 5 5 2 3 PDR1 PDR3 YOR1 Drug resistance 5 5 2 3 RIM101 IME1 DIT1 sporulation-specific 6 6 3 4 SPT16 SWI4/SWI6 CLN1 Cell cycle and mating type switch 7 -1 4 5 SPT16 SWI4/SWI6 CLN2 Cell cycle and mating type switch 7 -1 -1 5 SPT16 SWI4/SWI6 HO Cell cycle and mating type switch 7 -1 -1 -1 TUP1 ALPHA1 MFALPHA1 Mating factor alpha 1 1 -1 5 UME6 INO2/INO4 CHO1 Phospholipid biosynthesis 8 6 5 4 UME6 INO2/INO4 CHO2 Phospholipid biosynthesis 8 6 5 4 UME6 INO2/INO4 OPI3 Phospholipid biosynthesis 8 6 5 4 Frac. coherently labeled 29 / 29 25 / 29 23 / 29 23 / 29 Accuracy 0.97 0.82 0.68 0.76 S7.3 Motif M6 in the English Wikipedia article network The English Wikipedia network (5759) consists of 4.21 million nodes (representing articles) and 101.31 million edges, where an edge from node i to node j means that there is a hyperlink from the ith article to the jth article. The network data was downloaded from http://law. di.unimi.it/webdata/enwiki-2013/. We used Algorithm 1 to find a motif-based cluster for motif M6 and Medge (the algorithm was run on the largest connected component of the motif adjacency matrix). The clusters are shown in Figure S13. The nodes in the motif-based cluster are cities and barangays (small S59 administrative divisions) in the Philippines. The cluster has a set of nodes with many outgoing links that form the source node in motif M6. In total, the cluster consists of 22 nodes and 338 edges. The linking pattern appears anomalous and suggests that perhaps the pages uplinking should receive reciprocated links. On the other hand, the edge-based cluster is much larger cluster and does not have too much structure. The cluster consists of several high-degree nodes and their neighbors. S60 Figure S13: Clusters from the English Wikipedia hyperlink network (5759). AC: Motifbased cluster (A) for motif M6 (B). The cluster consists of cities and small administrative divisions in the Philippines. The green nodes have many bi-direction links with each other and many incoming links from orange nodes at the bottom of the figure. The spy plot illustrates this network structure (C). DF: Cluster (D) for undirected edges (E). The cluster has a few very high-degree nodes, as evidenced by the spy plot (F). S61 S7.4 Motif M6 in the Twitter follower network We also analyzed the complete 2010 Twitter follower graph (58, 59, 71). The graph consists 41.65 million nodes (users) and 1.47 billion edges, where an edge from node i to node j signifies that user i is followed by user j on the social network. The network data was downloaded from We used Algorithm 1 to find a motif-based cluster for motif M6 (the algorithm was run on the largest connected component of the motif adjacency matrix). The cluster contains 151 nodes and consists of two disconnected components. Here, we consider the smaller of the two components, which consists of 38 nodes. We also found an edge-based cluster on the undirected graph (using Algorithm 1 with motif Medge). This cluster consists of 44 nodes. Figure S14 illustrates the motif-based and edge-based clusters. Both clusters capture anomalies in the graph. The motif-based cluster consists of holding accounts for a photography company. The nodes that form bi-directional links have completed profiles (contain a profile picture) while several nodes with incomplete profiles (without a profile picture) are followed by the completed accounts. The edge-based cluster is a near clique, where the user screen names all begin with LC . We suspect that the similar usernames are either true social communities, holding accounts, or bots. (For the most part, their tweets are protected, so we could not verify if any of these scenarios are true). Interestingly, both M6 and Medge find anomalous clusters. However, their structures are quite different. We conclude that M6 can lead to the detection of new anomalous clusters in social networks. S62 Figure S14: Clusters in the 2010 Twitter follower network (58, 59, 71). AC: Motif-based cluster (A) for motif M6 (B). All accounts are holding accounts for a photography company. The green nodes correspond to accounts that have completed profiles, while the orange accounts have incomplete profiles. The spy plot illustrates how the cluster is formed around this motif (C). DF: Cluster (D) for edge-based clustering (E). The cluster consists of a near-clique (F) where all users have the prefix LC . S63 S7.5 Motif M7 in the Stanford web graph The Stanford web graph (7, 56) consists of 281,903 nodes and 2,312,497 edges, where an edge from node i to node j means that there is a hyperlink from the ith web page to the jth web page. Here, all of the web pages come from the Stanford domain. The network data was downloaded from We used Algorithm 1 to find a motif-based cluster for motif M7, a motif that is overexpressed in web graphs (1). An illustration of the cluster and an edge-based cluster (i.e., using Algorithm 1 with Medge) are in Figure S15. Interestingly, both clusters exhibits a coreperiphery structure, albeit markedly different ones. The motif-based cluster contains several core nodes with large in-degree. Such core nodes comprise the sink node in motif M7. On the periphery are several clusters within which are many bi-directional links (as illustrated by the spy plot in Figure S15). The nodes in these clusters then up-link to the core nodes. This type of organizational unit suggests an explanation for why motif M7 is over-expressed: clusters of similar pages tend to uplink to more central pages. The edge-based cluster also has a few nodes with large in-degree, serving as a small core. On the periphery are the neighbors of these nodes, which themselves tend not to be connected (as illustrated by the spy plot). S64 Figure S15: Clusters in the Stanford web graph (7). AC: Motif-based cluster (A) for motif M7 (B). The cluster has a core group of nodes with many incoming links (serving as the sink node in M7; shown in orange) and several periphery groups that are tied together (the bi-directional link in M7; shown in green) and also up-link to the core. This is evident from the spy plot (C). DF: Cluster (C) for undirected edges (B). The cluster contains a few high-degree nodes and their neighbors, and the neighbors tend to not be connected, as illustrated by the splot (F). S65 S7.6 Semi-cliques in collaboration networks We used Algorithm 1 to identify clusters of a four-node motif (the semi-clique) that has been studied in conjunction with researcher productivity in collaboration networks (72) (see Figure S16). We found a motif-based cluster in two different collaboration networks. Each one is derived from co-authorship in papers submitted to the arXiv under a certain category; here, we analyze the High Energy PhysicsTheory (HepTh) and Condensed Matter Physics (CondMat) categories (56,73). The HepTh network has 23,133 nodes and 93,497 edges and the CondMat network has 9,877 nodes and 25,998 edges. The HepTh network data was downloaded from and the CondMat network data was downloaded from html. Figure S16 shows the two clusters for each of the collaboration networks. In both networks, the motif-based cluster consists of a core group of nodes and similarly-sized groups on the periphery. The core group of nodes correspond to the nodes of degree 3 in the motif and the periphery group nodes correspond to the nodes of degree 2. One explanation for this organization is that there is a small small group of authors that writes papers with different research groups. Alternatively, the co-authorship could come from a single research group, where senior authors are included on all of the papers and junior authors on a subset of the papers. On the other hand, the edge-based clusters (i.e., result of Algorithm 1 for Medge) are a clique in the HepTh netowork and a clique with a few dangling nodes in the CondMat network. The dense clusters are quite different from the sparser clusters based on the semi-clique. Such dense clusters are not that surprising. For example, a clique could arise from a single paper published by a group of authors. S66 Figure S16: Clusters in co-authorship networks (73). AE: Best motif-based cluster for the semi-clique motif (E) in the High Energy PhysicsTheory collaboration network (A) and the Condensed Matter Physics collaboration network (C). Corresponding spy plots are shown in (B) and (D). FI: Best edge-based (I) cluster in the High Energy PhysicsTheory collaboration network (F) and the Condensed Matter Physics collaboration network (H). Corresponding spy plots are shown in (G) and (I). S67\nAll data is available at our project web site at The web site includes links to datasets used for experiments throughout the supplementary material (7, 56, 5860, 7483). S68 References and Notes 1. R. Milo, et al., Science 298, 824 (2002). 2. S. Mangan, A. Zaslaver, U. Alon, Journal of molecular biology 334, 197 (2003). 3. J. Yang, J. Leskovec, Proceedings of the IEEE 102, 1892 (2014). 4. P. W. Holland, S. Leinhardt, American Journal of Sociology pp. 492513 (1970). 5. M. Rosvall, A. V. Esquivel, A. Lancichinetti, J. D. West, R. Lambiotte, Nature communi- cations 5 (2014). 6. N. Przulj, D. G. Corneil, I. Jurisica, Bioinformatics 20, 3508 (2004). 7. J. Leskovec, K. J. Lang, A. Dasgupta, M. W. Mahoney, Internet Mathematics 6, 29 (2009). 8. O. N. Yaveroglu, et al., Scientific reports 4 (2014). 9. S. Mangan, U. Alon, Proceedings of the National Academy of Sciences 100, 11980 (2003). 10. C. J. Honey, R. Kotter, M. Breakspear, O. Sporns, Proceedings of the National Academy of Sciences 104, 10240 (2007). 11. S. E. Schaeffer, Computer Science Review 1, 27 (2007). 12. Minimizing M(S) is NP-hard, which follows from the NP-hardness of the traditional def- inition of conductance (68). 13. See the Supplementary Material. 14. Formally, when the motif has three nodes, the selected cluster S satisfies M(S)  4  M  1, where M is the smallest motif conductance of any possible node set S. This inequality is proved in the Supplementary Material. S69 15. The normalized motif Laplacian matrix is LM = D1/2(DWM)D1/2, where D is a di- agonal matrix with the row-sums of WM on the diagonal (Dii =  j(WM)ij), and D 1/2 is the same matrix with the inverse square-roots on the diagonal (D1/2ii = 1/  j(WM)ij). The spectral ordering  is the by-value ordering of D1/2z, where z is the eigenvector corresponding to the second smallest eigenvalue of LM , i.e., i is the index of D1/2z with the ith smallest value. 16. C. Seshadhri, A. Pinar, T. G. Kolda, Statistical Analysis and Data Mining: The ASA Data Science Journal 7, 294 (2014). 17. R. Andersen, F. Chung, K. Lang, Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (2006), pp. 475486. 18. J. J. Whang, I. S. Dhillon, D. F. Gleich, SIAM Data Mining (2015). 19. A. Y. Ng, M. I. Jordan, Y. Weiss, Advances in Neural Information Processing Systems 14 (2002), pp. 849856. 20. D. Boley, Data Mining and Knowledge Discovery 2, 325 (1998). 21. D. L. Riddle, T. Blumenthal, B. J. Meyer, et al., eds., C. elegans II (Cold Spring Harbor Laboratory Press, 1997), second edn. 22. H. Lee, et al., Nature neuroscience 15, 107 (2012). 23. B. J. Frey, D. Dueck, Science 315, 972 (2007). 24. B. Serrour, A. Arenas, S. Gomez, Computer Communications 34, 629 (2011). 25. T. Michoel, A. Joshi, B. Nachtergaele, Y. Van de Peer, Molecular BioSystems 7, 2769 (2011). S70 26. A. R. Benson, D. F. Gleich, J. Leskovec, SIAM Data Mining (2015). 27. F. Krzakala, et al., Proceedings of the National Academy of Sciences 110, 20935 (2013). 28. M. Kaiser, C. C. Hilgetag, PLoS Computational Biology 2, e95 (2006). 29. U. Alon, Nature Reviews Genetics 8, 450 (2007). 30. O. Sporns, R. Kotter, PLoS Biology 2, e369 (2004). 31. A. Inokuchi, T. Washio, H. Motoda, Principles of Data Mining and Knowledge Discovery (Springer, 2000), pp. 1323. 32. F. R. Chung, Proceedings of ICCM (Citeseer, 2007), vol. 2, p. 378. 33. J. R. Lee, S. O. Gharan, L. Trevisan, Journal of the ACM 61, 37 (2014). 34. F. Chung, Annals of Combinatorics 9, 1 (2005). 35. D. Boley, G. Ranjan, Z.-L. Zhang, Linear Algebra and its Applications 435, 224 (2011). 36. F. D. Malliaros, M. Vazirgiannis, Physics Reports 533, 95 (2013). 37. G. Karypis, R. Aggarwal, V. Kumar, S. Shekhar, Very Large Scale Integration (VLSI) Sys- tems, IEEE Transactions on 7, 69 (1999). 38. S. Agarwal, K. Branson, S. Belongie, Proceedings of the 23rd International Conference on Machine Learning (ACM, 2006), pp. 1724. 39. D. Zhou, J. Huang, B. Scholkopf, Advances in Neural Information Processing Systems 19 (MIT Press, 2006), pp. 16011608. 40. J. Rodrguez, Linear and Multilinear Algebra 50, 1 (2002). S71 41. L. Trevisan, Lecture notes on expansion, sparsest cut, and spectral graph theory, http:// www.eecs.berkeley.edu/luca/books/expanders.pdf. Accessed June 28, 2015. 42. S. Demeyer, et al., PloS ONE 8, e61183 (2013). 43. M. Houbraken, et al., PLoS ONE 9, e97896 (2014). 44. S. Wernicke, IEEE/ACM Transactions on Computational Biology and Bioinformatics 3, 347 (2006). 45. S. Wernicke, F. Rasche, Bioinformatics 22, 1152 (2006). 46. C. R. Aberger, A. Notzli, K. Olukotun, C. Re, arXiv preprint arXiv:1503.02368 (2015). 47. M. Latapy, Theoretical Computer Science 407, 458 (2008). 48. J. W. Berry, et al., Proceedings of the 5th Conference on Innovations in Theoretical Com- puter Science (ACM, New York, NY, USA, 2014), pp. 225234. 49. D. Marcus, Y. Shavitt, IEEE 30th International Conference on Distributed Computing Sys- tems Workshops (2010), pp. 9298. 50. N. Chiba, T. Nishizeki, SIAM Journal on Computing 14, 210 (1985). 51. T. Schank, D. Wagner, Experimental and Efficient Algorithms (Springer, 2005), pp. 606 609. 52. L. Becchetti, P. Boldi, C. Castillo, A. Gionis, Proceedings of the 14th ACM SIGKDD inter- national conference on Knowledge discovery and data mining (ACM, 2008), pp. 1624. 53. J. Cohen, Computing in Science & Engineering 11, 29 (2009). S72 54. B. N. Parlett, The Symmetric Eigenvalue Problem, vol. 7 (SIAM, 1980). 55. K. J. Maschhoff, D. C. Sorensen, Applied Parallel Computing Industrial Computation and Optimization (Springer, 1996), pp. 478486. 56. J. Leskovec, A. Krevl, SNAP Datasets: Stanford large network dataset collection, http: //snap.stanford.edu/data (2014). 57. P. Boldi, B. Codenotti, M. Santini, S. Vigna, Software: Practice and Experience 34, 711 (2004). 58. P. Boldi, S. Vigna, Proceedings of the 13th International Conference on World Wide Web (ACM, 2004), pp. 595602. 59. P. Boldi, M. Rosa, M. Santini, S. Vigna, Proceedings of the 20th International Conference on World Wide Web (ACM, 2011), pp. 587596. 60. P. Boldi, A. Marino, M. Santini, S. Vigna, Proceedings of the companion publication of the 23rd international conference on World wide web companion (International World Wide Web Conferences Steering Committee, 2014), pp. 227228. 61. A. Azad, A. Buluc, J. R. Gilbert, Proceedings of the IPDPSW, Workshop on Graph Algo- rithm Building Blocks (GABB) (2015), pp. 804811. 62. M. Rosvall, C. T. Bergstrom, Proceedings of the National Academy of Sciences 105, 1118 (2008). 63. V. D. Blondel, J.-L. Guillaume, R. Lambiotte, E. Lefebvre, Journal of statistical mechanics: theory and experiment 2008, P10008 (2008). S73 64. R. E. Ulanowicz, C. Bondavalli, M. S. Egnotovich, Trophic Dynamics in South Florida Ecosystem, FY 97: The Florida Bay Ecosystem, Tech. Rep. CBL 98-123, Chesapeake Biological Laboratory, Solomons, MD (1998). 65. J. Bascompte, C. J. Melian, E. Sala, Proceedings of the National Academy of Sciences of the United States of America 102, 5443 (2005). 66. J. Bascompte, et al., Science 325, 416 (2009). 67. D. B. Stouffer, J. Camacho, W. Jiang, L. A. N. Amaral, Proceedings of the Royal Society of London B: Biological Sciences 274, 1931 (2007). 68. D. Wagner, F. Wagner, Proceedings of the 18th International Symposium on Mathematical Foundations of Computer Science (1993), pp. 744750. 69. C. D. Manning, P. Raghavan, H. Schutze, et al., Introduction to Information Retrieval, vol. 1 (Cambridge university press Cambridge, 2008). 70. R. Dobrin, Q. K. Beg, A.-L. Barabasi, Z. N. Oltvai, BMC bioinformatics 5, 10 (2004). 71. H. Kwak, C. Lee, H. Park, S. Moon, Proceedings of the 19th International Conference on World Wide Web (ACM, 2010), pp. 591600. 72. T. Chakraborty, N. Ganguly, A. Mukherjee, Advances in Social Networks Analysis and Mining (ASONAM), 2014 IEEE/ACM International Conference on (IEEE, 2014), pp. 130 137. 73. J. Leskovec, J. Kleinberg, C. Faloutsos, ACM Transactions on Knowledge Discovery from Data (TKDD) 1, 2 (2007). S74 74. R. West, H. S. Paskov, J. Leskovec, C. Potts, Transactions of the Association for Computa- tional Linguistics 2, 297 (2014). 75. J. Leskovec, J. Kleinberg, C. Faloutsos, Proceedings of the eleventh ACM SIGKDD inter- national conference on Knowledge discovery in data mining (ACM, 2005), pp. 177187. 76. J. Gehrke, P. Ginsparg, J. Kleinberg, ACM SIGKDD Explorations Newsletter 5, 149 (2003). 77. R. Albert, H. Jeong, A.-L. Barabasi, Nature 401, 130 (1999). 78. J. Leskovec, L. A. Adamic, B. A. Huberman, ACM Transactions on the Web (TWEB) 1, 5 (2007). 79. J. Leskovec, D. P. Huttenlocher, J. M. Kleinberg, ICWSM (2010). 80. J. Leskovec, J. J. Mcauley, Advances in neural information processing systems (2012), pp. 539547. 81. L. Takac, M. Zabovsky, International Scientific Conference and International Workshop Present Day Trends of Innovations (2012), pp. 16. 82. L. Backstrom, D. Huttenlocher, J. Kleinberg, X. Lan, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (ACM, 2006), pp. 4454. 83. J. Yang, J. Leskovec, 2012 IEEE 12th International Conference on Data Mining (IEEE, 2012), pp. 745754. S75 Authors would like to thank Rok Sosic for insightful comments. ARB acknowledges the support of a Stanford Graduate Fellowship. DFG acknowledges the support of NSF CCF1149756 and IIS-1422918 and DARPA SIMPLEX. JL acknowledges the support of NSF IIS1149837 and CNS-1010921, NIH BD2K, DARPA XDATA and SIMPLEX, Boeing, Lightspeed, and Volkswagen. S76\n", "prediction": "we develop a framework for higher-order clustering of networks.<n> the framework is based on a generalized version of the notion of a motif, which is a fundamental tool for understanding complex systems in biology and social science.the framework provides mathematical guarantees on the optimality of clusters and scales with billions of edges for networks including networks and transportation networks.we also show that our framework can be used to efficiently find clusters in a wide range of network topologies including hypergraphs and hypercubes.this framework allows us to further understand the structure of higher - order clustering and to provide new insights into the behavior of complex networks in terms of their connectivity and clustering properties.keywords : network clustering ; network organization _ msc2000 subject classifications_.correspondence to the above article + rajesh r. parwani,@xmath0vahid karimipour + department of physics + sharif university of technology + p.o.box 11365 - 9161 + tehran 91167 - iran + phone:86 - 23 - 8934 - 6167 + email:r.parwani@sharif.ac.ir + web: http://www.shafonline.com/journals/ncomms/2007/136/1/ 1 ] [ cols=\"\",options=\"header \" ) we present a new framework of graph clustering in the context of eigenvectors for network clusters of graphs and graph clusters ( in which we extend the graph of cluster clusters based in graph ( e - clusters with a graph s clustering ( k - clustering clusters to a cluster of clustering with the set of an e clustering clustering cluster in network of @ @ k clusters that is the cluster ( @ 1 @ ( the clustering @ the @@ k @ a clustering graph @ and @ <n> @ = k = ( j = @(@ @ j ij = = 1 ( ( 1 = the number = j - ( = a @ - @ 2 ( b = 2 = [ @ 0 ( p = + @ + ( [ = b ( 2 @ * = 0 = 3 = number t = to @ is @= @ [ ( + = p ( 3 ( is ( a (  ( x = we is =  = e = m = * @=1 ( we = is to ( v = an @. @ p is is [ 1 is j ( i = i ( *  @ b  is number @ v ( 0! @... ( - to [ [ j. = v - j @ to j + j is 2 is  to be @[[( j and the cut = x - [ 2 to  with @ x ( number of 2 - the [  has to an j to k  in j([2 = h =... @ has @, @) @ 3 (( = and is 1 - is + + [ + to is k is b is in @_ ( and a number and [ k and ( h - k + the is - = r = 4 ( to b - b and j, [[@ j[1 ([n = c (... [... is e is of j j) is p @ m ( with j@ 2 + is an [ b. j [ is 0 is x ]. @ h is h  the ( r - e  and th = g - number ( g   for the hypergraph @<n> = se = z = w ( m @ i @ r ( w - h ( c - a  j_(v = d = one () ). @] @ e + b(s = cut "}
{"ground_truth": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.\nGenerative methods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord et al., 2016a), image-to-image translation (Zhu et al., 2017; Liu et al., 2017; Wang et al., 2017), and image inpainting (Iizuka et al., 2017). Currently the most prominent approaches are autoregressive models (van den Oord et al., 2016b;c), variational autoencoders (VAE) (Kingma & Welling, 2014), and generative adversarial networks (GAN) (Goodfellow et al., 2014). Currently they all have significant strengths and weaknesses. Autoregressive models  such as PixelCNN  produce sharp images but are slow to evaluate and do not have a latent representation as they directly model the conditional distribution over pixels, potentially limiting their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improving this (Kingma et al., 2016). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress (Salimans et al., 2016; Gulrajani et al., 2017; Berthelot et al., 2017; Kodali et al., 2017). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani & Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016). Typically, a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest  the discriminator is an adaptive loss function that gets discarded once the generator has been trained. There are multiple potential problems with this formulation. When we measure the distance between the training distribution and the generated distribution, the gradients can point to more or less random directions if the distributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky & Bottou, 2017). Originally, Jensen-Shannon divergence was used as a distance metric (Goodfellow et al., 2014), and recently that formulation has been improved (Hjelm et al., 2017) and a number of more stable alternatives have been proposed, including least squares (Mao et al., 2016b), absolute deviation with margin (Zhao et al., 2017), and Wasserstein distance (Arjovsky et al., 2017; Gulrajani ar X iv :1 71 0. 10 19 6v 3 [ cs .N E ] 2 6 Fe b 20 18 et al., 2017). Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improved Wasserstein loss, but also experiment with least-squares loss. The generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apart from training images (Odena et al., 2017), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraints, further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, starting from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section 2. The GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation, but that view has been recently challenged (Odena et al., 2017). The degree of preserved variation is currently receiving attention and various methods have been suggested for measuring it, including inception score (Salimans et al., 2016), multi-scale structural similarity (MS-SSIM) (Odena et al., 2017; Wang et al., 2003), birthday paradox (Arora & Zhang, 2017), and explicit tests for the number of discrete modes discovered (Metz et al., 2016). We will describe our method for encouraging variation in Section 3, and propose a new metric for evaluating the quality and variation in Section 5. Section 4.1 discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layers. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section 4.2). We evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets. We improve the best published inception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CELEBA dataset that allows experimentation with output resolutions up to 1024  1024 pixels. This dataset and our full implementation are available at trained networks can be found at along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at primary contribution is a training methodology for GANs where we start with low-resolution images, and then progressively increase the resolution by adding layers to the networks as visualized in Figure 1. This incremental nature allows the training to first discover large-scale structure of the image distribution and then shift attention to increasingly finer scale detail, instead of having to learn all scales simultaneously. We use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure 2. This avoids sudden shocks to the already well-trained, smaller-resolution layers. Appendix A describes structure of the generator and discriminator in detail, along with other training parameters. We observe that the progressive training has several benefits. Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes (Odena et al., 2017). By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors to e.g. 10242 images. This approach has conceptual similarity to recent work by Chen & Koltun (2017). In practice it stabilizes the training sufficiently for us to reliably synthesize megapixel-scale images using WGAN-GP loss (Gulrajani et al., 2017) and even LSGAN loss (Mao et al., 2016b). throughout the process. Here N N refers to convolutional layers operating on N  N spatial resolution. This allows stable synthesis in high resolutions and also speeds up training considerably. One the right we show six example images generated using progressive growing at 1024 1024. Another benefit is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 26 times faster, depending on the final output resolution. The idea of growing GANs progressively is related to the work of Wang et al. (2017), who use multiple discriminators that operate on different spatial resolutions. That work in turn is motivated by Durugkar et al. (2016) who use one generator and multiple discriminators concurrently, and Ghosh et al. (2017) who do the opposite with multiple generators and one discriminator. Hierarchical GANs (Denton et al., 2015; Huang et al., 2016; Zhang et al., 2017) define a generator and discriminator for each level of an image pyramid. These methods build on the same observation as our work  that the complex mapping from latents to high-resolution images is easier to learn in steps  but the crucial difference is that we have only a single GAN instead of a hierarchy of them. In contrast to early work on adaptively growing networks, e.g., growing neural gas (Fritzke, 1995) and neuro evolution of augmenting topologies (Stanley & Miikkulainen, 2002) that grow networks greedily, we simply defer the introduction of pre-configured layers. In that sense our approach resembles layer-wise training of autoencoders (Bengio et al., 2007).\nGANs have a tendency to capture only a subset of the variation found in training data, and Salimans et al. (2016) suggest minibatch discrimination as a solution. They compute feature statistics not only from individual images but also across the minibatch, thus encouraging the minibatches of generated and training images to show similar statistics. This is implemented by adding a minibatch layer towards the end of the discriminator, where the layer learns a large tensor that projects the input activation to an array of statistics. A separate set of statistics is produced for each example in a minibatch and it is concatenated to the layers output, so that the discriminator can use the statistics internally. We simplify this approach drastically while also improving the variation. Our simplified solution has neither learnable parameters nor new hyperparameters. We first compute the standard deviation for each feature in each spatial location over the minibatch. We then average these estimates over all features and spatial locations to arrive at a single value. We replicate the value and concatenate it to all spatial locations and over the minibatch, yielding one additional (constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it best to insert it towards the end (see Appendix A.1 for details). We experimented with a richer set of statistics, but were not able to improve the variation further. In parallel work, Lin et al. (2017) provide theoretical insights about the benefits of showing multiple images to the discriminator. residual block, whose weight  increases linearly from 0 to 1. Here 2 and 0.5 refer to doubling and halving the image resolution using nearest neighbor filtering and average pooling, respectively. The toRGB represents a layer that projects feature vectors to RGB colors and fromRGB does the reverse; both use 1  1 convolutions. When training the discriminator, we feed in real images that are downscaled to match the current resolution of the network. During a resolution transition, we interpolate between two resolutions of the real images, similarly to how the generator output combines two resolutions. Alternative solutions to the variation problem include unrolling the discriminator (Metz et al., 2016) to regularize its updates, and a repelling regularizer (Zhao et al., 2017) that adds a new loss term to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. The multiple generators of Ghosh et al. (2017) also serve a similar goal. We acknowledge that these solutions may increase the variation even more than our solution  or possibly be orthogonal to it  but leave a detailed comparison to a later time.\nGANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization (Ioffe & Szegedy, 2015; Salimans & Kingma, 2016; Ba et al., 2016) in the generator, and often also in the discriminator. These normalization methods were originally introduced to eliminate covariate shift. However, we have not observed that to be an issue in GANs, and thus believe that the actual need in GANs is constraining signal magnitudes and competition. We use a different approach that consists of two ingredients, neither of which include learnable parameters.\nWe deviate from the current trend of careful weight initialization, and instead use a trivial N (0, 1) initialization and then explicitly scale the weights at runtime. To be precise, we set wi = wi/c, where wi are the weights and c is the per-layer normalization constant from Hes initializer (He et al., 2015). The benefit of doing this dynamically instead of during initialization is somewhat subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent methods such as RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015). These methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. This is a scenario modern initializers cause, and thus it is possible that a learning rate is both too large and too small at the same time. Our approach ensures that the dynamic range, and thus the learning speed, is the same for all weights. A similar reasoning was independently used by van Laarhoven (2017).\nTo disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer. We do this using a variant of local response normaliza- tion (Krizhevsky et al., 2012), configured as bx,y = ax,y/  1 N N1 j=0 (a j x,y)2 + , where = 108, N is the number of feature maps, and ax,y and bx,y are the original and normalized feature vector in pixel (x, y), respectively. We find it surprising that this heavy-handed constraint does not seem to harm the generator in any way, and indeed with most datasets it does not change the results much, but it prevents the escalation of signal magnitudes very effectively when needed.\nIn order to compare the results of one GAN to another, one needs to investigate a large number of images, which can be tedious, difficult, and subjective. Thus it is desirable to rely on automated methods that compute some indicative metric from large image collections. We noticed that existing methods such as MS-SSIM (Odena et al., 2017) find large-scale mode collapses reliably but fail to react to smaller effects such as loss of variation in colors or textures, and they also do not directly assess image quality in terms of similarity to the training set. We build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid (Burt & Adelson, 1987) representations of generated and target images, starting at a low-pass resolution of 16  16 pixels. As per standard practice, the pyramid progressively doubles until the full resolution is reached, each successive level encoding the difference to an up-sampled version of the previous level. A single Laplacian pyramid level corresponds to a specific spatial frequency band. We randomly sample 16384 images and extract 128 descriptors from each level in the Laplacian pyramid, giving us 221 (2.1M) descriptors per level. Each descriptor is a 7  7 pixel neighborhood with 3 color channels, denoted by x  R773 = R147. We denote the patches from level l of the training set and generated set as {xli}2 21 i=1 and {yli}2 21 i=1, respectively. We first normalize {xli} and {yli} w.r.t. the mean and standard deviation of each color channel, and then estimate the statistical similarity by computing their sliced Wasserstein distance SWD({xli}, {yli}), an efficiently computable randomized approximation to earthmovers distance, using 512 projections (Rabin et al., 2011). Intuitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowestresolution 16  16 images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise.\nIn this section we discuss a set of experiments that we conducted to evaluate the quality of our results. Please refer to Appendix A for detailed description of our network structures and training configurations. We also invite the reader to consult the accompanying video for additional result images and latent space interpolations. In this section we will distinguish between the network structure (e.g., convolutional layers, resizing), training configuration (various normalization layers, minibatch-related operations), and training loss (WGAN-GP, LSGAN).\nWe will first use the sliced Wasserstein distance (SWD) and multi-scale structural similarity (MSSSIM) (Odena et al., 2017) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-theart loss function (WGAN-GP) and training configuration (Gulrajani et al., 2017) in an unsupervised setting using CELEBA (Liu et al., 2015) and LSUN BEDROOM (Yu et al., 2015) datasets in 1282 resolution. CELEBA is particularly well suited for such comparison because the training images contain noticeable artifacts (aliasing, compression, blur) that are difficult for the generator to reproduce faithfully. In this test we amplify the differences between training configurations by choosing a relatively low-capacity network structure (Appendix A.2) and terminating the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged. Table 1 lists the numerical values for SWD and MS-SSIM in several training configurations, where our individual contributions are cumulatively enabled one by one on top of the baseline (Gulrajani et al., 2017). The MS-SSIM numbers were averaged from 10000 pairs of generated images, and SWD was calculated as described in Section 5. Generated CELEBA images from these configurations are shown in Figure 3. Due to space constraints, the figure shows only a small number of examples for each row of the table, but a significantly broader set is available in Appendix H. Intuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in colors, textures, and viewpoints. However, this is not captured by MS-SSIM: we can immediately see that configuration (h) generates significantly better images than configuration (a), but MS-SSIM remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does indicate a clear improvement. The first training configuration (a) corresponds to Gulrajani et al. (2017), featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables progressive growing of the networks, which results in sharper and more believable output images. SWD correctly finds the distribution of generated images to be more similar to the training set. Our primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in (c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix A.2). As an intermediate test (e), we enable minibatch discrimination (Salimans et al., 2016), which somewhat surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation. In contrast, our minibatch standard deviation (e) improves the average SWD scores and images. We then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD and subjective visual quality. Finally, in (h) we use a non-crippled network and longer training  we feel the quality of the generated images is at least comparable to the best published results so far.\nFigure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput. The first two plots correspond to the training configuration of Gulrajani et al. (2017) without and with progressive growing. We observe that the progressive variant offers two main benefits: it converges to a considerably better optimum and also reduces the total training time by about a factor of two. The improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity. Without progressive growing, all layers of the generator and discriminator are tasked with simultaneously finding succinct intermediate representations for both the large-scale variation and the small-scale detail. With progressive growing, however, the existing low-resolution layers are likely to have already converged early on, so the networks are only tasked with refining the representations by increasingly smaller-scale effects as new layers are introduced. Indeed, we see in Figure 4(b) that the largest-scale statistical similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the rest of the training. The smaller-scale curves (32, 64, 128) level off one by one as the resolution is increased, but the convergence of each curve is equally consistent. With non-progressive training in Figure 4(a), each scale of the SWD metric converges roughly in unison, as could be expected. The speedup from progressive growing increases as the output resolution grows. Figure 4(c) shows training progress, measured in number of real images shown to the discriminator, as a function of training time when the training progresses all the way to 10242 resolution. We see that progressive growing gains a significant head start because the networks are shallow and quick to evaluate at the beginning. Once the full resolution is reached, the image throughput is equal between the two methods. The plot shows that the progressive variant reaches approximately 6.4 million images in 96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours to reach the same point. In this case, the progressive growing offers roughly a 5.4 speedup.\nTo meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset. However, virtually all publicly available datasets previously used in GAN literature are limited to relatively low resolutions ranging from 322 to 4802. To this end, we created a high-quality version of the CELEBA dataset consisting of 30000 of the images at 1024  1024 resolution. We refer to Appendix C for further details about the generation of this dataset. Our contributions allow us to deal with high output resolutions in a robust and efficient fashion. Figure 5 shows selected 1024  1024 images produced by our network. While megapixel GAN results have been shown before in another dataset (Marchesi, 2017), our results are vastly more varied and of higher perceptual quality. Please refer to Appendix F for a larger set of result images as well as the nearest neighbors found from the training data. The accompanying video shows latent space interpolations and visualizes the progressive training. The interpolation works so that we first randomize a latent code for each frame (512 components sampled individually from N (0, 1)), then blur the latents across time with a Gaussian ( = 45 frames @ 60Hz), and finally normalize each vector to lie on a hypersphere. We trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed qualitative differences between the results of consecutive training iterations. Our implementation used an adaptive minibatch size depending on the current output resolution so that the available memory budget was optimally utilized. In order to demonstrate that our contributions are largely orthogonal to the choice of a loss function, we also trained the same network using LSGAN loss instead of WGAN-GP loss. Figure 1 shows six examples of 10242 images produced using our method using LSGAN. Further details of this setup are given in Appendix B.\nFigure 6 shows a purely visual comparison between our solution and earlier results in LSUN BEDROOM. Figure 7 gives selected examples from seven very different LSUN categories at 2562. A larger, non-curated set of results from all 30 LSUN categories is available in Appendix G, and the video demonstrates interpolations. We are not aware of earlier results in most of these categories, and while some categories work better than others, we feel that the overall quality is high.\nThe best inception scores for CIFAR10 (10 categories of 32  32 RGB images) we are aware of are 7.90 for unsupervised and 8.87 for label conditioned setups (Grinblat et al., 2017). The large difference between the two numbers is primarily caused by ghosts that necessarily appear between classes in the unsupervised setting, while label conditioning can remove many such transitions. When all of our contributions are enabled, we get 8.80 in the unsupervised setting. Appendix D shows a representative set of generated images along with a more comprehensive list of results from earlier methods. The network and training setup were the same as for CELEBA, progression limited to 32  32 of course. The only customization was to the WGAN-GPs regularization term ExPx [(||xD(x)||2  )2/2]. Gulrajani et al. (2017) used  = 1.0, which corresponds to 1-Lipschitz, but we noticed that it is in fact significantly better to prefer fast transitions ( = 750) to minimize the ghosts. We have not tried this trick with other datasets.\nWhile the quality of our results is generally high compared to earlier work on GANs, and the training is stable in large resolutions, there is a long way to true photorealism. Semantic sensibility and understanding dataset-dependent constraints, such as certain objects being straight rather than curved, leaves a lot to be desired. There is also room for improvement in the micro-structure of the images. That said, we feel that convincing realism may now be within reach, especially in CELEBA-HQ.\nWe would like to thank Mikael Honkavaara, Tero Kuosmanen, and Timi Hietanen for the compute infrastructure. Dmitry Korobchenko and Richard Calderwood for efforts related to the CELEBA-HQ dataset. Oskar Elek, Jacob Munkberg, and Jon Hasselgren for useful comments.\nA.1 1024 1024 NETWORKS USED FOR CELEBA-HQ Table 2 shows network architectures of the full-resolution generator and discriminator that we use with the CELEBA-HQ dataset. Both networks consist mainly of replicated 3-layer blocks that we introduce one by one during the course of the training. The last Conv 1  1 layer of the generator corresponds to the toRGB block in Figure 2, and the first Conv 1  1 layer of the discriminator similarly corresponds to fromRGB. We start with 4  4 resolution and train the networks until we have shown the discriminator 800k real images in total. We then alternate between two phases: fade in the first 3-layer block during the next 800k images, stabilize the networks for 800k images, fade in the next 3-layer block during 800k images, etc. Our latent vectors correspond to random points on a 512-dimensional hypersphere, and we represent training and generated images in [-1,1]. We use leaky ReLU with leakiness 0.2 in all layers of both networks, except for the last layer that uses linear activation. We do not employ batch normalization, layer normalization, or weight normalization in either network, but we perform pixelwise normalization of the feature vectors after each Conv 33 layer in the generator as described in Section 4.2. We initialize all bias parameters to zero and all weights according to the normal distribution with unit variance. However, we scale the weights with a layer-specific constant at runtime as described in Section 4.1. We inject the across-minibatch standard deviation as an additional feature map at 4 4 resolution toward the end of the discriminator as described in Section 3. The upsampling and downsampling operations in Table 2 correspond to 2  2 element replication and average pooling, respectively. We train the networks using Adam (Kingma & Ba, 2015) with  = 0.001, 1 = 0, 2 = 0.99, and = 108. We do not use any learning rate decay or rampdown, but for visualizing generator output at any given point during the training, we use an exponential running average for the weights of the generator with decay 0.999. We use a minibatch size 16 for resolutions 421282 and then gradually decrease the size according to 2562  14, 5122  6, and 10242  3 to avoid exceeding the available memory budget. We use the WGAN-GP loss, but unlike Gulrajani et al. (2017), we alternate between optimizing the generator and discriminator on a per-minibatch basis, i.e., we set ncritic = 1. Additionally, we introduce a fourth term into the discriminator loss with an extremely small weight to keep the discriminator output from drifting too far away from zero. To be precise, we set L = L+ driftExPr [D(x)2], where drift = 0.001. A.2 OTHER NETWORKS Whenever we need to operate on a spatial resolution lower than 1024 1024, we do that by leaving out an appropriate number copies of the replicated 3-layer block in both networks. Furthermore, Section 6.1 uses a slightly lower-capacity version, where we halve the number of feature maps in Conv 3  3 layers at the 16  16 resolution, and divide by 4 in the subsequent resolutions. This leaves 32 feature maps to the last Conv 3  3 layers. In Table 1 and Figure 4 we train each resolution for a total 600k images instead of 800k, and also fade in new layers for the duration of 600k images. For the Gulrajani et al. (2017) case in Table 1, we follow their training configuration as closely as possible. In particular, we set  = 0.0001, 2 = 0.9, ncritic = 5, drift = 0, and minibatch size 64. We disable progressive resolution, minibatch stddev, as well as weight scaling at runtime, and initialize all weights using Hes initializer (He et al., 2015). Furthermore, we modify the generator by replacing LReLU with ReLU, linear activation with tanh in the last layer, and pixelwise normalization with batch normalization. In the discriminator, we add layer normalization to all Conv 3 3 and Conv 4 4 layers. For the latent vectors, we use 128 components sampled independently from the normal distribution. B LEAST-SQUARES GAN (LSGAN) AT 1024 1024 We find that LSGAN is generally a less stable loss function than WGAN-GP, and it also has a tendency to lose some of the variation towards the end of long runs. Thus we prefer WGAN-GP, but have also produced high-resolution images by building on top of LSGAN. For example, the 10242 images in Figure 1 are LSGAN-based. On top of the techniques described in Sections 24, we need one additional hack with LSGAN that prevents the training from spiraling out of control when the dataset is too easy for the discriminator, and the discriminator gradients are at risk of becoming meaningless as a result. We adaptively increase the magnitude of multiplicative Gaussian noise in discriminator as a function of the discriminators output. The noise is applied to the input of each Conv 3  3 and Conv 4  4 layer. There is a long history of adding noise to the discriminator, and it is generally detrimental for the image quality (Arjovsky et al., 2017) and ideally one would never have to do that, which according to our tests is the case for WGAN-GP (Gulrajani et al., 2017). The magnitude of noise is determined as 0.2  max(0, dt  0.5)2, where dt = 0.1d + 0.9dt1 is an exponential moving average of the discriminator output d. The motivation behind this hack is that LSGAN is seriously unstable when d approaches (or exceeds) 1.0.\nIn this section we describe the process we used to create the high-quality version of the CELEBA dataset, consisting of 30000 images in 1024  1024 resolution. As a starting point, we took the collection of in-the-wild images included as a part of the original CELEBA dataset. These images are extremely varied in terms of resolution and visual quality, ranging all the way from 43  55 to 6732  8984. Some of them show crowds of several people whereas others focus on the face of a single person  often only a part of the face. Thus, we found it necessary to apply several image processing steps to ensure consistent quality and to center the images on the facial region. Our processing pipeline is illustrated in Figure 8. To improve the overall image quality, we preprocess each JPEG image using two pre-trained neural networks: a convolutional autoencoder trained to remove JPEG artifacts in natural images, similar in structure to the proposed by Mao et al. (2016a), and an adversarially-trained 4x super-resolution network (Korobchenko & Foco, 2017) similar to Ledig et al. (2016). To handle cases where the facial region extends outside the image, we employ padding and filtering to extend the dimensions of the image as illustrated in Fig.8(cd). We then select an oriented crop rectangle based on the facial landmark annotations included in the original CELEBA dataset as follows: x = e1  e0 y = 1 2 (e0 + e1) 1 2 (m0 +m1) c = 1 2 (e0 + e1) 0.1  y s = max (4.0  |x|, 3.6  |y|) x = Normalize (x  Rotate90(y)) y = Rotate90(x) e0, e1, m0, and m1 represent the 2D pixel locations of the two eye landmarks and two mouth landmarks, respectively, c and s indicate the center and size of the desired crop rectangle, and x and y indicate its orientation. We constructed the above formulas empirically to ensure that the crop rectangle stays consistent in cases where the face is viewed from different angles. Once we have calculated the crop rectangle, we transform the rectangle to 4096 4096 pixels using bilinear filtering, and then scale it to 1024 1024 resolution using a box filter. We perform the above processing for all 202599 images in the dataset, analyze the resulting 1024 1024 images further to estimate the final image quality, sort the images accordingly, and discard all but the best 30000 images. We use a frequency-based quality metric that favors images whose power spectrum contains a broad range of frequencies and is approximately radially symmetric. This penalizes blurry images as well as images that have conspicuous directional features due to, e.g., visible halftoning patterns. We selected the cutoff point of 30000 images as a practical sweet spot between variation and image quality, because it appeared to yield the best results.\nFigure 9 shows non-curated images generated in the unsupervised setting, and Table 3 compares against prior art in terms of inception scores. We report our scores in two different ways: 1) the highest score observed during training runs (here  refers to the standard deviation returned by the inception score calculator) and 2) the mean and standard deviation computed from the highest scores seen during training, starting from ten random initializations. Arguably the latter methodology is much more meaningful as one can be lucky with individual runs (as we were). We did not use any kind of augmentation with this dataset.\nMetz et al. (2016) describe a setup where a generator synthesizes MNIST digits simultaneously to 3 color channels, the digits are classified using a pre-trained classifier (0.4% error rate in our case), and concatenated to form a number in [0, 999]. They generate a total of 25,600 images and count how many of the discrete modes are covered. They also compute KL divergence as KL(histogram || uniform). Modern GAN implementations can trivially cover all modes at very low divergence (0.05 in our case), and thus Metz et al. specify a fairly low-capacity generator and two severely crippled discriminators (K/2 has  2000 params and K/4 only about  500) to tease out differences between training methodologies. Both of these networks use batch normalization. As shown in Table 4, using WGAN-GP loss with the networks specified by Metz et al. covers much more modes than the original GAN loss, and even more than the unrolled original GAN with the smaller (K/4) discriminator. The KL divergence, which is arguably a more accurate metric than the raw count, acts even more favorably. Replacing batch normalization with our normalization (equalized learning rate, pixelwise normalization) improves the result considerably, while also removing a few trainable parameters from the discriminators. The addition of a minibatch stddev layer further improves the scores, while restoring the discriminator capacity to within 0.5% of the original. Progression does not help much with these tiny images, but it does not hurt either.\nFigure 10 shows the nearest neighbors found for our generated images. Figure 11 gives additional generated examples from CELEBA-HQ. We enabled mirror augmentation for all tests using CELEBA and CELEBA-HQ. In addition to the sliced Wasserstein distance (SWD), we also quote the recently introduced Frechet Inception Distance (FID) (Heusel et al., 2017) computed from 50K images.\nFigures 1217 show representative images generated for all 30 LSUN categories. A separate network was trained for each category using identical parameters. All categories were trained using 100k images, except for BEDROOM and DOG that used all the available data. Since 100k images is a very limited amount of training data for most categories, we enabled mirror augmentation in these tests (but not for BEDROOM or DOG).\nFigure 18 shows larger collections of images corresponding to the non-converged setups in Table 1. The training time was intentionally limited to make the differences between various methods more visible.\n", "prediction": "in this paper, we propose a new methodology for discriminators that are adaptive to the spatial structure of images.<n> our method is based on a gradient - based adaptation of the generative model of an image to its local density of states ( ldos ) and local spatial and temporal structure ; we call this mechanism _ adaptive spatial disciminator_. we show that this method can achieve state - of - the - art results for high - resolution images ( up to @xmath0 pixels in size and with no loss in signal - to - noise ratio ( snr))@xcite and that it is robust to low - frequency noise ( down to a few hz in the worst case of low resolution and to several tens of mhz in high resolution : see the paper by wang and tanaka [ 2016 ] for more information on this approach and the associated software <n> see also the co - ordinates of our lab paper at https://github.com/sequential/adaptive-spatial-disciser/ [ sec_abstract]the field of image processing has seen rapid progress in recent years ( see for example the seminal papers of ( ssang & nguyen ( 2012 ), wu et al.(2013,2014)and the authors of arxiv:1502.04144v2 [ http://arxiv.org/abs / thms/1512.1 ( 2014 - 2.2 - 3.3 - 4.4 - 5. 4th - 6. 5 - 7. 6 - 8. 8 - 9. 9 -4. 2 - 10. 7 -3. 3 - 11. 1 -6 -2 -5 - 1 - a. in ( 2 -8. 10 -7. ( in 4 in 2 2 ( to 2 in 6 to 6 ( ( 6 for 2 to 8 to 1 in 1 to 4 to 5 to 7 to 3 to see 2 and 2 for the 4 ( 1 ( 3 ( the 2 1 for 4  2 [ [ 6 [ 1 [ to [ 2 @ 1 and 3 in 3 [ 3 for a 4 [ 8 [ 4 for 6 in [ ( 4 and 6  to ( a [ 9 [ 5 [ @ @ 2<n> [ a  2 while to do the first ( [ in a ( @ [ and we do ( while the [ for [ al t ( for ( we also to add the analysis for all to be the @ ( and [ the results to  ( i. [ e. to find the generator ( j.  for  in @ a while [  [ see ( e ])<n> to we have to increase the coefficients for p ( p. while we are the second ( but the matrix for an [ as we were the most [... [[[(([e (<n> in ], while in p ].<n> ( as the ( c. and @. @[2 ( x-[n ( it was the]) and a @ and also [. for it  until the... to... ( analysis of [ x.. as a to x and [i and is the distribution of @<n> for which is to an analysis ( al (... for @... and it for x - [<n> @] and<n> and... in x[x and for this  ).<n> we find [ which [ it - @  as []. to make the probability for... ]] and an @ - in which to p and additionally to which  while a) for<n> the]. [ we refer to one ( that is also ... until we found the] "}
{"ground_truth": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.\nConvolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsitythat each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharingthat the same filter is applied to each patch. However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6]. In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7]. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm. In this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying Computer Science Department, Stanford University, Stanford, CA 94305. Email: zhangyuc@cs.stanford.edu. Computer Science Department, Stanford University, Stanford, CA 94305. Email: pliang@cs.stanford.edu. Department of Electrical Engineering and Computer Science and Department of Statistics, University of California Berkeley, Berkeley, CA 94720. Email: wainwrig@eecs.berkeley.edu. ar X iv :1 60 9. 01 00 0v 1 [ cs .L G ] 4 S ep 2 01 two-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Further relaxing the low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs. On the theoretical front, we prove an oracle inequality on generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite dataa quantity to which we refer as the oracle riskplus a model complexity term that decays to zero polynomially in the sample size. Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. We then apply CCNNs to the MNIST handwritten digit dataset as well as four variation datasets [43], and find that it achieves the state-of-the-art performance. On the CIFAR-10 dataset, CCNNs outperform CNNs of the same depths, as well as other baseline methods that do not involve nonconvex optimization. We also demonstrate that building CCNNs on top of existing CNN filters improves the performance of CNNs. The remainder of this paper is organized as follows. We begin in Section 2 by introducing convolutional neural networks, and setting up the empirical risk minimization problem studied in this paper. In Section 3, we describe the algorithm for learning two-layer CCNNs, beginning with the simple case of convexifying CNNs with a linear activation function, then proceeding to convexify CNNs with a nonlinear activation. We show that the generalization error of a CCNN converges to that of the best possible CNN. In Section 4, we describe several extensions to the basic CCNN algorithm, including averaging pooling, multi-channel input processing, and the layer-wise learning of multi-layer CNNs. In Section 5, we report the empirical evaluations of CCNNs. We survey related work in Section 6 and conclude the paper in Section 7. Notation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let A be its nuclear norm, A2 be its spectral norm (i.e., maximal singular value), and AF be its Frobenius norm. We use `2(N) to denote the set of countable dimensional vectors v = (v1, v2, . . . ) such that  `=1 v 2 ` < . For any vectors u, v  `2(N), the inner product u, v :=  `=1 uivi and the `2-norm u2 :=  u, u are well defined.\nIn this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem.\nAt a high level, a two-layer CNN1 is a particular type of function that maps an input vector x  Rd0 (e.g., an image) to an output vector in y  Rd2 (e.g., classification scores for the d2 classes). This mapping is formed in the following manner:  First, we extract a collection of P vectors {zp(x)}Pj=1 of the full input vector x. Each vector zp(x)  Rd1 is referred to as a patch, and these patches may depend on overlapping components of x.  Second, given some choice of activation function  : R  R and a collection of weight vectors {wj}rj=1 in Rd1 , we compute the functions hj(z) := (w > j z) for each patch z  Rd1 . (1) Each function hj (for j  [r]) is known as a filter, and note that the same filters are applied to each patchthis corresponds to the parameter sharing of a CNN.  Third, for each patch index p  [P ], filter index j  [r], and output coordinate k  [d2], we introduce a coefficient k,j,p  R that governs the contribution of the filter hj on patch zp(x) to output fk(x). The final form of the CNN is given by f(x) : = (f1(x), . . . , fd2(x)), where the k th component is given by fk(x) := r j=1 P p=1 k,j,phj(zp(x)). (2) Taking the patch functions {zp}Pp=1 and activation function  as fixed, the parameters of the CNN are the filter vectors w := {wj  Rd1 : j  [r]} along with the collection of coefficient vectors  := {k,j  RP : k  [d2], j  [r]}. We assume that all patch vectors zp(x)  Rd1 are contained in the unit `2-ball. This assumption can be satisfied without loss of generality by normalization: By multiplying a constant  > 0 to every patch zp(x) and multiplying 1/ to the filter vectors w, the assumption will be satisfied without changing the the output of the network. Given some positive radii B1 and B2, we consider the model class Fcnn(B1, B2) := { f of the form (2) : max j[r] wj2  B1 and max k[d2],j[r] k,j2  B2 } . (3) When the radii (B1, B2) are clear from context, we adopt Fcnn as a convenient shorthand.\nGiven an input-output pair (x, y) and a CNN f , we let L(f(x); y) denote the loss incurred when the output y is predicted via f(x). We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument. As a concrete example, for multiclass classification with d2 classes, the output vector y takes values in the discrete set [d2] = {1, 2, . . . , d2}. 1Average pooling and multiple channels are also an integral part of CNNs, but these do not present any new technical challenges, so that we defer these extensions to Section 4. For example, given a vector f(x) = (f1(x), . . . , fd2(y))  Rd2 of classification scores, the associated multiclass logistic loss for a pair (x, y) is given by L(f(x); y) := fy(x) + log (d2 y=1 exp(fy(x)) ) . Given n training examples {(xi, yi)}ni=1, we would like to compute an empirical risk minimizer fcnn  arg min fFcnn n i=1 L(f(xi); yi). (4) Recalling that functions f  Fcnn depend on the parameters w and  in a highly nonlinear way (2), this optimization problem is nonconvex. As mentioned earlier, heuristics based on stochastic gradient methods are used in practice, which makes it challenging to gain a theoretical understanding of their behavior. Thus, in the next section, we describe a relaxation of the class Fcnn that allows us to obtain a convex formulation of the associated empirical risk minimization problem.\nWe now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function. Although the linear case is not of practical interest, it provides intuition for our more general convexification procedure, described in Section 3.2, which applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space (RKHS) allows us to again reduce to the linear setting.\nIn order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function (t) = t. In this case, the filter hj when applied to the patch vector zp(x) outputs a Euclidean inner product of the form hj(zp(x)) = zp(x), wj. For each x  Rd0 , we first define the P  d1-dimensional matrix Z(x) := z1(x) > ... zP (x) >  . (5) We also define the P -dimensional vector k,j := (k,j,1, . . . , k,j,P ) >. With this notation, we can rewrite equation (2) for the kth output as fk(x) = r j=1 P p=1 k,j,pzp(x), wj = r j=1 >k,jZ(x)wj = tr ( Z(x) ( r j=1 wj > k,j )) = tr(Z(x)Ak), (6) where in the final step, we have defined the d1P -dimensional matrix Ak := r j=1wj > k,j . Observe that fk now depends linearly on the matrix parameter Ak. Moreover, the matrix Ak has rank at most r, due to the parameter sharing of CNNs. See Figure 1 for a graphical illustration of this model structure. Letting A := (A1, . . . , Ad2) be a concatenation of these matrices across all d2 output coordinates, we can then define a function fA : Rd1  Rd2 of the form fA(x) := (tr(Z(x)A1), . . . , tr(Z(x)Ad2)). (7) Note that these functions have a linear parameterization in terms of the underlying matrix A. Our model class corresponds to a collection of such functions based on imposing certain constraints on the underlying matrix A: in particular, we define Fcnn(B1, B2) := { fA : max j[r] wj2  B1 and max k[d2] j[r] k,j2  B2    Constraint (C1) and rank(A) = r   Constraint (C2) } . This is simply an alternative formulation of our original class of CNNs. Notice that if the filter weights wj are not shared across all patches, then the constraint (C1) still holds, but constraint (C2) no longer holds. Thus, the parameter sharing of CNNs is realized by the low-rank constraint (C2). The matrix A of rank r can be decomposed as A = UV >, where both U and V have r columns. The column space of matrix A contains the convolution parameters {wj}, and the row space of A contains to the output parameters {k,j}. The rank-r matrices satisfying constraints (C1) and (C2) form a nonconvex set. A standard convex relaxation of a rank constraint is based on the nuclear norm A corresponding to the sum of the singular values of A. It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as A  B1B2r  d2. Consequently, if we define the function class Fccnn := { fA : A  B1B2r  d2 } , (8) then we are guaranteed that Fccnn  Fcnn. Overall, we propose to minimize the empirical risk (4) over Fccnn instead of Fcnn; doing so defines a convex optimization problem over this richer class of functions fccnn := arg min fAFccnn n i=1 L(fA(xi); yi). (9) In Section 3.3, we describe iterative algorithms that can be used to solve this form of convex program in the more general setting of nonlinear activation functions.\nFor nonlinear activation functions , we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). As we show, this relaxation allows us to reduce the problem to the linear activation case. Let K : Rd1  Rd1  R be a positive semidefinite kernel function. For particular choices of kernels (e.g., the Gaussian RBF kernel) and some sufficiently smooth activation function , we are able to show that the filter h : z 7 (w, z) is contained in the RKHS induced by the kernel function K. See Section 3.4 for the choice of the kernel function and the activation function. Let S := {zp(xi) : p  [P ], i  [n]} be the set of patches in the training dataset. The representer theorem then implies that for any patch zp(xi)  S, the function value can be represented by h(zp(xi)) =  (i,p)[n][P ] ci,pk(zp(xi), zp(xi)) (10) for some coefficients {ci,p}(i,p)[n][P ]. Filters taking the form (10) are members of the RKHS, because they are linear combinations of basis functions z 7 k(z, zp(xi)). Such filters are parametrized by a finite set of coefficients, which can be estimated via empirical risk minimization. Let K  RnPnP be the symmetric kernel matrix, where with rows and columns indexed by the example-patch index pair (i, p)  [n] [P ]. The entry at row (i, p) and column (i, p) of matrix K is equal to K(zp(xi), zp(xi)). So as to avoid re-deriving everything in the kernelized setting, we perform a reduction to the linear setting of Section 3.1. Consider a factorization K = QQ> of the kernel matrix, where Q  RnPm; one example is the Cholesky factorization with m = nP . We can interpret each row Q(i,p)  Rm as a feature vector in place of the original zp(xi)  Rd1 , and rewrite equation (10) as h(zp(xi)) = Q(i,p), w where w :=  (i,p) ci,pQ(i,p). In order to learn the filter h, it suffices to learn the m-dimensional vector w. To do this, define patch matrices Z(xi)  RPm for each i  [n] so that its p-th row is Q(i,p). Then we carry out all of Section 3.1; solving the ERM gives us a parameter matrix A  RmPd2 . The only difference is that the B1 norm constraint needs to be relaxed as well. See Appendix B for details. At test time, given a new input x  Rd0 , we can compute a patch matrix Z(x)  RPm as follows:  The p-th row of this matrix is the feature vector for patch p, which is equal to Qv(zp(x))  Rm. Here, for any patch z, the vector v(z) is defined as a nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). We note that if x is an instance xi in the training set, then the vector Qv(zp(x)) is exactly equal to Q(i,p). Thus the mapping Z(x) applies to both training and testing. Algorithm 1: Learning two-layer CCNNs Input: Data {(xi, yi)}ni=1, kernel function K, regularization parameter R > 0, number of filters r. 1. Construct a kernel matrix K  RnPnP such that the entry at column (i, p) and row (i, p) is equal to K(zp(xi), zp(xi)). Compute a factorization K = QQ> or an approximation K  QQ>, where Q  RnPm. 2. For each xi, construct patch matrix Z(xi)  RPm whose p-th row is the (i, p)-th row of Q, where Z() is defined in Section 3.2. 3. Solve the following optimization problem to obtain a matrix A = (A1, . . . , Ad2): A  argmin AR L(A) where L(A) := n i=1 L (( tr(Z(xi)A1), . . . , tr(Z(xi)Ad2) ) ; yi ) . (12) 4. Compute a rank-r approximation A  U V > where U  Rmr and V  RPd2r. Output: Return the predictor fccnn(x) := ( tr(Z(x)A1), . . . , tr(Z(x)Ad2) ) and the convolutional layer output H(x) := U>(Z(x))>.  We can then compute the predictor fk(x) = tr(Z(x)Ak) via equation (6). Note that we do not explicitly need to compute the filter values hj(zp(x)) to compute the output under the CCNN. Retrieving filters. However, when we learn multi-layer CCNNs, we need to compute the filters explicitly. Recall from Section 3.1 that the column space of matrix A corresponds to parameters of the convolutional layer, and the row space of A corresponds to parameters of the output layer. Thus, once we obtain the parameter matrix A, we compute a rank-r approximation A  U V >. Then set the j-th filter hj to the mapping z 7 Uj , Qv(z) for any patch z  Rd1 , (11) where Uj  Rm is the j-th column of matrix U , and Qv(z) represents the feature vector for patch z. The matrix V > encodes parameters of the output layer, thus doesnt appear in the filter expression (11). It is important to note that the filter retrieval is not unique, because the rankr approximation of the matrix A is not unique. One feasible way is to form the singular value decomposition A = UV >, then define U to be the first r columns of U , and define V > to be the first r rows of V >. When we apply all of the r filters to all patches of an input x  Rd0 , the resulting output is H(x) := U>(Z(x))>  this is an r  P matrix whose element at row j and column p is equal to hj(zp(x)).\nThe algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2. In order to solve the optimization problem (12), the simplest approach is to via projected gradient descent: At iteration t, using a step size t > 0, it forms the new matrix At+1 based on the previous iterate At according to: At+1 = R ( At  t AL(At) ) . (13) Here AL denotes the gradient of the objective function defined in (12), and R denotes the Euclidean projection onto the nuclear norm ball {A : A  R}. This nuclear norm projection can be obtained by first computing the singular value decomposition of A, and then projecting the vector of singular values onto the `1-ball. This latter projection step can be carried out efficiently by the algorithm of Duchi et al. [16]. There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46]. All these algorithms can be executed in a stochastic fashion, so that each gradient step processes a mini-batch of examples. The computational complexity of each iteration depends on the width m of the matrix Q. Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystrom approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q  RnPm such that K  QQ>. Typically, the parameter m is chosen to be much smaller than nP . In order to compute the matrix Q, the Nystrom approximation method takes O(m2nP ) time. The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27]. The complexity of computing a gradient vector on a batch of b images is O(mPd2b). The complexity of projecting the parameter matrix onto the nuclear norm ball is O(min{m2Pd2,mP 2d22}). Thus, the approximate algorithms provide substantial speed-ups on the projected gradient descent steps.\nIn this section, we upper bound the generalization error of Algorithm 1, proving that it converges to the best possible generalization error of CNN. We focus on the binary classification case where the output dimension is d2 = 1. 2 The learning of CCNN requires a kernel function K. We consider kernel functions whose associated RKHS is large enough to contain any function taking the following form: z 7 q(w, z), where q is an arbitrary polynomial function and w  Rd1 is an arbitrary vector. As a concrete example, we consider the inverse polynomial kernel: K(z, z) := 1 2 z, z , z2  1, z2  1. (14) This kernel was studied by Shalev-Shwartz et al. [36] for learning halfspaces, and by Zhang et al. [48] for learning fully-connected neural networks. We also consider the Gaussian RBF kernel: K(z, z) := exp(z  z22), z2 = z2 = 1,  > 0. (15) As we show in Appendix A, the inverse polynomial kernel and the Gaussian kernel satisfy the above notion of richness. We focus on these two kernels for the theoretical analysis. 2We can treat the multiclass case by performing a standard one-versus-all reduction to the binary case. Let fccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above. Our main theoretical result is that for suitably chosen activation functions, the generalization error of fccnn is comparable to that of the best CNN model. In particular, the following theorem applies to activation functions  of the following types: (a) arbitrary polynomial functions (e.g., used by [10, 29]). (b) sinusoid activation function (t) := sin(t) (e.g., used by [39, 22]). (c) erf function erf(t) := 2/    t 0 e z2dz, which represents an approximation to the sigmoid function (See Figure 2(a)). (d) a smoothed hinge loss sh(t) :=  t  1 2(erf(z) + 1)dz, which represents an approximation to the ReLU function (See Figure 2(b)). To understand why these activation functions pair with our choice of kernels, we consider polynomial expansions of the above activation functions: (t) =  j=0 ajt j , and note that the smoothness of these functions are characterized by the rate of their coefficients {aj}j=0 converging to zero. If  is a polynomial in category (a), then the richness of the RKHS guarantees that it contains the class of filters activated by function . If  is a non-polynomial function in categories (b),(c),(d), then as Appendix A shows, the RKHS contains the filter only if the coefficients {aj}j=0 converge quickly enough to zero (the criterion depends on the choice of the kernel). Concretely, the inverse polynomial kernel is shown to capture all of the four categories of activations: they are referred as valid activation functions for the inverse polynomial kernel. The Gaussian kernel induces a smaller RKHS, and is shown to capture categories (a),(b), so that these functions are referred as valid activation functions for the Gaussian kernel. In contrast, the sigmoid function and the ReLU function are not valid for either kernel, because their polynomial expansions fail to converge quickly enough, or more intuitively speaking, because they are not smooth enough functions to be contained in the RKHS. We are ready to state the main theoretical result. In the theorem statement, we use K(X)  RPP to denote the random kernel matrix obtained from an input vector X  Rd0 drawn randomly from the population. More precisely, the (p, q)-th entry of K(X) is given by K(zp(X), zq(X)). Theorem 1. Assume that the loss function L(; y) is L-Lipchitz continuous for every y  [d2] and that K is the inverse polynomial kernel or the Gaussian kernel. For any valid activation function , there is a constant C(B1) such that with the radius R := C(B1)B2r, the expected generalization error is at most EX,Y [L(fccnn(X);Y )]  inf fFcnn EX,Y [L(f(X);Y )] + c LC(B1)B2r  log(nP ) EX [K(X)2] n , (16) where c > 0 is a universal constant. Proof sketch The proof of Theorem 1 consists of two parts: First, we consider a larger function class that contains the class of CNNs. This function class is defined as: Fccnn := { x 7 r j=1 P p=1 j,phj(zp(x)) : r  < and r j=1 j2hjH  C(B1)B2d2 } . where H is the norm of the RKHS associated with the kernel. This new function class relaxes the class of CNNs in two ways: 1) the filters are relaxed to belong to the RKHS, and 2) the `2-norm bounds on the weight vectors are replaced by a single constraint on j2 and hjH. We prove the following property for the predictor fccnn: it must be an empirical risk minimizer of Fccnn, even though the algorithm has never explicitly optimized the loss within this nonparametric function class. Second, we characterize the Rademacher complexity of this new function class Fccnn, proving an upper bound for it based on the matrix concentration theory. Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of fccnn converges to the least possible generalization error of Fccnn. The later loss is bounded by the generalization loss of CNNs (because Fcnn  Fccnn), which establishes the theorem. See Appendix C for the full proof of Theorem 1. Remark on activation functions. It is worth noting that the quantity C(B1) depends on the activation function , and more precisely, depends on the convergence rate of the polynomial expansion of . Appendix A shows that if  is a polynomial function of degree `, then C(B1) = O(B`1). If  is the sinusoid function, the erf function or the smoothed hinge loss, then the quantity C(B1) will be exponential in B1. In an algorithmic perspective, we dont need to know the activation function for executing Algorithm 1. In a theoretical perspective, however, the choice of  is relevant from the point of Theorem 1 to compare fccnn with the best CNN, whose representation power is characterized by the choice of . Therefore, if a CNN with a low-degree polynomial  performs well on a given task, then CCNN also enjoys correspondingly strong generalization. Empirically, this is actually borne out: in Section 5, we show that the quadratic activation function performs almost as well as the ReLU function for digit classification. Remark on parameter sharing. In order to demonstrate the importance of parameter sharing, consider a CNN without parameter sharing, so that we have filter weights wj,p for each filter index j and patch index p. With this change, the new CNN output (2) is f(x) = r j=1 P p=1 j,p(w > j,pzp(x)), where j,p  R and wj,p  Rd1 . (17) Note that the hidden layer of this new network has P times more parameters than that of the convolutional neural network with parameter sharing. These networks without parameter sharing can be learned by the recursive kernel method proposed by Zhang et al. [48]. This paper shows that under the norm constraints wj2  B1 and r j=1 P p=1 |j,p|  B2, the excess risk of the recursive kernel method is at most O(LC(B1)B2  Kmax/n), where Kmax = maxz:z21K(z, z) is the maximal value of the kernel function. Plugging in the norm constraints of the function class Fcnn, we have B1 = B1 and B2 = B2r  P . Thus, the expected risk of the estimated f is bounded by: EX,Y [L(f(X);Y )]  inf fFcnn EX,Y [L(f(X);Y )] + c LC(B1)B2r  PKmax n . (18) Comparing this bound to Theorem 1, we see that (apart from the logarithmic terms) they differ in the multiplicative factors of  P Kmax versus  E[K(X)2]. Since the matrix K(X) is P -dimensional, we have K(X)2  max p[P ]  q[P ] |K(zp(X), zq(X))|  P Kmax. This demonstrates that  P Kmax is always greater than  E[K(X)2]. In general, the first term can be up to factor of  P times greater, which implies that the sample complexity of the recursive kernel method is up to P times greater than that of the CCNN. This difference corresponds to the fact that the recursive kernel method learns a model with P times more parameters. Although comparing the upper bounds doesnt rigorously show that one method is better than the other, it gives the right intuition for understanding the importance of parameter sharing.\nIn this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to the top. Before presenting the multi-layer algorithm, we present two extensions, average pooling and multi-channel inputs. Average pooling. Average pooling is a technique to reduce the output dimension of the convolutional layer from dimensions P  r to dimensions P   r with P  < P . Suppose that the filter hj applied to all the patch vectors produces the output vector Hj(x) := (hj(z1(x)),    , hj(zP (x)))  RPr. Average pooling produces a P   r matrix, where each row is the average of the rows corresponding to a small subset of the P patches. For example, we might average every pair of adjacent patches, which would produce P  = P/2 rows. The operation of average pooling can be represented via left-multiplication using a fixed matrix G  RP P . Algorithm 2: Learning multi-layer CCNNs Input:Data {(xi, yi)}ni=1, kernel function K, number of layers m, regularization parameters R1, . . . , Rm, number of filters r1, . . . , rm. Define H1(x) = x. For each layer s = 2, . . . ,m:  Train a two-layer network by Algorithm 1, taking {(Hs1(xi), yi)}ni=1 as training examples and Rs, rs as parameters. Let Hs be the output of the convolutional layer and fs be the predictor. Output: Predictor fm and the top convolutional layer output Hm. For the CCNN model, if we apply average pooling after the convolutional layer, then the kth output of the CCNN model becomes tr(GZ(x)Ak) where Ak  RmP  is the new (smaller) parameter matrix. Thus, performing a pooling operation requires only replacing every matrix Z(xi) in problem (12) by the pooled matrix GZ(xi). Note that the linearity of the CCNN allows us to effectively pool before convolution, even though for the CNN, pooling must be done after applying the nonlinear filters. The resulting ERM problem is still convex, and the number of parameters have been reduced by P/P -fold. Although average pooling is straightforward to incorporate in our framework, unfortunately, max pooling does not fit into our framework due to its nonlinearity. Processing multi-channel inputs. If our input has C channels (corresponding to RGB colors, for example), then the input becomes a matrix x  RCd0 . The c-th row of matrix x, denoted by x[c]  Rd0 , is a vector representing the c-th channel. We define the multi-channel patch vector as a concatenation of patch vectors for each channel: zp(x) := (zp(x[1]), . . . , zp(x[C]))  RCd1 . Then we construct the feature matrix Z(x) using the concatenated patch vectors {zp(x)}Pp=1. From here, everything else of Algorithm 1 remains the same. We note that this approach learns a convex relaxation of filters taking the form ( C c=1wc, zp(x[c])), parametrized by the vectors {wc}Cc=1. Multi-layer CCNN. Given these extensions, we are ready to present the algorithm for learning multi-layer CCNNs. The algorithm is summarized in Algorithm 2. For each layer s, we call Algorithm 1 using the output of previous convolutional layers as inputnote that this consists of r channels (one from each previous filter) and thus we must use the multi-channel extension. Algorithm 2 outputs a new convolutional layer along with a prediction function, which is kept only at the last layer. We optionally use averaging pooling after each successive layer. to reduce the output dimension of the convolutional layers.\nIn this section, we compare the CCNN approach with other methods. The results are reported on the MNIST dataset and its variations for digit recognition, and on the CIFAR-10 dataset for object classification.\nSince the basic MNIST digits are relatively easy to classify, we also consider more challenging variations [43]. These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]). Figure 3 shows a number of sample images from these different datasets. All the images are of size 28  28. For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This 10k/2k/50k partitioning is standard for MNIST variations [43]. Implementation details. For the CCNN method and the baseline CNN method, we train twolayer and three-layer models respectively. The models with k convolutional layers are denoted by CCNN-k and CNN-k. Each convolutional layer is constructed on 5  5 patches with unit stride, followed by 2 2 average pooling. The first and the second convolutional layers contains 16 and 32 filters, respectively. The loss function is chosen as the 10-class logistic loss. We use the Gaussian kernel K(z, z) = exp(z  z22) and set hyperparameters  = 0.2 for the first convolutional layer and  = 2 for the second. The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second. Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12]. The convex optimization problem is solved by projected SGD with mini-batches of size 50. As a baseline approach, the CNN models are activated by the ReLU function (t) = max{0, t} or the quadratic function (t) = t2. We train them using mini-batch SGD. The input images are preprocessed by global contrast normalization and ZCA whitening [see, e.g. 40]. We compare our method against several alternative baselines. The CCNN-1 model is compared against an SVM with the Gaussian RBF kernel (SVMrbf ) and a fully connected neural network with one hidden layer (NN-1). The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9]. Results. Table 1 shows the classification errors on the test set. The models are grouped with respect to the number of layers that they contain. For models with one convolutional layer, the errors of CNN-1 are significantly lower than that of NN-1, highlighting the benefits of parameter sharing. The CCNN-1 model outperforms CNN-1 on all datasets. For models with two or more hidden layers, the CCNN-2 model outperforms CNN-2 on all datasets, and is competitive against the state-of-the-art. In particular, it achieves the best accuracy on the rand, img and img+rot dataset, and is comparable to the state-of-the-art on the remaining two datasets. In order to understand the key factors that affect the training of CCNN filters, we evaluate five variants: (1) replace the Gaussian kernel by a linear kernel; (2) remove the ZCA whitening in preprocessing; (3) use fewer random features (m = 200 rather than m = 500) to approximate the kernel matrix; (4) regularize the parameter matrix by the Frobenius norm instead of the nuclear norm; (5) stop the mini-batch SGD early before it converges. We evaluate the obtained filters by training a second convolutional layer on top of them, then evaluating the classification error on the hardest dataset img+rot. As Table 2 shows, switching to the linear kernel or removing the ZCA whitening significantly degenerates the performance. This is because that both variants equivalently modify the kernel function. Decreasing the number of random features also has a non-negligible effect, as it makes the kernel approximation less accurate. These observations highlight the impact of the kernel function. Interestingly, replacing the nuclear norm by a Frobenius norm or stopping the algorithm early doesnt hurt the performance. To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix A. The effective rank obtained by the last two variants are equal to 77 and 24, greater than that of the original CCNN (equal to 12). It reveals that the last two variants have damaged the algorithms capability of enforcing a low-rank solution. However, the CCNN filters are retrieved from the top-r singular vectors of the parameter matrix, hence the performance will remain stable as long as the top singular vectors are robust to the variation of the matrix. In Section 3.4, we showed that if the activation function is a polynomial function, then the CCNN requires lower sample complexity to match the performance of the best possible CNN. More precisely, if the activation function is degree-` polynomial, then C(B) in the upper bound will be controlled by O(B`). This motivates us to study the performance of low-degree polynomial activations. Table 1 shows that the CNN-2 model with a quadratic activation function achieves error rates comparable to that with a ReLU activation: CNN-2 (Quad) outperforms CNN-2 (ReLU) on the basic and rand datasets, and is only slightly worse on the rot and img dataset. Since the performance of CCNN matches that of the best possible CNN, the good performance of the quadratic activation in part explains why the CCNN is also good.\nIn order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24]. The dataset consists of 60000 images divided into 10 classes. Each image has 3232 pixels in RGB colors. We use 50k images for training and 10k images for testing. Implementation details. We train CNN and CCNN models with two, three, and four layers Each convolutional layer is constructed on 5 5 patches with unit stride, followed by 3 3 average pooling with two-pixel stride. We train 32, 32, 64 filters for the three convolutional layers from bottom to the top. For any s s input, zero pixels are padded on its borders so that the input size becomes (s+4) (s+4), and the output size of the convolutional layer is (s/2) (s/2). The CNNs are activated by the ReLU function. For CCNNs, we use the Gaussian kernel with hyperparameter  = 1, 2, 2 (for the three convolutional layers). The feature matrix Z(x) is constructed via random feature approximation with dimension m = 2000. The preprocessing steps are the same as in the MNIST experiments. It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 2424 patches of the image, and test on the central 2424 patch. We also apply random cropping to training the the first and the second layer of the CCNN. We compare the CCNN against other baseline methods that dont involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30]. Results. We report the classification errors on in Table 3. For models of all depths, the CCNN model outperforms the CNN model. The CCNN-2 and the CCNN-3 model also outperform the three baseline methods. The advantage of the CCNN is substantial for learning two-layer networks, when the optimality guarantee of CCNN holds. The performance improves as more layers are stacked, but as we observe in Table 3, the marginal gain of CCNN diminishes as the network grows deeper. We suspect that this is due to the greedy fashion in which the CCNN layers are constructed. Once trained, the low-level filters of the CCNN are no longer able to adapt to the final classifier. In contrast, the low-level filters of the CNN model are continuously adjusted via backpropagation. It is worth noting that the performance of the CNN can be further improved by adding more layers, switching from average pooling to max pooling, and being regularized by local response normalization and dropout (see, e.g. [25]). The figures in Table 3 are by no means the state-ofthe-art result on CIFAR-10. However, it does demonstrate that the convex relaxation is capable of improving the performance of convolutional neural networks. For future work, we propose to study a better way for convexifying deep CNNs. In Figure 4, we compare the computational efficiency of CNN-3 to its convexified version CCNN3. Both models are trained by mini-batch SGD (with batchsize equal to 50) on a single processor. We optimized the choice of step-size for each algorithm. From the plot, it is easy to identify the three stages of the CCNN-3 curve for training the three convolutional layers from bottom to the top. We also observe that the CCNN converges faster than the CNN. More precisely, the CCNN takes half the runtime of the CNN to reach an error rate of 28%, and one-fifth of the runtime to reach an error rate of 23%. The per-iteration cost for training the first layer of CCNN is about 109% of the per-iteration cost of CNN, but the per-iteration cost for training the remaining two layers are about 18% and 7% of that of CNN. Thus, training CCNN scales well to large datasets. Training a CCNN on top of a CNN. Instead of training a CCNN from scratch, we can also train CCNNs on top of existing CNN layers. More concretely, once a CNN-k model is obtained, we train a two-layer CCNN by taking the (k 1)-th hidden layer of CNN as input. This approach preserves the low-level features learned by CNN, only convexifying its top convolutional layer. The underlying motivation is that the traditional CNN is good at learning low-level features through backpropagation, while the CCNN is optimal in learning two-layer networks. In this experiment, we convexify the top convolutional layer of CNN-2 and CNN-3 using the CCNN approach, with a smaller Gaussian kernel parameter (i.e.  = 0.1) and keeping other hyperparameters the same as in the training of CCNN-2 and CCNN-3. The results are shown in Table 4. The convexified CNN achieves better accuracy on all network depths. It is worth noting that the time for training a convexified layer is only a small fraction of the time for training the original CNN.\nWith the empirical success of deep neural networks, there has been an increasing interest in theoretical understanding. Bengio et al. [5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3]. Zhang et al. [47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting. Other relevant works for learning fully-connected networks include [35, 23, 29]. Aslan et al. [1, 2] propose a method for learning multi-layer latent variable models. They showed that for certain activation functions, the proposed method is a convex relaxation for learning the fully-connected neural network. Another line of work is devoted to understanding the energy landscape of a neural network. Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11]. If this property holds, then gradient descent can find a solution that is good enough. Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20]. However, these results are not applicable to a CNN, since the underlying assumptions are not satisfied by CNNs. Past work has studied learning translation invariant features without backpropagation. Mairal et al. [30] present convolutional kernel networks. They propose a translation-invariant kernel whose feature mapping can be approximated by a composition of the convolution, non-linearity and pooling operators, obtained through unsupervised learning. However, this method is not equipped with the optimality guarantees that we have provided for CCNNs in this paper, even for learning one convolution layer. The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper. Daniely et al. [13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization.\nIn this paper, we have shown how convex optimization can be used to efficiently optimize CNNs as well as understand them statistically. Our convex relaxation consists of two parts: the nuclear norm relaxation for handling parameter sharing, and the RKHS relaxation for handling non-linearity. For the two-layer CCNN, we proved that its generalization error converges to that of the best possible two-layer CNN. We handled multi-layer CCNNs only heuristically, but observed that adding more layers improves the performance in practice. On real data experiments, we demonstrated that CCNN outperforms the traditional CNN of the same depth, is computationally efficient, and can be combined with the traditional CNN to achieve better performance. A major open problem is to formally study the convex relaxation of deep CNNs.\nThis work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, Office of Naval Research grant ONR-N00014, National Science Foundation Grant CIF-31712-23800, as well as a Microsoft Faculty Research Award to the second author. A Inverse polynomial kernel and Gaussian kernel In this appendix, we describe the properties of the two types of kernels  the inverse polynomial kernel (14) and the Gaussian RBF kernel (15). We prove that the associated reproducing kernel Hilbert Spaces (RKHS) of these kernels contain filters taking the form h : z 7 (w, z) for particular activation functions . A.1 Inverse polynomial kernel We first verify that the function (14) is a kernel function. This holds since that we can find a mapping  : Rd1  `2(N) such that K(z, z) = (z), (z). We use zi to represent the i-th coordinate of an infinite-dimensional vector z. The (k1, . . . , kj)-th coordinate of (z), where j  N and k1, . . . , kj  [d1], is defined as 2 j+1 2 xk1 . . . xkj . By this definition, we have (x), (y) =  j=0 2(j+1)  (k1,...,kj)[d1]j zk1 . . . zkjz  k1 . . . z  kj . (19) Since z2  1 and z2  1, the series on the right-hand side is absolutely convergent. The inner term on the right-hand side of equation (19) can be simplified to (k1,...,kj)[d1]j zk1 . . . zkjz  k1 . . . z  kj = (z, z)j . (20) Combining equations (19) and (20) and using the fact that |z, z|  1, we have (z), (z) =  j=0 2(j+1)(z, z)j (i)= 1 2 z, z = K(z, z), which verifies that K is a kernel function and  is the associated feature map. Next, we prove that the associated RKHS contains the class of nonlinear filters. The lemma was proved by Zhang et al. [48]. We include the proof to make the paper self-contained. Lemma 1. Assume that the function (x) has a polynomial expansion (t) =  j=0 ajt j. Let C() :=  j=0 2 j+1a2j 2j. If C(w2) <, then the RKHS induced by the inverse polynomial kernel contains function h : z 7 (w, z) with Hilbert norm hH = C(w2). Proof. Let  be the feature map that we have defined for the polynomial inverse kernel. We define vector w  `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j  N and k1, . . . , kj  [d1], is equal to 2 j+1 2 ajwk1 . . . wkj . By this definition, we have (w, z) =  t=0 aj(w, z)j =  j=0 aj  (k1,...,kj)[d1]j wk1 . . . wkjzk1 . . . zkj = w,(z), (21) where the first equation holds since (x) has a polynomial expansion (x) =  j=0 ajx j , the second by expanding the inner product, and the third by definition of w and (z). The `2-norm of w is equal to: w22 =  j=0 2j+1a2j  (k1,...,kj)[d1]j w2k1w 2 k2   w 2 kj =  j=0 2j+1a2jw 2j 2 = C 2 (w2) <. (22) By the basic property of the RKHS, the Hilbert norm of h is equal to the `2-norm of w. Combining equations (21) and (22), we conclude that h  H and hH = w2 = C(w2). According to Lemma 1, it suffices to upper bound C() for a particular activation function . To make C() < , the coefficients {aj}j=0 must quickly converge to zero, meaning that the activation function must be sufficiently smooth. For polynomial functions of degree `, the definition of C implies that C() = O(`). For the sinusoid activation (t) := sin(t), we have C() =   j=0 22j+2 ((2j + 1)!)2  (2)2j+1  2e2 . For the erf function and the smoothed hinge loss function defined in Section 3.4, Zhang et al. [48, Proposition 1] proved that C() = O(ec 2 ) for universal numerical constant c > 0. A.2 Gaussian kernel The Gaussian kernel also induces an RKHS that contains a particular class of nonlinear filters. The proof is similar to that of Lemma 1. Lemma 2. Assume that the function (x) has a polynomial expansion (t) =  j=0 ajt j. Let C() :=  j=0 j!e2 (2)j a2j 2j. If C(w2) < , then the RKHS induced by the Gaussian kernel contains the function h : z 7 (w, z) with Hilbert norm hH = C(w2). Proof. When z2 = z2 = 1, It is well-known [see, e.g. 41] the following mapping  : Rd1  `2(N) is a feature map for the Gaussian RBF kernel: the (k1, . . . , kj)-th coordinate of (z), where j  N and k1, . . . , kj  [d1], is defined as e((2)j/j!)1/2xk1 . . . xkj . Similar to equation (21), we define a vector w  `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j  N and k1, . . . , kj  [d1], is equal to e((2)j/j!)1/2ajwk1 . . . wkj . By this definition, we have (w, z) =  t=0 aj(w, z)j =  j=0 aj  (k1,...,kj)[d1]j wk1 . . . wkjzk1 . . . zkj = w,(z). (23) The `2-norm of w is equal to: w22 =  j=0 j!e2 (2)j a2j  (k1,...,kj)[d1]j w2k1w 2 k2   w 2 kj =  j=0 j!e2 (2)j a2jw 2j 2 = C 2 (w2) <. (24) Combining equations (21) and (22), we conclude that h  H and hH = w2 = C(w2). Comparing Lemma 1 and Lemma 2, we find that the Gaussian kernel imposes a stronger condition on the smoothness of the activation function. For polynomial functions of degree `, we still have C() = O(`). For the sinusoid activation (t) := sin(t), it can be verified that C() = e2  j=0 1 (2j + 1)!  (2 2 )2j+1  e2/(4)+ . However, the value of C() is infinite when  is the erf function or the smoothed hinge loss, meaning that the Gaussian kernels RKHS doesnt contain filters activated by these two functions.\nIn this appendix, we provide a detailed derivation of the relaxation for nonlinear activation functions that we previously sketched in Section 3.2. Recall that the filter output is (wj , z). Appendix A shows that given a sufficiently smooth activation function , we can find some kernel function K : Rd1Rd1  R and a feature map  : Rd1  `2(N) satisfying K(z, z)  (z), (z), such that (wj , z)  wj , (z). (25) Here wj  `2(N) is a countable-dimensional vector and  := (1, 2, . . . ) is a countable sequence of functions. Moreover, the `2-norm of wj is bounded as wj2  C(wj2) for a monotonically increasing function C that depends on the kernel (see Lemma 1 and Lemma 2). As a consequence, we may use (z) as the vectorized representation of the patch z, and use wj as the linear transformation weights, then the problem is reduced to training a CNN with the identity activation function. The filter is parametrized by an infinite-dimensional vector wj . Our next step is to reduce the original ERM problem to a finite-dimensional one. In order to minimize the empirical risk, one only needs to concern the output on the training data, that is, the output of wj , (zp(xi)) for all (i, p)  [n]  [P ]. Let T be the orthogonal projector onto the linear subspace spanned by the vectors {(zp(xi)) : (i, p)  [n] [P ]}. Then we have  (i, p)  [n] [P ] : wj , (zp(xi)) = wj , T(zp(xi)) = Twj , (zp(xi)). The last equation follows since the orthogonal projector T is self-adjoint. Thus, for empirical risk minimization, we can without loss of generality assume that wj belongs to the linear subspace spanned by {(zp(xi)) : (i, p)  [n] [P ]} and reparametrize it by: wj =  (i,p)[n][P ] j,(i,p)(zp(xi)). (26) Let j  RnP be a vector whose whose (i, p)-th coordinate is j,(i,p). In order to estimate wj , it suffices to estimate the vector j . By definition, the vector satisfies the relation  > j Kj = wj22, where K is the nP  nP kernel matrix defined in Section 3.2. As a consequence, if we can find a matrix Q such that QQ> = K, then we have the norm constraint Q>j2 =  >j Kj = wj2  C(wj2)  C(B). (27) Let v(z)  RnP be a vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then by equations (25) and (26), the filter output can be written as  ( wj , z )  wj , (z)  j , v(z). (28) For any patch zp(xi) in the training data, the vector v(zp(xi)) belongs to the column space of the kernel matrix K. Therefore, letting Q represent the pseudo-inverse of matrix Q, we have  (i, p)  [n] [P ] : j , v(zp(xi)) = >j QQv(zp(xi)) = (Q>)Q>j , v(zp(xi)). It means that if we replace the vector j on the right-hand side of equation (28) by the vector (Q>)Q>j , then it wont change the empirical risk. Thus, for ERM we can parametrize the filters by hj(z) := (Q>)Q>j , v(z) = Qv(z), Q>j. (29) Let Z(x) be an P  nP matrix whose p-th row is equal to Qv(zp(x)). Similar to the steps in equation (6), we have fk(x) = r j=1 >k,jZ(x)K 1/2j = tr ( Z(x) ( r j=1 K1/2j > k,j )) = tr(Z(x)Ak), where Ak := r j=1Q >j > k,j . If we let A := (A1, . . . , Ad2) denote the concatenation of these matrices, then this larger matrix satisfies the constraints: Constraint (C1): max j[r] Q>j2  C(B1) and max (k,j)[d2][r] k,j2  B2. Constraint (C2): The matrix A has rank at most r. We relax these two constraints to the nuclear norm constraint: A  C(B1)B2r  d2. (30) By comparing constraints (8) and (30), we see that the only difference is that the term B1 in the norm bound has been replaced by C(B1). This change is needed because we have used the kernel trick to handle nonlinear activation functions.\nSince the output is one-dimensional in this case, we can adopt the simplified notation (A,j,p) for the matrix (A1, 1,j,p). Letting H be the RKHS associated with the kernel function K, and letting H be the associated Hilbert norm, consider the function class Fccnn := { x 7 r j=1 P p=1 j,phj(zp(x)) : r  < and r j=1 j2hjH  C(B1)B2d2 } . (31) Here j,p denotes the p-th entry of vector j  RP , whereas the quantity C(B1) only depends on B1 and the activation function . The following lemma shows that the function class Fccnn is rich enough so that it contains family of CNN predictors as a subset. The reader should recall the notion of a valid activation function, as defined prior to the statement of Theorem 1. Lemma 3. For any valid activation function , there is a quantity C(B1), depending only on B1 and , such that Fcnn  Fccnn. See Appendix C.1 for the proof. Next, we connect the function class Fccnn to the CCNN algorithm. Recall that fccnn is the predictor trained by the CCNN algorithm. The following lemma shows that fccnn is an empirical risk minimizer within Fccnn. Lemma 4. With the CCNN hyper-parameter R = C(B1)B2d2, the predictor fccnn is guaranteed to satisfy the inclusion fccnn  arg min fFccnn n i=1 L(f(xi); yi). See Appendix C.2 for the proof. Our third lemma shows that the function class Fccnn is not too big, which we do by upper bounding its Rademacher complexity. The Rademacher complexity of a function class F = {f : X  R} with respect to n i.i.d. samples {Xi}ni=1 is given by Rn(F) := EX, [ sup fF 1 n n i=1 if(Xi) ] , where { i}ni=1 are an i.i.d. sequence of uniform {1,+1}-valued variables. Rademacher complexity plays an important role in empirical process theory, and in particular can be used to bound the generalization loss of our empirical risk minimization problem. We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity. The following lemma involves the kernel matrix K(x)  RPP whose (i, j)-th entry is equal to K(zi(x), zj(x)), as well as the expectation E[K(X)2] of the spectral norm of this matrix when X is drawn randomly. Lemma 5. There is a universal constant c such that Rn(Fccnn)  c C(B1)B2r  log(nP )E[K(X)2] n . (32) See Appendix C.3 for the proof of this claim. Combining Lemmas 3 through 5 allows us to compare the CCNN predictor fccnn against the best model in the CNN class. Lemma 4 shows that fccnn is the empirical risk minimizer within function class Fccnn. Thus, the theory of Rademacher complexity [4] guarantees that E[L(Fccnn(X);Y )]  inf fFccnn E[L(f(x); y)] + 2L  Rn(Fccnn) + c n , (33) where c is a universal constant. By Lemma 3, we have inf fFccnn E[L(f(X);Y )]  inf fFcnn E[L(f(X);Y )]. Plugging this upper bound into inequality (33) and applying Lemma 5 completes the proof. C.1 Proof of Lemma 3 With the activation functions specified in the lemma statement, Lemma 1 and Lemma 2 show that there is a quantity C(B1), such any filter of CNN belongs to the reproducing kernel Hilbert space H and its Hilbert norm is bounded by C(B1). As a consequence, any function f  Fcnn can be represented by f(x) := r j=1 P p=1 j,phj(zp(x)) where hjH  C(B1) and j2  B2. It is straightforward to verify that function f satisfies the constraint in equation (31), and consequently belongs to Fccnn. C.2 Proof of Lemma 4 Let CR denote the function class { x 7 tr(Z(x)A) : A  R } . We first prove that CR  Fccnn. Consider an arbitrary function fA(x) : = tr(Z(x)A) belonging to CR. Note that the matrix A has a singular value decomposition (SVD) A = r j=1 jwju > j for some r  < , where wj and uj are unit vectors and j are real numbers. Using this notation, the function fA can be represented as the sum fA(x) = r j=1 ju > j Z(x)wj . Let v(z) be an nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then Qv(zp(x)) is the p-th row of matrix Z(x). Letting hj denote the mapping z 7 Qv(z), wj, we have fA(x) = r j=1 P p=1 juj,phj(zp(x)). The function hj can also be written as z 7 (Q>)wj , v(z). Equation (27) implies that the Hilbert norm of this function is equal to Q>(Q>)wj2, which is bounded by wj2 = 1. Thus we have r j=1 juj2hjH = r j=1 |j | = A  C(B1)B2r, which implies that fA  Fccnn. Next, it suffices to prove that for some empirical risk minimizer f in function class Fccnn, it also belongs to the function class CR. Recall that any function f  Fccnn can be represented in the form f(x) = r j=1 P p=1 j,phj(zp(x)). where the filter hj belongs to the RKHS. Let  : Rd1  `2(N) be a feature map of the kernel function. The function hj(z) can be represented by w, (z) for some w  `2(N). In Appendix B, we have shown that any function taking this form can be replaced by (Q>)Q>j , v(z) for some vector j  RnP without changing the output on the training data. Thus, there exists at least one empirical risk minimizer f of Fccnn such that all of its filters take the form hj(z) = (Q>)Q>j , v(z). (34) By equation (27), the Hilbert norm of these filters satisfy: hjH = Q>(Q>)Q>j2 = Q>j2. According to Appendix B, if all filters take the form (34), then the function f can be represented by tr(Z(x)A) for matrix A := r j=1Q >j > j . Consequently, the nuclear norm is bounded as A  r j=1 j2Q>j2 = r j=1 j2hjH  C(B1)B2r = R, which establishes that the function f belongs to the function class CR. C.3 Proof of Lemma 5 Throughout this proof, we use the shorthand notation R := C(B1)B2r. Recall that any function f  Fccnn can be represented in the form f(x) = r j=1 P p=1 j,phj(zp(x)) where hj  H, (35) and the Hilbert space H is induced by the kernel function K. Since any patch z belongs to the compact space {z : z2  1} and K is a continuous kernel satisfying K(z, z)  1, Mercers theorem [41, Theorem 4.49] implies that there is a feature map  : Rd1  `2(N) such that  `=1 `(z)`(z ) converges uniformly and absolutely to K(z, z). Thus, we can write K(z, z) = (z), (z). Since  is a feature map, every any function h  H can be written as h(z) = , (z) for some   `2(N), and the Hilbert norm of h is equal to 2. Using this notation, we can write the filter hj in equation (35) as hj(z) = j , (z) for some vector j  `2(N), with Hilbert norm hjH = j2. For each x, let (x) denote the linear operator that maps any sequence   `2(N) to the vector in RP with elements[ , (z1(x)) . . . , (zP (x)) ]T . Informally, we can think of (x) as a matrix whose p-th row is equal to (zp(x)). The function f can then be written as f(xi) = r j=1 >j (xi)j = tr (xi)( r j=1 j > j ) . (36) The matrix r j=1 j > j satisfies the constraint r j=1 j > j    r j=1 j2  j2 = r j=1 j2  hjH  R. (37) Combining equation (36) and inequality (37), we find that the Rademacher complexity is bounded by Rn(Fccnn) = 1 n E [ sup fFccnn n i=1 if(xi) ]  1 n E [ sup A:AR tr (( n i=1 i(xi) ) A )] = R n E [ n i=1 i(xi)  2 ] , (38) where the last equality uses Holders inequalitythat is, the duality between the nuclear norm and the spectral norm. As noted previously, we may think informally of the quantity n i=1 i(xi) as a matrix with P rows and infinitely many columns. Let (d)(xi) denote the submatrix consisting of the first d columns of (xi) and let  (d)(xi) denote the remaining sub-matrix. We have E [ n i=1 i(xi)  2 ]  E [ n i=1 i (d)(xi)  2 ] + E  n i=1 i (d)(xi)  2 F 1/2  E [ n i=1 i (d)(xi)  2 ] + ( nP  E [  `=d+1 2` (z) ])1/2 . Since  `=1  2 ` (z) uniformly converges to K(z, z), the second term on the right-hand side converges to zero as d  . Thus it suffices to bound the first term and take the limit. In order to upper bound the spectral norm n i=1 i (d)(xi)  2 , we use a matrix Bernstein inequality due to Minsker [31, Theorem 2.1]. In particular, whenever tr((d)(xi)( (d)(xi)) >)  C1, there is a universal constant c such that the expected spectral norm is upper bounded as E [ n i=1 i (d)(xi)  2 ]  c  log(nC1)E [( n i=1 (d)(xi)((d)(xi))>2 )1/2]  c  log(nC1) ( nE[(X)>(X)2] )1/2 . Note that the uniform kernel expansion K(z, z) =  `=1 `(z)`(z ) implies the trace norm bound tr((d)(xi)( (d)(xi)) >)  tr(K(xi)). Since all patches are contained in the unit `2-ball, the kernel function K is uniformly bounded by 1, and hence C1  P . Taking the limit d, we find that E [ n i=1 i(xi)  2 ]  c  log(nP ) ( nE[K(X)2] )1/2 . Finally, substituting this upper bound into inequality (38) yields the claimed bound (32).\n", "prediction": "we introduce a new class of neural networks, convexified convolutional networks ( ccns ).<n> we prove that this class converges to the optimal solution of the problem in the sense that the convergence rate is the rate of convergence of a convex matrix representing a filter applied to each input vector ; this is in contrast with the standard approach of solving a nonconvex optimization problem by using gradient methods to find a solution : the solution is only of optimal convergence in a certain sense and this rate does nt converge in general to any solution for which this approach can be used to solve the same problem for all input vectors of interest ( e.g. @xmath0 for classification ] and for training the filters of this new neural network in such a way that they have the best possible performance for the given problem when compared to other well - established methods of learning for this problem ).<n> this result allows us to propose an algorithm for learning the convexity classes of these networks and we show that our algorithm is efficient and that it achieves competitive performance with other state - of - the - art methods including fully -connected networks that are trained by backpropagation and denoising autoencoders and other methods based on convexification of matrix parameters ( which we refer to as low - rank matrix constraints in this paper and in our final formulation of our paper as well as an inequality on theoretical fronts [28] find that we achieveirically with a low-ir to achieve competitive competitive and competitive with competitive to compete with an empirical performance by the performance of competitive ( i. we also prove an iterative iterative  i for a s in [ j[n [ ( j. j@n[j[i t[ j j ij[[ j - j] j(j] <n> j, j and j in jj _ j- j [ jh  j_j - [j@ j = j is j]. j) jn j [ a j[d [ [[@j = f - f(n = a [ @ j ( th  [(d = d[s is a p[r ], j for j= j=1 j @ @[e ].<n> is @j is f = p(r = b - p = @ @[f  is [ f  @<n> p. [ d = r - d - @([x - x( j + j with j does j<n> [ x - r = 2. d(s j], @@@x(@  ( [ p - (  for [<n> d. p is p  =  a @ [ b = x = g - g  d  the @] is d@ d is is  p @ = [ w - z - b(x = m = k - a. f[w = z.  [ r. x[] @ d] for d [ k = w.[g - w = + d) is r  b. ( d_[m = the d and @ - is x. r is g. b ( p) and p] with p ( f-[a - e - k. is b ]) and the output @ p_d  + p and d ( b[p is.] - 2, p p<n> @ x  with d + @. w  z = n. = 1] + a is + [ 2] to f. + g + b is for p@s - + f and a - v - i -[. z  - m - n -."}
{"ground_truth": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.\nA machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavor, from performing menial jobs for us to helping the advancement of basic and applied research. Given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines. Still, since solving AI seems too complex a task to be pursued all at once, in the last decades the computational community has preferred to focus on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose intelligent machines. In this article, we propose an alternative approach: we first define the general characteristics we think intelligent machines should possess, and then we present a concrete roadmap to develop them in realistic, small steps, that are however incrementally structured in such a way that, jointly, they should lead us close to the ultimate goal of implementing a powerful AI. The article is organized as follows. In Section 2 we specify the two fundamental characteristics that we consider crucial for developing intelligenceat least the sort of intelligence we are interested innamely communication and learning. Our goal is ar X iv :1 51 1. 08 13 0v 2 [ cs .A I] 2 6 Fe b to build a machine that can learn new concepts through communication at a similar rate as a human with similar prior knowledge. That is, if one can easily learn how subtraction works after mastering addition, the intelligent machine, after grasping the concept of addition, should not find it difficult to learn subtraction as well. Since, as we said, achieving the long-term goal of building an intelligent machine equipped with the desired features at once seems too difficult, we need to define intermediate targets that can lead us in the right direction. We specify such targets in terms of simplified but self-contained versions of the final machine we want to develop. At any time during its education, the target machine should act like a stand-alone intelligent system, albeit one that will be initially very limited in what it can do. The bulk of our proposal (Section 3) thus consists in the plan for an interactive learning environment fostering the incremental development of progressively more intelligent behavior. Section 4 briefly discusses some of the algorithmic capabilities we think a machine should possess in order to profitably exploit the learning environment. Finally, Section 5 situates our proposal in the broader context of past and current attempts to develop intelligent machines. As that review should make clear, our plan encompasses many ideas that have already appeared in different research strands. What we believe to be novel in our approach is the way in which we are combining such ideas into a coherent program.\nRather than attempting to formally characterize intelligence, we propose here a set of desiderata we believe to be crucial for a machine to be able to autonomously make itself helpful to humans in their endeavors. The guiding principles we implicitly considered in formulating the desiderata are to minimize the complexity of the machine, and to maximize interpretability of its behavior by humans.\nAny practical realization of an intelligent machine will have to communicate with us. It would be senseless to build a machine that is supposed to perform complex operations if there is no way for us to specify the aims of these operations, or to understand the output of the machine. While other communication means could be entertained, natural language is by far the easiest and most powerful communication device we possess, so it is reasonable to require an intelligent machine to be able to communicate through language. Indeed, the intelligent machine we aim for could be seen as a computer that can be programmed through natural language, or as the interface between natural language and a traditional programming language. Importantly, humans have encoded a very large portion of their knowledge into natural language (ranging from mathematics treatises to cooking books), so a system mastering natural language will have access to most of the knowledge humans have assembled over the course of their history. Communication is, by its very nature, interactive: the possibility to hold a conversation is crucial both to gather new information (asking for explanation, clarification, instructions, feedback, etc.) and to optimize its transmission (compare a good lecture or studying with a group of peers to reading a book alone). Our learning environment will thus emphasize the interactive nature of communication. Natural language can also channel, to a certain extent, non-linguistic information, because much of the latter can be conveyed through linguistic means. For example, we can use language to talk about what we perceive with our senses, or to give instructions on how to operate in the world (see Louwerse, 2011, among others, for evidence that language encodes many perceptual aspects of our knowledge). Analogously, in the simulation we discuss below, a Teacher uses natural language to teach the Learner (the intelligent machine being trained) a more limited and explicit language (not unlike a simple programming language) in which the Learner can issue instructions to its environment through the same communication channels it uses to interact with the Teacher. The intelligent machine can later be instructed to browse the Internet by issuing commands in the appropriate code through its usual communication channels, mastering in this way a powerful tool to interact with the world at large. Language can also serve as an interface to perceptual components, and thus update the machine about its physical surroundings. For example, an object recognition system could transform raw pixel data into object labels, allowing the machine to see its real-life environment through a controlled-language modality. Still, we realize that our focus on the language-mediated side of intelligence may limit the learning machine in the development of skills that we naturally gain by observing the world around us. There seems to be a fundamental difference between the symbolic representations of language and the continuous nature of the world as we perceive it. If this will turn out to be an issue, we can extend the training phase of the machine (its development in a simulated environment such as the one we will sketch below) with tasks that are more perception-oriented. While in the tasks we will describe here the machine will be taught how to use its I/O channels to receive and transmit linguistic symbols, the machine could also be exposed, through the same interface, to simple encodings (bit streams) of continuous input signals, such as images. The machine could thus be trained, first, to understand the basic properties of continuous variables, and then to perform more complex operations in a continuous space, such as identifying shapes in 2D images. Note that including such tasks would not require us to change the design of our learning framework, only to introduce novel scripts. One big advantage of the single-interface approach we are currently pursuing is that the machine only needs to be equipped with bit-based I/O channels, thus being maximally simple in its interface. The machine can learn an unlimited number of new codes enabling it to interface, through the same channels, with all sorts of interlocutors (people, other machines, perceptual data encoded as described above, etc.). By equipping the machine with only a minimalistic I/O bit-stream interface, we ensure moreover that no prior knowledge about the challenges the machine will encounter is encoded into the structure of the input and output representations, harming the generality of the strategies the machine will learn (compare the difficulty of processing an image when its already encoded into pixels vs. as raw bits). Finally, while we propose language as the general interface to the machine, we are agnostic about the nature of the internal representations the machine must posit to deal with the challenges it faces. In particular, we are not making claims about the internal representations of the machine being based on an interpretable language of thought (Fodor, 1975). In other words, we are not claiming that the machine should carry out its internal reasoning in a linguistic form: only that its input and output are linguistic in nature. To give a few examples of how a communication-based intelligent machine can be useful, consider a machine helping a scientist with research. First of all, the communication-endowed machine does not need to pre-encode a large static database of facts, since it can retrieve the relevant information from the Internet. If the scientist asks a simple question such as: What is the density of gold?, the machine can search the Web to answer: 19.3g/cm3. Most questions will however require the machine to put together multiple sources of information. For example, one may ask: What is a good starting point to study reinforcement learning?. The machine might visit multiple Web sites to search for materials and get an idea of their relative popularity. Moreover, interaction can make even a relatively simple query such as the latter more successful. For example, the machine can ask the user if she prefers videos or articles, what is the mathematical background to be assumed, etc. However, what we are really interested in is a machine that can significantly speed up research progress by being able to address questions such as: What is the most promising direction to cure cancer, and where should I start to meaningfully contribute? This question may be answered after the machine reads a significant number of research articles online, while keeping in mind the perspective of the person asking the question. Interaction will again play a central role, as the best course of action for the intelligent machine might involve entering a conversation with the requester, to understand her motivation, skills, the time she is willing to spend on the topic, etc. Going further, in order to fulfill the request above, the machine might even conduct some independent research by exploiting information available online, possibly consult with experts, and direct the budding researcher, through multiple interactive sessions, towards accomplishing her goal.\nArguably, the main flaw of good old symbolic AI research (Haugeland, 1985) lied in the assumption that it would be possible to program an intelligent machine largely by hand. We believe it is uncontroversial that a machine supposed to be helping us in a variety of scenarios, many unforeseen by its developers, should be endowed with the capability of learning. A machine that does not learn cannot adapt or modify itself based on experience, as it will react in the same way to a given situation for its whole lifetime. However, if the machine makes a mistake that we want to correct, it is necessary for it to change its behaviorthus, learning is a mandatory component. Together with learning comes motivation. Learning allows the machine to adapt itself to the external environment, helping it to produce outputs that maximize the function defined by its motivation. Since we want to develop machines that make themselves useful to humans, the motivation component should be directly controlled by users through the communication channel. By specifying positive and negative rewards, one may shape the behavior of the machine so that it can become useful for concrete tasks (this is very much in the spirit of reinforcement learning, see, e.g., Sutton and Barto, 1998, and discussion in Section 5 below). Note that we will often refer to human learning as a source of insight and an ideal benchmark to strive for. This is natural, since we would like our machines to develop human-like intelligence. At the same time, children obviously grow in a very different environment from the one in which we tutor our machines, they soon develop a sophisticated sensorimotor system to interact with the world, and they are innately endowed with many other cognitive capabilities. An intelligent machine, on the other hand, has no senses, and it will start its life as a tabula rasa, so that it will have to catch up not only on human ontogeny, but also on their phylogeny (the history of AI indicates that letting a machine learn from data is a more effective strategy than manually pre-encoding innate knowledge into it). On the positive side, the machine is not subject to the same biological constraints of children, and we can, for example, expose it to explicit tutoring at a rate that would not be tolerable for children. Thus, while human learning can provide useful inspiration, we are by no means trying to let our machines develop in human-like ways, and we claim no psychological plausibility for the methods we propose.\nion-based intelligent machines In this section, we describe a simulated environment designed to teach the basics of linguistic interaction to an intelligent machine, and how to use it to learn to operate in the world. The simulated ecosystem should be seen as a kindergarten providing basic education to intelligent machines. The machines are trained in this controlled environment to later be connected to the real world in order to learn how to help humans with their various needs. The ecosystem I/O channels are controlled by an automatic mechanism, avoiding the complications that would arise from letting the machine interact with the real world from the very beginning, and allowing us to focus on challenges that should directly probe the effectiveness of new machine learning techniques. The environment must be challenging enough to force the machine to develop sophisticated learning strategies (essentially, it should need to learn how to learn). At the same time, complexity should be manageable, i.e., a human put into a similar environment should not find it unreasonably difficult to learn to communicate and act within it, even if the communication takes place in a language the human is not yet familiar with. After mastering the basic language and concepts of the simulated environment, the machine should be able to interact with and learn from human teachers. This puts several restrictions on the kind of learning the machine must come to be able to perform: most importantly, it will need to be capable to extract the correct generalizations from just a few examples, at a rate comparable to human learners. Our ecosystem idea goes against received wisdom from the last decades of AI research. This received wisdom suggests that systems should be immediately exposed to real-world problems, so that they dont get stuck into artificial blocks worlds (Winograd, 1971), whose experimenter-designed properties might differ markedly from those characterizing realistic setups. Our strategy is based on the observation, that we will discuss in Section 4, that current machine learning techniques cannot handle the sort of genuinely incremental learning of algorithms that is necessary for the development of intelligent machines, because they lack the ability to store learned skills in long-term memory and compose them. To bring about an advance in such techniques, we have of course many choices. It seems sensible to pick the simplest one. The environment we propose is sufficient to demonstrate the deficiencies of current techniques, yet it is simple enough that we can fully control the structure and nature of the tasks we propose to the machines, make sure they have a solution, and use them to encourage the development of novel techniques. Suppose we were instead to work in a more natural environment from the very beginning, for example from video input. This would impose large infrastructure requirements on the developers, it would make data pre-processing a big challenge in itself, and training even the simplest models would be very time-consuming. Moreover, it would be much more difficult to formulate interrelated tasks in a controlled way, and define the success criterion. Once we have used our ecosystem to develop a system capable of learning compositional skills from extremely sparse reward, it should be simple to plug in more natural signals, e.g., through communication with real humans and Internet access, so that the system would learn how to accomplish the tasks that people really want it to perform. The fundamental difference between our approach and classic AI blocks worlds is that we do not intend to use our ecosystem to script an exhaustive set of functionalities, but to teach the machine the fundamental ability to learn how to efficiently learn by creatively combining already acquired skills. Once such machine gets connected with the real world, it should quickly learn to perform any new task its Teacher will choose. Our environment can be seen as analogous to explicit schooling. Pupils are taught math in primary school through rather artificial problems. However, once they have interiorized basic math skills in this setup, they can quickly adapt them to the problems they encounter in their real life, and rely on them to rapidly acquire more sophisticated mathematical techniques.\nAgents To develop an artificial system that is able to incrementally acquire new skills through linguistic interaction, we should not look at the training data as a static set of labeled examples, as in common machine learning setups. We propose instead a dynamic ecosystem akin to that of a computer game. The Learner (the system to be trained) is an actor in this ecosystem. The second fundamental agent in the ecosystem is the Teacher. The Teacher assigns tasks and rewards the Learner for desirable behaviour, and it also provides helpful information, both spontaneously and in response to Learners requests. The Teachers behaviour is entirely scripted by the experimenters. Again, this might be worryingly reminiscent of entirely hand-coded good-old AIs. However, the Teacher need not be a very sophisticated program. In particular, for each task it presents to the learner, it will store a small set of expected responses, and only reward the Learner if its behaviour exactly matches one response. Similarly, when responding to Learners requests, the Teacher is limited to a fixed list of expressions it knows how to respond to. The reason why this suffices is that the aim of our ecosystem is to kickstart the Learners efficient learning capabilities, and not to provide enough direct knowledge for it to be self-sufficient in the world. For example, given the limitations of the scripted Teacher, the Learner will only be able to acquire a very impoverished version of natural language in the ecosystem. At the same time, the Learner should acquire powerful learning and generalization strategies. Using the minimal linguistic skills and strong learning abilities it acquired, the Learner should then be able to extend its knowledge of language fast, once it is put in touch with actual human users. Like in classic text-based adventure games (Wikipedia, 2015b), the Environment is entirely linguistically defined, and it is explored by the Learner by giving orders, asking questions and receiving feedback (although graphics does not play an active role in our simulation, it is straightforward to visualize the 2D world in order to better track the Learners behaviour, as we show through some examples below). The Environment is best seen as the third fundamental agent in the ecosystem. The Environment behaviour is also scripted. However, since interacting with the Environment serves the purpose of observation and navigation of the Learner surroundings (sensorimotor experience), the Environment uses a controlled language that, compared to that of the Teacher, is more restricted, more explicit and less ambiguous. One can thus think of the Learner as a higher-level programming language, that accepts instructions from the programmer (the Teacher) in a simple form of natural language, and converts them into the machine code understood by the Environment. In the examples to follow, we assume the world defined by the Environment to be split into discrete cells that the Learner can traverse horizontally and vertically. The world includes barriers, such as walls and water, and a number of objects the Learner can interact with (a pear, a mug, etc). Note that, while we do not explore this possibility here, it might be useful to add other actors to the simulation: for example, training multiple Learners in parallel, encouraging them to teach/communicate with each other, while also interacting with the scripted Teacher. Interface channels The Learner experience is entirely defined by generic input and output channels. The Teacher, the Environment and any other language-endowed agent write to the input stream. Reward (a scalar value, as discussed next) is also written to the input stream (we assume, however, that the Learner does not need to discover which bits encode reward, as it will need this information to update its objective function). Ambiguities are avoided by prefixing a unique string to the messages produced by each actor (e.g., messages from the Teacher might be prefixed by the string T:, as in our examples below). The Learner writes to its output channel, and it is similarly taught to use unambiguous prefixes to address the Teacher, the Environment and any other agent or service it needs to communicate with. Having only generic input and output communication channels should facilitate the seamless addition of new interactive entities, as long as the Learner is able to learn the language they communicate in. Reward Reward can be positive or negative (1/-1), the latter to be used to speed up instruction by steering away the Learner from dead ends, or even damaging behaviours. The Teacher, and later human users, control reward in order to train the Learner. We might also let the Environment provide feedback through hard-coded rewards, simulating natural events such as eating or getting hurt. Like in realistic biological scenarios, reward is sparse, mostly being awarded after the Learner has accomplished some task. As intelligence grows, we expect the reward to become very sparse, with the Learner able to elaborate complex plans that are only rewarded on successful completion, and even displaying some degree of self-motivation. Indeed, the Learner should be taught that short-term positive reward might lead to loss at a later stage (e.g., hoarding on food with poor nutrition value instead of seeking further away for better food), and that sometimes reward can be maximized by engaging in activities that in the short term provide no benefit (learning to read might be boring and time-consuming, but it can enormously speed up problem solvingand the consequent reward accrual by making the Learner autonomous in seeking useful information on the Internet). Going even further, during the Learner adulthood explicit external reward could stop completely. The Learner will no longer be directly motivated to learn in new ways, but ideally the policies it has already acquired will include strategies such as curiosity (see below) that would lead it to continue to acquire new skills for its own sake. Note that, when we say that reward could stop completely, we mean that users do not need to provide explicit reward, in the form of a scalar value, to the Learner. However, from a human perspective, we can look at this as the stage in which the Learner has interiorized its own sources of reward, and no longer needs external stimuli. We assume binary reward so that human users need not worry about relative amounts of reward to give to the Learner (if they do want to control the amount of reward, they can simply reward the Learner multiple times). The Learner objective should however maximize average reward over time, naturally leading to different degrees of cumulative reward for different courses of action (this is analogous to the notion of expected cumulative reward in reinforcement learning, which is a possible way to formalize the concept). Even if two solutions to a task are rewarded equally on its completion, the faster strategy will be favored, as it leaves the Learner more time to accumulate further reward. This automatically ensures that efficient solutions are preferred over wasteful ones. Moreover, by measuring time independently from the number of simulation steps, e.g., using simple wall-clock time, one should penalize inefficient learners spending a long time performing offline computations. As already mentioned, our approach to reward-based learning shares many properties with reinforcement learning. Indeed, our setup fits into the general formulation of the reinforcement learning problem (Kaelbling et al., 1996; Sutton and Barto, 1998) see Section 5 for further discussion of this point. Incremental structure In keeping with the game idea, it is useful to think of the Learner as progressing through a series of levels, where skills from earlier levels are required to succeed in later ones. Within a level, there is no need to impose a strict ordering of tasks (even when our intuition suggests a natural incremental progression across them), and we might let the Learner discover its own optimal learning path by cycling multiple times through blocks of them. At the beginning, the Teacher trains the Learner to perform very simple tasks in order to kick-start linguistic communication and the discovery of very simple algorithms. The Teacher first rewards the Learner when the latter repeats single characters, then words, delimiters and other control strings. The Learner is moreover taught how to repeat and manipulate longer sequences. In a subsequent block of tasks, the Teacher leads the Learner to develop a semantics for linguistic symbols, by encouraging it to associate linguistic expressions with actions. This is achieved through practice sessions in which the Learner is trained to repeat strings that function as Environment commands, and it is rewarded only when it takes notice of the effect the commands have on its state (we present concrete examples below). At this stage, the Learner should become able to associate linguistic strings to primitive moves and actions (turn left). Next, the Teacher will assign tasks involving action sequences (find an apple), and the Learner should convert them into sets of primitive commands (simple programs). The Teacher will, increasingly, limit itself to specify an abstract end goal (bring back food), but not recipes to accomplish it, in order to spur creative thinking on behalf of the Learner (e.g., if the Learner gets trapped somewhere while looking for food, it may develop a strategy to go around obstacles). In the process of learning to parse and execute higher-level commands, the Learner should also be trained to ask clarification questions to the Teacher (e.g., by initially granting reward when it spontaneously addresses the Teacher, and by the repetition-based strategy we illustrate in the examples below). With the orders becoming more general and complex, the language of the Teacher will also become (within the limits of what can be reasonably scripted) richer and more ambiguous, challenging the Learner capability to handle restricted specimens of common natural language phenomena such as polysemy, vagueness, anaphora and quantification. To support user scenarios such as the ones we envisaged in Section 2 above and those we will discuss at the end of this section, the Teacher should eventually teach the Learner how to read natural text, so that the Learner, given access to the Internet, can autonomously seek for information online. Incidentally, notice that once the machine can read text, it can also exploit distributional learning from large amounts of text (Erk, 2012; Mikolov et al., 2013; Turney and Pantel, 2010) to induce word and phrase representations addressing some of the challenging natural language phenomena we just mentioned, such as polysemy and vagueness. The Learner must take its baby steps first, in which it is carefully trained to accomplish simple tasks such as learning to compose basic commands. However, for the Learner to have any hope to develop into a fully-functional intelligent machine, we need to aim for a snow-balling effect to soon take place, such that later tasks, despite being inherently more complex, will require a lot less explicit coaching, thanks to a combinatorial explosion in the background abilities the Leaner can creatively compose (like for humans, learning how to surf the Web should take less time than learning how to spell). Time off Throughout the simulation, we foresee phases in which the Learner is free to interact with the Environment and the Teacher without a defined task. Systems should learn to exploit this time off for undirected exploration, that should in turn lead to better performance in active training stages, just like, in the dead phases of a video-game, a player is more likely to try out her options than to just sit waiting for something to happen, or when arriving in a new city wed rather go sightseeing than staying in the hotel. Since curiosity is beneficial in many situations, such behaviour should naturally lead to higher later rewards, and thus be learnable. Time off can also be used to think or take a nap, in which the Learner can replay recent experiences and possibly update its inner structure based on a more global view of the knowledge it has accumulated, given the extra computational resources that the free time policy offers. Evaluation Learners can be quantitatively evaluated and compared in terms of the number of new tasks they accomplish successfully in a fixed amount of time, a measure in line with the reward-maximization-over-time objective we are proposing. Since the interactive, multi-task environment setup does not naturally support a distinction between a training and a test phase, the machine must carefully choose rewardmaximizing actions from the very beginning. In contrast, evaluating the machine only on its final behavior would overlook the number of attempts it took to reach the solution. Such alternative evaluation would favor models which are simply able to memorize patterns observed in large amounts of training data. In many practical domains, this approach is fine, but we are interested in machines capable of learning truly general problem-solving strategies. As the tasks become incrementally more difficult, the amount of required computational resources for naive memorization-based approaches scales exponentially, so only a machine that can efficiently generalize can succeed in our environment. We will discuss the limitations of machines that rely on memorization instead of algorithmic learning further in Section 4.3 below. We would like to foster the development of intelligent machines by employing our ecosystem in a public competition. Given what we just said, the competition would not involve distributing a static set of training/development data similar in nature to the final test set. We foresee instead a setup in which developers have access to the full pre-programmed environment for a fixed amount of time. The Learners are then evaluated on a set of new tasks that are considerably different from the ones exposed in the development phase. Examples of how test tasks might differ from those encountered during development include the Teacher speaking a new language, a different Environment topography, new obstacles and objects with new affordances, and novel domains of endeavor (e.g., test tasks might require selling and buying things, when the Learner was not previously introduced to the rules of commerce).\nPreliminaries At the very beginning, the Learner has to learn to pay attention to the Teacher, to identify the basic units of language (find regularity in bit patterns, learn characters, then words and so on). It must moreover acquire basic sequence repetition and manipulation skills, and develop skills to form memory and learn efficiently. These very initial stages of learning are extremely important, as we believe they constitute the building blocks of intelligence. However, as bit sequences do not make for easy readability, we focus here on an immediately following phase, in which the Learner has already learned how to pay attention to the Teacher and manipulate character strings. We show how the Teacher guides the Learner from these basic skills to being able to solve relatively sophisticated Environment navigation problems by exploiting interactive communication. Because of the fractal-like structure we envisage in the acquisition of increasingly higher-level skills, these steps will illustrate many of the same points we could have demonstrated through the lower-level initial routines. The tasks we describe are also incrementally structured, starting with the Learner learning to issue Environment commands, then being led to take notice of the effect these commands have, then understanding command structure, in order to generalize across categories of actions and objects, leading it in turn to being able to process higher-level orders. At this point, the Learner is initiated to interactive communication. Note that we only illustrate here polite turn-taking, in which messages do not overlap, and agents start writing to the communication channels only after the endof-message symbol has been issued. We do not however assume that interaction must be constrained in this way. On the contrary, there are advantages in letting entities write to the communication channels whenever they want: for example, the Teacher might interrupt the Learner to prevent him from completing a command that would have disastrous consequences, or the Learner may interrupt the Teacher as soon as it figured out what to do, in order to speed up reward (a simple priority list can be defined to solve conflicts, e.g., Teachers voice is louder than that of Environment, etc.). Note also that our examples are meant to illustrate specific instances from a larger set of trials following similar templates, that should involve a variety of objects, obstacles and possible actions. Moreover, the presented examples do not aim to exhaustively cover all learning-fostering strategies that might be implemented in the ecosystem. Finally, we stress again that we are not thinking of a strict ordering of tasks (not least because it would be difficult to fix, a priori, an ordering that is based on some objective, learning-model-independent difficulty criterion), but rather about clusters of related tasks organized into levels, such that the Learner could cycle through them and discover its own optimal progression to solve them. Notation We present the input to the Learner (messages from Teacher and Environment, as well as Reward, prefixed by T:, E: and R:, respectively) in the left column, and the Learner output on the right (messages addressed to the Teacher are prefixed by @T:, those for the Environment are prefixed by @E:). We assume different I/O identification prefixes to disambiguate meta-linguistic statements (e.g., T: give order @E: I move. in the first exchange below, where its only the Teacher speaking all the way). The full stop serves as end-of-message delimiter (line breaks are used for formatting purposes, and should be treated as white space; similarly, hyphens are inserted for readability and should be ignored). We use an ellipsis (three dots) to mark a sequence of messages on any channel we do not report (e.g., the Learner exploring a number of solutions before zeroing in on the right one). Finally, we interleave the conversations with comments, shown in grey. The Learner learns to issue Environment commands We start with the Teacher teaching the Learner to produce Environment commands, first by rote repetition channeled to the right message recipient (@E), then by encouraging it to translate from the Teachers own natural language to the controlled Environment language. At this stage, reward is not tied to understanding what happened (the semantics of the commands). Still, a good Learner might take note of the actions triggered by the commands. The following are example conversations that may happen during the learning process: Input: Output: T: give order @E: I move. fdakjf13ktj53u9gs5 5 3#. character soup without addressee, no reward . . . @T: dsfjkasjfk;aljsf. character soup with wrong addressee, no reward . . . @E: fglk4$3wfgg. right addressee, character soup, no reward . . . give order @E: I move. E: you moved. R: 1. Learner moves and is rewarded, although the solution is inefficient T: give order @E: I move. . . . @E: I move. E: you moved. R: 1. Learner figured out minimal string leading to reward Further skills the Learner will need to develop include segmentation of Teachers input, and production of separate commands for the Environment. At first, this may be rather trivial, as shown in this example: Input: Output: T: give orders @E: I move and @E: I look. @E: I move and @E: I look. no reward, because Environment does not parse complex commands . . . T: give orders @E: I move and @E: I look. @E: I move. E: you moved. @E: I look. E: there is an apple. Input: Output: R: 1. Eventually, the Learner should learn to switch fluently between Teacher and Environment communication codes, translating the Teachers linguistic messages into motor commands to the Environment (an example of such exchange is visualized in Figure 1). Associating language to actions Next, the Learner is encouraged to take notice of what happens in the Environment and associate the Teachers language to states of the world, by being rewarded only when its orders result in the corresponding actions being effectively carried out in the world. An example task is illustrated in Figure 2, where the Learner will not get reward because it is no longer sufficient for it to correctly translate instructions into Environment commands: the commands must also have the intended effect on the world. In this specific case, order execution fails because the Learner encountered a barrier. Learning to generalize By adding sufficient variety to the trials above, the Learner should start to get hold of the compositional nature of commands (turn right and turn left share systematic characteristics, e.g., they both cause a change in what the Learner sees; the Learner can pick X only when X is in the cell in front of him, etc.). The next trials help the Learner further move away from literal command string memorization, and generalize across categories of actions and objects (turn left/right are instances of turning, apples and pears are objects, etc.). This is demonstrated in the following example: Input: Output: T: turn and move. @E: I turn. E: (no action) . . . @E: I turn left. E: you turned left. @E: I move. E: you cant move. there is a barrier in front of Learner . . . @E: I turn right. E: you turned right. @E: I move. E: you moved. R: 1. In the next example, the Learner is asked to pick some object that is in front of him, without specifying what the object is (we assume that the simple, unambiguous language of the Environment has no words for general categories such as that of object, so the Learner must eventually find out that it must name specific objects instead): Input: Output: T: pick an object. @E: I pick an object. E: (no response) @E: I pick the apple. E: (no response) @E: I pick the pear. E: you picked the pear. R: 1. As we just illustrated, initially the Learner will apply an exhaustive search strategy, listing all objects it knows of to find one it can pick. Later, the Teacher should teach the look command, and the Learner should be able to discover a faster strategy than exhaustive search: Input: Output: T: pick an object. @E: I look. E: you see a pear. @E: I pick the pear. E: you picked the pear. R: 1. Learning new strategies of course does not imply that the Learner can safely forget the ones it has previously acquired. For example, in some tasks the look command might not work (because, say, it is too dark and the Learner cannot see what is in front of him). In such case, an efficient Learner should find it easy to re-use previously learned skills, such as exhaustive search. Understanding higher-level orders The next batch of trials aims at developing the ability to decompose higher-level commands into a sequence of basic actions, first fixed (move twice), then flexible (find an apple). The general teaching strategy is to provide sets of paired trials: In the first trial, the Teacher describes the task explicitly as a sequence of primitive actions, and gives the high-level name of the order. In the second trial, the Teacher issues an equivalent high-level order, and the Learner is rewarded if it goes through the same sequence as in the first trial (we are aware that many complex linguistic expressions are not amenable to a straightforward decompositional analysis, see, e.g., Boleda and Erk, 2015: we will use some that are in order to teach the Learner about how to decompose complex concepts into step-by-step plans). The Teacher could start by teaching iteration counters: Input: Output: T: move and move. @E: I move. E: you moved. @E: I move. E: you moved. R: 1. T: this is called move two times. . . . T: move two times. . . . @E: I move. E: you moved. @E: I move. E: you moved. R: 1. . . . T: move and move and move. @E: I move. E: you moved. @E: I move. E: you moved. @E: I move. E: you moved. R: 1. T: this is called move three times. . . . T: move three times. . . . @E: I move. E: you moved. @E: I move. E: you moved. @E: I move. E: you moved. R: 1. The Learner should eventually be able to apply modifiers productively to different actions without much further training (e.g., apply three times to turn left). Next, the Teacher can show how to segment high-level tasks, such as finding some- thing, into atomic action sequences. For example, to find an apple, one can develop a simple strategy of going forward until an apple is found. The Teacher might initiate the Learner to this new skill as follows: Input: Output: T: move and look. @E: I move. E: you moved. @E: I look. E: you see grass. T: move and look. @E: I move. E: you moved. @E: I look. E: you see an apple. R: 1. T: this is called find an apple. Note that one such example is not sufficient for the Learner to understand what exactly we call finding an apple, as multiple interpretations are valid: maybe we just wanted it to execute the given commands twice. Thus, there should be multiple training sequences of variable length, to clarify that the task is really to apply an until loop, i.e., to iterate move and look commands until the object the Learner is searching for is found. Further tasks can define composite skills, such as getting an apple, which would consist of first finding an apple, and then picking it up. Another generalization would be to include multiple objects the Learner should be searching for. Adding obstacles to the Environment would further complicate the challenge. The previous strategy of simply going forward until an object is found will not work anymore, as the Learner would stop at the first obstacle. We can either expect the Learner to independently develop better search strategies involving turning, or we can add further examples where the Teacher shows the Learner how to improve through more direct supervision. Interactive communication Tasks such as finding an apple without directions might involve a long random search. Thus, we want to kick-start interactive communication, so that the Learner can be efficiently directed by the Teacher (and eventually by humans) to the right cell. In a first set of trials, the Learner is rewarded for repeating a how to request uttered by the Teacher (addressing it back to the teacher via the @T: prefix), and following the precise instructions produced by the Teacher in response to the request: Input: Output: T: ask me how to find an apple. Input: Output: @E: ask me how to find an apple. wrong addressee, wrong request prefix . . . @T: how to find an apple. T: turn right and move and move and pick the apple. @E: I turn right. E: you turned right. @E: I move. E: you moved. @E: I move. E: you moved. @E: I pick the apple. E: you picked the apple. R: 1. Trials such as this one are later interspersed with trials where the Learner is assigned a task it can in principle accomplish by random search, but taking the initiative by issuing a how to request and then following the precise directions provided by the Teacher will considerably speed up reward. Algorithmic knowledge Some tasks illustrated above require understanding basic control flow structures. For example, parsing action modifiers implies a simple form of counting, and in order to find things the Learner must implement an until (equivalently, while not) loop. Similarly, the command get out of the grass calls for a while loop. Efficient completion of more advanced tasks, e.g., return home, implies development of more complex algorithms, such as path-finding. After acquiring a bank of such algorithms (which might be encoded in the internal machine representation by something very different from programming language constructs), the Learner should be able, in advanced stages of the simulation, to productively combine them in order to succeed in full-fledged novel missions that involve accomplishing a large number of hierarchically-structured sub-goals (find somebody who will trade two apples for a banana). As we discussed in Section 3.1, the Learners functionality could essentially be interpreted as learning how to compose programs based on the descriptions given in natural language by the Teacher. The programs produce very simple instructions that are understood by the Environment, which can be seen as a sort of CPU. From this point of view, the intelligent system we aim to train is a bridge between the Teacher (later to be replaced by a human operator) and a traditional computer that understands only a limited set of basic commands and needs to be manually programmed for each single task. Thus, we believe that successful construction of intelligent machines could automate computer programming, which will likely be done in the future simply through communication in natural language.\nTo conclude the illustration of our plan, we provide a motivating example of how an intelligent machine schooled in our ecosystem could later make itself useful in the real world. We consider a scenario in which the machine works as an assistant to Alice, an elderly person living alone. Bob is Alices son, and he also interacts with the machine. We assume that, as part of its training, the machine has been taught how to issue Internet commands and process their outcomes. In the example dialogue, we give a general idea of how the machine would interface to the Internet, without attempting to precisely define the syntax of this interaction. Most importantly, the Internet queries in the example are meant to illustrate how the machine does not need to store all the knowledge it needs to accomplish its duties, as it can retrieve useful information from the Web on demand, and reason about it. Input: Output: Bob: I just spoke to the doctor, who said my mother needs to move for at least one hour per day, please make sure she does get enough exercise. . . . following conversation takes place the next day: @Alice: Are you in the mood for some light physical exercise today? Alice: Yes, but no more than 15 minutes, please. @INTERNET: [query search engine for keywords elderly, light activity, 15 minutes ] shortly afterwards. . . @Alice: I have downloaded a YouTube video with a 15-minute yoga routine, please watch it whenever you can. a few hours later. . . Alice: Im going out to buy groceries. @INTERNET: [query search engine with keywords average walking speed, elderly person] Input: Output: @INTERNET: [search maps app for distance to grocery stores in Alices neighborhood] @Alice: Hey, why dont you walk to the Yummy Food Market today? It should take you about 45 minutes to and from, so youll get the rest of your daily exercise. @Alice: Thanks for the suggestion. The communication-based intelligent machine should adapt to a whole range of tasks it was not explicitly programmed for. If necessary, the user can give it further explicit positive and negative reward to motivate it to change its behavior. This may be needed only rarely, as the machine should be shipped to the end users after it already mastered good communication abilities, and further development should mostly occur through language. For example, when the user says No, dont do this again, the machine will understand that repeating the same type of behavior might lead to negative reward, and it will change its course of action even when no explicit reward signal is given (again, another way to put this is that the machine should associate similar linguistic strings to an interiorized negative reward). The range of tasks for intelligent machines can be very diverse: besides the everydaylife assistant we just considered, it could explain students how to accomplish homework assignments, gather statistical information from the Internet to help medical researchers (see also the examples in Section 2.1 above), find bugs in computer programs, or even write programs on its own. Intelligent machines should extend our intellectual abilities in the same way current computers already function as an extension to our memory. This should enable us to perform intellectual tasks beyond what is possible today. We realize the intelligent machines we aim to construct could become powerful tools that may be possibly used for dubious purposes (the same could be said about any advanced technology, including airplanes, space rockets and computers). We believe the perception of AI is skewed by popular science fiction movies. Instead of thinking of computers that take over the world for their own reasons, we think AI will be realized as a tool: A machine that will extend our capability to reason and solve complex problems. Further, given the current state of the technology, we believe any discussion on friendliness of the AI is at this moment premature. We expect it will take years, if not decades to scale basic intelligent machines to become competitive with humans, giving us enough time to discuss any possible existential threats.\nIn this section, we will outline some of our ideas about how to build intelligent machines that would benefit from the learning environment we described. While we do not have a concrete proposal yet about how exactly such machines should be implemented, we will discuss some of the properties and components we think are needed to support the desired functionalities. We have no pretense of completeness, we simply want to provide some food for thought. As in the previous sections, we try to keep the complexity of the machine at the minimum, and only consider the properties that seem essential.\nThere are many types of behavior that we collectively call learning, and it is useful to discuss some of them first. Suppose our goal is to build an intelligent machine working as a translator between two languages (we take here a simplified word-based view of the translation task). First, we will teach the machine basic communication skills in our simulated environment so that it can react to requests given by the user. Then, we will start teaching it, by example, how various words are translated. There are different kinds of learning happening here. To master basic communication skills, the machine will have to understand the concept of positive and negative reward, and develop complex strategies to deal with novel linguistic inputs. This requires discovery of algorithms, and the ability to remember facts, skills and even learning strategies. Next, in order to translate, the machine needs to store pairs of words. The number of pairs is unknown and a flexible growing mechanism may be required. However, once the machine understands how to populate the dictionary with examples, the learning left to do is of a very simple nature: the machine does not have to update its learning strategy, but only to store and organize the incoming information into long-term memory using previously acquired skills. Finally, once the vocabulary memorization process is finished and the machine starts working as a translator, no further learning might be required, and the functionality of the machine can be fixed. The more specialized and narrow the functionality of the machine is, the less learning is required. For very specialized forms of behavior, it should be possible to program the solution manually. However, as we move from roles such as a simple translator of words, a calculator, a chess player, etc., to machines with open-ended goals, we need to rely more on general learning from a limited number of examples. One can see the current state of the art in machine learning as being somewhere in the middle of this hierarchy. Tasks such as automatic speech recognition, classification of objects in images or machine translation are already too hard to be solved purely through manual programming, and the best systems rely on some form of statistical learning, where parameters of hand-coded models are estimated from large datasets of examples. However, the capabilities of state-of-the-art machine learning systems are severely limited, and only allow a small degree of adaptability of the machines functionality. For example, a speech recognition system will never be able to perform speech translation by simply being instructed to do soa human programmer is required to implement additional modules manually.\nWe see a special kind of long-term memory as the key component of the intelligent machine. This long-term memory should be able to store facts and algorithms corresponding to learned skills, making them accessible on demand. In fact, even the ability to learn should be seen as a set of skills that are stored in the memory. When the learning skills are triggered by the current situation, they should compose new persistent structures in the memory from the existing ones. Thus, the machine should have the capacity to extend itself. Without being able to store previously learned facts and skills, the machine could not deal with rather trivial assignments, such as recalling the solution to a task that has been encountered before. Moreover, it is often the case that the solution to a new task is related to that of earlier tasks. Consider for example the following sequence of tasks in our simulated environment:  find and pick an apple;  bring the apple back home;  find two apples;  find one apple and two bananas and bring them home. Skills required to solve these tasks include:  the ability to search around the current location;  the ability to pick things;  the ability to remember the location of home and return to it;  the ability to understand what one and two mean;  the ability to combine the previous skills (and more) to deal with different requests. The first four abilities correspond to simple facts or skills to be stored in memory: a sequence of symbols denoting something, the steps needed to perform a certain action, etc. The last ability is an example of a compositional learning skill, with the capability of producing new structures by composing together known facts and skills. Thanks to such learning skills, the machine will be able to combine several existing abilities to create a new one, often on the fly. In this way, a well-functioning intelligent machine will not need a myriad of training examples whenever it faces a slightly new request, but it could succeed given a single example of the new functionality. For example, when the Teacher asks the Learner to find one apple and two bananas and bring them home, if the Learner already understands all the individual abilities involved, it can retrieve the relevant compositional learning skill to put together a plan and execute it step by step. The Teacher may even call the new skill generated in this way prepare breakfast, and refer to it later as such. Understanding this new concept should not require any further training of the Learner, and the latter should simply store the new skill together with its label in its long-term memory. As we have seen in the previous examples, the Learner can continue extending its knowledge of words, commands and skills in a completely unsupervised way once it manages to acquire skills that allow it to compose structures in its long-term memory. It may be that discovering the basic learning skills, something we usually take for granted, is much more intricate than it seems to us. But once we will be able to build a machine which can effectively construct itself based on the incoming signals even when no explicit supervision in the form of rewards is given, as discussed above we should be much closer to the development of intelligent machines.\nAnother aspect of the intelligent machine that deserves discussion is the computational model that the machine will be based on. We are convinced that such model should be unrestricted, that is, able to represent any pattern in the data. Humans can think of and talk about algorithms without obvious limitations (although, to apply them, they might need to rely on external supports, such as paper and pencil). A useful intelligent machine should be able to handle such algorithms as well. A more precise formulation of our claim in the context of the theory of computation is that the intelligent machine needs to be based on a Turing-complete computational model. That is, it has to be able to represent any algorithm in fixed length, just like the Turing machine (the very fact that humans can describe Turing-complete systems shows that they are, in practical terms, Turing-complete: it is irrelevant, for our purposes, whether human online processing capabilities are strictly Turing-complete what matters is that their reasoning skills, at least when aided by external supports, are). Note that there are many Turing-complete computational systems, and Turing machines in particular are a lot less efficient than some alternatives, e.g., Random Access Machines. Thus, we are not interested in building the intelligent machine around the concept of the Turing machine; we just aim to use a computational model that does not have obvious limitations in ability to represent patterns. A system that is weaker than Turing-complete cannot represent certain patterns in the data efficiently, which in turn means it cannot truly learn them in a general sense. However, it is possible to memorize such complex patterns up to some finite level of complexity. Thus, even a computationally restricted system may appear to work as intended up to some level of accuracy, given that a sufficient number of training examples is provided. For example, we may consider a sequence repetition problem. The machine is supposed to remember a sequence of symbols and reproduce it later. Further, lets assume the machine is based on a model with the representational power of finite state machines. Such system is not capable to represent the concept of storing and reproducing a sequence. However, it may appear to do so if we design our experiment imperfectly. Assume there is a significant overlap between what the machine sees as training data, and the test data we use to evaluate performance of the machine. A trivial machine that can function as a look-up table may appear to work, simply by storing and recalling the training examples. With an infinite number of training examples, a look-up-table-based machine would appear to learn any regularity. It will work indistinguishably from a machine that can truly represent the concept of repetition; however, it will need to have infinite size. Clearly, such memorizationbased system will not perform well in our setting, as we aim to test the Learners ability to generalize from a few examples. Since there are many Turing-complete computational systems, one may wonder which one should be preferred as the basis for machine intelligence. We cannot answer this question yet, however we hypothesize that the most natural choice would be a system that performs computation in a parallel way, using elementary units that can grow in number based on the task at hand. The growing property is necessary to support the long-term memory, if we assume that the basic units themselves are finite. An example of an existing computational system with many of the desired properties is the cellular automaton of Von Neumann et al. (1966). We might also be inspired by string rewriting systems, for example some versions of the L-systems (Prusinkiewicz and Lindenmayer, 2012). An apparent alternative would be to use a non-growing model with immensely large capacity. There is however an important difference. In a growing model, the new cells can be connected to those that spawned them, so that the model is naturally able to develop a meaningful topological structure based on functional connectivity. We conjecture that such structure would in itself contribute to learning in a crucial way. On the other hand, it is not clear if such topological structure can arise in a large-capacity unstructured model. Interestingly, some of the more effective machinelearning models available today, such as recurrent and convolutional neural networks, are characterized by (manually constrained) network topologies that are well-suited to the domains they are applied to.\nWe owe, of course, a large debt to the seminal work of Turing (1950). Note that, while Turings paper is most often cited for the imitation game, there are other very interesting ideas in it, worthy of more attention from curious readers, especially in the last section on learning machines. Turing thought that a good way to construct a machine capable of passing his famous test would be to develop a child machine, and teach it further skills through various communication channels. These would include sparse rewards shaping the behavior of the child machine, and other information-rich channels such as language input from a teacher and sensory information. We share Turings goal of developing a child machine capable of independent communication through natural language, and we also stress the importance of sparse rewards. The main distinction between his and our vision is that Turing assumed that the child machine would be largely programmed (he gives an estimate of sixty programmers working on it for fifty years). We rather think of starting with a machine only endowed with very elementary skills, and focus on the capability to learn as the fundamental ability that needs to be developed. This further assumes educating the machine at first in a simulated environment where an artificial teacher will train it, as we outlined in our roadmap. We also diverge with respect to the imitation game, since the purpose of our intelligent machine is not to fool human judges into believing it is actually a real person. Instead, we aim to develop a machine that can perform a similar set of tasks to those a human can do by using a computer, an Internet connection and the ability to communicate. There has been a recent revival of interest in tasks measuring computational intelligence, spurred by the empirical advances of powerful machine-learning architectures such as multi-layered neural networks (LeCun et al., 2015), and by the patent inadequacy of the classic version of Turing test (Wikipedia, 2015c). For example, Levesque et al. (2012) propose to test systems on their ability to resolve coreferential ambiguities (The trophy would not fit in the brown suitcase because it was too big. . . What was too big? ). Geman et al. (2015) propose a visual Turing test in which a computational system is asked to answer a set of increasingly specific questions about objects, attributes and relations in a picture (Is there a person in the blue region? Is the person carrying something? Is the person interacting with any other object? ). Similar initiatives differ from ours in that they focus on a specific set of skills (coreference, image parsing) rather than testing if an agent can learn new skills. Moreover, these are traditional evaluation benchmarks, unlike the hybrid learning/evaluation ecosystem we are proposing. The idea of developing an AI living in a controlled synthetic environment and interacting with other agents through natural language is quite old. The Blocks World of Winograd (1971) is probably the most important example of early research in this vein. The approach was later abandoned, when it became clear that the agents developed within this framework did not scale up to real-world challenges (see, e.g., Morelli et al., 1992). The knowledge encoded in the systems tested by these early simulations was manually programmed by their creators, since they had very limited learning capabilities. Consequently, scaling up to the real world implied manual coding of all the knowledge necessary to cope with it, and this proved infeasible. Our simulation is instead aiming at systems that encode very little prior knowledge and have strong capabilities to learn from data. Importantly, our plan is not to try to manually program all possible scripts our system might encounter later, as in some of the classic AI systems. We plan to program only the initial environment, in order to kickstart the machines ability to learn and adapt to different problems and scenarios. After the simulated environment is mastered, scaling up the functionality of our Learner will not require further manual work on scripting new situations, but will rather focus on integrating real world inputs, such as those coming from human users. The toy world itself is already designed to feature novel tasks of increasing complexity, explicitly testing the abilities of systems to autonomously scale up. Still, we should not underestimate the drawbacks of synthetic simulations. The tasks in our environment might directly address some challenging points in the development of AI, such as learning with very weak supervision, being able to form a structured long-term memory, and the ability of the child machine to grow in size and complexity when encountering new problems. However, simulating the real world can only bring us so far, and we might end up overestimating the importance of some arbitrary phenomena at the expense of others, that might turn out to be more common in natural settings. It may be important to bring reality into the picture relatively soon. Our toy world should let the intelligent machine develop to the point at which it is able to learn from and cooperate with actual humans. Interaction with real-life humans will then naturally lead the machine to deal with real-world problems. The issue of when exactly a machine trained in our controlled synthetic environment is ready to go out in the human world is open, and it should be explored empirically. However, at the same time, we believe that having the machine interact with humans before it can deal with basic problems in the controlled environment would be pointless, and possibly even strongly misleading. Our intelligent machine shares some of its desired functionalities with the current generation of automated personal assistants such as Apples Siri ad Microsofts Cortana. However, these are heavily engineered systems that aim to provide a natural language interface for human users to perform a varied but fixed set of tasks (similar considerations also apply to artificial human companions and digital pets such as Tamagotchi, see Wikipedia, 2015a). Such systems can be developed by defining the most frequent use cases, choosing those that can be solved with the current technology (e.g., book an air ticket, look at the weather forecast and set the alarm clock for tomorrows morning), and implementing specific solutions for each such use case. Our intelligent machine is not intended to handle just a fixed set of tasks. As exemplified by the example in Section 3.3, the machine should be capable to learn efficiently how to perform tasks such as those currently handled by personal assistants, and more, just from interaction with the human user (without a programmer or machine learning expert in the loop). Architectures for software agents, and more specifically intelligent agents, are widely studied in AI and related fields (Nwana, 1996; Russell and Norvig, 2009). We cannot review this ample literature here, in order to position our proposal precisely with respect to it. We simply remark that we are not aware of other architectures that are as centered on learning and communication as ours. Interaction plays a central role in the study of multiagent systems (Shoham and Leyton-Brown, 2009). However, the emphasis in this research tradition is on how conflict resolution and distributed problem solving evolve in typically large groups of simple, mostly scripted agents. For example, traffic modeling is a classic application scenario for multiagent systems. This is very different from our emphasis on linguistic interaction for the purposes of training a single agent that should become independently capable of very complex behaviours. Tenenbaum (2015), like us, emphasizes the need to focus on basic abilities that form the core of intelligence. However, he takes naive physics problems as the starting point, and discusses specific classes of probabilistic models, rather than proposing a general learning scenario. There are also some similarities between our proposal and the research program of Luc Steels (e.g., Steels, 2003, 2005), who lets robots evolve vocabularies and grammatical constructions through interaction in a situated environment. However, on the one hand his agents are actual robots subject to the practical hardware limitations imposed by the need to navigate a complex natural environment from the start; on the other, the focus of the simulations is narrowly on language acquisition, with no further aim to develop broadly intelligent agents. We have several points of contact with the semantic parsing literature, such as navigation tasks in an artificial world (MacMahon et al., 2006) and reward-based learning from natural language instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). The agents developed in this area can perform tasks, such as learning to execute instructions in natural environments by interacting with humans (Thomason et al., 2015), or improving performance on real-life video-games by consulting the instruction manual (Branavan et al., 2012), that we would want our intelligent machines to also be able to carry out. However, current semantic-parsing-based systems achieve these impressive feats by exploiting architectures tuned to the specific tasks at hand, and they rely on a fair amount of hard-wired expert knowledge, in particular about language structures (although recent work is moving towards a more knowledge-lean direction, see for example Narasimhan et al., 2015, who train a neural network to play text-based adventure games using only text descriptions as input and game reward as signal). Our framework is meant to encourage the development of systems that should eventually be able to perform similar tasks, but getting there incrementally, starting with almost no prior knowledge and first learning from their environment a set of simpler skills, and how to creatively merge them to tackle more ambitious goals. The last twenty years have witnessed several related proposals on learning to learn (Thrun and Pratt, 1997), lifelong learning (Silver et al., 2013) and continual learning (Ring, 1997). Much of this work is theoretical in nature and focuses on algorithms rather than on empirical challenges for the proposed models. Still, the general ideas being pursued are in line with our program. Ring (1997), in particular, defines a continual-learning agent whose experiences occur sequentially, and what it learns at one time step while solving one task, it can use later, perhaps to solve a completely different task. Rings desiderata for the continual learner are remarkably in line with ours. It is an autonomous agent. It senses, takes actions, and responds to the rewards in its environment. It learns behaviors and skills while solving its tasks. It learns incrementally. There is no fixed training set; learning occurs at every time step; and the skills the agent learns now can be used later. It learns hierarchically. Skills it learns now can be built upon and modified later. It is a black box. The internals of the agent need not be understood or manipulated. All of the agents behaviors are developed through training, not through direct manipulation. Its only interface to the world is through its senses, actions, and rewards. It has no ultimate, final task. What the agent learns now may or may not be useful later, depending on what tasks come next. Our program is definitely in the same spirit, with an extra emphasis on interaction. Mitchell et al. (2015) discuss NELL, the most fully realized concrete implementation of a lifelong learning architecture. NELL is an agent that has been reading the Web for several years to extract a large knowledge base. Emphasis is on the neverending nature of the involved tasks, on their incremental refinement based on what NELL has learned, and on sharing information across tasks. In this latter respect, this project is close to multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Collobert et al., 2011), that focuses on the idea of parameter sharing across tasks. It is likely that a successful learner in our framework will exploit similar strategies, but our current focus lies on defining the tasks, rather than on how to pursue them. Bengio et al. (2009) propose the related idea of curriculum learning, whereby training data for a single task are ordered according to a difficulty criterion, in the hope that this will lead to better learning. This is motivated by the observation that humans learn incrementally when developing complex skills, an idea that has also previously been studied in the context of recurrent neural network training by Elman (1993). The principle of incremental learning is also central to our proposal. However, the fundamental aspect for us is not a strict ordering of the training data for a specific task, but incrementality in the skills that the intelligent machine should develop. This sort of incrementality should in turn be boosted by designing separate tasks with a compositional structure, such that the skills acquired from the simpler tasks will help to solve the more advanced ones more efficiently. The idea of incremental learning, motivated by the same considerations as in the papers we just mentioned, also appears in Solomonoff (2002), a work which has much earlier roots in research on program induction (Solomonoff, 1964, 1997; Schmidhuber, 2004). Within this tradition, Schmidhuber (2015) reviews a large literature and presents some general ideas on learning that might inspire our search for novel algorithms. Genetic programming (Poli et al., 2008) also focuses on the reuse of previously found sub-solutions, speeding up the search procedure in this way. Our proposal is also related to that of Bottou (2014), in its vision of compositional machine learning, although he only considers composition in limited domains, such as sentence and image processing. We share many ideas with the reinforcement learning framework (Sutton and Barto, 1998). In reinforcement learning, the agent chooses actions in an environment in order to maximize some cumulative reward over time. Reinforcement learning is particularly popular for problems where the agent can collect information only by interacting with the environment. Given how broad this definition is, our framework could be considered as a particular instance of it. Our proposal is however markedly different from standard reinforcement learning work (Kaelbling et al., 1996) in several respects. Specifically, we emphasize language-mediated, interactive communication, we focus on incremental strategies that encourage agents to solve tasks by reusing previously learned knowledge and we aim to limit the number of trials an agent gets in order to accomplish a certain goal. Mnih et al. (2015) recently presented a single neural network architecture capable of learning a set of classic Atari games using only pixels and game scores as input (see also the related idea of general game playing, e.g., Genesereth et al., 2005). We pursue a similar goal of learning from a low-level input stream and reward. However, unlike these authors, we do not aim for a single architecture that can, disjointly, learn an array of separate tasks, but for one that can incrementally build on skills learned on previous tasks to perform more complex ones. Moreover, together with reward, we emphasize linguistic interaction as a fundamental mean to foster skill extension. Sukhbaatar et al. (2015) introduce a sandbox to design games with the explicit purpose to train computational agents in planning and reasoning tasks. Moreover, they stress a curriculum strategy to foster learning (making the agent progress through increasingly more difficult versions of the game). Their general program is aligned with ours, and the sandbox might be useful to develop our environment. However, they do not share our emphasis on communication and interaction, and their approach to incremental learning is based on increasingly more difficult versions of the same task (e.g., increasing the number of obstacles), rather than on defining progressively more complex tasks, such that solving the later ones requires composing solutions to earlier ones, as we are proposing. Furthermore, the tasks currently considered within the sandbox do not seem to be challenging enough to require new learning approaches, and may be solvable with current techniques or minor modifications thereof. Mikolov (2013) originally discussed a preliminary version of the incremental taskbased approach we are more fully outlying here. In a similar spirit, Weston et al. (2015) present a set of question answering tasks based on synthetically generated stories. They also want to foster non-incremental progress in AI, but their approach differs from ours in several crucial aspects. Again, there is no notion of interactive, language-mediated learning, a classic train/test split is enforced, and the tasks are not designed to encourage compositional skill learning (although Weston and colleagues do emphasize that the same system should be used for all tasks). Finally, the evaluation metric is notably different from ours - while we aim to minimize the number of trials it takes for the machine to master the tasks, their goal is to have a good performance on held out data. This could be a serious drawback for works that involve artificial tasks, as in our view the goal should be to develop a machine that can learn as fast as possible, to have any hope to scale up and be able to generalize in more complex scenarios. One could think of solving sequence-manipulation problems such as those constituting the basis of our learning routine with relatively small extensions of established machine learning techniques (Graves et al., 2014; Grefenstette et al., 2015; Joulin and Mikolov, 2015). As discussed in the previous section, for simple tasks that involve only a small, finite number of configurations, one could be apparently successful even just by using a look-up table storing all possible combinations of inputs and outputs. The above mentioned works, that aim to learn algorithms from data, also add a long-term memory (e.g., a set of stacks), but they use it to store the data only, not the learned algorithms. Thus, such approaches fail to generalize in environments where solutions to new tasks are composed of already learned algorithms. Similar criticism holds for approaches that try to learn certain algorithms by using an architecture with a strong prior towards their discovery, but not general enough to represent even small modifications. To give an example from our own work: a recurrent neural network augmented with a stack structure can form a simple kind of long-term memory and learn to memorize and repeat sequences in the reversed order, but not in the original one (Joulin and Mikolov, 2015). We expect a valid solution to the algorithmic learning challenge to utilize a small number of training examples, and to learn tasks that are closely related at an increasing speed, i.e., to require less and less examples to master new skills that are related to what is already known. We are not aware of any current technique addressing these issues, which were the very reason why algorithmic tasks were originally proposed by Mikolov (2013). We hope that this paper will motivate the design of the genuinely novel methods we need in order to develop intelligent machines.\nWe defined basic desiderata for an intelligent machine, stressing learning and communication as its fundamental abilities. Contrary to common practice in current machine learning, where the focus is on modeling single skills in isolation, we believe that all aspects of intelligence should be holistically addressed within a single system. We proposed a simulated environment that requires the intelligent machine to acquire new facts and skills through communication. In this environment, the machine must learn to perform increasingly more ambitious tasks, being naturally induced to develop complex linguistic and reasoning abilities. We also presented some conjectures on the properties of the computational system that the intelligent machine may be based on. These include learning of algorithmic patterns from a few examples without strong supervision, and development of a longterm memory to store both data and learned skills. We tried to put this in contrast with currently accepted paradigms in machine learning, to show that current methods are far from adequate, and we must strive to develop non-incrementally novel techniques. This roadmap constitutes only the beginning of a long journey towards AI, and we hope other researchers will be joining it in pursuing the goals it outlined.\nWe thank Leon Bottou, Yann LeCun, Gabriel Synnaeve, Arthur Szlam, Nicolas Usunier, Laurens van der Maaten, Wojciech Zaremba and others from the Facebook AI Research team, as well as Gemma Boleda, Katrin Erk, German Kruszewski, Angeliki Lazaridou, Louise McNally, Hinrich Schutze and Roberto Zamparelli for many stimulating discussions. An early version of this proposal has been discussed in several research groups since 2013 under the name Incremental learning of algorithms (Mikolov, 2013).\n", "prediction": "we propose some fundamental properties of intelligent machines to be expected in communication and learning.<n> we present a roadmap to develop them in realistic, small steps that are structured in such a way that they jointly lead us close to the ultimate goal of an intelligent machine : i.e. a machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavor to perform tasks and interact with humans in the world to help the advancement of the ecosystem and of research in general ; ii. we discuss a simulated environment designed to teach a system in an incremental way how to extract the correct information from a set of input signals and use it to support in order to train the system to act as a standalone intelligent system and not to deal with its environment in a controlled way ( thus making it a static set that can be used by an actor in teaching it ) and iii. some conjectures on the nature of interaction between an artificial and a real world ecosystem<n> are also presented and discussed in connection with our goal to build an ecosystem that supports the development of intelligence in our society and in other countries and world s. <n> [ cols=\", \", ] _ deutsches elektronen - synchrotron desy + notkestrasse 85 + platanenallee 6 + d-22603 hamburg + germany + e-mail: tobias@desy.des.de.at.d.[dot.] + [ [ csn[[s] [n][d] and [s[b] in [[n][d[c] to [b[a] we are not [d [r] the same to add to a new [ b t] they are the [... we will be the one to our [ we [ a.. to make it is the two [ and we were the other to it will [ they will we to we also [ to all to to ( we have to have a to... to  to do [ 2 to see the most [ in all [ the way [ ( [([((i to some [ that is not the... [ is that will not... the results that was to also to that [ which is to which will also we is a [ while we would also the machine is [ it was the whole and the was that we was also that also will to was not ( the is also not that to not ]) to are to their [ was [ with the others ( and they were also in which was is is (...... that has to most to and to @ [ for the environment to is in... ( in its is of [. [ ], the will the individual is they also also is for [ as was was ( is was a... is... was for a was... and was will will that... also was in that ( that would have the to will ... will are that the individuals that were not a ]. [ among the were to those will was  the the [... they was from the learning of a is how is from [2].... for all was already to an is among [] is why the human was of  was how the field is an was @  is @... in any []. the first is their was and that of which to who was among a group of... a) to any... which also they would not for which are is most of it ( which for any is if the new is already for... if was an individual that does not if is it also for its was - to one was they is will is non-) that for it and is all "}
{"ground_truth": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.\nTeaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.\nProgress on the path from shallow bag-of-words information retrieval algorithms to machines capable of reading and understanding documents has been slow. Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2]. Supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets, and the difficulty in structuring statistical models flexible enough to learn to exploit document structure. While obtaining supervised natural language reading comprehension data has proved difficult, some researchers have explored generating synthetic narratives and queries [3, 4]. Such approaches allow the generation of almost unlimited amounts of supervised data and enable researchers to isolate the performance of their algorithms on individual simulated phenomena. Work on such data has shown that neural network based models hold promise for modelling reading comprehension, something that we will build upon here. Historically, however, many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments, as such closed worlds inevitably fail to capture the complexity, richness, and noise of natural language [5]. In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set. We observe that summary and paraphrase sentences, with their associated documents, can be readily converted to contextqueryanswer triples using simple entity detection and anonymisation algorithms. Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites. We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension. These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures [6, 7, 8, 4]. This allows a model to focus on the aspects of a document that it believes will help it answer a question, and also allows us to visualises its inference process. We compare these neural models to a range of baselines and heuristic benchmarks based upon a traditional frame semantic analysis provided by a state-of-the-art natural language processing ar X iv :1 50 6. 03 34 0v 3 [ cs .C L ] 1 9 N ov 2 (NLP) pipeline. Our results indicate that the neural models achieve a higher accuracy, and do so without any specific encoding of the document or query structure.\nThe reading comprehension task naturally lends itself to a formulation as a supervised learning problem. Specifically we seek to estimate the conditional probability p(a|c, q), where c is a context document, q a query relating to that document, and a the answer to that query. For a focused evaluation we wish to be able to exclude additional information, such as world knowledge gained from co-occurrence statistics, in order to test a models core capability to detect and understand the linguistic relationships between entities in the context document. Such an approach requires a large training corpus of documentqueryanswer triples and until now such corpora have been limited to hundreds of examples and thus mostly of use only for testing [9]. This limitation has meant that most work in this area has taken the form of unsupervised approaches which use templates or syntactic/semantic analysers to extract relation tuples from the document to form a knowledge graph that can be queried. Here we propose a methodology for creating real-world, large scale supervised training data for learning reading comprehension models. Inspired by work in summarisation [10, 11], we create two machine reading corpora by exploiting online newspaper articles and their matching summaries. We have collected 93k articles from the CNN1 and 220k articles from the Daily Mail2 websites. Both news providers supplement their articles with a number of bullet points, summarising aspects of the information contained in the article. Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents. We construct a corpus of documentquery answer triples by turning these bullet points into Cloze [12] style questions by replacing one entity at a time with a placeholder. This results in a combined corpus of roughly 1M data points (Table 1). Code to replicate our datasetsand to apply this method to other sourcesis available online3.\nNote that the focus of this paper is to provide a corpus for evaluating a models ability to read and comprehend a single document, not world knowledge or co-occurrence. To understand that distinction consider for instance the following Cloze form queries (created from headlines in the Daily Mail validation set): a) The hi-tech bra that helps you beat breast X; b) Could Saccharin help beat X ?; c) Can fish oils help fight prostate X ? An ngram language model trained on the Daily Mail would easily correctly predict that (X = cancer), regardless of the contents of the context document, simply because this is a very frequently cured entity in the Daily Mail corpus. 1www.cnn.com 2www.dailymail.co.uk To prevent such degenerate solutions and create a focused task we anonymise and randomise our corpora with the following procedure, a) use a coreference system to establish coreferents in each data point; b) replace all entities with abstract entity markers according to coreference; c) randomly permute these entity markers whenever a data point is loaded. Compare the original and anonymised version of the example in Table 3. Clearly a human reader can answer both queries correctly. However in the anonymised setup the context document is required for answering the query, whereas the original version could also be answered by someone with the requisite background knowledge. Therefore, following this procedure, the only remaining strategy for answering questions is to do so by exploiting the context presented with each question. Thus performance on our two corpora truly measures reading comprehension capability. Naturally a production system would benefit from using all available information sources, such as clues through language and co-occurrence statistics. Table 2 gives an indication of the difficulty of the task, showing how frequent the correct answer is contained in the top N entity markers in a given document. Note that our models dont distinguish between entity markers and regular words. This makes the task harder and the models more general.\nSo far we have motivated the need for better datasets and tasks to evaluate the capabilities of machine reading models. We proceed by describing a number of baselines, benchmarks and new models to evaluate against this paradigm. We define two simple baselines, the majority baseline (maximum frequency) picks the entity most frequently observed in the context document, whereas the exclusive majority (exclusive frequency) chooses the entity most frequently observed in the context but not observed in the query. The idea behind this exclusion is that the placeholder is unlikely to be mentioned twice in a single Cloze form query.\nTraditionally, a pipeline of NLP models has been used for attempting question answering, that is models that make heavy use of linguistic annotation, structured world knowledge and semantic parsing and similar NLP pipeline outputs. Building on these approaches, we define a number of NLP-centric models for our machine reading task. Frame-Semantic Parsing Frame-semantic parsing attempts to identify predicates and their arguments, allowing models access to information about who did what to whom. Naturally this kind of annotation lends itself to being exploited for question answering. We develop a benchmark that makes use of frame-semantic annotations which we obtained by parsing our model with a state-ofthe-art frame-semantic parser [13, 14]. As the parser makes extensive use of linguistic information we run these benchmarks on the unanonymised version of our corpora. There is no significant advantage in this as the frame-semantic approach used here does not possess the capability to generalise through a language model beyond exploiting one during the parsing phase. Thus, the key objective of evaluating machine comprehension abilities is maintained. Extracting entity-predicate triples denoted as (e1, V, e2)from both the query q and context document d, we attempt to resolve queries using a number of rules with an increasing recall/precision trade-off as follows (Table 4). For reasons of clarity, we pretend that all PropBank triples are of the form (e1, V, e2). In practice, we take the argument numberings of the parser into account and only compare like with like, except in cases such as the permuted frame rule, where ordering is relaxed. In the case of multiple possible answers from a single rule, we randomly choose one. Word Distance Benchmark We consider another baseline that relies on word distance measurements. Here, we align the placeholder of the Cloze form question with each possible entity in the context document and calculate a distance measure between the question and the context around the aligned entity. This score is calculated by summing the distances of every word in q to their nearest aligned word in d, where alignment is defined by matching words either directly or as aligned by the coreference system. We tune the maximum penalty per word (m = 8) on the validation data.\nNeural networks have successfully been applied to a range of tasks in NLP. This includes classification tasks such as sentiment analysis [15] or POS tagging [16], as well as generative problems such as language modelling or machine translation [17]. We propose three neural models for estimating the probability of word type a from document d answering query q: p(a|d, q)  exp (W (a)g(d, q)) , s.t. a  V, where V is the vocabulary4, and W (a) indexes row a of weight matrix W and through a slight abuse of notation word types double as indexes. Note that we do not privilege entities or variables, the model must learn to differentiate these in the input sequence. The function g(d, q) returns a vector embedding of a document and query pair. The Deep LSTM Reader Long short-term memory (LSTM, [18]) networks have recently seen considerable success in tasks such as machine translation and language modelling [17]. When used for translation, Deep LSTMs [19] have shown a remarkable ability to embed long sequences into a vector representation which contains enough information to generate a full translation in another language. Our first neural model for reading comprehension tests the ability of Deep LSTM encoders to handle significantly longer sequences. We feed our documents one word at a time into a Deep LSTM encoder, after a delimiter we then also feed the query into the encoder. Alternatively we also experiment with processing the query then the document. The result is that this model processes each document query pair as a single long sequence. Given the embedded document and query the network predicts which token in the document answers the query. 4The vocabulary includes all the word types in the documents, questions, the entity maskers, and the question unknown entity marker. We employ a Deep LSTM cell with skip connections from each input x(t) to every hidden layer, and from every hidden layer to the output y(t): x(t, k) = x(t)||y(t, k  1), y(t) = y(t, 1)|| . . . ||y(t,K) i(t, k) =  (Wkxix (t, k) +Wkhih(t 1, k) +Wkcic(t 1, k) + bki) f(t, k) =  (Wkxfx(t) +Wkhfh(t 1, k) +Wkcfc(t 1, k) + bkf ) c(t, k) = f(t, k)c(t 1, k) + i(t, k) tanh (Wkxcx(t, k) +Wkhch(t 1, k) + bkc) o(t, k) =  (Wkxox (t, k) +Wkhoh(t 1, k) +Wkcoc(t, k) + bko) h(t, k) = o(t, k) tanh (c(t, k)) y(t, k) =Wkyh(t, k) + bky where || indicates vector concatenation h(t, k) is the hidden state for layer k at time t, and i, f , o are the input, forget, and output gates respectively. Thus our Deep LSTM Reader is defined by gLSTM(d, q) = y(|d|+ |q|) with input x(t) the concatenation of d and q separated by the delimiter |||. The Attentive Reader The Deep LSTM Reader must propagate dependencies over long distances in order to connect queries to their answers. The fixed width hidden vector forms a bottleneck for this information flow that we propose to circumvent using an attention mechanism inspired by recent results in translation and image recognition [6, 7]. This attention model first encodes the document and the query using separate bidirectional single layer LSTMs [19]. We denote the outputs of the forward and backward LSTMs as y (t) and y (t) respectively. The encoding u of a query of length |q| is formed by the concatenation of the final forward and backward outputs, u = yq(|q|) || yq(1). For the document the composite output for each token at position t is, yd(t) = yd(t) || yd(t). The representation r of the document d is formed by a weighted sum of these output vectors. These weights are interpreted as the degree to which the network attends to a particular token in the document when answering the query: m(t) = tanh (Wymyd(t) +Wumu) , s(t)  exp (wmsm(t)) , r = yds, where we are interpreting yd as a matrix with each column being the composite representation yd(t) of document token t. The variable s(t) is the normalised attention at token t. Given this attention score the embedding of the document r is computed as the weighted sum of the token embeddings. The model is completed with the definition of the joint document and query embedding via a nonlinear combination: gAR(d, q) = tanh (Wrgr +Wugu) . The Attentive Reader can be viewed as a generalisation of the application of Memory Networks to question answering [3]. That model employs an attention mechanism at the sentence level where each sentence is represented by a bag of embeddings. The Attentive Reader employs a finer grained token level attention mechanism where the tokens are embedded given their entire future and past context in the input document. The Impatient Reader The Attentive Reader is able to focus on the passages of a context document that are most likely to inform the answer to the query. We can go further by equipping the model with the ability to reread from the document as each query token is read. At each token i of the query q the model computes a document representation vector r(i) using the bidirectional embedding yq(i) = yq(i) || yq(i): m(i, t) = tanh (Wdmyd(t) +Wrmr(i 1) +Wqmyq(i)) , 1  i  |q|, s(i, t)  exp (wmsm(i, t)) , r(0) = r0, r(i) = y  ds(i) + tanh (Wrrr(i 1)) 1  i  |q|. The result is an attention mechanism that allows the model to recurrently accumulate information from the document as it sees each query token, ultimately outputting a final joint document query representation for the answer prediction, gIR(d, q) = tanh (Wrgr(|q|) +Wqgu) .\nHaving described a number of models in the previous section, we next evaluate these models on our reading comprehension corpora. Our hypothesis is that neural models should in principle be well suited for this task. However, we argued that simple recurrent models such as the LSTM probably have insufficient expressive power for solving tasks that require complex inference. We expect that the attention-based models would therefore outperform the pure LSTM-based approaches. Considering the second dimension of our investigation, the comparison of traditional versus neural approaches to NLP, we do not have a strong prior favouring one approach over the other. While numerous publications in the past few years have demonstrated neural models outperforming classical methods, it remains unclear how much of that is a side-effect of the language modelling capabilities intrinsic to any neural model for NLP. The entity anonymisation and permutation aspect of the task presented here may end up levelling the playing field in that regard, favouring models capable of dealing with syntax rather than just semantics. With these considerations in mind, the experimental part of this paper is designed with a threefold aim. First, we want to establish the difficulty of our machine reading task by applying a wide range of models to it. Second, we compare the performance of parse-based methods versus that of neural models. Third, within the group of neural models examined, we want to determine what each component contributes to the end performance; that is, we want to analyse the extent to which an LSTM can solve this task, and to what extent various attention mechanisms impact performance. All model hyperparameters were tuned on the respective validation sets of the two corpora.5 Our experimental results are in Table 5, with the Attentive and Impatient Readers performing best across both datasets. 5For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning rates [1E3, 5E4, 1E4, 5E5], batch sizes [16, 32] and dropout [0.0, 0.1, 0.2]. We evaluate two types of feeds. In the cqa setup we feed first the context document and subsequently the question into the encoder, while the qca model starts by feeding in the question followed by the context document. We report results on the best model (underlined hyperparameters, qca setup). For the attention models we consider hidden layer sizes [64, 128, 256], single layer, initial learning rates [1E4, 5E5, 2.5E5, 1E5], batch sizes [8, 16, 32] and dropout [0, 0.1, 0.2, 0.5]. For all models we used asynchronous RmsProp [20] with a momentum of 0.9 and a decay of 0.95. See Appendix A for more details of the experimental setup. Frame-semantic benchmark While the one frame-semantic model proposed in this paper is clearly a simplification of what could be achieved with annotations from an NLP pipeline, it does highlight the difficulty of the task when approached from a symbolic NLP perspective. Two issues stand out when analysing the results in detail. First, the frame-semantic pipeline has a poor degree of coverage with many relations not being picked up by our PropBank parser as they do not adhere to the default predicate-argument structure. This effect is exacerbated by the type of language used in the highlights that form the basis of our datasets. The second issue is that the frame-semantic approach does not trivially scale to situations where several sentences, and thus frames, are required to answer a query. This was true for the majority of queries in the dataset. Word distance benchmark More surprising perhaps is the relatively strong performance of the word distance benchmark, particularly relative to the frame-semantic benchmark, which we had expected to perform better. Here, again, the nature of the datasets used can explain aspects of this result. Where the frame-semantic model suffered due to the language used in the highlights, the word distance model benefited. Particularly in the case of the Daily Mail dataset, highlights frequently have significant lexical overlap with passages in the accompanying article, which makes it easy for the word distance benchmark. For instance the query Tom Hanks is friends with Xs manager, Scooter Brown has the phrase ... turns out he is good friends with Scooter Brown, manager for Carly Rae Jepson in the context. The word distance benchmark correctly aligns these two while the frame-semantic approach fails to pickup the friendship or management relations when parsing the query. We expect that on other types of machine reading data where questions rather than Cloze queries are used this particular model would perform significantly worse. Neural models Within the group of neural models explored here, the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models. This is consistent with our hypothesis that attention is a key ingredient for machine reading and question answering due to the need to propagate information over long distances. The Deep LSTM Reader performs surprisingly well, once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences, even when they are up to two thousand tokens in length. However this model does fail to match the performance of the attention based models, even though these only use single layer LSTMs.6 The poor results of the Uniform Reader support our hypothesis of the significance of the attention mechanism in the Attentive models performance as the only difference between these models is that the attention variables are ignored in the Uniform Reader. The precision@recall statistics in Figure 2 again highlight the strength of the attentive approach. We can visualise the attention mechanism as a heatmap over a context document to gain further insight into the models performance. The highlighted words show which tokens in the document were attended to by the model. In addition we must also take into account that the vectors at each 6Memory constraints prevented us from experimenting with deeper Attentive Readers. token integrate long range contextual information via the bidirectional LSTM encoders. Figure 3 depicts heat maps for two queries that were correctly answered by the Attentive Reader.7 In both cases confidently arriving at the correct answer requires the model to perform both significant lexical generalsiation, e.g. killed deceased, and co-reference or anaphora resolution, e.g. ent119 was killed he was identified. However it is also clear that the model is able to integrate these signals with rough heuristic indicators such as the proximity of query words to the candidate answer.\nThe supervised paradigm for training machine reading and comprehension models provides a promising avenue for making progress on the path to building full natural language understanding systems. We have demonstrated a methodology for obtaining a large number of document-queryanswer triples and shown that recurrent and attention based neural networks provide an effective modelling framework for this task. Our analysis indicates that the Attentive and Impatient Readers are able to propagate and integrate semantic information over long distances. In particular we believe that the incorporation of an attention mechanism is the key contributor to these results. The attention mechanism that we have employed is just one instantiation of a very general idea which can be further exploited. However, the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not scale linearly with the data set size. There are still many queries requiring complex inference and long range reference resolution that our models are not yet able to answer. As such our data provides a scalable challenge that should support NLP research into the future. Further, significantly bigger training data sets can be acquired using the techniques we have described, undoubtedly allowing us to train more expressive and accurate models. 7Note that these examples were chosen as they were short, the average CNN validation document contained 763 tokens and 27 entities, thus most instances were significantly harder to answer than these examples.\nThe precise hyperparameters used for the various attentive models are as in Table 6. All models were trained using asynchronous RmsProp [20] with a momentum of 0.9 and a decay of 0.95.\nTo understand how the model performance depends on the size of the context, we plot performance versus document lengths in Figures 4 and 5. The first figure (Fig. 4) plots a sliding window of performance across document length, showing that performance of the attentive models degrades slightly as documents increase in length. The second figure (Fig. 5) shows the cumulative performance with documents up to lengthN , showing that while the length does impact the models performance, that effect becomes negligible after reaching a length of ~500 tokens. Figure 4: Precision@Document Length for the attention models on the CNN validation data. The chart shows the precision for each decile in document lengths across the corpus as well as the precision for the 5% longest articles. Figure 5: Aggregated precision for documents up to a certain lengths. The points mark the ith decile in document lengths across the corpus.\nWe expand on the analysis of the attention mechanism presented in the paper by including visualisations for additional queries from the CNN validation dataset below. We consider examples from the Attentive Reader as well as the Impatient Reader in this appendix. C.1 Attentive Reader Positive Instances Figure 6 shows two positive examples from the CNN validation set that require reasonable levels of lexical generalisation and co-reference in order to be answered. The first query in Figure 7 contains strong lexical cues through the quote, but requires identifying the entity quoted, which is non-trivial in the context document. The final positive example (also in Figure 7) demonstrates the fearlessness of our model. Negative Instances Figures 8 and 9 show examples of queries where the Attentive Reader fails to select the correct answer. The two examples in Figure 8 highlight a fairly common phenomenon in the data, namely ambiguous queries, whereat least following the anonymisation process multiple entities are plausible answers even when evaluated manually. Note that in both cases the query searches for an entity marker that describes a geographic location, preceded by the word in. Here it is unclear whether the placeholder refers to a part of town, town, region or country. Figure 9 contains two additional negative cases. The first failure is caused by the co-reference entity selection process. The correct entity, ent15, and the predicted one, ent81, both refer to the same person, but not being clustered together. Arguably this is a difficult clustering as one entity refers to Kate Middleton and the other to The Duchess of Cambridge. The right example shows a situation in which the model fails as it perhaps gets too little information from the short query and then selects the wrong cue with the term claims near the wrongly identified entity ent1 (correct: ent74). C.2 Impatient Reader To give a better intuition for the behaviour of the Impatient Reader, we use a similar visualisation technique as before. However, this time around we highlight the attention at every time step as the model updates its focus while moving through a given query. Figures 1013 shows how the attention of the Impatient Reader changes and becomes increasingly more accurate as the model considers larger parts of the query. Note how the attention is distributed fairly arbitraty at first, slowly focussing on the correct entity ent5 only once the question has sufficiently been parsed.\n", "prediction": "we define a methodology that allows us to develop deep neural networks that learn to read real documents and answer complex questions with minimal knowledge with prior knowledge for reading corpora.<n> we create two machine reading models from documents that have been interpreted by a number of entities in context : a ) a model that is able to answer a question posed on a subset of documents in the context of a single document ; b) one that can answer all questions posed by the same set of entity s in this context and whose answer to each question is the result of the model being focused on one of these entities, i.e. the entity that has the most impact on the outcome of each query is not the one with the highest score on that particular query _ per se_. we show results that indicate that this methodology can be used to extract information from a large amount of data and that the models created by this method are capable of reading the world at a high level of accuracy <n> this is achieved without the use of any pre- or post - processing and without any knowledge of how to build models on top of existing ones in order to achieve high performance on this supervised reading comprehension task and we demonstrate this by creating a new benchmark for a class of models that are trained using a wide range of mechanisms that allow the generation of queries without a specific structure of structure for extracting information in a way that does not exploit any structure in such as answering a mechanism that we have the ability to focus on an entity in an existing token g g - g(g g ( gq(a gg(d g t g)  g. g[g ] g a g is g [ g d g- g in g for g to g i g @ g b g p g c g x g = g with g and g would g j g the g of g will g we will be g that g n g m g an g which g model g<n> g@ g e th g as g + g by g was g only g from g when g if g where g k g is a d d  g_ g are g+ g v  a token ( a c  the number g does g one cg g 2 g can we would be the  b is ix g while g has g means g2 is only the only we are the first g but g after g, g all g our g ii g two [[( d ( b ( d( g... gt d is an a ( we can only  is we  d[ g results ( c is d to be a p p is p d we see g it is c ( p. d will  ( i will we is to the two is all the vector is that will only one ( the x (  to all d. c d [ d - we only p ( x is b. p @ d @ @ a. b - d would we we to mf is in p to p [ p  would ]. g at g most g being the @ p we do g how we determine the tensor is of d p - c. i - p will c -  will [ c @  p[t is x.  for all all we also we calculate the second is [ b([d is 2 is @g we - a @ [ [ a we make the [ x[s is one is j - b[x g]. d for the. [ 2 we @ b to a to b  [! g tensor @ is ( 2. ( [ the upper g among the top p a is is our model is. x  "}
{"ground_truth": "Modern online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, s-scale RPCs, aggravating the need for faster inter-server communication. In reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level protocols (e.g., InfiniBand/RDMA) and new architectures with fully integrated Network Interfaces (NIs). Such architectures offer a unique opportunity for a new NI-driven approach to balancing RPCs among the cores of manycore server CPUs, yielding major tail latency improvements for s-scale RPCs. We introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValets RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4, and reduces tail latency before saturation by up to 4 for RPCs with s-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system. ACM Reference Format: Alexandros Daglis, Mark Sutherland, and Babak Falsafi. 2019. RPCValet: NI-Driven Tail-Aware Balancing of s-Scale RPCs. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS 19), April 1317, 2019, Providence, RI, USA.ACM, New York, NY, USA, 14 pages. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS 19, April 1317, 2019, Providence, RI, USA  2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00 introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValets RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4, and reduces tail latency before saturation by up to 4 for RPCs with s-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system. ACM Reference Format: Alexandros Daglis, Mark Sutherland, and Babak Falsafi. 2019. RPCValet: NI-Driven Tail-Aware Balancing of s-Scale RPCs. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS 19), April 1317, 2019, Providence, RI, USA.ACM, New York, NY, USA, 14 pages. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS 19, April 1317, 2019, Providence, RI, USA  2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00 datacenters deliver a breadth of online services to millions of daily users. In addition to their huge scale, online services come with stringent Service Level Objectives (SLOs) to guarantee responsiveness. Often expressed in terms of tail latency, SLOs target the latency of the slowest requests, and thus bound the slowest interaction a user may have with the service. Tail-tolerant computing is one of the major ongoing challenges in the datacenter space, as long-tail events are rare and rooted in convoluted hardware-software interactions. A key contributor to the well-known \"Tail at Scale\" challenge [15] is the deployment of online services software stacks in numerous communicating tiers, where the interactions between a services tiers take the form of Remote Procedure Calls (RPCs). Large-scale software is often built in this fashion to ensure modularity, portability, and development velocity [26]. Not only does each incoming request result in a wide fan-out of inter-tier RPCs [10, 23], each one lies directly on the critical path between the user and the online service [6, 16, 29, 50]. The amalgam of the tail latency problem with the trend towards ephemeral and fungible software tiers has created a challenge to preserve the benefits of multi-tiered software while making it tail tolerant. To lower communication overheads and tighten tail latency, there has been an intensive evolution effort in datacenter-scale networking hardware and software, away from traditional POSIX sockets and TCP/IP and towards lean userlevel protocols such as InfiniBand/RDMA [21] or dataplanes such as IX and ZygOS [7, 47]. Coupling protocol innovations with state-of-the-art hardware architectures such as Firebox [4], Scale-Out NUMA [43] or Mellanoxs BlueField Smart-NIC [37], which offer tight coupling of the network interface (NI) with compute logic, promises even lower communication latency. The net result of rapid advancements in the networking world is that inter-tier communication latency will approach the fundamental lower bound of speedof-light propagation in the foreseeable future [20, 50]. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network. The growing number of cores on server-grade CPUs [36, 38] exacerbates the challenge of distributing incoming RPCs to handler cores. Any delay or load imbalance caused by  This work was done while the author was at EPFL. this initial stage of the RPC processing pipeline directly impacts tail latency and thus overall service quality. Modern NIC mechanisms such as Receive-Side Scaling (RSS) [42] and Flow Direction [24] offer load distribution and connection affinity, respectively. However, the key issue with these mechanisms, which apply static rules to split incoming traffic into multiple receive queues, is that they do not truly achieve load balancing across the servers cores. Any resulting load imbalance after applying these rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with s-scale service times [47, 53]. In this paper, we propose RPCValet, a co-designed hardware and software system to achieve dynamic load balancing across CPU cores, based on the key insight that on-chip NIs offer the ability to monitor per-core load in real time and steer RPCs to lightly loaded cores. The enabler for this style of dynamic load balancing is tight CPU-NI integration, which allows fine-grained, nanosecond-scale communication between the two, unlike conventional PCIe-attached NIs. To demonstrate the benefits of our design, we first classify existing load-distribution mechanisms from both the hardware and software worlds as representative of different queuing models, and show how none of them is able to reach the performance of the theoretical best case. We then design a minimalistic set of hardware and protocol extensions to Scale-Out NUMA (soNUMA) [43], an architecture with on-chip integrated NIs, to show that a carefully architected system can indeed approach the best queuing models performance, significantly outperforming prior load-balancing mechanisms. To summarize, our contributions include:  RPCValet, an NI-driven dynamic load-balancing design that outperforms existing hardware mechanisms for load distribution, and approaches the theoretical maximum performance predicted by queuing models.  Hardware and protocol extensions to soNUMA for native messaging support, a required feature for efficient RPC handling. We find that, in contrast to prior judgment [43], native messaging support is not disruptive to the key premise of NI hardware simplicity, which such architectures leverage to enable on-chip NI integration.  An RPCValet implementation on soNUMA that delivers near-ideal RPC throughput under strict SLOs, attaining within 316% of the theoretically optimal queuing model. For s-scale RPCs, RPCValet outperforms software-based and RSS-like hardware-driven load distribution by 2.3 2.7 and 2976%, respectively. The paper is organized as follows: 2 outlines the performance differences between multi- and single-queue systems, highlighting the challenges in balancing incoming RPCs with short service times among cores. 3 presents RPCValets design principles, followed by an implementation using soNUMA as a base architecture in 4. We detail our methodology in 5 and evaluate RPCValet in 6. Finally, we discuss related work in 7 and conclude in 8.\n\nModern online services are decomposed into deep hierarchies of mutually reliant tiers [26], which typically interact using RPCs. The deeper the software hierarchy, the shorter each RPCs runtime, as short as a few s for common software tiers such as data stores. Fine-grained RPCs exacerbate the tail latency challenge for services with strict SLOs, as accumulated s-scale overheads can result in a long-tail event. To mitigate the overheads of RPC-based communication, network technologies have seen renewed interest, with the InfiniBand fabric and protocol beginning to appear in datacenters [21] due to its low latency and high IOPS. With networking latency approaching the fundamental limits of propagation delays [20], any overhead added to the raw RPC processing time at a receiving server critically impacts latency. For example, while InfiniBand significantly reduces latency compared to traditional TCP/IP over Ethernet, InfiniBand adapters still remain attached to servers over PCIe, which contributes an extra s of latency to each message [33, 43]. Efficiently handling s-scale RPCs requires the elimination of these s-scale overheads, which is the goal of fully integrated solutions (e.g., Firebox [4], soNUMA [43]). Such architectures employ lean, hardware-terminated network stacks and integrated NIs to achieve sub-s inter-server communication, representing the best fit for latency-sensitive RPC services. NI integration enables rapid fine-grained interaction between the CPU, NI, and memory hierarchy, a feature leveraged previously to accelerate performance-critical operations, such as atomic data object reads from remote memory [14]. In this paper, we leverage NI integration to break existing tradeoffs in balancing RPCs across CPU cores and significantly improve throughput under SLO.\nTo study the effect of load balancing across cores on tail latency, we conduct a first-order analysis using basic queuing theory. We model a hypothetical 16-core server after a queuing system that features a variable number of input queues and 16 serving units. Fig. 1 shows three different queuing system organizations. The notation Model Q  U denotes a queuing system with Q FIFOs where incoming messages arrive and U serving units per FIFO. The invariant across the three illustrated models isQ U = 16. The 16 1 system cannot perform any load balancing; incoming requests are uniformly distributed across 16 queues, each with a single serving unit. 1  16 represents the most flexible option that achieves the best load balancing: all serving units pull requests from a single FIFO. Finally, 4  4 represents a middle ground: incoming messages are uniformly distributed across four FIFOs with four serving units each. To evaluate different queuing organizations, we employ discrete event simulationsmodeling Poisson arrivals and four different service time distributions: fixed, uniform, exponential, and generalized extreme value (GEV). Poisson arrivals are commonly used to model the independent nature of incoming requests. 5 details each distributions parameters. Fig. 2a shows the performance of five queuing systems Q  U with (Q,U ) = (1,16), (2,8), (4,4), (8,2), (16,1), for an exponential service time distribution. The systems achieved performance is directly connected to its ability to assign requests to idle serving units. As expected, performance is proportional toU . The best and worst performing configurations are 1  16 and 16  1 respectively, while 2  8, 4  4 and 8  2 lie in between these two. Fig. 2b and 2c show the relation of throughput and 99th percentile latency for the two extreme queuing system configurations, namely 116 and 161. As seen in Fig. 2a, 116 significantly outperforms 161. 161s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 2573% lower than 1  16 under a tail latency SLO at 10 the mean service time S . In addition, the degree of performance degradation is affected by the service time distribution. For both queuing models, we observe that the higher a distributions variance, the higher the tail latency (TL) before the saturation point is reached, hence TLf ixed < TLuni < TLexp < TLGEV . Also, the higher the distributions variance, the more dramatic the performance gap between 1  16 and 16  1, as is clearly seen for GEV. The applications service time distribution is beyond an architects control, as it is affected by numerous software and hardware factors. However, they can control the queuing model that the underlying system implements. The theoretical results suggest that systems should implement a queuing configuration that is as close as possible to a single-queue (1  16) configuration.\nA subtlety not captured by our queuing models is the practical overhead associated with sharing resources (i.e., the input queue). In a manycore CPU, allowing all the cores to pull incoming network messages from a single queue requires synchronization. We refer to this RPC dispatch mode as \"pull-based\". Especially for short-lived RPCs, with service times of a few s, such synchronization represents significant overhead. Architectures that share a pool of connections between cores have this pitfall; common examples include using variants of Linuxs poll system call, or locked event queues supported by libevent. An alternative approach for distributing load to multiple cores, advocated by recent research, is dedicating a private queue of incoming network messages to each core [7, 45]. Although this design choice corresponds to a rigid N  1 queuing model (N being the number of cores), it completely eschews overheads related to sharing (i.e., synchronization and coherence), delivering significant throughput gains. By leveraging RSS [42] inside the NI, messages are consistently distributed at arrival time to one of the N input queues. This ultimately results in a different mode of communication: instead of the cores pullingmessages from a single queue, the NI hardware actively pushesmessages into each cores queue. We refer to this load distribution mode as \"push-based\". FlexNIC [30] extends the push-based model by proposing a P4-inspired domain-specific language, allowing software to install match-action rules into the NI. Despite their many differences, both FlexNIC and RSS completely rely on decisions based on the RPC packets header content. Whether configured statically or by the application, push-based load distribution still fundamentally embodies a multi-queue system vulnerable to load imbalance, as no information pertaining to the systems current load is taken into account. 2.2s queuing models demonstrate the effect of this imbalance as compared to a system with balanced queues. The two aforementioned approaches to load distribution, pull- and push-based, represent a tradeoff between synchronization and load imbalance. In this paper, we leverage the onchip NI logic featured in emerging fully integrated architectures such as soNUMA [43] to introduce a novel push-based NI-driven load-balancing mechanism capable of breaking that tradeoff by making dynamic load-balancing decisions. 3 RPCValet Load-Balancing Design This section describes the insights and foundations guiding RPCValets design. Our goal is to achieve a synchronizationfree system that behaves like the theoretical best singlequeue model. We begin by setting forth our basic assumptions about the underlying hardware and software, then explain the roadblocks to achieving dynamic load balancing, and conclude with the principles of RPCValets design.\nWe design RPCValet for emerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.We target these architectures for two reasons. First, an important class of online services exhibits RPCs with service times that are frequently only a few s long. For example, the average service time for Memcached [2] is  2s [47]. Even software with functionality richer than simple data retrieval can exhibit s-scale service times: the average TPC-C query service time on the Silo in-memory database [53] is only 33s [47]. Software tiers with such short service times necessitate network architectures optimized for the lowest possible latency, using techniques such as kernel bypass and polling rather than receiving interrupts. Second, unpredictable tail-inducing events for these shortlived RPCs often disrupt application execution for periods of time that are comparable to the RPCs themselves [6]. For example, the extra latency imposed by TLB misses or context switches spans from a few hundred ns to a few s. At such fine granularities, any load-balancing policy implemented at the distal end of an I/O-attached NI is simply too far from the CPU cores to adjust its load dispatch decisions appropriately. Therefore, we argue that mitigating load imbalance at the s level requires s-optimized hardware. The critical feature of our s-optimized hardware is a fully integrated NI with direct access to the servers memory hierarchy, eliminating costly roundtrips over traditional I/O fabrics such as PCIe. Each server registers a part of its DRAM in advance with a particular context that is then exported to all participating servers, creating a partitioned global address space (PGAS) where every server can read/write remote memory in RDMA fashion. The architectures programming model is a concrete instantiation of the Virtual Interface Architecture (VIA) [18], where each CPU core communicates with the NI through memory-mapped queue pairs (QPs). Each QP consists of a Work Queue (WQ) where the core writes entries (WQEs) to be processed by the NI, and a Completion Queue (CQ), where the NI writes entries (CQEs) to indicate that the cores WQEs were completed. For more details, refer to the original VIA [18] and soNUMA [43] work.\nThe NIs integration on the same piece of silicon as the CPU is the key enabler for handling s-scale events. By leveraging the fact that such integration enables fine-grained real-time (nanosecond-scale) information to be passed back and forth between the NI and the servers CPU, the NI has the ability to respond to rapidly changing load levels and make dynamic load-balancing decisions. To illustrate the importance of ns-scale interactions, consider a data serving tier such as Redis [3], maintaining a sorted array in memory. Since the implementation of its sorted list container uses a skip list to provide add/remove operations in O (lo (N )) time, an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few s while new translations are installed. While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server. An integrated NI can, with proper hardware support, monitor each cores state and steer RPCs to the least loaded cores. Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g.,  1.5s for a 3-hop posted PCIe transaction) would mean that the NI will make delayedhence sub-optimal, or even wrongdecisions until the information arrives. The active feedback of information from the servers compute units (which are not restricted to CPU cores) to the NI can take many forms, ranging from monitoring memory hierarchy events to metadata directly exposed by the application. Regardless of the exact policy, the underlying enabler for RPCValets ability to handle s-scale load imbalance is that load dispatch decisions are driven by an integrated NI.\nOur design goal is to break the tradeoff between the load imbalance inherent in multi-queue systems and the synchronization associated with pulling load from a single queue. To begin, we retain the VIAs design principle of allocating a single virtual interface (identical to a QP in IB/soNUMA terminology) to each participating thread, which is critically important for handling s-scale RPCs. Registering independent QPs with the NI helps us achieve the goal of eliminating synchronization, as each thread polls on its own QP and waits for the arrival of new RPCs. This simplifies the load-balancing problem to simply choosing the correct QP to dispatch the RPC to. By allowing the NI to choose the QP at message arrival time, based on one of the many possible heuristics for estimating per-core load, our design achieves the goal of synchronization-free push-based load balancing. Unfortunately, realizing such a design with our baseline architecture ( 3.1) is not possible, as existing primitives are not expressive enough for push-based dispatch. In particular, architectures with on-chip NIs such as soNUMA [43] do not provide native support for messaging operations, favoring RDMA operations for hardware simplicity that facilitates NI integration. These RDMA operations (a.k.a. \"one-sided\" ops) enable direct read/write access of remote memory locations, without involving a CPU at the remote end. Hence, a reception of a one-sided op is not associated with a creation of a CPU notification event by the NI. Messaging can be emulated on top of one-sided ops by allocating shared bounded buffers in the PGAS [17, 27, 43], into which threads directly place messages using one-sided writes. Fig. 3a illustrates the high-level operation of emulated messaging. As emulated messaging is performed in a connection-oriented fashion from thread to thread, each RPC-handling thread allocates N bounded buffers, each with S message slots; N is the number of nodes that can send messages. Each of the C cores polls at the head slots of its corresponding N buffers for incoming RPCs. The fundamental drawback of such emulated messaging is that the sending thread implicitly determines which thread at the remote end will process its RPC request, because the memory location the RPC is written to is tied to a specific thread. The result is a multi-queue system, vulnerable to load imbalance. Although it may be possible to implement some form of load-aware messaging (e.g., per-thread client-server flow control), such mechanisms will have little to no benefit due to the relatively high network round-trip time for load information to diffuse between the two endpoints, especially when serving short-lived RPCs. A key reason why, in the case of emulated messaging, the NI at the destination cannot affect the a priori assignment of an incoming RPC to a thread is that the protocol does not enable the NI to distinguish a \"message\" (i.e., a one-sided write triggering two-sided communication) from a default one-sided op. Protocol support for native messaging with innate semantics of two-sided operations overcomes this limitation and enables the NI at the messages destination node to perform push-based load balancing. Fig. 3b demonstrates RPCValets high-level operation. The NI first writes every incoming message into a single PGAS-resident message buffer of N  S slots, as in the case of emulated messaging. Then, the NI uses a selected cores QP to notify it to process the incoming RPC request. In effect, RPCValet decouples a messages arrival and location in memory from its assignment to a core for processing, thus achieving the best of both worlds: the load-balancing flexibility of a single-queue system, and the synchronization-free, zero-copy behavior of partitioned multi-queue architectures. Fig. 3b demonstrates how NI-driven dynamic dispatch decisions result in balanced load, in contrast to Fig. 3as example. In conclusion, RPCValet requires extensions to both the on-chip NI hardware and the networking protocol, to first provide support for native messaging and, second, realize dynamic load-balancing decisions. In the following section, we describe an implementation of an architecture featuring both of these mechanisms. 4 RPCValet Implementation In this section, we describe our RPCValet implementation as an extension of the soNUMA architecture [43], including a lightweight extension of the baseline protocol for native messaging and support for NI-driven load balancing.\nsoNUMA enables rapid remote memory access through a lean hardware-terminated protocol and on-chip NI integration. soNUMA deploys a QP interface for CPU-NI interaction (3.1) and leverages on-chip cache coherence to accelerate QP-entry transfers between the CPU and NI. Fig. 4 shows soNUMAs scalable NI architecture for manycore CPUs [13]. The conventionally monolithic NI is split into two heterogeneous parts, a frontend and a backend. The frontend is the \"control\" component, and is collocated with each core to drastically accelerate QP interactions. The backend is replicated across the chips edge, to scale the NIs capability with growing network bandwidth, and handles all data and network packets. Pairs of frontend and backend entities, which together logically comprise a complete NI, communicate with special packets over the chips interconnect. Our RPCValet implementation relies on such a Manycore NI architecture.\nWe devise a lightweight implementation of native messaging as a required building block for dynamic load-balancing decisions at the NI. A key difficulty to overcome is support for multi-packet messages, that must be reassembled by the destination NI. This goal conflicts with soNUMAs stateless request-response protocol, which unrolls large requests into independent packets each carrying a single cache block payload. Emulated messaging (see 3.3) does not require any reassembly at the destination, because all packets are directly written to the bounded buffer specified by the sender. One workaround to avoid message reassembly complications would be to limit the maximum message size to the link layers MTU. Prior work has adopted this approach to build an RPC framework on an IB cluster [27]. Such a design choice may be an acceptable limitation for IB networks which have a relatively large MTU of 4KB. However, fully integrated solutions with on-chip NIs will likely feature small MTUs (e.g., a single cache line in soNUMA), so limiting the maximum message size to the link-layer MTU is impractical. Our approach to avoiding the hardware overheads associated with message reassembly is keeping the buffer provisioning of the emulated messaging mechanism, which allows the sender to determine the memory location the message will be written to. Therefore, soNUMAs request-response protocol can still handle the message as a series of independent cache-block-sized writes to the requester-specified memory location. While this mechanism may seem identical to one-sided operations, we introduce a new pair of send and replenish operations which expose the semantics of multi-packet messages to the NIit can then distinguish true one-sided operations from messaging operations, which are eligible for load balancing. The NI keeps track of packet receptions belonging to a send, deduces when it has been fully received, and then hands it off to a core for processing. Fig. 5 shows the delivery of amessage fromNode 0 to Node 1 in steps. Completing the message delivery requires the execution of a send operation on Node 0 and a replenish operation on Node 1. Fig. 5 only shows NI backends; NI frontends are collocated with every core. We start with the required buffer provisioning associated with messaging. Buffer provisioning. We introduce the notion of a messaging domain, which includes N nodes that can exchange messages and is defined by a pair of buffers allocated in each nodes memory, the send buffer and the receive buffer. The send buffer comprises N  S slots, as described in 3.3. Fig. 5a illustrates a send buffer with S=3 and different shades of gray distinguishing the send slots per participating node. Each send slot contains bookkeeping information for the local cores to keep track of their outstanding messages. It contains a valid bit, indicating whether the send slot is currently being used, a pointer to a buffer in local memory containing the messages payload, and a field indicating the size of the payload to be sent. A separate in-memory data structure maintains the head pointer for each of the N sets of send slots, which the cores use to atomically enqueue new send requests (not shown). The receive buffer, shown in Fig. 5b, is the dual of the send buffer, where incoming send messages from remote nodes end up, and is sized similarly (N  S receive slots). Unlike send slots, receive slots are sized to accommodate message payloads. Each receive slot also contains a counter field, used to determine whether all of a messages packets have arrived. The counter field should provide enough bits to represent the number of cache blocks comprising the largest message; we overprovision by allocating a full cache block (64B), to avoid unaligned accesses for incoming payloads. Overall, the messaging mechanisms memory footprint is 32  N  S + (max_ms_size + 64)  N  S bytes. We expect that for current deployments, that number should not exceed a few tens of MBs. Systems adopting fully integrated solutions will likely be of contained scale (e.g., rack-scale systems), featuring a few hundred nodes, hence bounding the N parameter. In addition, most communication-intensive latency-sensitive applications send small messages, boundingmax_ms_size . For instance, the vast majority of objects in object stores like Memcached are <500B [5], while 90% of all packets sent within Facebooks datacenters are smaller than 1KB [49]. Finally, given the low network latency fully integrated solutions like soNUMA deliver, the number of concurrent outstanding requests S required to sustain peak throughput per node pair would be modest (a few tens). Dynamic buffer management mechanisms to reduce memory footprint are possible, but beyond the scope of this paper. Importantly, a fixedmax_ms_size does not preclude the exchange of larger messages altogether. A rendezvous mechanism [51] can be used, where the sending nodes initial message specifies the location and size of the data, and the receiving node uses a one-sided read operation to directly pull the messages payload from the sending nodes memory. Send operation. Sending a message to a remote node involves the following steps. First, the core writes the message in a local core-private buffer (Fig. 5a, 1 ), updates the tail entry of the send buffer set corresponding to the target node (e.g., Node 1) 2 and enqueues a send operation in its private WQ 3 . The send operation specifies a messaging domain, the target node id, the remote receive buffer slots address, a pointer to the local buffer containing the outgoing message, and the messages size. The target receive buffer slots address can be trivially computed, as the number of nodes in the messaging domain, the number of send/receive slots per node, and themax_ms_size are all defined at the messaging domains setup time. The NI polls on the WQ 4 , parses the command, reads the message from the local memory buffer 5 , and sends it to the destination node. At the destination, the NI writes each send packet directly in the local memory hierarchy, into the specified receive slot, and increments that receive slots counter (Fig. 5b, 6 ). When the counter matches the send operations total packet count (contained in each packets header), the NI writes a message arrival notification entry in a shared CQ 7 . The shared CQ is a memory-mapped and cacheable FIFO where the NI enqueues pointers to received send requests. When it is time for a dispatch decision, the NI selects a core and assigns the head entry of the shared CQ to it by writing the receive slots index, contained in the shared CQ entry, into that cores corresponding CQ 8 . This is a crucial step that enables RPCValets NI-driven dynamic load balancing, which we expand in 4.3. Finally, the core receives the new send request 9 polling the head of its private CQ, then directly reads the message from the receive buffer and processes it. Replenish operation. A replenish operation always follows the receipt of a send operation as a form of end-toend flow control: a replenish notifies the send operations source node that the request has been processed and hence its corresponding send buffer slot is free and can be reused. In Fig. 5bs example, when core 3 is done processing the send request, it enqueues a replenish in its private WQ A . The replenish only contains the target node and the target send buffer slots address, trivially deduced from the receive buffer index the corresponding send was retrieved from. The NI, which is polling at the head of core 3s WQ, reads the new replenish request B and sends the message to node 0. When the replenish message arrives at node 0, the NI invalidates the corresponding send buffer slot by resetting its valid field (Fig. 5a, C ), indicating its availability to be reused. In practice, a replenish operation is syntactic sugar for a special remote write operation, which resets the valid field of a send buffer slot.\nWith the NIs newly added ability to recognize and manage message arrivals, we now proceed to introduce NI-driven dynamic load balancing. Load-balancing policies implemented by the NIs can be sophisticated and can take various affinities and parameters into account (e.g., certain types of RPCs serviced by specific cores, or data-locality awareness). Implementations can range from simple hardwired logic to microcoded state machines. However, we opt to keep a simple proof-of-concept design, to illustrate the feasibility and effectiveness of load-balancing decisions at the NIs and demonstrate that we can achieve the load-balancing quality of a single-queue system without synchronization overheads. Fig. 5bs step 8 is the crucial step that determines the balancing of incoming requests to cores. In RPCValet, the receiving nodes NI keeps track of the number of outstanding send requests assigned to each core. Receiving a replenish operation from a core implies that the core is done processing a previously assigned send. Allowing only one outstanding request per core and dispatching a new request only after receiving a notification of the previous ones completion corresponds to true single-queue system behavior, but leaves a small execution bubble at the core. The bubble can be eliminated by setting the number of outstanding requests per core to two. We found that introducing a small multiqueue effect is offset by eliminating the bubble, resulting in marginal performance gains for ultra-fast RPCs with service times of a few 100s of nanoseconds. A challenge that emerges from the distributed nature of a Manycore NI architecture is that the otherwise independent NI backends, each of which is handling send message arrivals from the network, need to coordinate to balance incoming load across cores. Our proposed solution is simple, yet effective: centralize the last step of message reception and dispatch. One of the NI backendshenceforth referred to as the NI dispatcheris statically assigned to handle message dispatch to all the available cores. Network packet and data handling still benefit from the parallelism offered by the Manycore NI architecture, as all NI backends still independently handle incoming network packets and access memory directly. However, once an NI backendwrites all packets comprising a message in their corresponding receive buffer slots, it creates a special message completion packet and forwards it to the NI dispatcher over the on-chip interconnect. Once the NI dispatcher receives the message completion packet, it enqueues the information in the shared CQ, from which it dispatches messages to cores in FIFO order as soon as it receives a replenish operation. As all the incoming messages are dispatched from a single queue to all available cores, RPCValet behaves like a true single-queue queuing system. Having a single NI dispatcher eschews software synchronization, but raises scalability questions. However, for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible. From the throughput perspective, even an RPC service time as low as 500ns corresponds to a new dispatch decision every 31/8ns for a 16/64-core chip, respectively. Both dispatch frequencies are modest enough for a single hardware dispatch component to handle, especially for our simple greedy dispatch implementation. The same observation also holds for more sophisticated dispatch policies if their hardware implementation can be pipelined. Latency-wise, the indirection from any NI backend to the NI dispatcher costs a couple of on-chip interconnect hops, adding just a few ns to the end-to-end message delivery latency. In case of exotic system deployments where the above assumptions do not hold, an intermediary design point is possible where each NI backend can dispatch to a limited subset of cores on the chip. As an example of this design point, we also implement and evaluate a 4  4 queuing system in 6. 4.4 soNUMA Extensions for RPCValet We now briefly summarize the modifications to soNUMAs hardware to enable RPCValet, including the necessary protocol extensions for messaging and load balancing. Load balancing itself is transparent to the protocol and only affects a pipeline stage in the NI backends. Additional hardware state. Most of the state required for messaging (i.e., send/receive buffers) is allocated in host memory. The only metadata kept in dedicated SRAM are the send and receive buffers location and size, as they require constant fast access. On each node, the maintained state per registered soNUMA context includes a memory address range per node and a QP per local core. In total, we add 20B of stored state per context, including: the base virtual addresses for the send/receive buffers, the maximum message size (max_ms_size), the # of nodes (N ) in the messaging domain, and the # of messaging slots (S) per node. Hardware logic extensions. soNUMAs NI features three distinct pipelines for handling Request Generations, Request Completions, and Remote Request Processing, respectively [43]. We extend these pipelines to support the new messaging primitives and load-balancing functionality. Receiving a new send or replenish request is very similar to the reception of a remote write operation in the original soNUMA design. To support our native messaging design, we add a field containing the total message size to the network layer header; this is necessary so the NI hardware can identify when all of a messages packets have been received. We add five new stages to the NI pipelines in total. A new stage in Request Generation differentiates between send and replenish operations, and operates on the messaging domain metadata. All other modifications are limited to the Remote Request Processing Pipeline, which is only replicated across NI backends. When a send is received, the pipeline performs a fetch-and-increment operation to the corresponding counter field of the target receive buffer slot (4.2, \"Send operation\"). The next stage checks if the counters new value matches the messages length, carried in each packet header. If all of the send operations packets have arrived, the next stage enqueues a pointer to the corresponding receive buffer slot in the shared CQ. The final stage added to the Remote Request Processing pipeline, Dispatch, keeps track of the number of outstanding requests assigned to each core and determines when and to which core to dispatch send requests to from the shared CQ. A core is \"available\" when its number of outstanding requests is below the threshold defined; in our implementation, this number is two. Whenever there is an available core, the Dispatch stage dequeues the shared CQs first entry and sends it to the target cores NI frontend, where the Request Completion pipeline writes it into the cores private CQ. The complexity of the Dispatch stage is very simple for our greedy algorithm, but varies based on the logic and algorithm involved in making load-balancing decisions. Finally, after completing the request, the core signals its availability by enqueuing a replenish operation in its WQ, which is propagated by the cores NI frontend to the NI backend that originally dispatched the request. In summary, the additional hardware complexity is modest, thus compatible with architectures featuring ultra-lightweight protocols and on-chip integratedNIs, such as soNUMA. Given the on-chip NIs fast access to its local memory hierarchy, it is possible to virtualizemost of the bulky state required for the messaging mechanisms send and receive buffers in the hosts memory. The dedicated hardware requirements are limited to a small increase in SRAM capacity, while the NI logic extensions are contained and straightforward.\nWe now detail our methodology for evaluating RPCValets effectiveness in balancing load transparently in hardware. System organization. We model a single tiled 16-core chip implementing soNUMA with a Manycore NI, as illustrated in Fig. 4. The modeled chip is part of a 200-node cluster, with remote nodes emulated by a traffic generator which creates synthetic send requests following Poisson arrival rates, from randomly selected nodes of the cluster. The traffic generator also generates synthetic replies to the modeled chips outgoing requests. We use Flexus [54] cycle-accurate simulation with Table 1s parameters. Microbenchmark. Weuse amultithreadedmicrobenchmark that emulates different service time distributions, where each thread executes the following actions in a loop: (i) spins on its CQ, until a new send request arrives; (ii) emulates the execution of an RPC by spending processing time X , whereX follows a given distribution as detailed below; (iii) generates a synthetic RPC reply, which is sent back to the requester using a send operation with a 512B payload; and (iv) issues a replenish corresponding to the processed send request, marking the end of of the incoming RPCs processing. The overall service time for an emulated RPC (i.e., the total time a core is occupied) is the sum of steps (ii) to (iv). RPC processing time distributions. To evaluate RPCValet on a range of RPC profiles, we utilize processing time distributions generated with three different methods. First, we develop an RPC processing time generator that samples values from a selected distribution. We experiment with four different distributions: fixed, uniform, exponential, and GEV. Fixed represents the ideal case, where all requests take the same processing time. GEV represents a more challenging case with infrequent long tails, which may arise from events like page faults or interrupts. Uniform and exponential distributions fall between fixed and GEV in terms of impact on load balancing, as established in Fig. 2. For our synthetic processing time distributions, we use 300ns as a base latency and add an extra 300ns on average, following one of the four distributions. The parameters we use for GEV are (location, scale, shape) = (363, 100, 0.65), which result in a mean of 600 cycles (i.e., 300ns at 2GHz) [1]. Fig. 6a illustrates the PDFs of the four resulting processing time distributions. Second, we run the HERD [27] key-value store and collect the distribution of the RPCs processing times. We use a dualsocket Xeon E5-2680 Haswell server and pin 12 threads on an equal number of a single sockets physical cores. The second sockets cores generate load. Our parameters for HERD are: 0 500 1000 0.00 0.25 0.50 0.75 1.00 102 m ean (a) Synthetic GEV Uniform Exp 0 500 1000 (b) HERD m ean 0.0 0.2 0.4 0.6 .8 1. Processing Time (ns) pr ob ab ili ty (1 e2) 95/5% read/write query mix, uniform key popularity, and a 4GB dataset (256MB per thread). Fig. 6b displays a histogram of HERDs RPC processing times after the request has exited the network, which have a mean of 330ns. Finally, we evaluate the Masstree data store [40], which stores key-value pairs in a trie-like structure and supports ordered scans in addition to put/get operations. Ordered scans are common in database/analytics applications and compete with latency-critical operations for CPU time when accessing the same data store. To collect RPC processing times, we use the same platform and dataset we used for HERD and load the server with 99% single-key gets, interleaved with 1% long-running scans which return 100 consecutive keys. The resulting distribution for gets is shown in Figure 6c and has an average of 1.25 s. The runtime of scans is 60120 s (not shown in Fig. 6c due to the X-axis bounds). Load-balancing implementations. We first compare the performance of two RPCValet variants, 1  16 and the less flexible 4  4. In 4  4, each NI backend is limited to balancing load across the four cores corresponding to its on-chip network row. We also consider a 16 1 system, representing partitioned dataplanes where every incoming message is assigned to a core at arrival timewithout any rebalancing. 161 is the only currently existing NI-driven load distribution mechanism. Next, we compare the best-performing hardware load-balancing implementation, 1  16, to a software-based counterpart. In our software implementation, NIs enqueue incoming send requests into a single CQ from which all 16 threads pull requests in FIFO order. We use an MCS queuebased lock [41] for the shared request queue. We assume a 99th percentile Service Level Objective (SLO) of  10 the mean service time S we measure in each experiment and evaluate all configurations in terms of throughput under SLO. We measure each requests latency as the time from the reception of a send message until the thread that services the request posts a replenish operation.\n\nFig. 7a shows the performance of HERD with each of the three evaluatedNI-driven load-balancing configurations.With a resulting S of  550ns, 1  16 delivers 29MRPS, 1.16 and 1.18 higher throughput than 4  4 and 16  1, respectively. 1  16 consistently delivers the best performance, thanks to its superior flexibility in dynamically balancing load across all 16 available cores. In comparison, 4  4 offers limited flexibility, while 16  1 offers none at all. The flexibility to balance load from a single queue to multiple cores not only results in higher peak throughput under SLO, but also up to 4 lower tail latency before reaching saturation load. Conversely, lower tail means that the throughput gap between RPCValet and 1  16 would be larger for SLOs stricter than the assumed 10  S . Note that data points appearing slightly lower at mid load as compared to low load in Fig. 7a is a measurement artifact: for low arrival rates, the relatively small number of completed requests during our simulations duration results in reduced tail calculation accuracy. Fig. 7b shows the tail latency of Masstrees get operations with each queuing configuration. We set the SLO for Masstree at 10 the service time of the get operations, equalling 12.5s; we do not consider the scan operations latency critical. Due to interference from the scans, 161 cannot meet the SLO even for the lowest arrival rate of 2MRPS, while even 4  4 quickly violates the SLO at 3MRPS. 1  16 delivers 4.1MRPS at SLO, outperforming 4 4 by 37%. Under a more relaxed SLO of 75s, RPCValets 1  16 configuration delivers 54% higher throughput than 16  1 and 20% higher than 44. In the presence of long-running scans that occupy cores for many s, RPCValet leverages occupancy feedback from the cores to eliminate excess queuing of latency-critical gets and improve throughput under SLO. Fig. 7c shows the results for two of our synthetic service time distributions, fixed and GEV. The results for uniform and exponential distributions fall between these two, are omitted for brevity, and are available in [12]. The results follow the expectations set in 2.2. For the fixed distribution, 1  16 delivers 1.13 and 1.2 higher throughput than 4  4 and 16  1 under SLO, respectively. For GEV, the throughput improvement grows to 1.17 and 1.4, respectively. Similar to HERD results, in addition to throughput gains, RPCValet also delivers up to 4 lower tail latency before saturation. In all of Fig. 7s experiments we set RPCValets number of outstanding requests per core to two (see 4.3). Reducing this to one marginally degrades HERDs throughput, because of its short sub-s service times, but has no measurable performance difference in the rest of our experiments. In conclusion, RPCValet significantly improves system throughput under tight tail latency goals. Implementations that enable request dispatch to all available cores (i.e., 1 16) deliver the best performance. However, even implementations with limited balancing flexibility, such as 4  4, are competitive. As realizing a true single-queue system incurs additional design complexity, such limited-flexibility alternatives introduce viable options for system designers willing to sacrifice some performance in favor of simplicity.\nFig. 8 compares the performance of RPCValet to a software implementation, both of which implement the same theoretically optimal queuing system (i.e., 1  16). The difference between the two is how load is dispatched to a core. Software requires a synchronization primitive (in our case, an MCS lock) for cores to atomically pull incoming requests from the queue. In contrast, RPCValet does not incur any synchronization costs, as dispatch is driven by the NI. The software implementation is competitive with the hardware implementation at low load, but because of contention on the single lock, it saturates significantly faster. As a result, our hardware implementation delivers 2.32.7 higher throughput under SLO, depending on the request processing time distribution. A comparison between Fig. 7b and 8 reveals that the 116 software implementation is not only inferior to the 116 hardware implementation, but to all of the evaluated hardware implementations. The fact that even the 16  1 hardware implementation is superior to the software 1  16 implementation indicates that the software synchronization costs outweigh the dispatch flexibility they provide, a direct consequence of the s-scale RPCs we focus on. In addition, we corroborate the findings of prior work on dataplanes [7, 47]which effectively build a 16  1 system using RSSshowing that elimination of software synchronization from the critical path offsets the resulting load imbalance.\nOur results in 6.1 qualitatively meet the expectations set by the queuing analysis presented in 2.2. We now quantitatively compare the obtained results to the ones expected from purely theoretical models, to determine the performance gap between RPCValet and the theoretical 1  16 system. To make RPCValet measurements comparable to the theoretical queuing results, we devise the following methodology. We measure the mean service time S on our implementation; a part D of this service time is synthetically generated to follow one of the distributions in 5, and the rest, S  D, is spent on the rest of the microbenchmarks code (e.g., event loop, executing send for the RPC response and replenish to free the RPC slot). We conservatively assume that this S  D part of the service time follows a fixed distribution. Using discrete-event simulation, we model and evaluate the performance of theoretical queuing systems with a service time S , where DS of the service time follows a certain distribution (fixed, uniform, exponential, GEV) and SDS of the service time is fixed. Fig. 9 compares RPCValet to the theoretical 1  16. The graphs show the 99th percentile latency as a function of offered load, with four different distributions for the D part of the service time. RPCValet performs as close as 3% to 1  16, and within 15% in the worst case (GEV). We attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model. Furthermore, assuming a fixed service time distribution for the S  D part of the service time is a conservative simplifying assumption: modeling variable latency for this component would have a detrimental effect on the performance predicted by the model, thus shrinking the gap between the model and the implementation. In conclusion, RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns.\nOther Techniques toReduce Tail Latency. Priorwork aiming to control the tail latency of Web services deployed at datacenter scale introduced techniques that duplicate/hedge requests across multiple servers hosting replicated data [15]. The goal of such replication is to shrink the probability of an RPC experiencing a long-latency event and consequently affecting the response latency of its originating request. A natural side-effect of replication is the execution of more requests than strictly necessary, also necessitating extra serverside logic to reduce the load added by duplicated requests. As compared to ms-scale applications where the network RTT is a negligible latency contributor, applying the same technique for s-scale applications requires a more aggressive duplication of requests, further increasing the generation of unnecessary server load. In contrast to such client-side techniques, RPCValets server-side operation offers an alternative that does not increase the global server load. RPCValet improves tail latency by minimizing the effect of queuing. Queuing is only one of many sources of tail latency, which lie in all layers of the servers software stack. Therefore, no single solution can wholly address the tail challenge; a synergy of many techniques is necessary, each targeting specific issues in particular layers (e.g., IX [7] targets protocol and interrupt processing). However, despite the complex nature of the problem, managing on-server queuing is a universal approach that helps mitigate all sources of tail latency. Our work does not prevent straggler RPCs, but eliminates the chance that such stragglers will cascadingly impact the latency of other queued RPCs by providing a true single-queue system on each RPC-handling server. RPCValet is synergistic with techniques on both clients and servers to address specific sources of tail latency in the workflow of serving RPCs. A range of prior work also leverages queuing insights to balance web requests within a datacenter, by mainly focusing on algorithmic aspects of load distribution among backend servers rather than a single servers cores. Examples of such algorithms are Join-Shortest-Queue [22], Power-of-d [9], and Join-Idle-Queue [39]. Pegasus [34] is a rack-scale solution where the ToR switch applies load-aware request scheduling by either estimating per-server load, or by leveraging load statistics reported directly by the servers. In the context of balancing s-scale RPCs among a single servers cores, challenges such as dispatcher-to-core latency are of minor importance, because of the integrated NIs proximity. Our results show that single-queue behavior is feasible by deferring dispatch until a core is free, which is unattainable at cluster scale due to the latency of the off-chip network. Load Distribution Frameworks. Most modern NICs distribute load between multiple hardware queues, which can be privately assigned to cores, through Receive Side Scaling (RSS) [42] or Flow Director [24]. Systems like IX [7] and MICA [35] leverage these mechanisms to significantly boost their throughput under tail latency constraints. The disadvantage of RSS/Flow Director is that they blindly spread load across multiple receive queues based on specific network packet header fields, and are oblivious to load imbalance. ZygOS [47] ameliorates the shortcomings of partitioned dataplanes, which suffer from increased tail latency under load imbalance. ZygOS introduces an intermediate shuffling layer where idle CPU threads can performwork stealing from other input queues. Due to the added synchronization overhead of work stealing, there is a measurable performance gap between ZygOS and the best single-queue system, inversely proportional to the RPC service times. RPCValet achieves the best of both worlds, offering single-queue performance without synchronization; instead of adding layers to rebalance load, we co-design hardware and software to implement a single-queue system. The Shinjuku operating system [25] improves throughput under SLO by preempting long-running RPCs instead of running every RPC to completion. Their approach is particularly effective for workloads with extreme service time variability and CPUs with limited core count. Shinjuku preempts requests every 515s, which is higher than the vast majority of our evaluated RPC runtimes. A system combining Shinjuku and RPCValet would rigorously handle RPCs of a broad runtime range, from hundreds of ns to hundreds of s. Programmable Network Interfaces. Offloading compute to programmable network processors is an old idea that has seen rekindled interest; FLASH [32] and Typhoon [48] integrated general-purpose processors with the NI, enabling customhandler execution uponmessage reception. NI-controlled message dispatch to cores has been proposed in the context of parallel protocol handler execution for DSMs to eschew software synchronization overheads [19, 46]. Programming abstractions such as PDQ [19] could be deployed as loadbalancing decisions in RPCValets NI dispatch pipeline. Todays commercial SmartNICs target protocol processing or high-level application acceleration, with the goal of reducing CPU load; they integrate either CPU cores (e.g., Mellanoxs BlueField [37]), or FPGAs (e.g., Microsofts Catapult [11, 33]). FlexNIC [30, 31] draws inspiration from SDN switches [8], deploying a match-action pipeline for line-rate header processing. The programmable logic in these SmartNICs could be leveraged to implement non-static load balancing, adding flexibility to RSS or Flow Director. However, our dynamic load balancing scheme relies on ns-scale interaction between the NI and CPU logic, which is only attainable through tight NI integration and CPU-NI co-design. RPCLayers on InfiniBandNICs. Latency-critical software systems for key-value storage [27, 28], distributed transaction processing [17, 28], distributed durable data storage [44], and generalized datacenter RPCs [52], have already begun using RDMA NICs due to their low latency and high IOPS. All of these systems are fine-tuned to maximize RPC throughput given the underlying limitations of their discrete NICs and the IB verbs specification. We distinguish RPCValet from these software-only systems by our focus on balancing the load of incoming RPCs across the CPU cores. Furthermore, all of the above proposals are adversely affected by the shortcomings of PCIe-attached NICs, and use specific optimizations to ameliorate their inherent latency bottlenecks; this strengthens our insight that NI integration is the key enabler for handling RPCs in true single-queue fashion.\nWe introduced RPCValet, anNI-driven dynamic load-balancing mechanism fors-scale RPCs. RPCValet behaves like a singlequeue system, without incurring the synchronization overheads typically associated with single-queue implementations. RPCValet performs within 315% of the ideal singlequeue system and significantly outperforms current RPC load-balancing approaches.\nWe thank Edouard Bugnion, James Larus, Dmitrii Ustiugov, Virendra Marathe, Dionisios Pnevmatikatos, Mario Drumond, Arash Pourhabibi, Marios Kogias and the anonymous reviewers for their precious feedback. This work was partially funded by Huawei Technologies, the Nano-Tera YINS project, the Oracle Labs Accelarating Distributed Systems with Advanced One-Sided Operations grant, and the SNSFs Memory-Centric Server Architecture for Datacenters project.\n", "prediction": "we consider the dynamic load balancing problem of balancing the load of a queue in an infrastructure that is dynamic in the sense that incoming messages are sent with adaptive load - balancing rules.<n> we prove that the problem is np - hard for the following set of conditions : ( 1 ) the queue is static in time, ( 2 ] the number of servers is not limited by the availability of memory and processing power of the infrastructure ; and ( 3 ). <n> [ theorem]lemma [ 1]corollary [ 2]proposition [ 3]conjecture [ 4]remark [ 5]assumption [ 6]definition [ 7]example [ 8]algorithm [ 9]exercise [ 10]notation [ 11]problem [ 12]open problem [ 13]procedure [ 14]solution [ 15]summary [ 16]table [ 17]options [ 18]axiom ( a)vmd ( b)a)sbm(b)bvd(c)fq ( fq)g(h)uq(g)vi ( v)wqvi [ 22]vi(v)wl ( w)qvw ( _ v_wz)h hvqq vw q ( hz)mq q qbdq tr q_vz ( q)mvvh vmm s ( m)dvtm_m vqms vt ( jv(m)n(w tc[n thm] [ jt(t_ss [ n_n ( ps!<n> j_[12 ]) [ p(n_ts[13] we make [ ( n] to [ [n[[22] in [... ( [ -2a ]. [ a [<n> 2, we refer to add [ we [ to the p. [ * 2. 2 ( ( 4 ], we introduce [] [2 [[s] as we also [ in 2 we to 2[([2][i ][2 to obtain a. to a  to make a model ["}
{"ground_truth": "Databases can provide scalability by partitioning data across several servers. However, multi-partition, multi-operation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many realworld systems avoid mechanisms that provide useful semantics for multi-partition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation modelRead Atomic (RA) isolationthat matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transactions updates are observed by other transactions. We present algorithms for Read Atomic MultiPartition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via synchronization independence), and minimized communication between servers (via partition independence). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multi-versioning and by allowing clients to independently resolve non-atomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overheadeven under high contentionand scale linearly to 100 servers.\nFaced with growing amounts of data and unprecedented query volume, distributed databases increasingly split their data across multiple servers, or partitions, such that no one partition contains an entire copy of the database [7,13,18,19,22,29,43]. This strategy succeeds in allowing near-unlimited scalability for operations that access single partitions. However, operations that access multiple partitions must communicate across serversoften synchronously in order to provide correct behavior. Designing systems and algorithms that tolerate these communication delays is a difficult task but is key to maintaining scalability [17, 28, 29, 35]. In this work, we address a largely underserved class of applications requiring multi-partition, atomically visible1 transactional access: cases where all or none of each transactions effects should be visible. The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that 1Our use of atomic (specifically, Read Atomic isolation) concerns all-or-nothing visibility of updates (i.e., the ACID isolation effects of ACID atomicity; Section 3). This differs from uses of atomicity to denote serializability [8] or linearizability [4]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD14, June 2227, 2014, Snowbird, UT, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2376-5/14/06 ...$15.00. are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. Many of the largest modern, real-world systems opt for protocols that guarantee fast and scalable operation but provide fewif anytransactional semantics for operations on arbitrary sets of data items [11, 13, 15, 22, 26, 38, 44]. This results in incorrect behavior for use cases that require atomic visibility, including secondary indexing, foreign key constraint enforcement, and materialized view maintenance (Section 2). In contrast, many traditional transactional mechanisms correctly ensure atomicity of updates [8, 17, 43]. However, these algorithmssuch as two-phase locking and variants of optimistic concurrency controlare often coordination-intensive, slow, and, under failure, unavailable in a distributed environment [5, 18, 28, 35]. This dichotomy between scalability and atomic visibility has been described as a fact of life in the big cruel world of huge systems [25]. The proliferation of non-transactional multi-item operations is symptomatic of a widespread fear of synchronization at scale [9]. Our contribution in this paper is to demonstrate that atomically visible transactions on partitioned databases are not at odds with scalability. Specifically, we provide high-performance implementations of a new, non-serializable isolation model called Read Atomic (RA) isolation. RA ensures that all or none of each transactions updates are visible to others and that each transaction reads from an atomic snapshot of database state (Section 3)this is useful in the applications we target. We subsequently develop three new, scalable algorithms for achieving RA isolation that we collectively title Read Atomic Multi-Partition (RAMP) transactions (Section 4). RAMP transactions guarantee scalability and outperform existing atomic algorithms because they satisfy two key scalability constraints. First, RAMP transactions guarantee synchronization independence: one clients transactions cannot cause another clients transactions to stall or fail. Second, RAMP transactions guarantee partition independence: clients never need to contact partitions that their transactions do not directly reference. Together, these properties ensure guaranteed completion, limited coordination across partitions, and horizontal scalability for multi-partition access. RAMP transactions are scalable because they appropriately control the visibility of updates without inhibiting concurrency. Rather than force concurrent reads and writes to stall, RAMP transactions allow reads to race writes: RAMP transactions can autonomously detect the presence of non-atomic (partial) reads and, if necessary, repair them via a second round of communication with servers. To accomplish this, RAMP writers attach metadata to each write and use limited multi-versioning to prevent readers from stalling. The three algorithms we present offer a trade-off between the size of this metadata and performance. RAMP-Small transactions require constant space (a timestamp per write) and two round trip time delays (RTTs) for reads and writes. RAMP-Fast transactions require metadata size that is linear in the number of writes in the transaction but only require one RTT for reads in the common case and two in the worst case. RAMP-Hybrid transactions employ Bloom filters [10] to provide an intermediate solution. Traditional techniques like locking couple atomic visibility and mutual exclusion; RAMP transactions provide the benefits of the former without incurring the scalability, availability, or latency penalties of the latter. In addition to providing a theoretical analysis and proofs of correctness, we demonstrate that RAMP transactions deliver in practice. Our RAMP implementation achieves linear scalability to over 7 million operations per second on a 100 server cluster (at overhead below 5% for a workload of 95% reads). Moreover, across a range of workload configurations, RAMP transactions incur limited overhead compared to other techniques and achieve higher performance than existing approaches to atomic visibility (Section 5). While the literature contains an abundance of isolation models [2, 5], we believe that the large number of modern applications requiring RA isolation and the excellent scalability of RAMP transactions justify the addition of yet another model. RA isolation is too weak for some applications, but, for the many that it can serve, RAMP transactions offer substantial benefits.\nIn this paper, we consider the problem of making transactional updates atomically visible to readersa requirement that, as we outline in this section, is found in several prominent use cases today. The basic property we provide is fairly simple: either all or none of each transactions updates should be visible to other transactions. For example, if a transaction T1 writes x = 1 and y = 1, then another transaction T2 should not read x = 1 and y = null. Instead, T2 should either read x = 1 and y = 1 or, possibly, x = null and y = null. Informally, each transaction reads from an unchanging snapshot of database state that is aligned along transactional boundaries. We call this property atomic visibility and formalize it via the Read Atomic isolation guarantee in Section 3. The classic strategy for providing atomic visibility is to ensure mutual exclusion between readers and writers. For example, if a transaction like T1 above wants to update data items x and y, it can acquire exclusive locks for each of x and y, update both items, then release the locks. No other transactions will observe partial updates to x and y, ensuring atomic visibility. However, this solution has a drawback: while one transaction holds exclusive locks on x and y, no other transactions can access x and y for either reads or writes. By using mutual exclusion to enforce the atomic visibility of updates, we have also limited concurrency. In our example, if x and y are located on different servers, concurrent readers and writers will be unable to perform useful work during communication delays. These communication delays form an upper bound on throughput: effectively, 1message delay operations per second. To avoid this upper bound, we separate the problem of providing atomic visibility from the problem of maintaining mutual exclusion. By achieving the former but avoiding the latter, the algorithms we develop in this paper are not subject to the scalability penalties of many prior approaches. To ensure that all servers successfully execute a transaction (or that none do), our algorithms employ an atomic commitment protocol (ACP). When coupled with a blocking concurrency control mechanism like locking, ACPs are harmful to scalability and availability: arbitrary failures can (provably) cause any ACP implementation to stall [8]. (Optimistic concurrency control mechanisms can similarly block during validation.) We instead use ACPs with non-blocking concurrency control mechanisms; this means that individual transactions can stall due to failures or communication delays without forcing other transactions to stall. In a departure from traditional concurrency control, we allow multiple ACP rounds to proceed in parallel over the same data. The end resultour RAMP transactionsprovide excellent scalability and performance under contention (e.g., in the event of write hotspots) and are robust to partial failure. RAMP transactions nonblocking behavior means that they cannot provide certain guarantees like preventing concurrent updates. However, applications that can use Read Atomic isolation will benefit from our algorithms. The remainder of this section identifies several relevant use cases from industry that require atomic visibility for correctness.\nAs a simple example, consider a social networking application: if two users, Sam and Mary, become friends (a bi-directional relationship), other users should never see that Sam is a friend of Mary but Mary is not a friend of Sam: either both relationships should be visible, or neither should be. A transaction under Read Atomic isolation would correctly enforce this behavior, and we can further classify three general use cases for Read Atomic isolation: 1.) Foreign key constraints. Many database schemas contain information about relationships between records in the form of foreign key constraints. For example, Facebooks TAO [11], LinkedIns Espresso [38], and Yahoo! PNUTS [15] store information about business entities such as users, photos, and status updates as well as relationships between them (e.g., the friend relationships above). Their data models often represent bi-directional edges as two distinct uni-directional relationships. For example, in TAO, a user performing a like action on a Facebook page produces updates to both the LIKES and LIKED_BY associations [11]. PNUTSs authors describe an identical scenario [15]. These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access. Violations of atomic visibility surface as broken bi-directional relationships (as with Sam and Mary above) and dangling or incorrect references (e.g., Frank is an employee of department.id=5, but no such department exists in the department table). With RAMP transactions, when inserting new entities, applications can bundle relevant entities from each side of a foreign key constraint into a transaction. When deleting associations, users can tombstone the opposite end of the association (i.e., delete any entries with associations via a special record that signifies deletion) [45] to avoid dangling pointers. 2.) Secondary indexing. Data is typically partitioned across servers according to a primary key (e.g., user ID). This allows fast location and retrieval of data via primary key lookups but makes access by secondary attributes (e.g., birth date) challenging. There are two dominant strategies for distributed secondary indexing. First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server [7,38]. This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads [11, 17, 38]. Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data [7, 15]. This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two). Real-world services employ either local secondary indexing (e.g., Espresso [38], Cassandra, and Google Megastores local indexes [7]) or non-atomic (incorrect) global secondary indexing (e.g., Espresso and Megastores global indexes, Yahoo! PNUTSs proposed secondary indexes [15]). The former is non-scalable but correct, while the latter is scalable but incorrect. For example, in a database partitioned by id with an incorrectly-maintained global secondary index on salary, the query SELECT id, salary WHERE salary > 6 ,  might return records with salary less than $60,000 and omit some records with salary greater than $60,000. With RAMP transactions, the secondary index entry for a given attribute can be updated atomically with base data. For example, if a secondary index is stored as a mapping from secondary attribute values to sets of item-versions matching the secondary attribute (e.g., the secondary index entry for users with blue hair would contain a list of user IDs and last-modified timestamps corresponding to all of the users with attribute hair-color=blue), then insertions of new primary data require additions to the corresponding index entry, deletions require removals, and updates require a tombstone deletion from one entry and an insertion into another. 3.) Materialized view maintenance. Many applications precompute (i.e., materialize) queries over data, as in Twitters Rainbird service [44], Googles Percolator [36], and LinkedIns Espresso systems [38]. As a simple example, Espresso stores a mailbox of messages for each user along with statistics about the mailbox messages: for Espressos read-mostly workload, it is more efficient to maintain (i.e., pre-materialize) a count of unread messages rather than scan all messages every time a user accesses her mailbox [38]. In this case, any unread message indicators should remain in sync with the messages in the mailbox. However, atomicity violations will allow materialized views to diverge from the base data (e.g., Susans mailbox displays a notification that she has unread messages but all 63,201 messages in her inbox are marked as read). With RAMP transactions, base data and views can be updated atomically. The physical maintenance of a view depends on its specification [14, 27], but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition. For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance [46]). For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter [40]. In brief: Status Quo. Despite application requirements for Read Atomic isolation, few large-scale production systems provide it. For example, the authors of Tao, Espresso, and PNUTS describe several classes of atomicity anomalies exposed by their systems, ranging from dangling pointers to the exposure of intermediate states and incorrect secondary index lookups, often highlighting these cases as areas for future research and design [11, 15, 38]. These systems are not exceptions: data stores like Bigtable [13], Dynamo [22], and many popular NoSQL [34] and even some NewSQL [5] stores do not provide transactional guarantees for multi-item operations. The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics. Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described.\nIn this section, we formalize Read Atomic isolation and, to capture scalability, formulate a pair of strict scalability criteria: synchronization and partition independence. Readers more interested in RAMP algorithms may wish to proceed to Section 4.\nTo formalize RA isolation, as is standard [2], we consider ordered sequences of reads and writes to arbitrary sets of items, or transactions. We call the set of items a transaction reads from and writes to its read set and write set. Each write creates a version of an item and we identify versions of items by a unique timestamp taken from a totally ordered set (e.g., rational numbers). Timestamps induce a total order on versions of each item (and a partial order across versions of different items). We denote version i of item x as xi. A transaction Tj exhibits fractured reads if transaction Ti writes versions xm and yn (in any order, with x possibly but not necessarily equal to y), Tj reads version xm and version yk, and k < n. A system provides Read Atomic isolation (RA) if it prevents fractured reads anomalies and also prevents transactions from reading uncommitted, aborted, or intermediate data. Thus, RA provides transactions with a snapshot view of the database that respects transaction boundaries (see the Appendix for more details, including a discussion of transitivity). RA is simply a restriction on write visibilityif the ACID Atomicity property requires that all or none of a transactions updates are performed, RA requires that all or none of a transactions updates are made visible to other transactions.\nAs outlined in Section 2.1, RA isolation matches many of our use cases. However, RA is not sufficient for all applications. RA does not prevent concurrent updates or provide serial access to data items. For example, RA is an incorrect choice for an application that wishes to maintain positive bank account balances in the event of withdrawals. RA is a better fit for our friend operation because the operation is write-only and correct execution (i.e., inserting both records) is not conditional on concurrent updates. From a programmers perspective, we have found RA isolation to be most easily understandable (at least initially) with read-only and write-only transactions; after all, because RA allows concurrent writes, any values that are read might be changed at any time. However, read-write transactions are indeed well defined under RA.\nWe consider databases that are partitioned, with the set of items in the database spread over multiple servers. Each item has a single logical copy, stored on a servercalled the items partitionwhose identity can be calculated using the item. Clients forward operations on each item to the items partition, where they are executed. Transaction execution terminates in commit, signaling success, or abort, signaling failure. In our examples, all data items have the null value (?) at database initialization. We do not model replication of data items within a partition; this can happen at a lower level of the system than our discussion (see Section 4.6) as long as operations on each item are linearizable [4]. Scalability criteria. As we hinted in Section 1, large-scale deployments often eschew transactional functionality on the premise that it would be too expensive or unstable in the presence of failure and degraded operating modes [9, 11, 13, 15, 22, 25, 26, 38, 44]. Our goal in this paper is to provide robust and scalable transactional functionality, and, so we first define criteria for scalability: Synchronization independence ensures that one clients transactions cannot cause another clients to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition). This prevents one transaction from causing another to abortwhich is particularly important in the presence of partial failuresand guarantees that each client is able to make useful progress. In the absence of failures, this maximizes useful concurrency. In the distributed systems literature, synchronization independence for replicated transactions is called transactional availability [5]. Note that strong isolation models like serializability and Snapshot Isolation violate synchronization independence and limit scalability. While many applications can limit their data accesses to a single partition via explicit data modeling [7,19,25,38] or planning [18,35], this is not always possible. In the case of secondary indexing, there is a tangible cost associated with requiring single-partition updates (scatter-gather reads), while, in social networks like Facebook and large-scale hierarchical access patterns as in Rainbird, perfect partitioning of data accesses is near-impossible. Accordingly: Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access. Thus, a partition failure only affects transactions that access items contained on the partition. This also reduces load on servers not directly involved in a transactions execution. In the distributed systems literature, partition independence for replicated data is called replica availability [5] or genuine partial replication [39]. In addition to the above requirements, we limit the metadata overhead of algorithms. There are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state. As a straw-man solution, each transaction could send copies of all of its writes to every partition it accesses so that readers observe all of its writes by reading a single item. This provides RA isolation but requires considerable storage. Other solutions may require extra data storage proportional to the number of servers in the cluster or, worse, the database size (Section 6). We will attempt to minimize this metadatathat is, data that the transaction did not itself write but which is required for correct execution. In our algorithms, we will specifically provide constant-factor metadata overheads (RAMP-S, RAMP-H) or else overhead linear in transaction size (but independent of data size; RAMP-F).\nGiven specifications for RA isolation and scalability, we present algorithms for achieving both. For ease of understanding, we first focus on providing read-only and write-only transactions with a last writer wins overwrite policy, then subsequently discuss how to perform read/write transactions. Our focus in this section is on intuition and understanding; we defer all correctness and scalability proofs to the Appendix, providing salient details inline. At a high level, RAMP transactions allow reads and writes to proceed concurrently. This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transactions writes, violating RA (i.e., fractured reads might occur). Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions. To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions. In this section, we present three algorithms that provide a trade-off between the amount of metadata required and the expected number of extra reads to fetch missing writes. As discussed in Section 2, if techniques like distributed locking couple mutual exclusion with atomic visibility of writes, RAMP transactions correctly control visibility but allow concurrent and scalable execution.\nTo begin, we present a RAMP algorithm that, in the race-free case, requires one RTT for reads and two RTTs for writes, called RAMP-Fast (abbreviated RAMP-F; Algorithm 1). RAMP-F stores metadata in the form of write sets (overhead linear in transaction size). Overview. Each write in RAMP-F (lines 1421) contains a timestamp (line 15) that uniquely identifies the writing transaction as well as a set of items written in the transaction (line 16). For now, combining a unique client ID and client-local sequence number is sufficient for timestamp generation (see also Section 4.5). RAMP-F write transactions proceed in two phases: a first round of communication places each timestamped write on its respective partition. In this PREPARE phase, each partition adds the write to its local database (versions, lines 1, 1719). A second round of communication marks versions as committed. In this COMMIT phase, each partition updates an index containing the highest-timestamped committed version of each item (lastCommit, lines 2, 2021). RAMP-F read transactions begin by first fetching the last (highesttimestamped) committed version for each item from its respective partition (lines 2330). Using the results from this first round of reads, each reader can calculate whether it is missing any versions (that is, versions that were prepared but not yet committed on their partitions). Combining the timestamp and set of items from each version read (i.e., its metadata) produces a mapping from items to timestamps that represent the highest-timestamped write for each transaction that appears in this first-round read set (lines 2629). If the reader has read a version of an item that has a lower timestamp than indicated in the mapping for that item, the reader issues a second read to fetch the missing version (by timestamp) from its partition (lines 3032). Once all missing versions are fetched (which can be done in parallel), the client can return the resulting set of versionsthe first-round reads, with any missing versions replaced by the optional, second round of reads. By example. Consider the RAMP-F execution depicted in Figure 1. T1 writes to both x and y, performing the two-round write protocol on two partitions, Px and Py. However, T2 reads from x and y while T1 is concurrently writing. Specifically, T2 reads from Px after Px has committed T1s write to x, but T2 reads from Py before Py has committed T1s write to y. Therefore, T2s first-round reads return x = x1 and y = ?, and returning this set of reads would violate RA. Using the metadata attached to its first-round reads, T2 determines that it is missing y1 (since vlatest [y] = 1 and 1 >?) and so T2 subsequently issues a second read from Py to fetch y1 by version. After completing its second-round read, T2 can safely return its result set. T1s progress is unaffected by T2, and T1 subsequently completes by committing y1 on Py. Why it works. RAMP-F writers use metadata as a record of intent: a reader can detect if it has raced with an in-progress commit round and use the metadata stored by the writer to fetch the missing data. Accordingly, RAMP-F readers only issue a second round of reads in the event that they read from a partially-committed write transaction (where some but not all partitions have committed a write). In this event, readers will fetch the appropriate writes from the not-yetcommitted partitions. Most importantly, RAMP-F readers never have to stall waiting for a write that has not yet arrived at a partition: the two-round RAMP-F write protocol guarantees that, if a partition commits a write, all of the corresponding writes in the transaction are present on their respective partitions (though possibly not committed locally). As long as a reader can identify the corresponding version by timestamp, the reader can fetch the version from the respective partitions set of pending writes without waiting. To enable this, RAMP-F writes contain metadata linear in the size of the writing transactions write set (plus a timestamp per write). RAMP-F requires 2 RTTs for writes: one for PREPARE and one for COMMIT. For reads, RAMP-F requires one RTT in the absence of concurrent writes and two RTTs otherwise. RAMP timestamps are only used to identify specific versions and in ordering concurrent writes to the same item; RAMP-F transactions do not require a global timestamp authority. For example, if lastCommit[k] = 2, there is no requirement that a transaction with timestamp 1 has committed or even that such a transaction exists.\nWhile RAMP-F requires linearly-sized metadata but provides bestcase one RTT for reads, RAMP-Small (RAMP-S) uses constant-size metadata but always requires two RTT for reads (Algorithm 2). RAMP-S and RAMP-F writes are identical, but, instead of attaching the entire write set to each write, RAMP-S writers only store the transaction timestamp (line 7). Unlike RAMP-F, RAMP-S readers issue a first round of reads to fetch the highest committed timestamp for each item from its respective partition (lines 3, 911). Once RAMP-S readers have recieved the highest committed timestamp for each item, the readers send the entire set of timestamps they received to the partitions in a second round of communication (lines 1314). For each item in the read request, RAMP-S servers return the highesttimestamped version of the item that also appears in the supplied set of timestamps (lines 56). Readers subsequently return the results from the mandatory second round of requests. Algorithm 1 RAMP-Fast Server-side Data Structures 1: versions: set of versions hitem,value, timestamp tsv, metadata mdi 2: latestCommit[i]: last committed timestamp for item i Server-side Methods 3: procedure PREPARE(v : version) 4: versions.add(v) 5: return 6: procedure COMMIT(tsc : timestamp) 7: Its {w.item | w 2 versions^w.tsv = tsc} 8: 8i 2 Its, latestCommit[i] max(latestCommit[i], tsc) 9: procedure GET(i : item, tsreq : timestamp) 10: if tsreq = /0 then 11: return v 2 versions : v.item = i^ v.tsv = latestCommit[item] 12: else 13: return v 2 versions : v.item = i^ v.tsv = tsreq Client-side Methods 14: procedure PUT_ALL(W : set of hitem,valuei) 15: tstx generate new timestamp 16: Itx set of items in W 17: parallel-for hi,vi 2W 18: v hitem = i,value = v, tsv = tstx,md = (Itx {i})i 19: invoke PREPARE(v) on respective server (i.e., partition) 20: parallel-for server s : s contains an item in W 21: invoke COMMIT(tstx) on s 22: procedure GET_ALL(I : set of items) 23: ret {} 24: parallel-for i 2 I 25: ret[i] GET(i, /0) 26: vlatest {} (default value: 1) 27: for response r 2 ret do 28: for itx 2 r.md do 29: vlatest [itx] max(vlatest [itx],r.tsv) 30: parallel-for item i 2 I 31: if vlatest [i]> ret[i].tsv then 32: ret[i] GET(i,vlatest [i]) 33: return ret By example. In Figure 1, under RAMP-S, Px and Py would respectively return the sets {1} and {?} in response to T2s first round of reads. T2 would subsequently send the set {1,?} to both Px and Py, which would return x1 and y1. (Including ? in the second-round request is unnecessary, but we leave it in for ease of understanding.) Why it works. In RAMP-S, if a transaction has committed on some but not all partitions, the transaction timestamp will be returned in the first round of any concurrent read transaction accessing the committed partitions items. In the (required) second round of read requests, any prepared-but-not-committed partitions will find the committed timestamp in the reader-provided set and return the appropriate version. In contrast with RAMP-F, where readers explicitly provide partitions with a specific version to return in the (optional) second round, RAMP-S readers defer the decision of which version to return to the partition, which uses the reader-provided set to decide. This saves metadata but increases RTTs, and the size of the parameters of each second-round GET request is (worst-case) linear in the read set size. Unlike RAMP-F, there is no requirement to return the value of the last committed version in the first round (returning the version, lastCommit[k], suffices in line 3).\nRAMP-Hybrid (RAMP-H; Algorithm 3) strikes a compromise between RAMP-F and RAMP-S. RAMP-H and RAMP-S write protocols are identical, but, instead of storing the entire write set (as in RAMP-F), Algorithm 2 RAMP-Small Server-side Data Structures same as in RAMP-F (Algorithm 1) Server-side Methods PREPARE, COMMIT same as in RAMP-F 1: procedure GET(i : item, tsset : set of timestamps) 2: if tsset = /0 then 3: return v 2 versions : v.item = i^ v.tsv = latestCommit[k] 4: else 5: tsmatch = {t | t 2 tsset ^9v 2 versions : v.item = i^ v.tv = t} 6: return v 2 versions : v.item = i^ v.tsv = max(tsmatch) Client-side Methods 7: procedure PUT_ALL(W : set of hitem,valuei) same as RAMP-F PUT_ALL but do not instantiate md on line 18 8: procedure GET_ALL(I : set of items) 9: tsset {} 10: parallel-for i 2 I 11: tsset .add(GET(i, /0).tsv) 12: ret {} 13: parallel-for item i 2 I 14: ret[i] GET(i, tsset) 15: return ret RAMP-H writers store a Bloom filter [10] representing the transaction write set (line 1). RAMP-H readers proceed as in RAMP-F, with a first round of communication to fetch the last-committed version of each item from its partition (lines 35). Given this set of versions, RAMP-H readers subsequently compute a list of potentially highertimestamped writes for each item (lines 710). Any potentially missing versions are fetched in a second round of reads (lines 12). By example. In Figure 1, under RAMP-H, x1 would contain a Bloom filter with positives for x and y and y? would contain an empty Bloom filter. T2 would check for the presence of y in x1s Bloom filter (since x1s version is 1 and 1 > ?) and, finding a match, conclude that it is potentially missing a write (y1). T2 would subsequently fetch y1 from Py. Why it works. RAMP-H is effectively a hybrid between RAMP-F and RAMP-S. If the Bloom filter has no false positives, RAMP-H reads behave like RAMP-F reads. If the Bloom filter has all false positives, RAMP-H reads behave like RAMP-S reads. Accordingly, the number of (unnecessary) second-round reads (i.e., which would not be performed by RAMP-F) is controlled by the Bloom filter false positive rate, which is in turn (in expectation) proportional to the size of the Bloom filter. Any second-round GET requests are accompanied by a set of timestamps that is also proportional in size to the false positive rate. Therefore, RAMP-H exposes a trade-off between metadata size and expected performance. To understand why RAMP-H is safe, we simply have to show that any false positives (second-round reads) will not compromise the integrity of the result set; with unique timestamps, any reads due to false positives will return null.\nThe RAMP algorithms allow readers to safely race writers without requiring either to stall. The metadata attached to each write allows readers in all three algorithms to safely handle concurrent and/or partial writes and in turn allows a trade-off between metadata size and performance (Table 1): RAMP-F is optimized for fast reads, RAMP-S is optimized for small metadata, and RAMP-H is, as the name suggests, a middle ground. RAMP-F requires metadata linear in transaction size, while RAMP-S and RAMP-H require constant metadata. However, RAMP-S and RAMP-H require more RTTs for reads compared to RAMP-F when there is no race between readers and writers. Algorithm 3 RAMP-Hybrid Server-side Data Structures Same as in RAMP-F (Algorithm 1) Server-side Methods PREPARE, COMMIT same as in RAMP-F GET same as in RAMP-S Client-side Methods 1: procedure PUT_ALL(W : set of hitem,valuei) same as RAMP-F PUT_ALL but instantiate md on line 18 with Bloom filter containing Itx When reads and writes race, in the worst case, all algorithms require two RTTs for reads. Writes always require two RTTs to prevent readers from stalling due to missing, unprepared writes. RAMP algorithms are scalable because clients only contact partitions relative to their transactions (partition independence), and clients cannot stall one another (synchronization independence). More specifically, readers do not interfere with other readers, writers do not interfere with other writers, and readers and writers can proceed concurrently. When a reader races a writer to the same items, the writers new versions will only become visible to the reader (i.e., be committed) once it is guaranteed that the reader will be able to fetch all of them (possibly via a second round of communication). A reader will never have to stall waiting for writes to arrive at a partition (for details, see Invariant 1 in the Appendix).\nIn this section, we discuss relevant implementation details. Multi-versioning and garbage collection. RAMP transactions rely on multi-versioning to allow readers to access versions that have not yet committed and/or have been overwritten. In our initial presentation, we have used a completely multi-versioned storage engine; in practice, multi-versioning can be implemented by using a single-versioned storage engine for retaining the last committed version of each item and using a look-aside store for access to both prepared-but-not-yet-committed writes and (temporarily) any overwritten versions. The look-aside store should make prepared versions durable but canat the risk of aborting transactions in the event of a server failuresimply store any overwritten versions in memory. Thus, with some work, RAMP algorithms are portable to legacy, non-multi-versioned storage systems. In both architectures, each partitions data will grow without bound if old versions are not removed. If a committed version of an item is not the highest-timestamped committed version (i.e., a committed version v of item k where v < lastCommit[k]), it can be safely discarded (i.e., garbage collected, or GCed) as long as no readers will attempt to access it in the future (via second-round GET requests). It is easiest to simply limit the running time of read transactions and GC overwritten versions after a fixed amount of real time has elapsed. Any read transactions that take longer than this GC window can be restarted [32, 33]. Therefore, the maximum number of versions retained for each item is bounded by the items update rate, and servers can reject any client GET requests for versions that have been GCed (and the read transaction can be restarted). As a more principled solution, partitions can also gossip the timestamps of items that have been overwritten and have not been returned in the first round of any ongoing read transactions. Read-write transactions. Until now, we have focused on readonly and write-only transactions. However, we can extend our algorithms to provide read-write transactions. If transactions predeclare the data items they wish to read, then the client can execute a GET_ALL transaction at the start of transaction execution to prefetch all items; subsequent accesses to those items can be served from this pre-fetched set. Clients can buffer any writes and, upon transaction commit, send all new versions to servers (in parallel) via a PUT_ALL request. As in Section 3, this may result in anomalies due to concurrent update but does not violate RA isolation. Given the benefits of pre-declared read/write sets [18, 35, 43] and write buffering [17, 41], we believe this is a reasonable strategy. For secondary index lookups, clients can first look up secondary index entries then subsequently (within the same transaction) read primary data (specifying versions from index entries as appropriate). Timestamps. Timestamps should be unique across transactions, and, for session consistency (Appendix), increase on a per-client basis. Given unique client IDs, a client ID and sequence number form unique transaction timestamps without coordination. Without unique client IDs, servers can assign unique timestamps with high probability using UUIDs and by hashing transaction contents. Overwrites. In our algorithms, we have depicted a policy in which versions are overwritten according to a highest-timestamp-wins policy. In practice, and, for commutative updates, users may wish to employ a different policy upon COMMIT: for example, perform set union. In this case, lastCommit[k] contains an abstract data type (e.g., set of versions) that can be updated with a merge operation [22, 42] (instead of updateI f Greater) upon commit. This treats each committed record as a set of versions, requiring additional metadata (that can be GCed as in Section 4.7).\nRAMP transactions operate in a distributed setting, which poses challenges due to latency, partial failure, and network partitions. Synchronization independence ensures that failed clients do not cause other clients to fail, while partition independence ensures that clients only have to contact partitions for items in their transactions. This provides fault tolerance and availability as long as clients can access relevant partitions, but here we further elucidate RAMP interactions with replication and stalled operations. Replication. A variety of mechanisms including traditional database master-slave replication with failover, quorum-based protocols, and state machine replication and can ensure availability of individual partitions in the event of individual server failure [8]. To control durability, clients can wait until the effects of their operations (e.g., modifications to versions and lastCommit) are persisted locally on their respective partitions and/or to multiple physical servers before returning from PUT_ALL calls (either via master-to-slave replication or via quorum replication and by performing two-phase commit across multiple active servers). Notably, because RAMP transactions can safely overlap in time, replicas can process different transactions PREPARE and COMMIT requests in parallel. Stalled Operations. RAMP writes use a two-phase atomic commitment protocol that ensures readers never block waiting for writes to arrive. As discussed in Section 2, every ACP may block during failures [8]. However, due to synchronization independence, a blocked transaction (due to failed clients, failed servers, or network partitions) cannot cause other transactions to block. Blocked writes instead act as resource leaks on partitions: partitions will retain prepared versions indefinitely unless action is taken. To free these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP) described in [8]. CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT. In CTP, if a server Sp has performed PREPARE for transaction T but times out waiting for a COMMIT, Sp can check the status of T on any other partitions for items in T s write set. If another server Sc has received COMMIT for T , then Sp can COMMIT T . If Sa, a server responsible for an item in T , has not received PREPARE for T , Sa and Sp can promise never to PREPARE or COMMIT T in the future and Sp can safely discard its versions. A client recovering from a failure can read from the servers to determine if they unblocked its write. Writes that block mid-COMMIT will also become visible on all partitions. CTP (evaluated in Section 5) only runs when writes block (or time-outs fire) and runs asynchronously with respect to other operations. CTP requires that PREPARE messages contain a list of servers involved in the transaction (a subset of RAMP-F metadata but a superset of RAMP-H and RAMP-S) and that servers remember when they COMMIT and abort writes (e.g., in a log file). Compared to alternatives (e.g., replicating clients [24]), we have found CTP to be both lightweight and effective.\nRAMP algorithms also allow several possible optimizations: Faster commit detection. If a server returns a version in response to a GET request and the versions timestamp is greater than the highest committed version of that item (i.e., lastCommit), then transaction writing the version has committed on at least one partition. In this case, the server can mark the version as committed. This scenario will occur when all partitions have performed PREPARE and at least one server but not all partitions have performed COMMIT (as in CTP). This allows faster updates to lastCommit (and therefore fewer expected RAMP-F and RAMP-H RTTs). Metadata garbage collection. Once all of transaction T s writes are committed on each respective partition (i.e., are reflected in lastCommit), readers are guaranteed to read T s writes (or later writes). Therefore, non-timestamp metadata for T s writes stored in RAMP-F and RAMP-H (write sets and Bloom filters) can therefore be discarded. Detecting that all servers have performed COMMIT can be performed asynchronously via a third round of communication performed by either clients or servers. One-phase writes. We have considered two-phase writes, but, if a user does not wish to read her writes (thereby sacrificing session guarantees outlined in the Appendix), the client can return after issuing its PREPARE round (without sacrificing durability). The client can subsequently execute the COMMIT phase asynchronously, or, similar to optimizations presented in Paxos Commit [24], the servers can exchange PREPARE acknowledgements with one another and decide to COMMIT autonomously. This optimization is safe because multiple PREPARE phases can safely overlap.\nWe proceed to experimentally demonstrate RAMP transaction scalability as compared to existing transactional and non-transactional mechanisms. RAMP-F, RAMP-H, and often RAMP-S outperform existing solutions across a range of workload conditions while exhibiting overheads typically within 8% and no more than 48% of peak throughput. As expected from our theoretical analysis, the performance of our RAMP algorithms does not degrade substantially under contention and scales linearly to over 7.1 million operations per second on 100 servers. These outcomes validate our choice to pursue synchronization- and partition-independent algorithms.\nTo demonstrate the effect of concurrency control on performance and scalability, we implemented several concurrency control algorithms in a partitioned, multi-versioned, main-memory database prototype. Our prototype is in Java and employs a custom RPC system with Kryo 2.20 for serialization. Servers are arranged as a distributed hash table with partition placement determined by random hashing. As in stores like Dynamo [22], clients can connect to any server to execute operations, which the server will perform on their behalf (i.e., each server acts as a client in our RAMP pseudocode). We implemented RAMP-F, RAMP-S, and RAMP-H and configure a wall-clock GC window of 5 seconds as described in Section 4.5. RAMP-H uses a 256-bit Bloom filter based on an implementation of MurmurHash2.0, with four hashes per entry; to demonstrate the effects of filter saturation, we do not modify these parameters in our experiments. Our prototype utilizes the Faster commit detection optimization from Section 4.5 but we chose not to employ the latter two optimizations in order to preserve session guarantees and because metadata overheads were generally minor. Algorithms for comparison. As a baseline, we do not employ any concurrency control (denoted NWNR, for no write and no read locks); reads and writes take one RTT and are executed in parallel. We also consider three lock-based mechanisms: long write locks and long read locks, providing Repeatable Read isolation (PL-2.99; denoted LWLR), long write locks with short read locks, providing Read Committed isolation (PL-2L; denoted LWSR; does not provide RA), and long write locks with no read locks, providing Read Uncommitted isolation [2] (LWNR; also does not provide RA). While only LWLR provides RA, LWSR and LWNR provide a useful basis for comparison, particularly in measuring concurrency-related locking overheads. To avoid deadlocks, the system lexicographically orders lock requests by item and performs them sequentially. When locks are not used (as for reads in LWNR and reads and writes for NWNR), the system parallelizes operations. We also consider an algorithm where, for each transaction, designated coordinator servers enforce RA isolationeffectively, the Eiger systems 2PC-PCI mechanism [33] (denoted E-PCI; Section 6). Writes proceed via prepare and commit rounds, but any reads that arrive at a partition and overlap with a concurrent write to the same item must contact a (randomly chosen, per-write-transaction) coordinator partition to determine whether the coordinators prepared writes have been committed. Writes require two RTTs, while reads require one RTT during quiescence and two RTTs in the presence of concurrent updates (to a variable number of coordinator partitionslinear in the number of concurrent writes to the item). Using a coordinator violates partition independence but not synchronization independence. We optimize 2PC-PCI reads by having clients determine a read timestamp for each transaction (eliminating an RTT) and do not include happens-before metadata. This range of lock-based strategies (LWNR, LWSR, LWNR), recent comparable approach (E-PCI), and best-case (NWNR; no concurrency control) baseline provides a spectrum of strategies for comparison. Environment and benchmark. We evaluate each algorithm using the YCSB benchmark [16] and deploy variably-sized sets of servers on public cloud infrastructure. We employ cr1.8xlarge instances on Amazon EC2 and, by default, deploy five partitions on five servers. We group sets of reads and sets of writes into read-only and write-only transactions (default size: 4 operations), and use the default YCSB workload (workloada, with Zipfian distributed item accesses) but with a 95% read and 5% write proportion, reflecting read-heavy applications (Section 2, [11, 33, 44]; e.g., Taos 500 to 1 reads-to-writes [11, 33], Espressos 1000 to 1 Mailbox application [38], and Spanners 3396 to 1 advertising application [17]). By default, we use 5000 concurrent clients split across 5 separate EC2 instances and, to fully expose our metadata overheads, use a value size of 1 byte per write. We found that lock-based algorithms were highly inefficient for YCSBs default 1K item database, so we increased the database size to 1M items by default. Each version contains a timestamp (64 bits), and, with YCSB keys (i.e., item IDs) of size 11 bytes and a transaction length L, RAMP-F requires 11L bytes of metadata per version, while RAMP-H requires 32 bytes. We successively vary several parameters, including number of clients, read proportion, transaction length, value size, database size, and number of servers and report the average of three sixty-second trials.\nOur first set of experiments focuses on two metrics: performance compared to baseline and performance compared to existing techniques. The overhead of RAMP algorithms is typically less than 8% compared to baseline (NWNR) throughput, is sometimes zero, and is never greater than 50%. RAMP-F and RAMP-H always outperform the lock-based and E-PCI techniques, while RAMP-S outperforms lock-based techniques and often outperforms E-PCI. We proceed to demonstrate this behavior over a variety of conditions: Number of clients. RAMP performance scales well with increased load and incurs little overhead (Figure 2). With few concurrent clients, there are few concurrent updates and therefore few secondround reads; performance for RAMP-F and RAMP-H is close to or even matches that of NWNR. At peak throughput (at 10,000 clients), RAMP-F and RAMP-H pay a throughput overhead of 4.2% compared to NWNR. RAMP-F and RAMP-H exhibit near-identical performance; the RAMP-H Bloom filter triggers few false positives (and therefore few extra RTTs compared to RAMP-F). RAMP-S incurs greater overhead and peaks at almost 60% of the throughput of NWNR. Its guaranteed two-round trip reads are expensive and it acts as an effective lower bound on RAMP-F and RAMP-H performance. In all configurations, the algorithms achieve low latency (RAMP-F, RAMP-H, NWNR less than 35ms on average and less than 10 ms at 5,000 clients; RAMP-S less than 53ms, 14.3 ms at 5,000 clients). In comparison, the remaining algorithms perform less favorably. In contrast with the RAMP algorithms, E-PCI servers must check a coordinator server for each in-flight write transaction to determine whether to reveal writes to clients. For modest load, the overhead of these commit checks places E-PCI performance between that of RAMP-S and RAMP-H. However, the number of in-flight writes increases with load (and is worsened due to YCSBs Zipfian distributed accesses), increasing the number of E-PCI commit checks. This in turn decreases throughput, and, with 10,000 concurrent clients, E-PCI performs so many commit checks per read (over 20% of reads trigger a commit check, and, on servers with hot items, each commit check requires indirected coordinator checks for an average of 9.84 transactions) that it underperforms the LWNR lockbased scheme. Meanwhile, multi-partition locking is expensive [35]: with 10,000 clients, the most efficient algorithm, LWNR, attains only 28.6% of the throughput of NWNR, while the least efficient, LWLR, attains only 1.6% (peaking at 3,412 transactions per second). We subsequently varied several other workload parameters, which we briefly discuss below and plot in Figure 3: Read proportion. Increased write activity leads to a greater number of races between reads and writes and therefore additional second-round RTTs for RAMP-F and RAMP-H reads. With all write transactions, all RAMP algorithms are equivalent (two RTT) and achieve approximately 65% of the throughput of NWNR. With all reads, RAMP-F, RAMP-S, NWNR, and E-PCI are identical, with a single RTT. Between these extremes, RAMP-F and RAMP-S scale nearlinearly with the write proportion. In contrast, lock-based protocols fare poorly as contention increases, while E-PCI again incurs penalties due to commit checks. Transaction length. Increased transaction lengths have variable impact on the relative performance of RAMP algorithms. Synchronization independence does not penalize long-running transactions, but, with longer transactions, metadata overheads increase. RAMP-F relative throughput decreases due to additional metadata (linear in transaction length) and RAMP-H relative performance also decreases as its Bloom filters saturate. (However, YCSBs Zipfian-distributed access patterns result in a non-linear relationship between length and throughput.) As discussed above, we explicitly decided not to tune RAMP-H Bloom filter size but believe a logarithmic increase in filter size could improve RAMP-H performance for large transaction lengths (e.g., 1024 bit filters should lower the false positive rate for transactions of length 256 from over 92% to slightly over 2%). Value size. Value size similarly does not seriously impact relative throughput. At a value size of 1B, RAMP-F is within 2.3% of NWNR. However, at a value size of 100KB, RAMP-F performance nearly matches that of NWNR: the overhead due to metadata decreases, and write request rates slow, decreasing concurrent writes (and subse- 1 10 100 1000 10000 100000 Value Size (bytes) 0 30K 60K 90K 120K 150K 180K Th ro ug hp ut (tx n/ s) 10 100 1000 10K 100K 1M 10M Database Size (items) 0 30K 60K 90K 120K 150K 180K Th ro ug hp ut (tx n/ s) Figure 3: Algorithm performance across varying workload conditions. RAMP-F and RAMP-H exhibit similar performance to NWNR baseline, while RAMP-Ss 2 RTT reads incur a greater performance penalty across almost all configurations. RAMP transactions consistently outperform RA isolated alternatives. quently second-round RTTs). Nonetheless, absolute throughput drops by a factor of 24 as value sizes moves from 1B to 100KB. Database size. RAMP algorithms are robust to high contention for a small set of items: with only 1000 items in the database, RAMP-F achieves throughput within 3.1% of NWNR. RAMP algorithms are largely agnostic to read/write contention, although, with fewer items in the database, the probability of races between readers and inprogress writers increases, resulting in additional second-round reads for RAMP-F and RAMP-H. In contrast, lock-based algorithms fare poorly under high contention, while E-PCI indirected commit checks again incurred additional overhead. By relying on clients (rather than additional partitions) to repair fractured writes, RAMP-F, RAMP-H, and RAMP-S performance is less affected by hot items. Overall, RAMP-F and RAMP-H exhibit performance close to that of no concurrency control due to their independence properties and guaranteed worst-case performance. As the proportion of writes increases, an increasing proportion of RAMP-F and RAMP-H operations take two RTTs and performance trends towards that of RAMP-S, which provides a constant two RTT overhead. In contrast, lockbased protocols perform poorly under contention while E-PCI triggers more commit checks than RAMP-F and RAMP-H trigger second round reads (but still performs well without contention and for particularly read-heavy workloads). The ability to allow clients to independently verify read sets enables good performance despite a range of (sometimes adverse) conditions (e.g., high contention).\nWe also evaluated the overhead of blocked writes in our implementation of the Cooperative Termination Protocol discussed in Section 4.6. To simulate blocked writes, we artificially dropped a percentage of COMMIT commands in PUT_ALL calls such that clients returned from writes early and partitions were forced to complete the commit via CTP. This behavior is worse than expected because blocked clients continue to issue new operations. The table below reports the throughput reduction as the proportion of blocked writes increases (compared to no blocked writes) for a workload of 100% RAMP-F write transactions: Blocked % 0.01% 0.1% 25% 50% Throughput No change 99.86% 77.53% 67.92% As these results demonstrate, CTP can reduce throughput because each commit check consumes resources (here, network and CPU capacity). However, CTP only performs commit checks in the event of blocked writes (or time-outs; set to 5s in our experiments), so a modest failure rate of 1 in 1000 writes has a limited effect. The higher failure rates produce a near-linear throughput reduction but, in practice, a blocking rate of even a few percent is likely indicative of larger systemic failures. As Figure 3 hints, the effect of additional metadata for the participant list in RAMP-H and RAMP-S is limited, and, for our default workload of 5% writes, we observe similar trends but with throughput degradation of 10% or less across the above configurations. This validates our initial motivation behind the choice of CTP: average-case overheads are small.\nWe finally validate our chosen scalability criteria by demonstrating linear scalability of RAMP transactions to 100 servers. We deployed an increasing number of servers within the us-west-2 EC2 region and, to mitigate the effects of hot items during scaling, configured uniform random access to items. We were unable to include more than 20 instances in an EC2 placement group, which guarantees 10 GbE connections between instances, so, past 20 servers, servers communicated over a degraded network. Around 40 servers, we exhausted the us-west-2b availability zone (datacenter) capacity and had to allocate our instances across the remaining zones, further degrading network performance. However, as shown in Figure 4, each RAMP algorithm scales linearly, even though in expectation, at 100 servers, all but one in 100M transactions is a multi-partition operation. In particular, RAMP-F achieves slightly under 7.1 million operations per second, or 1.79 million transactions per second on a set of 100 servers (71,635 operations per partition per second). At all scales, RAMP-F throughput was always within 10% of NWNR. With 100 servers, RAMP-F was within 2.6%, RAMP-S within 3.4%, and RAMP-S was within 45% of NWNR. In light of our scalability criteria, this behavior is unsurprising.\nReplicated databases offer a broad spectrum of isolation guarantees at varying costs to performance and availability [8]: Serializability. At the strong end of the isolation spectrum is serializability, which provides transactions with the equivalent of a serial execution (and therefore also provides RA). A range of techniques can enforce serializability in distributed databases [3, 8], multi-version concurrency control (e.g. [37]) locking (e.g. [31]), and optimistic concurrency control [41]. These useful semantics come with costs in the form of decreased concurrency (e.g., contention and/or failed optimistic operations) and limited availability during partial failure [5, 21]. Many designs [19, 29] exploit cheap serializability within a single partition but face scalability challenges for distributed operations. Recent industrial efforts like F1 [41] and Spanner [17] have improved performance via aggressive hardware advances but, their reported throughput is still limited to 20 and 250 writes per item per second. Multi-partition serializable transactions are expensive and, especially under adverse conditions, are likely to remain expensive [18, 28, 35]. Weak isolation. The remainder of the isolation spectrum is more varied. Most real-world databases offer (and often default to) nonserializable isolation models [5, 34]. These weak isolation levels allow greater concurrency and fewer system-induced aborts compared to serializable execution but provide weaker semantic guarantees. For example, the popular choice of Snapshot Isolation prevents Lost Update anomalies but not Write Skew anomalies [2]; by preventing Lost Update, concurrency control mechanisms providing Snapshot Isolation violate synchronization independence [5]. In recent years, many NoSQL designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation in many industrial databases such PNUTS [15], Dynamo [22], TAO [11], Espresso [38], Rainbird [44], and BigTable [13]. These systems avoid penalties associated with stronger isolation but in turn sacrifice transactional guarantees (and therefore do not offer RA). Related mechanisms. There are several algorithms that are closely related to our choice of RA and RAMP algorithm design. COPS-GTs two-round read-only transaction protocol [32] is similar to RAMP-F readsclient read transactions identify causally inconsistent versions by timestamp and fetch them from servers. While COPS-GT provides causal consistency (requiring additional metadata), it does not support RA isolation for multi-item writes. Eiger provides its write-only transactions [33] by electing a coordinator server for each write. As discussed in Section 5 (E-PCI), the number of commit checks performed during its read-only transactions is proportional to the number of concurrent writes. Using a coordinator violates partition independence but in turn provides causal consistency. This coordinator election is analogous to GStores dynamic key grouping [19] but with weaker isolation guarantees; each coordinator effectively contains a partitioned completed transaction list from [12]. Instead of relying on indirection, RAMP transaction clients autonomously assemble reads and only require constant factor (or, for RAMP-F, linear in transaction size) metadata size compared to Eigers PL-2L (worst-case linear in database size). RAMP transactions are inspired by our earlier proposal for Monotonic Atomic View (MAV) isolation: transactions read from a monotonically advancing view of database state [5]. MAV is strictly weaker than RA and does not prevent fractured reads, as required for our applications (i.e., reads are not guaranteed to be transactionally aligned). The prior MAV algorithm we briefly sketched in [5] is similar to RAMP-F but, as a consequence of its weaker semantics, allows one-round read transactions. The RAMP algorithms described here are portable to the highly available (i.e., nonlinearizable, AP/EL [1, 23]) replicated setting of [5], albeit with necessary penalties to latency between updates and their visibility. Overall, we are not aware of a concurrency control mechanism for partitioned databases that provides synchronization independence, partition independence, and at least RA isolation.\nThis paper described how to achieve atomically visible multipartition transactions without incurring the performance and availability penalties of traditional algorithms. We first identified a new isolation levelRead Atomic isolationthat provides atomic visibility and matches the requirements of a large class of real-world applications. We subsequently achieved RA isolation via scalable, contention-agnostic RAMP transactions. In contrast with techniques that use inconsistent but fast updates, RAMP transactions provide correct semantics for applications requiring secondary indexing, foreign key constraints, and materialized view maintenance while maintaining scalability and performance. By leveraging multi-versioning with a variable but small (and, in two of three algorithms, constant) amount of metadata per write, RAMP transactions allow clients to detect and assemble atomic sets of versions in one to two rounds of communication with servers (depending on the RAMP implementation). The choice of synchronization and partition independent algorithms allowed us to achieve near-baseline performance across a variety of workload configurations and scale linearly to 100 servers. While RAMP transactions are not appropriate for all applications, the many for which they are well suited will benefit measurably. Acknowledgments The authors would like to thank Peter Alvaro, Giselle Cheung, Neil Conway, Aaron Davidson, Mike Franklin, Aurojit Panda, Nuno Preguia, Edward Ribeiro, Shivaram Venkataraman, and the SIGMOD reviewers for their insightful feedback. This research is supported by NSF CISE Expeditions award CCF1139158 and DARPA XData Award FA8750-12-2-0331, the National Science Foundation Graduate Research Fellowship (grant DGE-1106400), and gifts from Amazon Web Services, Google, SAP, Apple, Inc., Cisco, Clearstory Data, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, General Electric, Hortonworks, Huawei, Intel, Microsoft, NetApp, NTT Multimedia Communications Laboratories, Oracle, Samsung, Splunk, VMware, WANdisco and Yahoo!.\n[1] D. J. Abadi. Consistency tradeoffs in modern distributed database system design: CAP is only part of the story. IEEE Computer, 45(2):3742, 2012. [2] A. Adya. Weak consistency: a generalized theory and optimistic implementations for distributed transactions. PhD thesis, MIT, 1999. [3] D. Agrawal and V. Krishnaswamy. Using multiversion data for non-interfering execution of write-only transactions. In SIGMOD 1991. [4] H. Attiya and J. Welch. Distributed Computing: Fundamentals, Simulations and Advanced Topics (2nd edition). John Wiley Interscience, March 2004. [5] P. Bailis, A. Davidson, A. Fekete, A. Ghodsi, J. M. Hellerstein, and I. Stoica. Highly Available Transactions: Virtues and Limitations. In VLDB 2014. [6] P. Bailis, A. Fekete, A. Ghodsi, J. M. Hellerstein, and I. Stoica. The potential dangers of causal consistency and an explicit solution. In SOCC 2012. [7] J. Baker, C. Bond, J. Corbett, J. Furman, et al. Megastore: Providing scalable, highly available storage for interactive services. In CIDR 2011. [8] P. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency control and recovery in database systems. Addison-wesley New York, 1987. [9] K. Birman, G. Chockler, and R. van Renesse. Toward a cloud computing research agenda. SIGACT News, 40(2):6880, June 2009. [10] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. CACM, 13(7):422426, 1970. [11] N. Bronson, Z. Amsden, G. Cabrera, P. Chukka, P. Dimov, et al. TAO: Facebooks distributed data store for the social graph. In USENIX ATC 2013. [12] A. Chan and R. Gray. Implementing distributed read-only transactions. IEEE Transactions on Software Engineering, (2):205212, 1985. [13] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, et al. Bigtable: A distributed storage system for structured data. In OSDI 2006. [14] R. Chirkova and J. Yang. Materialized views. Foundations and Trends in Databases, 4(4):295405, 2012. [15] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon, et al. PNUTS: Yahoo!s hosted data serving platform. In VLDB 2008. [16] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking cloud serving systems with YCSB. In ACM SOCC 2010. [17] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, et al. Spanner: Googles globally-distributed database. In OSDI 2012. [18] C. Curino, E. Jones, Y. Zhang, and S. Madden. Schism: a workload-driven approach to database replication and partitioning. In VLDB 2010. [19] S. Das, D. Agrawal, and A. El Abbadi. G-store: a scalable data store for transactional multi key access in the cloud. In ACM SOCC 2010. [20] K. Daudjee and K. Salem. Lazy database replication with ordering guarantees. In ICDE 2004, pages 424435. [21] S. Davidson, H. Garcia-Molina, and D. Skeen. Consistency in partitioned networks. ACM Computing Surveys, 17(3):341370, 1985. [22] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, et al. Dynamo: Amazons highly available key-value store. In SOSP 2007. [23] S. Gilbert and N. Lynch. Brewers conjecture and the feasibility of consistent, available, partition-tolerant web services. SIGACT News, 33(2):5159, 2002. [24] J. Gray and L. Lamport. Consensus on transaction commit. ACM TODS, 31(1):133160, Mar. 2006. [25] P. Helland. Life beyond distributed transactions: an apostates opinion. In CIDR 2007. [26] S. Hull. 20 obstacles to scalability. Commun. ACM, 56(9):5459, 2013. [27] N. Huyn. Maintaining global integrity constraints in distributed databases. Constraints, 2(3/4):377399, Jan. 1998. [28] E. P. Jones, D. J. Abadi, and S. Madden. Low overhead concurrency control for partitioned main memory databases. In SIGMOD 2010. [29] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, et al. H-Store: a high-performance, distributed main memory transaction processing system. In VLDB 2008. [30] R. J. Lipton and J. S. Sandberg. PRAM: a scalable shared memory. Technical Report TR-180-88, Princeton University, September 1988. [31] F. Llirbat, E. Simon, D. Tombroff, et al. Using versions in update transactions: Application to integrity checking. In VLDB 1997. [32] W. Lloyd, M. J. Freedman, et al. Dont settle for eventual: scalable causal consistency for wide-area storage with COPS. In SOSP 2011. [33] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen. Stronger semantics for low-latency geo-replicated storage. In NSDI 2013. [34] C. Mohan. History repeats itself: Sensible and NonsenSQL aspects of the NoSQL hoopla. In EDBT 2013. [35] A. Pavlo, C. Curino, and S. Zdonik. Skew-aware automatic database partitioning in shared-nothing, parallel OLTP systems. In SIGMOD 2012. [36] D. Peng and F. Dabek. Large-scale incremental processing using distributed transactions and notifications. In OSDI 2010. [37] S. H. Phatak and B. Badrinath. Multiversion reconciliation for mobile databases. In ICDE 1999. [38] L. Qiao, K. Surlaker, S. Das, T. Quiggle, et al. On brewing fresh Espresso: LinkedIns distributed data serving platform. In SIGMOD 2013. [39] N. Schiper, P. Sutra, and F. Pedone. P-store: Genuine partial replication in wide area networks. In IEEE SRDS 2010. [40] M. Shapiro et al. A comprehensive study of convergent and commutative replicated data types. Technical Report 7506, INRIA, 2011. [41] J. Shute et al. F1: A distributed SQL database that scales. In VLDB 2013. [42] D. B. Terry, A. J. Demers, K. Petersen, M. J. Spreitzer, M. M. Theimer, and B. B. Welch. Session guarantees for weakly consistent replicated data. In PDIS 1994. [43] A. Thomson, T. Diamond, S. Weng, K. Ren, P. Shao, and D. Abadi. Calvin: Fast distributed transactions for partitioned database systems. In SIGMOD 2012. [44] K. Weil. Rainbird: Real-time analytics at Twitter. Strata 2011 [45] S. B. Zdonik. Object-oriented type evolution. In DBPL, pages 277288, 1987. [46] J. Zhou et al. Lazy maintenance of materialized views. In VLDB 2007.\nRAMP-F Correctness. To prove RAMP-F provides RA isolation, we show that the two-round read protocol returns a transactionally atomic set of versions. To do so, we formalize criteria for atomic (read) sets of versions in the form of companion sets. We will call the set of versions produced by a transaction sibling versions and call two items from the same write set sibling items. Given two versions xi and y j , we say that xi is a companion to y j if xi is a transactional sibling of y j or x is a sibling item of y j and i > j. We say that a set of versions V is a companion set if, for every pair (xi,y j) of versions in V where x is a sibling item of y j , xi is a companion to y j . In Figure 1, the versions returned by T2s first round of reads ({x1,y?}) do not comprise a companion set because y? has a lower timestamp than x1s sibling version of y (that is, x1 has sibling version y1 and but ?< 1 so y? has too low of a timestamp). Subsets of companion sets are also companion sets and companion sets also have a useful property for RA isolation: Claim 1 (Companion sets are atomic). Companion sets do not contain fractured reads. Proof. Claim 1 follows from the definitions of companion sets and fractured reads. If V is a companion set, then every version xi 2V is also a companion to every other version y j 2V where v j contains x in its sibling items. If V contained fractured reads, V would contain two versions xi,y j such that the transaction that wrote y j also wrote a version xk , i < k. However, in this case, xi would not be a companion to y j , a contradiction. Therefore, V cannot contain fractured reads. To provide RA, RAMP-F clients assemble a companion set for the requested items (in vlatest ), which we prove below: Claim 2. RAMP-F provides Read Atomic isolation. Proof. Each write in RAMP-F contains information regarding its siblings, which can be identified by item and timestamp. Given a set of RAMP-F versions, recording the highest timestamped version of each item (as recorded either in the version itself or via sibling metadata) yields a companion set of item-timestamp pairs: if a client reads two versions xi and y j such that x is in y js sibling items but i < j, then vlatest [x] will contain j and not i. Accordingly, given the versions returned by the first round of RAMP-F reads, clients calculate a companion set containing versions of the requested items. Given this companion set, clients check the first-round versions against this set by timestamp and issue a second round of reads to fetch any companions that were not returned in the first round. The resulting set of versions will be a subset of the computed companion set and will therefore also be a companion set. This ensures that the returned results do not contain fractured reads. RAMP-F first-round reads access lastCommit, so each transaction corresponding to a first-round version is committed, and, therefore, any siblings requested in the (optional) second round of reads are also committed. Accordingly, RAMP-F never reads aborted or non-final (intermediate) writes. This establishes that RAMP-F provides RA. RAMP-F Scalability and Independence. RAMP-F also provides the independence guarantees from Section 3.3. The following invariant over lastCommit is core to RAMP-F GET request completion: Invariant 1 (Companions present). If a version xi is referenced by lastCommit (that is, lastCommit[x] = i), then each of xis sibling versions are present in versions on their respective partitions. Invariant 1 is maintained by RAMP-Fs two-phase write protocol. lastCommit is only updated once a transactions writes have been placed into versions by a first round of PREPARE messages. Siblings will be present in versions (but not necessarily lastCommit). Claim 3. RAMP-F provides synchronization independence. Proof. Clients in RAMP-F do not communicate or coordinate with one another and only contact servers. Accordingly, to show that RAMP-F provides synchronization independence, it suffices to show that server-side operations always terminate. PREPARE and COMMIT methods only access data stored on the local partition and do not block due to external coordination or other method invocations; therefore, they complete. GET requests issued in the first round of reads have tsreq =? and therefore will return the version corresponding to lastCommit[k], which was placed into versions in a previously completed PREPARE round. GET requests issued in the second round of client reads have tsreq set to the clients calculated vlatest [k]. vlatest [k] is a sibling of a version returned from lastCommit in the first round, so, due to Invariant 1, the requested version will be present in versions. Therefore, GET invocations are guaranteed access to their requested version and can return without waiting. The success of RAMP-F operations do not depend on the success or failure of other clients RAMP-F operations. Claim 4. RAMP-F provides partition independence. Proof. RAMP-F transactions do not access partitions that are unrelated to each transactions specified data items and servers do not contact other servers in order to provide a safe response for operations. RAMP-S Correctness. RAMP-S writes and first-round reads proceed identically to RAMP-F writes, but the metadata written and returned is different. Therefore, the proof is similar to RAMP-F, with a slight modification for the second round of reads. Claim 5. RAMP-S provides Read Atomic isolation. Proof. To show that RAMP-S provides RA, it suffices to show that RAMP-S second-round reads (resp) are a companion set. Given two versions xi,y j 2 resp such that x 6= y, if x is a sibling item of y j , then xi must be a companion to y j . If xi were not a companion to y j , then it would imply that x is not a sibling item of y j (so we are done) or that j > i. If j > i, then, due to Invariant 1 (which also holds for RAMP-S writes due to identical write protocols), y js sibling is present in versions on the partition for x and would have been returned by the server (line 6), a contradiction. Each second-round GET request returns only one version, so we are done. RAMP-S Scalability and Independence. RAMP-S provides synchronization independence and partition independence. For brevity, we again omit full proofs, which closely resemble those of RAMP-F. RAMP-H Correctness. The probabilistic behavior of the RAMP-H Bloom filter admits false positives. However, given unique transaction timestamps (Section 4.5), requesting false siblings by timestamp and item does not affect correctness: Claim 6. RAMP-H provides Read Atomic isolation. Proof. To show that RAMP-H provides Read Atomic isolation, it suffices to show that any versions requested by RAMP-H second-round reads that would not have been requested by RAMP-F second-round reads (call this set v f alse) do not compromise the validity of RAMP-Hs returned companion set. Any versions in v f alse do not exist: timestamps are unique, so, for each version xi, there are no versions x j of non-sibling items with the same timestamp as xi (i.e., where i = j). Therefore, requesting versions in v f alse do not change the set of results collected in the second round. RAMP-H Scalability and Independence. RAMP-H provides synchronization independence and partition independence. We omit full proofs, which closely resemble those of RAMP-F. The only significant difference from RAMP-F is that second-round GET requests may return ?, but, as we showed above, these empty responses correspond to false positives in the Bloom filter and therefore do not affect correctness. Comparison to other isolation levels. The fractured reads anomaly is similar to Adyas Missing Transaction Updates definition, only applied to immediate read dependencies (rather than all transitive dependencies). RA is stronger than PL-2 (Read Committed), but weaker than PL-SI, PL-CS, and PL-2.99 (notably, RA does not prevent anti-dependency cycles, or Adyas G2 or G-SIainformally, it allows concurrent updates) [2]. RA does not (by itself) provide ordering guarantees across transactions. Our RAMP implementations provide a variant of PRAM consistency, where, for each item, each users writes are serialized [30] (i.e., session ordering [20]), and, once a users operation completes, all other users will observe its effects (regular register semantics, applied at the transaction level). This provides transitivity with respect to each users operations. For example, if a user updates her privacy settings and subsequently posts a new photo, the photo cannot be read without the privacy setting change [15]. However, PRAM does not respect the happens-before relation [4] across users. If Sam reads Marys comment and replies to it, other users may read Sams comment without Marys comment. In this case, RAMP transactions can leverage explicit causality [6] via foreign key dependencies, but happensbefore is not provided by default. If required, we believe it is possible to enforce happens-before but, due to scalability concerns regarding metadata and partition independence (e.g., [6] and Section 5), do not further explore this possibility. An active-active replicated implementation can provide available [5, 23] operation at the cost of these recency guarantees.\n", "prediction": "this paper introduces a new model for transactional isolation.<n> the model is based on the observation that many applications that are partitioned across multiple servers do not necessarily provide access to the same partitions of the original database, and this observation leads us to introduce the notion of _ multipartition_. we provide a simple and effective way to partition a database into multiple partitions : each partition contains the data of all the partitions that have been partitioned and we prove that our model provides a useful semantics for any application that wishes to perform atomic updates on any partitioned database ( e.g. a relational database or an unstructured data set ) with a limited number of partitions ( typically one for every @xmath0 items in the database and a partition for each item on each server ; for example a table of size 100 can contain the contents of an entire table and an item can be partitioned into many partitions according to what is required of its contents ( see [18,1922 ] for an example of a large class of applications including secondary indexing and view maintenance of this work and [38,38 ], for a more detailed discussion of these applications see ( s. g. kim and t. c. williams  esa / spht analysis of relational data processing ''<n> ( 2013a1a2b2a3a4b5 ). <n> we also present a practical implementation of our approach and show that we are able to provide high-(e for the case ( [4,4a5b4 [5a6b7 ])!? [8 ]. [10 -2 -3 -4 -6 -5 -20 -15 -16 -10 -18 -22 -14 -12 [ [ 15. [ 2 [ 16 -21 -24 -26 -27 [[[14 [ 20 ][[2 [ 22 -29 [ 32] we identify [ 3 [ a [ ( 2 -30 [ -40 [ to [ 21   2 2. 2 - [ we [... [ for [ 10  to a further [ the results to ( to add the [ and to 2 ( we address [ in [ with [ while [ as [ e th [ j  the following the other [ that [ but [ it [i [. to  [ several [ some [ after [ 4  while we further to we we  in our other and the others [ which we make [ by [ one  that ng  for  as we would autres []. we use the new [ whereas [ another [ what we read [ based [ others ( while the analysis [([((i ( ( and others that is [ also  ( as the most [ many other to read and while while others to which ly and it ( that was [ all [ an []  and ( in a number for all to our [ p and also to all we was also we as a ( the com and other  it and its [)  by the updates ... and that ( a ]] and]. lauren and many [ x and all for which is the replication of data and replication and some ( which was not the databases  with the number (  a) and is a replication for replication (... the update and data for other]. while ( for its data to any data that would ( x - and was the state of any ), while that to update the output for it as that the]. as it was a subsets and as  until the structure of other for database that does and any [4]. ( it is not as (]. and in which [], while it for data in "}
{"ground_truth": "We are seeing an explosion of uncertain datai.e., data that is more properly represented by probability distributions or estimated values with error bounds rather than exact valuesfrom sensors in IoT, sampling-based approximate computations and machine learning algorithms. In many cases, performing computations on uncertain data as if it were exact leads to incorrect results. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. This paper proposes and evaluates an approach for tackling this challenge in DAG-based data processing systems. We present a framework for uncertainty propagation (UP) that allows developers to modify precise implementations of DAG nodes to process uncertain inputs with modest effort. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. For example, a social network trend analysis application that combines data sampling with UP can reduce execution time by 2.3x when the user can tolerate a maximum relative error of 5% in the final answer. These results demonstrate that our UP framework presents a compelling approach for handling uncertain data in DAG processing.\nCCS CONCEPTS  Computer systems organization  Architectures; KEYWORDS Uncertainty Propagation, DAG Data Processing ACM Reference Format: Ioannis Manousakis, igo Goiri, Ricardo Bianchini, Sandro Rigo, and Thu D. Nguyen. 2018. Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing, Carlsbad, CA, USA, October 1113, 2018 (SoCC 18), 12 pages. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SoCC 18, October 1113, 2018, Carlsbad, CA, USA  2018 Association for Computing Machinery. ACM ISBN 978-1-4503-6011-1/18/10. . . $15.00 is being produced and collected at a tremendous pace. The need to process this vast amount of data has led to the design and deployment of data processing systems such as MapReduce, Spark and Scope [7, 32, 37]. These frameworks typically allow data processing applications to be expressed as directed acyclic graphs (DAGs) of side-effect free computation nodes, with data flowing through the edges for processing. The frameworks then run applications on clusters of servers, transparently handling issues such as task scheduling, data movement, and fault tolerance. At the same time, there is an urgent need for processing an exploding body of data with uncertainties [4]. For example, data collected using sensors are always estimates that have uncertainties the differences between the estimated and true valuesdue to sensor inaccuracies. Data uncertainties also arise in many other contexts, including probabilistic modeling [10], machine learning [26], approximate storage [29], and the use of sampling-based approximation that produce estimated outputs with error bounds [2, 11]. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings [4]. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. Thus, in this paper, we propose and evaluate a general framework that significantly eases this challenging task. Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains. Our framework is based on techniques that allow programmers to modify precise implementations of DAG computation nodes to handle uncertain inputs with modest effort. Uncertainties can then be propagated locally across each node of the DAG from the point where they are first introduced to the final outputs of the computation. More specifically, we use Differential Analysis (DA) [3] to propagate uncertainties through DAG nodes that are continuous and differentiable functions. For semi-continuous functions, we propagate uncertainties through a combination of DA and Monte Carlo simulation, where our framework automatically selects the appropriate method based on the input distributions and the locations of function discontinuities. For all other function types, we use Monte Carlo simulation. As an example of how a developer uses our framework, suppose a company needs to run a revenue prediction model implemented by a two-node logical DAG 1 shown in Figure 1(a). The first node approximates the number of customers belonging to different age groups in a database using BlinkDB [2]. The second node then computes the revenue as a weighted average, with the (uncertain) weights representing the predicted revenue per customer in a given age group. While the approximation can significantly reduce the execution time of the first node, it produces estimates with uncertainties (error bounds), rather than precise values. A developer can use our proposed framework to handle these uncertainties in the second node by providing the derivatives for the weighted average, which are essentially just the weights, with very few code changes to the precise version. This small amount of additional work will allow the answer to be computed as a distribution rather than an exact value that gives a misleading impression of precision. In particular, a precise answer, e.g., red line in Figure 1(c), may predict high revenue leading to profit while ignoring the left side of the distribution in Figure 1(c), which indicates a significant possibility of low revenue leading to an overall loss. Ignoring this possibility can be dangerous if the company is risk-averse. We implement the proposed framework in UP-MapReduce, an extension of the Hadoop MapReduce, to handle uncertainty propagation (UP). UP-MapReduce allows programmers to develop applications with UP in much the same way as their precise counterparts. Added efforts come in the form of selecting the appropriate uncertain Mapper and Reducer classes provided by UP-MapReduce and respecting some required constraints on code structures (Section 5). Developers can optionally provide closed-form derivatives for DAG nodes that implement UP with DA to enhance performance. We then leverage UP-MapReduce to build a toolbox of operations (e.g., sum, multiply, logarithm) on uncertain data and modify ten applications, including AI/ML, image processing, trend analysis, 1As explained in Section 4, small logical DAGs will often map to extremely large execution DAGs with thousands of execution nodes running on large server clusters when processing large data sets. and model construction applications, to process uncertain data. Our experience shows that UP-MapReduce is easy to use. Running two of these applications on real data sets demonstrates the tremendous potential for combining sampling-based approximation (early in the DAG) with UP to reduce execution time while properly propagating the introduced uncertainties to the final outputs. This propagation allows users to intelligently trade off accuracy for execution time. For example, in one application, execution time can be reduced by 2.3x if the user can tolerate errors of up to 5%. Further, in one of the two applications, the original data set is a sample of network probes and so any computation on this sample necessarily has to deal with uncertainties. UP-MapReduce allows developers to easily tackle these uncertainties. We also perform extensive sensitivity analyses on small to large execution DAGs (ranging up to tens of thousands of nodes), using eight of the applications with synthetic data, which allows us to adjust various input characteristics. Specifically, we explore the impact of UP on the magnitudes of uncertainties (e.g., whether uncertainties become worse after propagation), the accuracy of our UP techniques, overheads of UP, and scalability. Our results show that our UP techniques are highly accurate in most cases. Furthermore, the performance overheads of UP using DA are very low  average of 6% performance degradation  when closed-form derivatives are provided. Performance overheads are more significant when using DA with numerical differentiation or Monte Carlo simulation as input size increases, but this performance impact can be reduced by adding computation resources. Recall that these overheads arise from the need to process uncertain data instead of exact values. Finally, our results demonstrate that UP-MapReduce scales well to a cluster with 512 servers. In summary, our contributions include: (1) identifying existing theories appropriate for UP and showing how to apply them to DAG-based data processing frameworks, (2) designing and implementing our proposed UP approach in a MapReduce framework called UP-MapReduce, (3) implementing a suite of data processing applications to explore the accuracy, performance, and scalability of UP-MapReduce, and (4) showing that our approach is highly effective in many scenarios, allowing applications to efficiently account for data uncertainties.\nIn this section we first motivate the necessity for UP by presenting a (non-exhaustive) list of common uncertainty sources where UP is required if the data uncertainties are not to be ignored. We then proceed to discuss related work and in particular recent approximate methods that generate uncertainty as byproducts of the approximation. Finally, we review previous work in uncertainty estimation and belief propagation. Sources of Uncertainty. Collecting data from imprecise instruments such as temperature, position or other analog sensors often introduces measurement uncertainty. In these applications, acquiring precise data is typically not an option, but it is usually possible to tune precision at the expense of resources such as more expensive sensors, higher response time or energy consumption. An example of such a trade-off is the potential for a sensor network to enter a low-power state to conserve energy at the expense of providing lower quality measurements. Similarly, model uncertainty is introduced when computational models used in applications do not precisely describe physical phenomena. For example, in structure strength analysis, one may simulate the macroscopic impact of wind on a high-level model of a bridge structure, rather than modeling the forces on the individual molecules (which might be intractable). Approximate computing is an emerging source of approximation uncertainty. In this setting, it may be possible for the user to tradeoff precision (how much uncertainty) against execution time and/or energy consumption. Examples include iterative refinement techniques or aggregate approximation schemes (via sampling) such as BlinkDB [2] and ApproxHadoop [11]. This is a particularly interesting scenario since execution time savings achieved via approximation may be offset by the necessity for UP in subsequent nodes of a computation DAG. Other types of approximation-induced uncertainty include statistical estimators (i.e., from maximum likelihood or a posteriori estimation) and approximate storage [29]. Approximate computing with bounded errors. Extensive past work has been done in approximate computing with quality estimates by the systems, hardware and database communities. The purpose of approximate computing is to reduce the required resources (e.g., execution time and/or energy consumption) by relaxing the precision of the output but also providing estimates on the (uncertain) output quality  for example, the mean and variance of the output values. Most prior works have focused on reducing the input set by sampling and/or dropping computation. For example, the database community has long considered the problem through approximate query processing. There, database systems sample the input data set and/or drop sub-queries to accelerate top-level queries with the ultimate goal of reducing response time, increasing throughput [1, 16, 38], and/or even providing response time guarantees [2]. Some works identify computational blocks (at compile or runtime) that can be dropped for tunable approximation with or without accuracy estimates [24, 28]. ApproxHadoop accelerates the computation of large-scale aggregations (i.e., sum, count, average) by combining sampling and computation drop [11] while providing error estimates. Others provide energy bounds by online tuning of the approximation levels [14]. Finally, the hardware community approaches the problem by trading hardware accuracy for energy efficiency, performance and transistors. For example, Esmaeilzadeh et al. [9] designed an ISA extension that provides approximate operations and proposed a micro-architecture that implements approximate functional units such as adders, multipliers, and approximate load-store units (a problem that was also tackled later by Miguel et al. [23]). Uncertainty estimation and belief propagation. Prior work has been proposed to handle the uncertainty introduced by approximate systems and to perform belief propagation where uncertainty and prior beliefs are combined to perform inference. Approximate programming, for example, seeks to design systems and programming languages that implement and bound the errors of various arithmetic and logical operators (addition, multiplication, and comparison) when handling uncertain (probabilistic) types. For example, Uncertain<T> [4] is a language construct that can be used to estimate the output distribution of a graph of basic operations that compose a program. Uncertain<T> can be used for inference as well, by using Bayesian statistics to derive the posterior distribution. Others have worked on probabilistic programming to implement type systems [5, 34] and compiler transformations [6, 25] to handle uncertainty, error bounding, and inference for uncertain programs. Sampson et al. [30] worked on decision making under uncertainty which is necessary to implement branches and assertions in programs. In contrast with arithmetic operations, comparison operators are more challenging, as they involve estimating the tail of the (unknown) distribution  much like our approach for UP through semi-continuous functions. Differentiation from prior work. We differentiate from past work in uncertainty estimation as being the first to bring uncertainty propagation techniques to large-scale computational DAGs. In contrast to prior work (e.g., Uncertain <T> and ApproxHadoop) where uncertainty estimation is performed only for basic arithmetic and logical operations, we can handle arbitrary functions. At this high level of abstraction, new challenges arise. For example, accounting for covariances between uncertain data items may become a limiting factor in the performance and scalability of computations on uncertain data. Our method also offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation. Several other UP methods, such as polynomial chaos expansion and fast integration can also be used (in fact to estimate the actual distribution of Y instead of just computing the first two moments) [13, 19]. However, these methods are very computationally expensive, especially with increasing number of variables as noted by Lee and Chen [19]. We also do not perform any inference or multi-dimensional convolutions (as in Uncertain <T>) which suffer from high computational complexity and limit their applicability to only a few hundred input variables per node in the execution DAG. On the contrary, we show that our approach can handle millions of input variables with relatively low overhead.\nIn this section we introduce our proposed methods for handling uncertain inputs at a DAG node. Specifically, we discuss how to (approximately) compute Y = f (X), where f is an arbitrary function without side effects, representing the computation of a DAG node, X is a set of random variables representing inputs with uncertainties, and Y is a set of random variables representing outputs with uncertainties. Depending on the nature of f (continuous, semi-continuous or discrete), we leverage a set of three statistical methods to approximate the mean Yi and the variance  2Yi for each Yi in Y. These methods are described below.\nWe use first-order Differential Analysis (DA) to approximate the first two moments of Y, i.e., mean and variance, for functions f that are continuous and differentiable [3]. The general strategy is to compute Y by approximating f using its first-order Taylor series at the expected value of X. This approximation is accurate if f is roughly linear around the support (in other words, neighborhood) of X; errors are being introduced otherwise. As shall be seen in Section 7.3, using the first-order Taylor series gives good accuracy for the majority of the applications we study. For simplicity, we present DA equations for a single output value Y; we refer the reader to [3] for the full derivation of the multiple input, multiple output case. Let Y = f (X), withX = {X1,X2, ...,Xn }. We can compute an approximation Y of Y using the first-order Taylor series around a given point X0 = {X 01 ,X 02 , ...,X 0n } as: Y = 0 + n i=1 i (Xi  X 0i ) (1) 0 = f (X0) and i =  f Xi (X0) We then compute an approximate mean Y by setting X0 = X = {X1 , X2 , ..., Xn } and computing the expected value of Y . Y = E[Y ] = E [ 0 + n i=1 i (Xi  Xi ) ] (2) = 0 + n i=1 (iE[Xi ]  i Xi ) = 0 + n i=1 (i Xi  i Xi ) = f (X) Analogously, we can derive an estimate of the variance  2Y using the first-order Taylor series:  2Y = n i=1 2i  2 Xi + n i=1 n j=1, j,i 2i  2 j  2 XiX j (3) where  2XiX j is the covariance of Xi and X j . If we assume that the inputs are independent, so that XiX j = 0, i , j, then Equation 3 reduces to the left summand. We illustrate the computation of the mean and variance for the single-input, single-output case (Y = f (X )) in Figure 2(a). For the general case with multiple inputs and multiple outputs, one must also be concerned with the covariances between the outputs as shown in Figure 2(b). In general, these covariances may be nonzero. Thus, if the multiple outputs are being used as inputs to a later stage of computation as in Y = f (X),Z = (Y), then we cannot assume that Yi and Yj , i , j , in Y are independent. Rather, it would be necessary to compute the covariances  2YiYj and use them when approximating Z [3].\nWe can leverage the above approach for semi-continuous functions when the support of each Xi in X falls mostly or entirely within a continuous and differentiable part of the function. We adopt two approaches for checking with high confidence which intervals of X lie in continuous parts. The first assumes each Xi is approximately normal allowing the estimation of the support using any desired confidence interval through the corresponding covariance matrix of the input. The second approach makes no assumption about the distribution of X. It instead uses a multivariate generalization of the Chebyshevs inequality [17] to bound the probability that X lies within any interval. For example, suppose we define a filter function as f (X ) = 1 when X >  and 0 otherwise. This is a simple semi-continuous function defined on two intervals. Our framework automatically performs the required run-time checks for each Xi . In this case, it will check if X lies entirely (or mostly) in (,+) or (,]. If the condition is satisfied, it will leverage DA to propagate through the filtering function which in this case leads to an exact result. If X s support spans the discontinuity, our framework is forced to resort to Monte Carlo simulation which we discuss next.\nWe use Monte Carlo simulation to approximate Y for functions f that do not meet (or the developers to not know whether they meet) the requirements for DA. Specifically, we evaluate f on n randomly drawn samples of X (input) and use the outputs as an approximation of Y. As n  , the empirical distribution obtained for each Yi converges to the true distribution. To choose n, we use the following expression which bounds the difference between the empirical and the true distribution [21]: P ( sup yR (Fi ,n (y)  Fi (y)) >  )  2e2n 2 (4) where Fi ,n (y) is the empirically derived CDF for Yi and Fi (Y ) is the actual CDF for Yi . For example, to approximate the CDF of Fi (y) with a 99% probability of achieving an accuracy of  = 0.05, one would need n = 53 samples. To generate accurate samples, one must know the joint density of X and pay the heavy computational cost of any rejection-sampling algorithm. Unfortunately, that cost grows exponentially with an increasing size of X and thus we resort to two approximations. The first generates samples from the input marginalsXi , when provided or previously estimated, and ignores covariances. In the absence of full distributional information, the second approximation assumes that each input is normally distributed with the same mean and covariance matrix as the unknown distribution. Surprisingly, although the estimated distribution Y is only a coarse approximation of the actual but unknown Y , their corresponding mean and variances are similar. To see why, recall that in Eq. 3 we showed that the mean and variance estimation of Y depends solely on the mean and variance of X. Thus, simulating (drawing samples) from any X that matches the required mean and variances, will accurately approximate the corresponding values for Y .\nWe now discuss how to apply the UP techniques introduced in the last section to data processing DAGs. Figure 3 shows a small example DAG, where uncertainty is introduced in the node labeled s (e.g., via a sampling-based approximation technique). Uncertainties then must be propagated through the two u nodes following s . Figure 4 shows an example detailed view of the two u nodes designed to highlight the challenges of implementing UP in DAG data processing. This example can correspond to transformations in a Spark program or Map and Reduce phases in a MapReduce program. This figure shows that, in general, we must handle UP through multi-input, multi-output functions for implementation in DAG data processing frameworks. Further, inputs may have nonzero covariances; e.g., Ym2 and Ym1 are generated from the same input, and thus are likely to have a non-zero covariance. Finally, the number of inputs and outputs may not be known statically at development time; e.g., a reduce() function in MapReduce has to accept an arbitrary number of values (> 0) for each key. It is relatively straightforward to implement UP through blackbox functions using Monte Carlo simulation (henceforth called UPMC) despite the above complexities. This technique treats any node of the DAG as a black box, dynamically generates samples from the input set (each sample contains a single random value drawn from the distribution of each input data item), and dynamically computes the mean and variance for each output using the empirically derived distributions. Recall that we assume normal input distributions in the absence of this information and we ignore covariances between the inputs when constructing samples (Section 3.3), both of which may lead to inaccuracies. The implementation of Differential Analysis (henceforth called UP-DA) is more challenging. Specifically, when a DAG node producesmultiple outputs, we view it as being implemented bymultiple sub-functions, each producing one of the output. For example, if a function H (X0,X1,X2) produces two outputs Y1 and Y2, then it is expressible as Y1 = h1(X0,X1,X2) and Y2 = h2(X0,X1,X2). In fact, each sub-function may depend only on a subset of the inputs; e.g., Y1 = h1(X0,X1) and Y2 = h2(X0,X2). In this case, the UP implementation must be able to identify the inputs used by each sub-function to correctly compute the (co) variances (Equation 3). Thus, if a function such as f or  in Figure 4 produces multiple output values, each output must be produced by an invocation of a sub-function. The output values can be produced by multiple invocations of the same sub-function, or invocation of several different sub-functions. Each invocation must go through an UP interface so that we can track the input-to-output dependencies. Input covariances can require additional data flow to be added to the DAG for computing output variances and covariances. For example, consider the scenario where X1 and Xn have a non-zero covariance; even though Y2 and Ym are generated by different invocations of f , the covariance between X1 and Xn will affect the variance estimates for Y2 and Ym . The (previously independent) computation of Y2 and Ym now requires the read-only covariance matrix to be present in all nodes. In general, to propagate covariances properly, each node of the DAG must have the complete covariance matrix of the sibling inputs. This requirement is challenging to implement in practice since it introduces additional data propagation and dependencies among execution DAG nodes, both of which may degrade performance and limit scalability. Our current implementation of UP in MapReduce (Section 5) does not handle all possible covariances, leaving the exploration of the full issue for future work. Meanwhile, our results in Section 7 show that this limitation does not affect accuracy significantly in most applications that we study. As shall be seen, having closed-form partial derivatives can significantly reduce the performance overheads of UP-DA compared to numerical differentiation. Thus, an UP-DA implementation should provide an interface for the programmer to provide closed-form partial derivative functions when available. Since the number of inputs may not be known at compile time, the interface must be sufficiently flexible to allow for a parameterized implementation of the partial derivatives. For example, a function that is symmetrical on all inputs (e.g.,  X 2i ) has the same partial derivative for all inputs (e.g., 2Xi ). In this case, the partial derivative can be implemented using a single function parameterized by X and the index i . Finally, Figure 4 has some interesting performance implications. In the absence of covariances, UP is computed independently at each DAG node, allowing DAGs with UP to be sped up with added computation resources similar to without UP. However, speedup will ultimately be limited by the longest executing node as this straggler will determine the minimum execution time of the DAG. For example,may be an aggregator function that takes inputs from many different invocations of f . If  has to aggregate a large number of inputs, then UP will require the evaluation of many partial derivatives (and possibly many numerical differentiations) for Differential Analysis or multiple evaluations of a function with many inputs for Monte Carlo. Thus, an invocation of  with a comparatively large number of inputs can become a performance bottleneck. Fortunately, we can limit the impact of these stragglers by giving more resources to them. In particular, numerical differentiation and derivative evaluations for different inputs are independent and so can be executed in parallel. Monte Carlo runs are also independent. Parallelizing execution in both cases is quite easy, especially for many-core servers.\nAs a proof of concept, we extend Hadoop MapReduce to include the above UP techniques in multi-stage DAG applications. We first show how our approach can be applied to the MapReduce paradigm. We then describe our implementation called UP-MapReduce.\nIn MapReduce, each program runs in two phases, Map and Reduce. In the Map phase, a user-written side-effect-free map() function is invoked per each input (key, value) pair, and produces a set of intermediate (key, value) pairs, where multiple pairs may have the same key. In the Reduce phase, a user-written side-effect-free reduce() function is called per intermediate key and the set of values associated with that key (produced by all invocations of map() during the Map phase), and produces a set of keys, each with an associated set of values [8]. MapReduce programs can further be chained together to form complex DAGs. Figure 4 now maps readily to a MapReduce program (except that the keys are not shown), with the Map phase invoking the map function f and the Reduce phase invoking the reduce function . It is important to note that while the MapReduce model defines that map() only takes one (key, value) pair as input, the value may be a set; e.g., a line of words. Thus, we implement both map() and reduce() as multi-input, multi-output functions. Assuming that only values are uncertain (keys are exact), the discussion in Section 4 applies directly to the implementation of UP-MapReduce. Each map() or reduce() invocation expands to one or multiple UP calls through UP-MapReduce, which automatically estimates the uncertain outputs. UP-MapReduce then streams uncertain outputs from map() reduce() while in the case of multiple chained programs, temporarily writes these values to HDFS where the next program in the DAG consumes them. To support functions with multiple outputs, we introduce the notions of sub-maps and sub-reduces, with each map() (reduce()) containing one or more distinct sub-maps (sub-reduces). Each output must then be produced by the invocation of a sub-map (subreduce) on the correct subset of inputs. We adopt similar approach for implementing semi-continuous map() (reduce()) functions (Section 3.2); user-specified continuous intervals pair with exactly one sub-map (or sub-reducer respectively). Due to MapReduce limitations, where map() or reduce() invocations are independent, we currently do not handle the case where input covariances require additional data flow for computing output covariances; e.g., the previously mentioned case of X1 and Xn in Figure 4 having a non-zero covariance. We do support covariances for the inputs and outputs of a single invocation.\nWe implement UP-MapReduce as an extension of Apache Hadoop 2.7. The extension comprises three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions, for Map and Reduce, respectively. Developers must choose the correct classes when implementing programs for UP-MapReduce. Our extension also introduces the uncertain type PV (probabilistic value) which implements random variables. A PV variable contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution. Below, we briefly describe the necessary Reducer classes. UP is implemented similarly for the Mapper classes. UPMCReducer. This class implements UP-MC for reduce. It contains a PV object used to store the outputs, two abstract methods eval() and reduce(), and a reduceWithUP() method. The programmer needs to implement the reduce function (e.g., sum) in eval(), which accepts a variable number of double inputs and returns a variable number of double outputs. reduce() accepts a string key and a variable number of inputs in serialized form. reduceWithUP() implements UP-MC, and accepts a variable number of PV inputs. It computes the PV outputs using multiple invocations of eval() using samples derived from the PV inputs. A developer would then write her Reducer class by inheriting from this class, implementing eval() and reduce(). reduce() should first parse the input, then call reduceWith UP(), and finally emit the PV object. It is critical that reduce() does not perform any computation on the inputs that affect the output outside of eval(). The developer can specify that eval() should be invoked multiple times, with each invocation processing a particular subset of the inputs. This feature implements the multi-input, multi-output design via sub-maps and sub-reduces. UPDAContinuousReducer. This class implements UP-DA for continuous functions. The class adds an abstract method derivative() that accepts the inputs X in the form of an array of doubles, the index i to compute fXi , an array of constants that can be used as weights and outputs a double representing fXi (X). The developer implements this method to provide a closed-form derivative for UP-DA. The class also implements a reduceWith UP()method that overrides its parents method with an implementation of UP-DA. This method uses derivative() if it has been implemented, and numerical differentiation otherwise. It also uses input covariances, but expect the input covariance matrix to be loaded into the Hadoop read-only cache externally and prior to the execution of the Reduce phase. Then, it calls eval() as needed for evaluating the reduce function. The developer must implement eval() and reduce() as described above. UPDASemiContinuousReducer. This class implements UP-DA for semi-continuous functions and inherits from UPDAContinuousReducer. It allows the developer to specify a list of discontinuities in the reduce function and the range of the support of each input that must be within a continuous portion of the function. It implements a reduceWith UP() method that checks the support of each input against the discontinuities (with the desired accuracy), and chooses to use UP-MC or UP-DA as appropriate. No further implementation is required from the developer. Example UP-MapReduce program. Figure 5 shows the code for an UP-MapReduce program that computes a weighted average (secondDAGnode in Figure 1) in the presence of input uncertainties. Changes compared to a precise version are quite minimal. Parallelization ofMCandnumerical differentiation.Wehave extended the UP Reducer implementations to use multiple threads to speed up the execution of Monte Carlo simulation and numerical differentiation on servers with multi-core and/or hyperthreaded processors.\nWe have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. We list the applications in Table 1, along with the kernels comprising each application and shorthand names which we use later in the evaluation section. Below, we briefly discuss each one. 1) Uncertain toolbox.We apply UP-DA to a variety of continuous operations such as summation, multiplication, logarithms, exponentiation and trigonometric functions with known simple closed-form derivatives. We also include comparison and min/max operations (via UP-DA and UP-MC, respectively). We combine all the above operations to create a toolbox of uncertain elementary operations which can be used as building blocks to construct richer applications. In UP-MapReduce, these uncertain blocks may represent either a logical UP-map or a logical UP-reducer but at runtime, they will expand according to the required dataflow to one or multiple nodes in the execution DAG. 2) Matrix multiplication (mm). The multiplication of two matricesA (nm) andB (mp) can be performed by computing the elements of the outputmatrixAB (np) asABi j = f (Arowi ,Bcolumnj ) = m k=1AikBk j (the inner product of Arowi and Bcolumnj ). A MapReduce implementation can use the Map phase to read A and B and emit pairs (ki j , Aik ) and (ki j , Bk j ) for 0 < i  n, 0 < j  p, and 0 < k  m. The reduce() function can then sort the Aik s and Bk j s into a sequence Ai ,1,Ai ,2, ...,Ai ,m,B1, j ,B2, j , ...,Bm, j , and then compute the inner product. Applying UP-DA is then done as follows. The only change needed for map() is the handling of PV rather than precise values. UP is not needed because no computation is being done. The reduce() is rewritten to call eval() after properly arranging the inputs, followed by a call to continuousUP(). eval() computes the inner product. The partial derivatives for inputs from A is f Aik = Bk j , and vice versa for inputs from B. 3) Regression (linreg). Fitting hyperplanes to observations is a frequent task in analytics. In particular, linear regression often relies on the least-squares method, where the sum of the squared differences between the hyperplane and the observed points is minimized. We base our application on linear regression, i.e, we are looking for Y = X +  . In the presence of noisy observations with known means and variances, we estimate the mean and variance of  and  . 4) Clustering (kmeans). Assigning observed data to clusters with k-means is frequent in data exploration. Given a fixed number of clusters and a sequence of observed data points, k-means performs an iterative algorithm, which (may) converge to a solution that minimizes the normed distance between all the points and their corresponding clusters. In the presence of uncertain data points, we extend the precise k-means algorithm with UP to estimate the mean and variance of the estimated cluster coordinates. The algorithm will then operate as a logical DAG with depth equal to the number of iterations required for k-means to converge. The logical DAG will then expand in runtime, to a large execution DAG where UPMapReduce will propagate the uncertainty at every node. As an example, ford data points, c centroids and n iterations the uncertain execution DAG will comprise of (d2  c + 1)  n nodes. 5) k-nearest neighbors (kNN). A common classification method is performed by estimating the k nearest neighbors around a data point. This computation primarily involves calculating p-norms, which measure distance in multi-dimensional spaces. We extend the traditional notion of norm with UP to estimate the mean and variance, when the input coordinates are uncertain. 6) Solving systems of linear equations (linsolve). In order to solve large (nn) systems of linear equations, in the form ofAx = b, one can use the Jacobi method to find the unknown x. Jacobi is an iterative procedure that progressively refines the solution x. We extend Jacobi to support uncertain A and/or b inputs. Then, we compute the mean and variance for each element of x. 7) Finding eigenpairs (eig).Computing eigenpairs (and especially the dominant eigenvalue and eigenvector) is the central task in solving differential equations and computing eigenfaces. The power iteration iteratively calculates the dominant eigenpair of an input matrix. We create our own version of the power iteration to handle uncertain input data. Specifically, we combine basic uncertain operations (division), mm and Euclidean norms as previously shown to build the necessary iteration. The output is then a random eigenvalue and a random eigenvector. 8) Compression (svd). An effective data compression method is the Singular Value Decomposition (SVD). The SVD of an input matrix A is the key kernel in solving problems such as data compression, but also principal component analysis, weather prediction, and signal processing. We can calculate the components of SVD (U , , and V ) by finding the eigenvalues of AA and AA. In case A is uncertain, we extend the precise SVD implementation with UP-DA and in particular by using the uncertain toolbox and eig. 9) Data filter (filter). Data filters are common data manipulation tasks in large-scale data processing systems, such as Apache Spark, and built-in procedures in programming languages such as Scala. We implement an uncertain compare-aggregator filter that handles uncertain inputs. During the compare phase, the (uncertain) input data are compared against a user-defined value. The statistics of the intermediate result are forwarded to an aggregator function which estimates the uncertainty of the final result. 10) Trends in social media (tsocial). A common task in social media analysis is to study potential trends between variables of the social graph. For example, one might want to discover correlations between peoples age and number of followers in a social media site. Assuming the data is stored in a database, a two-phase workflow (a DAG whose logical nodes execute on different DAG processing systems) will first execute a GROUP BY query with stratified sampling to approximate the average number of friends per age group (with each group representing one day). This stage outputs the mean number of friends and a variance for each age group. The second phase performs uncertain linreg between the uncertain number of friends vs. age using linear regression. It then outputs the mean and variance for the slope and intercept of the fitted line. 11) Mean US internet latency estimation (latency). Suppose that a content delivery network (CDN) operator wants to improve the average perceived latency of its customers [35]. He then seeks to maximize the US-wide 10-mile average latency by altering the position of the CDN endpoints. To perform this task, the operator first estimates (via sampling) the mean (10-mile) latency of some candidate locations in the US. Obviously, the operator cannot estimate the desired latency mean on every possible location in US, but instead interpolates the nearby (unobserved) locations. To correctly perform the interpolation though, one should consider that each estimated mean is actually a distribution, as every estimate is being constructed from the appropriate samples. We replicate such a scenario and illustrate how UP can be combined in a multi-stage uncertain workflow. The workflow comprises the following stages 1) collect traceroute measurements (within the US) from the iPlanes dataset [20], 2) estimate the mean for each observed location using the samples, 3) use UP-MapReduce to perform bi-linear interpolations to estimate the mean latencies of unobserved locations and 4) use UP-MapReduce with an uncertain weighted average to simulate the frequency of packet transmission from each location based on known population density to ultimately obtain the mean and variance of the final estimate (population adjusted 10-mile mean latency).\nIn this section, we evaluate UP-MapReduce by studying its accuracy, performance, and scalability. We begin by exploring the two applications, tsocial and latency, that include sampling-based approximations and trade precision for reduced execution times. We show that by developing these applications in UP-MapReduce we can drastically decrease the execution time of both, while propagating the uncertainties introduced by the approximations. We then explore the accuracy of our UP techniques, performance overheads, and scalability via an extensive sensitivity analysis.\nInput data sets.We leverage real datasets for the two approximate applications under study. Specifically, we evaluate tsocial using the Facebook social structure from SNAP social circles [22] and latency using traceroute measurements from iPlanes [20]. For the purpose of the sensitivity analysis (performance, precision and scalability), we generate synthetic input data sets with varying sizes and amounts of uncertainty for each application, similarly to the synthetic data generation in [40]. For each data set, we first choose a random mean value  for each input item according to a uniform distribution on a chosen range of values. We then set the variance  2 for each input item to achieve a specific relative error defined as 3/. Baseline. We ran a large Monte Carlo experiment that executes a precise version of each application multiple times to accurately compute the empirical distributions for the outputs. Specifically, each experiment consists of n = 104 runs of a precise application, where each run is given inputs drawn randomly according to the actual (known) input distributions. Note that this is different than using UP-MC for each node of an applications DAG. Here, the entire application is run from beginning to end in each run as shown in Figure 6. For an iterative application, each run executes all iterations for a given input to generate an output sample. This way, all correlations between data items passing through the DAG are correctly preserved. The output samples from the n runs are then used to construct empirical output distributions from which we extract the mean and variance for each output. We consider three different distributions for input uncertainty: normal (Baseline-Normal), skewed with +0.5 skewness (Baseline-Skewed), and uniform (Baseline-Uniform). Comparing UP with Baseline. We compare the mean value and relative error for each output computed by UP-MapReduce against the values produced by the corresponding Baseline experiments. When an application produces one or a small number of outputs (e.g., linreg), we show the comparison for the output with the largest difference between the two approaches.When an application outputs a vector or matrix (e.g., svd), we show the comparison using the norm of the means 2 and the relative error defined as 3 2/2. We expand on a case to show that using the norms do not obfuscate large differences for a subset of estimated outputs. We use the mean produced by Baseline-Normal to compute the relative error for UP in our comparisons (since the mean produced by UP is an estimate). All mean values computed by all methods were very close together, so this choice had little impact. Experimental platform. All (but scalability) experiments were run on a cluster of 2 servers. Each server is equipped with two Intel Xeon dual-core processors, 8 GB of DRAM, 1 Gbps network interface and two 480 GB HDDs. The servers in this cluster ran Ubuntu Linux Server LTS 14.04. Scalability experiments (Section 7.4) were run on a cluster of 512 servers, where each server is equipped with two Intel Xeon 16-core processors, 64GB of DRAM, a 10Gbps network interface and four 3TB HDDs. All servers in that cluster ran Windows Server 2012. Finally, all experiments were run with UP-MapReduce (Apache Hadoop 2.7).\nWe now leverage UP-MapReduce to build two multi-stage approximate workflows (tsocial and latency). Both first sample their initial dataset and produce uncertain intermediate values. Then, we leverage UP-MapReduce to process these uncertain values in subsequent stages, ultimately generating the final (uncertain) outputs. Our results show that UP is critical for propagating the introduced uncertainties, inform users of the magnitude of the final errors and provide guidelines to control them by adjusting the amount of initial approximation. Specifically, tsocial is a two-stage approximate workflow comprising 1) the execution of an approximate query in BlinkDB [2] on 2  107 registered individuals, followed by 2) an uncertain linear regression (linreg) in UP-MapReduce. The execution of the approximate query in BlinkDB drastically reduces the execution time of the stage compared to a precise execution, but introduces uncertainties in the form of estimated errors (variance). UP-MapReduce is then used to propagate these uncertainties through the second stage of the computation. The second four-stage workflow (latency) approximates the mean US latency on a grid (2000 locations) by performing latency measurements only on 68 locations. This workflow comprises of 1) latency measurements which generate uncertainty due to sampling 2) generate an uncertain 2-dimensional latency surface on the obtained estimates from these 68 locations 3) perform uncertain bilinear interpolation on the (unobserved) remaining 1932 locations and 4) perform an uncertain weighted average to generate the population-weighted latency average. Figure 7 (top) shows the execution times of tsocial (right yaxis) for sampling rates ranging from 0.1% to 100% (precise). It also R eg re ss io n er ro r ( % ) 0 10 20 30 0 5 10 15 Ex ec ut io n tim e (s ) Output Error - max(a,b) BlinkDB - Query UP-MapReduce - Regression -2 10-1 100 101 10210 100 0 50 100 0 50 100 Sampling rate (%) Ex ec ut io n tim e (s ) La st s ta ge e rro r ( % ) 101 102 Output Error UP-MapReduce Processing Sampling rate (%) tsocial latency Figure 7: Obtained relative errors and execution times for varying the sampling rate of two approximate workflows (tsocial-top and latency-bottom). shows the maximum relative error of the regression coefficients for slope and intercept (left y-axis). We only show UP-DA-numDiff for UP-MapReduce because execution times and errors for all three techniques are similar given the relatively small number of output items from BlinkDB (3 104). We observe that significant savings in overall execution time can be achieved despite the overheads of UP. For example, a 5% sampling rate in the first logical DAG node leads to a relative error of just 1.35% and 51% savings in execution time (4s for BlinkDB and 2.9s for UP-MapReduce compared to 14.1s for BlinkDB without sampling plus a negligible amount of time for the precise linear regression computation). Overheads from UP require a sampling rate of 80% before approximation can lead to time savings for the workflow. After that, reduction in execution time increases as the sampling rate decreases since the UP overheads are relatively constant. Reduction in workflow execution time continues to increase until the smallest sampling rate of 0.1% for a maximum of 67.8% savings. However, the relative error increases rapidly to 30% after a sampling rate of 1%. Similarly, Figure 7 (bottom) shows the execution times of postprocessing the obtained traceroute data (excluding the time to perform the traceroutes themselves) and the duration of the subsequent stages (UP-MapReduce bi-linear interpolation and weighed average) for sampling latencies ranging from 1% to 100%. Note that a 100% sampling rate (2500 samples per observed location) indicates that we process all the available data; it does not correspond to sampling the entire network (which is not be possible to achieve). The estimated means are still uncertain and they include errors which should be propagated with UP. Initially, and for sampling rates of 10  100%, we observe a generous reduction in execution time from 82  19s . For smaller sampling rates, the savings cap at 12s . The execution times for UP-MapReduce again stay unaltered as the number of observed (60) and interpolant locations (5940) are constant (8.1s). The output error of the weighted average, increases quadratically as we decrease sampling rate. For example, sampling just 25% of the data, we can reduce the execution time by 62.5% with an output error of 9.01%. Similarly to tsocial, we only show UP-DA-numDiff, as it was the UP method with the longest running times. Interestingly in this case, there is no trade-off between postprocessing and UP-MapReduce execution times (in contrast to tsocial). As we always estimate the means from samples, UPMapReduce is necessary to propagate the uncertainties. It is then evident that without UP-MapReduce, we would be unaware of the high potential workflow error (which can be as high as 92.8%).\nWe now perform a sensitivity analysis to evaluate the accuracy and performance of UP-MapReduce. We include results from all previously described applications except the toolbox,mm and tsocial, as they are included as part of the other applications under study. We start by exploring the accuracy of UP-MapReduce estimation of the means. Figure 8 plots the relative error (%) of the means (or the corresponding Euclidean norm in case of multivariate outputs) computed by UP-DA using numerical differentiation against the Baseline-Normal. These results are identical for UP-MC. We observe that UP-MapReduce estimates the means with very low bias, especially when the input relative errors are small (< 3%). We next study the accuracy of the estimated relative errors. Figure 9(left) plots the relative errors computed by the three variants of UP-MapReduce as a function of the input relative error for 3 representative applications. The figure also plots the values produced by the three Baseline variants. Figure 9(right) plots the execution times of UP-MapReduce as a function of input size (the relative error of the input does not affect execution time). The figure also plots the execution times of precise versions, where there is zero input variance. We observe that input uncertainties can be relatively stable, contract, or expand after propagation depending on the application. UP-MapReduce is highly accurate in most cases; i.e., its estimated relative errors are very close to the baseline values for 6 of the applications (linreg, kmeans, latency filter, kNN and linsolve). On the other hand, its estimated relative errors can also deviate noticeably from the baseline values (eig, and svd) when input errors are significant. In these cases, all three UP methods show similar deviations from the baseline although there are small differences between UP-MC and the other two approaches. Deviations for UPDA-numDiff and UP-DA-closedForm arise from the inaccuracies introduced by Differential Analysis. Deviations for UP-MC arise from the fact that UP-MapReduce performs the Monte Carlo computation independently for each computation node in the DAG, as opposed to executing the entire DAG multiple times as in the Baseline experiments. As previously mentioned, our current implementation does not account for all covariances and does not consider input covariances when drawing input samples in UP-MC, all of which also contribute to the observed deviations. To verify that the computed norms are not obfuscating large differences between the UP-MapReduce estimates and baseline results, we also study the differences for each output in the multioutput applications. For example, Figure 10 plots CDFs of relative errors produced by the Baseline-Normal and UP-DA-numDiff when running linsolve for a 50  50 linear system. Observe that UP accurately estimates the entire relative error CDF of multivariate outputs for 1% input relative errors (Figure 10(a)), while for larger relative errors of 15% UP precisely estimates a significant portion of errors (79%), with significant deviations for only a very few outputs. We observe similar trends in the remaining multivariate applications (svd, kmeans, linreg, eig and latency). Interestingly, UP-DA-closedForm adds very little overhead to the precise version. This is because the derivatives for all functions being evaluated in our applications are simple functions. For example, the partial derivative with respect to xi of the inner product x, y = ni=1 xiyi is simply yi , an O(1) computation. Thus, even though the number of evaluations of the derivative functions grows linearly with the number of inputs, each evaluation is extremely cheap and so the computations adds little overhead overall. Out of the eight applications, the maximum overhead (compared to the same application without UP) is 11.4% (kmeans) while the average across them is 6.0%. It is important to note that the overhead for UP-DA-closedForm in general depends on the complexity of the derivatives; however, in all applications under consideration it was less expensive to evaluate derivatives of a function than the function itself. Thus, we expect the overheads of UP-DA-closedForm to be routinely lower than the ones for the other two UP techniques.\nWe finally explore the scalability of UP-MapReduce by running applications 3-11 on a cluster of 512 servers.We also run the original precise applications. We choose the following input sizes: linreg (16106), kmeans (107), kNN (16106), linsolve (9106), eig (9106), svd (9  106), filter (16  106), latency (16  106 from 150 locations). We illustrate our results (speedups) vs. increasing number of servers from four representative applications in Figure 11. The rest follow similar trends. We draw the following conclusions. First, we observe that in all evaluated applications UP-DA-closed Form achieves similar (on average 1.6% difference) scalability as the precise version due to its low additive overhead. Thus, uncertainty propagation does not deteriorate scalability (which as shown in Figure 11 may be poor) of the original application. Second, UP-DAclosedForm and UP-MC-monteCarlo show better scalability in all applications (except kmeans) due to the increased work per task (map and/or reducer) which amortizes the framework overheads. We expect this improvement to hold in applications where UP does not cause heavy execution imbalance (following observation). Third, when evaluating kmeans with UP-DA-closedForm or UP-MC-monteCarlo we observe lower scalability (in contrary with the previous point). A study of executions shows that this is a result of straggler reducers which are caused from data imbalance between different intermediate keys (centroids) and amplified either by the numerical differentiation or the Monte Carlo simulation. The imbalance is not noticeable in the precise and UP-DA-closedForm versions but the other UP methods increase the running times causing reduced scalability. These effects are even more noticeable on UP-DA-numDiff without straggler parallelization which attains a maximum speedup of just 10 when we utilize all our available servers (not shown in Figure 11).\nIn this paper, we proposed an approach for propagating data uncertainties through DAG computations. Specifically, we showed how Differential Analysis can be used to propagate uncertainties through DAG nodes implementing continuous (and semi-continuous under certain conditions) and differentiable functions. Our approach falls back to Monte Carlo simulation of nodes otherwise, but uses statistical bounds to minimize overheads while achieving a target error bounds. Our approach also allows the inter-mixing of Differential Analysis and Monte Carlo simulation for different nodes within a DAG, offering flexibility in the operations supported and minimizing performance overheads We have shown how our UP approach can be applied to general DAG frameworks. We have also implemented it in the UPMapReduce system. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low  an average of 6% performance degradation  when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computing resources. Our scalability results show that UP-MapReduce scales well to a cluster with 512 servers. Finally, using two workflows that couple approximation with UP, we show that significant reductions in execution time can be achieved with approximation, despite the need for UP which propagates estimated uncertainties to the final output.\nThis work was partially supported by NSF grant CCF-1319755.\n", "prediction": "we present a framework for processing uncertain data that allows us to easily trade off performance for accuracy.<n> specifically, we show that our framework provides a compelling solution to the problem of processing ( imprecise ) data in a way that is easy to implement and that provides high accuracy with a small amount of effort : we introduce a simulation function that can be used as an approximation to an unknown function in [ 1 ] ; we use this framework to introduce an uncertainty propagation ( ie propagation of uncertainty in the same way as that of a function _ a priori_. <n> we also provide a performance guarantee on the output of our proposed framework that guarantees that the error introduced by error propagation is at most a constant over the range of the input- and output- uncertainty distributions for which we provide an error estimate for the approximation- s to be performed ( which is not required to perform the simulation- or to use the  error - propagation '' of error estimates for approximations of unknown functions to compute the expected value of an input function or of its covariance function for a given input - uncertainty distribution ).<n> our results demonstrate that we can easily introduce uncertainty into a system to solve a problem that has been solved with high precision and with errors that are not very large ( e.g. @xmath0 in our example ), instead of introducing uncertainty as a high-level approximation ( i.e. f = j = f ( f t = p = b = n = a f= f(f = @f f ( b fg = ( a = [ f @ f f + f - f i = g = i eq ( g bf = the first b @ @ ( @ g @ b b ( j @  f b is f is the @ i @ = ]) is a g f [ b i is @ [ [ g b [!  g g ( [ j 2,3,4  ], f g [ i [ @ a th ]. g to f * g * f to g is [ 2 [f [ e = 2 [ ( the second ][f @[n  for [ a [ p  to gravid  the [ to [[[([g  ( 2[ f for 2 ( ([i  in f with f[e  [ 3  with 2 to @ 2 @ to ( we are the function [ which  as [f to our [ as the f and the i to which [ the]. to we will be the number of [ 4  which to obtain the probability  from the] to e  that  is  2 we ... [ + [... to a @ which will  while the solution for f in which @ the e is 2 with the ( is to...  @ as f as ((g to p ( in 2. [].  + 2( f)  b[] with [] @(n to 2 + @... f]. [ ii  by [ we is p[2]  and [ that [(e [2]. g]. the], which which the]) to b and @ e) to add f], [ * ]]  e @ - [])  * @ and f... @ j[b  a] is]. @ p [4] 4]]]. in g] [n [],  = 3 ( + g], we have the functions for all [g]. f. e -[a  ii [ and we to + 2]) with which]. we].<n> [2 *(i] and a].]. for our]. 2] as 2, f]"}
{"ground_truth": "Many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. Mining these continuous data streams brings unique opportunities, but also new challenges. This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using off-the-shelf hardware. It uses Hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. We study VFDTs properties and demonstrate its utility through an extensive set of experiments on synthetic data. We apply VFDT to mining the continuous stream of Web access data from the whole University of Washington main campus.\nCategories and Subject Descriptors H.2.8 [Database Management]: Database Applications data mining ; I.2.6 [Artificial Intelligence]: Learning concept learning ; I.5.2 [Pattern Recognition]: Design Methodologyclassifier design and evaluation General Terms Decision trees, Hoeffding bounds, incremental learning, diskbased algorithms, subsampling\nKnowledge discovery systems are constrained by three main limited resources: time, memory and sample size. In traditional applications of machine learning and statistics, sample size tends to be the dominant limitation: the computational resources for a massive search are available, but carrying out such a search over the small samples available (typically less than 10,000 examples) often leads to overfitting or data dredging (e.g., [22, 16]). Thus overfitting avoidance becomes the main concern, and only a fraction of the available computational power is used [3]. In contrast, in many (if not most) present-day data mining applications, the bottleneck is time and memory, not examples. The latter are typically in over-supply, in the sense that it is impossible with current KDD systems to make use of all of them within the available computational resources. As a result, most of the available examples go unused, and underfitting may result: enough data to model very complex phenomena is available, but inappropriately simple models are produced because we are unable to take full advantage of the data. Thus the development of highly efficient algorithms becomes a priority. Currently, the most efficient algorithms available (e.g., [17]) concentrate on making it possible to mine databases that do not fit in main memory by only requiring sequential scans of the disk. But even these algorithms have only been tested on up to a few million examples. In many applications this is less than a days worth of data. For example, every day retail chains record millions of transactions, telecommunications companies connect millions of calls, large banks process millions of ATM and credit card operations, and popular Web sites log millions of hits. As the expansion of the Internet continues and ubiquitous computing becomes a reality, we can expect that such data volumes will become the rule rather than the exception. Current data mining systems are not equipped to cope with them. When new examples arrive at a higher rate than they can be mined, the quantity of unused data grows without bounds as time progresses. Even simply preserving the examples for future use can be a problem when they need to be sent to tertiary storage, are easily lost or corrupted, or become unusable when the relevant contextual information is no longer available. When the source of examples is an open-ended data stream, the notion of mining a database of fixed size itself becomes questionable. Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists. However, the available algorithms of this type (e.g., [20]) have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm. This paper proposes Hoeffding trees, a decision-tree learning method that overcomes this trade-off. Hoeffding trees can be learned in constant time per example (more precisely, in time that is worst-case proportional to the number of attributes), while being nearly identical to the trees a conventional batch learner would produce, given enough examples. The probability that the Hoeffding and conventional tree learners will choose different tests at any given node decreases exponentially with the number of examples. We also describe and evaluate VFDT, a decision-tree learning system based on Hoeffding trees. VFDT is I/O bound in the sense that it mines examples in less time than it takes to input them from disk. It does not store any examples (or parts thereof) in main memory, requiring only space proportional to the size of the tree and associated sufficient statistics. It can learn by seeing each example only once, and therefore does not require examples from an online stream to ever be stored. It is an anytime algorithm in the sense that a ready-to-use model is available at any time after the first few examples are seen, and its quality increases smoothly with time. The next section introduces Hoeffding trees and studies their properties. We then describe the VFDT system and its empirical evaluation. The paper concludes with a discussion of related and future work.\nThe classification problem is generally defined as follows. A set of N training examples of the form (x, y) is given, where y is a discrete class label and x is a vector of d attributes, each of which may be symbolic or numeric. The goal is to produce from these examples a model y = f(x) that will predict the classes y of future examples x with high accuracy. For example, x could be a description of a clients recent purchases, and y the decision to send that customer a catalog or not; or x could be a record of a cellular-telephone call, and y the decision whether it is fraudulent or not. One of the most effective and widely-used classification methods is decision tree learning [1, 15]. Learners of this type induce models in the form of decision trees, where each node contains a test on an attribute, each branch from a node corresponds to a possible outcome of the test, and each leaf contains a class prediction. The label y = DT (x) for an example x is obtained by passing the example down from the root to a leaf, testing the appropriate attribute at each node and following the branch corresponding to the attributes value in the example. A decision tree is learned by recursively replacing leaves by test nodes, starting at the root. The attribute to test at a node is chosen by comparing all the available attributes and choosing the best one according to some heuristic measure. Classic decision tree learners like ID3, C4.5 and CART assume that all training examples can be stored simultaneously in main memory, and are thus severely limited in the number of examples they can learn from. Disk-based decision tree learners like SLIQ [10] and SPRINT [17] assume the examples are stored on disk, and learn by repeatedly reading them in sequentially (effectively once per level in the tree). While this greatly increases the size of usable training sets, it can become prohibitively ex- pensive when learning complex trees (i.e., trees with many levels), and fails when datasets are too large to fit in the available disk space. Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets. This learner should require each example to be read at most once, and only a small constant time to process it. This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost. We achieve this by noting with Catlett [2] and others that, in order to find the best attribute to test at a given node, it may be sufficient to consider only a small subset of the training examples that pass through that node. Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively.1 We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound) [7, 9]. Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean r. The Hoeffding bound states that, with probability 1  , the true mean of the variable is at least r  , where = R2 ln(1/) 2n (1) The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations. The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same  and ). Let G(Xi) be the heuristic measure used to choose test attributes (e.g., the measure could be information gain as in C4.5, or the Gini index as in CART). Our goal is to ensure that, with high probability, the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples. Assume G is to be maximized, and let Xa be the attribute with highest observed G after seeing n examples, and Xb be the second-best attribute. Let G = G(Xa)  G(Xb)  0 be the difference between their observed heuristic values. Then, given a desired , the Hoeffding bound guarantees that Xa is the correct choice with probability 1   if n examples have been seen at this node and G > .2 In other words, if the ob- 1We assume the examples are generated by a stationary stochastic process (i.e., their distribution does not change over time). If the examples are being read from disk, we assume that they are in random order. If this is not the case, they should be randomized, for example by creating a random index and sorting on it. 2In this paper we assume that the third-best and lower attributes have sufficiently smaller gains that their probability of being the true best choice is negligible. We plan to lift this assumption in future work. If the attributes at a given node are (pessimistically) assumed independent, it simply involves a Bonferroni correction to  [11]. served G > then the Hoeffding bound guarantees that the true G  G  > 0 with probability 1  , and therefore that Xa is indeed the best attribute with probability 1  . This is valid as long as the G value for a node can be viewed as an average of G values for the examples at that node, as is the case for the measures typically used. Thus a node needs to accumulate examples from the stream until becomes smaller than G. (Notice that is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves. This leads to the Hoeffding tree algorithm, shown in pseudo-code in Table 1. The counts nijk are the sufficient statistics needed to compute most heuristic measures; if other quantities are required, they can be similarly maintained. Pre-pruning is carried out by considering at each node a null attribute X that consists of not splitting the node. Thus a split will only be made if, with confidence 1, the best split found is better according to G than not splitting. The pseudo-code shown is only for discrete attributes, but its extension to numeric ones is immediate, following the usual method of allowing tests of the form (Xi < xij)?, and computing G for each allowed threshold xij . The sequence of examples S may be infinite, in which case the procedure never terminates, and at any point in time a parallel procedure can use the current tree HT to make class predictions. If d is the number of attributes, v is the maximum number of values per attribute, and c is the number of classes, the Hoeffding tree algorithm requires O(dvc) memory to store the necessary counts at each leaf. If l is the number of leaves in the tree, the total memory required is O(ldvc). This is independent of the number of examples seen, if the size of the tree depends only on the true concept and is independent of the size of the training set. (Although this is a common assumption in the analysis of decision-tree and related algorithms, it often fails in practice. Section 3 describes a refinement to the algorithm to cope with this.) A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node). In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces. In order to make this statement precise, we need to define the notion of disagreement between two decision trees. Let P (x) be the probability that the attribute vector (loosely, example) x will be observed, and let I(.) be the indicator function, which returns 1 if its argument is true and 0 otherwise. Definition 1. The extensional disagreement e between two decision trees DT1 and DT2 is the probability that they will produce different class predictions for an example: e(DT1, DT2) = x P (x)I[DT1(x) 6= DT2(x)] Consider that two internal nodes are different if they contain different tests, two leaves are different if they contain different class predictions, and an internal node is different Table 1: The Hoeffding tree algorithm. Inputs: S is a sequence of examples, X is a set of discrete attributes, G(.) is a split evaluation function,  is one minus the desired probability of choosing the correct attribute at any given node. Output: HT is a decision tree. Procedure HoeffdingTree (S, X, G, ) Let HT be a tree with a single leaf l1 (the root). Let X1 = X  {X}. Let G1(X) be the G obtained by predicting the most frequent class in S. For each class yk For each value xij of each attribute Xi  X Let nijk(l1) = 0. For each example (x, yk) in S Sort (x, y) into a leaf l using HT . For each xij in x such that Xi  Xl Increment nijk(l). Label l with the majority class among the examples seen so far at l. If the examples seen so far at l are not all of the same class, then Compute Gl(Xi) for each attribute Xi  Xl  {X} using the counts nijk(l). Let Xa be the attribute with highest Gl. Let Xb be the attribute with second-highest Gl. Compute using Equation 1. If Gl(Xa)  Gl(Xb) > and Xa 6= X, then Replace l by an internal node that splits on Xa. For each branch of the split Add a new leaf lm, and let Xm = X  {Xa}. Let Gm(X) be the G obtained by predicting the most frequent class at lm. For each class yk and each value xij of each attribute Xi  Xm  {X} Let nijk(lm) = 0. Return HT . from a leaf. Consider also that two paths through trees are different if they differ in length or in at least one node. Definition 2. The intensional disagreement i between two decision trees DT1 and DT2 is the probability that the path of an example through DT1 will differ from its path through DT2: i(DT1, DT2) = x P (x)I[Path1(x) 6= Path2(x)] where Pathi(x) is the path of example x through tree DTi. Two decision trees agree intensionally on an example iff they are indistinguishable for that example: the example is passed down exactly the same sequence of nodes, and receives an identical class prediction. Intensional disagreement is a stronger notion than extensional disagreement, in the sense that DT1,DT2 i(DT1, DT2)  e(DT1, DT2). Let pl be the probability that an example that reaches level l in a decision tree falls into a leaf at that level. To simplify, we will assume that this probability is constant, i.e., l pl = p, where p will be termed the leaf probability. This is a realistic assumption, in the sense that it is typically approximately true for the decision trees that are generated in practice. Let HT be the tree produced by the Hoeffding tree algorithm with desired probability  given an infinite sequence of examples S, and DT be the asymptotic batch decision tree induced by choosing at each node the attribute with true greatest G (i.e., by using infinite examples at each node). Let E[i(HT, DT)] be the expected value of i(HT, DT), taken over all possible infinite training sequences. We can then state the following result. Theorem 1. If HT is the tree produced by the Hoeffding tree algorithm with desired probability  given infinite examples (Table 1), DT is the asymptotic batch tree, and p is the leaf probability, then E[i(HT, DT)]  /p. Proof. For brevity, we will refer to intensional disagreement simply as disagreement. Consider an example x that falls into a leaf at level lh in HT, and into a leaf at level ld in DT. Let l = min{lh, ld}. Let PathH(x) = (N H 1 (x), N H 2 (x), . . . , NHl (x)) be xs path through HT up to level l, where NHi (x) is the node that x goes through at level i in HT, and similarly for PathD(x), xs path through DT. If l = lh then NHl (x) is a leaf with a class prediction, and similarly for NDl (x) if l = ld. Let Ii represent the proposition PathH(x) = PathD(x) up to and including level i, with I0 = True. Notice that P (lh 6= ld) is included in P (NHl (x) 6= N D l (x)|Il1), because if the two paths have different lengths then one tree must have a leaf where the other has an internal node. Then, omitting the dependency of the nodes on x for brevity, P (PathH(x) 6= PathD(x)) = P (NH1 6= N D 1  N H 2 6= N D 2  . . .  N H l 6= N D l ) = P (NH1 6= N D 1 |I0) + P (N H 2 6= N D 2 |I1) + . . . +P (NHl 6= N D l |Il1) = l i=1 P (NHi 6= N D i |Ii1)  l i=1  = l (2) Let HT(S) be the Hoeffding tree generated from training sequence S. Then E[i(HT, DT)] is the average over all infinite training sequences S of the probability that an examples path through HT(S) will differ from its path through DT: E[i(HT, DT)] = S P (S) x P (x) I[PathH(x) 6= PathD(x)] = x P (x) P (PathH(x) 6= PathD(x)) =  i=1 xLi P (x) P (PathH(x) 6= PathD(x)) (3) where Li is the set of examples that fall into a leaf of DT at level i. According to Equation 2, the probability that an examples path through HT(S) will differ from its path through DT, given that the latter is of length i, is at most i (since i  l). Thus E[i(HT, DT)]   i=1 xLi P (x)(i) =  i=1 (i) xLi P (x) (4) The sum xLi P (x) is the probability that an example x will fall into a leaf of DT at level i, and is equal to (1  p)i1p, where p is the leaf probability. Therefore E[i(HT, DT)]   i=1 (i)(1  p)i1p = p  i=1 i(1  p)i1 = p  i=1 (1  p)i1 +  i=2 (1  p)i1 +    +  i=k (1  p)i1 +    = p 1 p + 1  p p +    + (1  p)k1 p +    =  1 + (1  p) +    + (1  p)k1 +    =   i=0 (1  p)i =  p (5) This completes the demonstration of Theorem 1. An immediate corollary of Theorem 1 is that the expected extensional disagreement between HT and DT is also asymptotically at most /p (although in this case the bound is much looser). Another corollary (whose proof we omit here in the interests of space) is that there exists a subtree of the asymptotic batch tree such that the expected disagreement between it and the Hoeffding tree learned on finite data is at most /p. In other words, if /p is small then the Hoeffding tree learned on finite data is very similar to a subtree of the asymptotic batch tree. A useful application of Theorem 1 is that, instead of , users can now specify as input to the Hoeffding tree algorithm the maximum expected disagreement they are willing to accept, given enough examples for the tree to settle. The latter is much more meaningful, and can be intuitively specified without understanding the workings of the algorithm or the Hoeffding bound. The algorithm will also need an estimate of p, which can easily be obtained (for example) by running a conventional decision tree learner on a manageable subset of the data. How practical are these bounds? Suppose that the best and second-best attribute differ by 10% (i.e., /R = 0.1). Then, according to Equation 1, ensuring  = 0.1% requires 380 examples, and ensuring  = 0.0001% requires only 345 additional examples. An exponential improvement in , and therefore in expected disagreement, can be obtained with a linear increase in the number of examples. Thus, even with very small leaf probabilities (i.e., very large trees), very good agreements can be obtained with a relatively small number of examples per node. For example, if p = 0.01%, an expected disagreement of at most 1% can be guaranteed with 725 examples per node. If p = 1%, the same number of examples guarantees a disagreement of at most 0.01%.\nWe have implemented a decision-tree learning system based on the Hoeffding tree algorithm, which we call VFDT (Very Fast Decision Tree learner). VFDT allows the use of either information gain or the Gini index as the attribute evaluation measure. It includes a number of refinements to the algorithm in Table 1: Ties. When two or more attributes have very similar Gs, potentially many examples will be required to decide between them with high confidence. This is presumably wasteful, because in this case it makes little difference which attribute is chosen. Thus VFDT can optionally decide that there is effectively a tie and split on the current best attribute if G < <  , where  is a user-specified threshold. G computation. The most significant part of the time cost per example is recomputing G. It is inefficient to recompute G for every new example, because it is unlikely that the decision to split will be made at that specific point. Thus VFDT allows the user to specify a minimum number of new examples nmin that must be accumulated at a leaf before G is recomputed. This effectively reduces the global time spent on G computations by a factor of nmin, and can make learning with VFDT nearly as fast as simply classifying the training examples. Notice, however, that it will have the effect of implementing a smaller  than the one specified by the user, because examples will be accumulated beyond the strict minimum required to choose the correct attribute with confidence 1  . (This increases the time required to build a node, but our experiments show that the net effect is still a large speedup.) Because  shrinks exponentially fast with the number of examples, the difference could be large, and the  input to VFDT should be correspondingly larger than the target. Memory. As long as VFDT processes examples faster than they arrive, which will be the case in all but the most demanding applications, the sole obstacle to learning arbitrarily complex models will be the finite RAM available. VFDTs memory use is dominated by the memory required to keep counts for all growing leaves. If the maximum available memory is ever reached, VFDT deactivates the least promising leaves in order to make room for new ones. If pl is the probability that an arbitrary example will fall into leaf l, and el is the observed error rate at that leaf, then plel is an upper bound on the error reduction achievable by refining the leaf. plel for a new leaf is estimated using the counts at the parent for the corresponding attribute value. The least promising leaves are considered to be the ones with the lowest values of plel. When a leaf is deactivated, its memory is freed, except for a single number required to keep track of plel. A leaf can then be reactivated if it becomes more promising than currently active leaves. This is accomplished by, at regular intervals, scanning through all the active and inactive leaves, and replacing the least promising active leaves with the inactive ones that dominate them. Poor attributes. Memory usage is also minimized by dropping early on attributes that do not look promising. As soon as the difference between an attributes G and the best ones becomes greater than , the attribute can be dropped from consideration, and the memory used to store the corresponding counts can be freed. Initialization. VFDT can be initialized with the tree produced by a conventional RAM-based learner on a small subset of the data. This tree can either be input as is, or over-pruned to contain only those nodes that VFDT would have accepted given the number of examples at them. This can give VFDT a head start that will allow it to reach the same accuracies at smaller numbers of examples throughout the learning curve. Rescans. VFDT can rescan previously-seen examples. This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times. This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once. The next section describes an empirical study of VFDT, where the utility of these refinements is evaluated.\n\nA system like VFDT is only useful if it is able to learn more accurate trees than a conventional system, given similar computational resources. In particular, it should be able to use to advantage the examples that are beyond a conventional systems ability to process. In this section we test this empirically by comparing VFDT with C4.5 release 8 [15] on a series of synthetic datasets. Using these allows us to freely vary the relevant parameters of the learning process. In order to ensure a fair comparison, we restricted the two systems to using the same amount of RAM. This was done by setting VFDTs available memory parameter to 40MB, and giving C4.5 the maximum number of examples that would fit in the same memory (100k examples).3 VFDT used information gain as the G function. Fourteen concepts were used for comparison, all with two classes and 100 binary attributes. The concepts were created by randomly generating decision trees as follows. At each level after the first three, a fraction f of the nodes was replaced by leaves; the rest became splits on a random attribute (that had not been used yet on a path from the root to the node being considered). When the decision tree reached a depth of 18, all the remaining growing nodes were replaced with leaves. Each leaf was randomly assigned a class. The size of the resulting concepts ranged from 2.2k leaves to 61k leaves with a median of 12.6k. A stream of training examples was then 3VFDT occasionally grew slightly beyond 40MB because the limit was only enforced on heap-allocated memory. C4.5 always exceeded 40MB by the size of the unpruned tree. generated by sampling uniformly from the instance space, and assigning classes according to the target tree. We added various levels of class and attribute noise to the training examples, from 0 to 30%.4 (A noise level of n% means that each class/attribute value has a probability of n% of being reassigned at random, with equal probability for all values, including the original one.) In each run, 50k separate examples were used for testing. C4.5 was run with all default settings. We ran our experiments on two Pentium 6/200 MHz, one Pentium II/400 MHz, and one Pentium III/500 MHz machine, all running Linux. Figure 1 shows the accuracy of the learners averaged over all the runs. VFDT was run with  = 107,  = 5%, nmin = 200, no leaf reactivation, and no rescans. VFDTboot is VFDT bootstrapped with an over-pruned version of the tree produced by C4.5. C4.5 is more accurate than VFDT up to 25k examples, and the accuracies of the two systems are similar in the range from 25k to 100k examples (at which point C4.5 is unable to consider further examples). Most significantly, VFDT is able to take advantage of the examples after 100k to greatly improve accuracy (88.7% for VFDT and 88.8% for VFDT-boot, vs. 76.5% for C4.5). C4.5s early advantage comes from the fact it reuses examples to make decisions on multiple levels of the tree it is inducing, while VFDT uses each example only once. As expected, VFDT-boots initialization lets it achieve high accuracy more quickly than without it. However, VFDT-boots performance is surprising in that its accuracy is much higher than C4.5s at 100k examples, when VFDT-boot has not seen any examples that C4.5 did not. An explanation for this is that many of the experiments reported in Figure 1 contained noise, and, as Catlett [2] showed, over-pruning can be very effective at reducing overfitting in noisy domains. Figure 2 shows the average number of nodes in the trees induced by each of the learners. Notice that VFDT and VFDT-boot induce trees with similar numbers of nodes, and that both achieve greater accuracy with far fewer nodes than C4.5. This suggests that using VFDT can substantially increase the comprehensibility of the trees induced relative to C4.5. It also suggests that VFDT is less prone than C4.5 to overfitting noisy data. Figure 3 shows how the algorithms respond to noise. It compares four runs on the same concept (with 12.6k leaves), but with increasing levels of noise added to the training examples. C4.5s accuracy reports are for training sets with 100k examples, and VFDT and VFDT-boots are for training sets of 20 million examples. VFDTs advantage compared to C4.5 increases with the noise level. This is further evidence that use of the Hoeffding bound is an effective pruning method. 4The exact concepts used were, in the form (f , noise level, #nodes, #leaves): (0.15, 0.10, 74449, 37225), (0.15, 0.10, 13389, 6695), (0.17, 0.10, 78891, 39446), (0.17, 0.10, 93391, 46696), (0.25, 0.00, 25209, 12605), (0.25, 0.20, 25209, 12605), (0.25, 0.30, 25209, 12605), (0.25, 0.00, 15917, 7959), (0.25, 0.10, 31223, 15612), (0.25, 0.15, 16781, 8391), (0.25, 0.20, 4483, 2242), (0.28, 0.10, 122391, 61196), (0.28, 0.10, 6611, 3306), (0.25, 0.10, 25209, 12605). The last set of parameters was also used as the basis for the lesion studies reported below. Figure 4 shows how the algorithms compare on six concepts of varying size.5 All the training sets had 10% noise. As before, C4.5s results are for learning on 100k examples, while VFDT and VFDT-boots are for 20 million. Both versions of VFDT do better than C4.5 on every concept size considered. However, contrary to what we would expect, as concept size increases the relative benefit seems to remain approximately constant for VFDT and VFDT-boot. Looking deeper, we find that with 20 million examples VFDT and VFDT-boot induce trees with approximately 9k nodes regardless of the size of the underlying concept. This suggests that they would take good advantage of even more training examples. We carried out all runs without ever writing VFDTs training examples to disk (i.e., generating them on the fly and passing them directly to VFDT). For time comparison purposes, however, we measured the time it takes VFDT to read examples from the (0.25, 0.10, 25209, 12605) data set from disk on the Pentium III/500 MHz machine. VFDT takes 5752 seconds to read the 20 million examples, and 625 seconds to process them. In other words, learning time is about an order of magnitude less than input time. On the same runs, C4.5 takes 36 seconds to read and process 100k examples, and VFDT takes 47 seconds. Finally, we generated 160 million examples from the (0.25, 0.10, 25209, 12605) concept. Figure 5 compares VFDT and C4.5 on this data set. VFDT makes progress over the entire data set, but begins to asymptote after 10 million examples; the final 150 million examples contribute 0.58% to accuracy. VFDT took 9501 seconds to process the examples (excluding I/O) and induced 21.9k leaves. In the near future we plan to carry out similar runs with more complex concepts and billions of examples.\nWe conducted a series of lesion studies to evaluate the effectiveness of some of the components and parameters of the VFDT system. Figure 6 shows the accuracy of the learners on the (0.25, 0.00, 25209, 12605) data set. It also shows a slight modification to the VFDT-boot algorithm, where the tree produced by C4.5 is used without first over-pruning it. All versions of VFDT were run with  = 107,  = 5%, nmin = 200, no leaf reactivation, and no rescans. C4.5 does better without noise than with it, but VFDT is still able to use additional data to significantly improve accuracy. VFDT-boot with the no over-prune setting is initially better than the over-pruning version, but does not make much progress and is eventually overtaken. We hypothesize that this is because it has difficulty overcoming the poor low-confidence decisions C4.5 made near its leaves. In the remainder of the lesion studies VFDT was run on the (0.25, 0.10, 25209, 12605) data set with  = 107,  = 5%, nmin = 200, no leaf reactivation, and no rescans. We evaluated the effect of disabling ties, so that VFDT does not make any splits until it is able to identify a clear winner. 5The concept (0.15, 0.10, 74449, 37225) turned out to be atypically easy, and is not included in the graph to avoid obscuring the trend. The observed accuracies for this concept were: C4.5  83.1%; VFDT  89.0%; VFDT-boot  89.7%. We conducted two runs, holding all parameters constant except that the second run never split with a tie. Without ties VFDT induced a tree with only 65 nodes and 72.9% accuracy, compared to 8k nodes and 86.9% accuracy with ties. VFDT-boot without ties produced 805 nodes and 83.3% accuracy, compared to 8k nodes and 88.5% accuracy with ties. We also carried out two runs holding all parameters constant except nmin, the number of new examples that must be seen at a node before Gs are recomputed. The first run recomputed G every 200 examples (nmin = 200), and the second did it for every example (nmin = 1). Doing the G computations for every example, VFDT gained 1.1% accuracy and took 3.8 times longer to run. VFDT-boot lost 0.9% accuracy and took 3.7 times longer. Both learners induced about 5% more nodes with the more frequent G computations. We then carried out two runs holding all parameters but VFDTs memory limit constant. The first run was allowed 40 MB of memory; the second was allowed 80 MB. VFDT and VFDT-boot both induced 7.8k more nodes with the additional memory, which improved VFDTs accuracy by 3.0% and VFDT-boots by 3.2%. Finally, we carried out two runs holding all parameters but  constant. The first run had a delta of 102, and the second had a delta of 107. With the lower , VFDT and VFDT-boot both induced about 30% fewer nodes than with the higher one. VFDTs accuracy was 2.3% higher and VFDT-boots accuracy was 1.0% higher with the lower .\nWe are currently applying VFDT to mining the stream of Web page requests emanating from the whole University of Washington main campus. The nature of the data is described in detail in [23]. In our experiments so far we have used a one-week anonymized trace of all the external web accesses made from the university campus. There were 23,000 active clients during this one-week trace period, and the entire university population is estimated at 50,000 people (students, faculty and staff). The trace contains 82.8 million requests, which arrive at a peak rate of 17,400 per minute. The size of the compressed trace file is about 20 GB.6 Each request is tagged with an anonymized organization ID that associates the request with one of the 170 organizations (colleges, departments, etc.) within the university. One purpose this data can be used for is to improve Web caching. The key to this is predicting as accurately as possible which hosts and pages will be requested in the near future, given recent requests. We applied decisiontree learning to this problem in the following manner. We split the campus-wide request log into a series of equal time slices T0, T1, . . . , Tt, . . . ; in the experiments we report, each time slice is an hour. For each organization O1, O2, . . . , Oi, . . . , O170 and each of the 244k hosts appearing in the logs H1, . . . , Hj , . . . , H244k, we maintain a count of how many times the organization accessed the host in the time slice, Cijt. We discretize these counts into four buckets, representing no requests, 1  12 requests, 13  25 requests and 26 or more requests. Then for each time slice and host accessed in that time slice (Tt, Hj) we generate an example with attributes t mod 24, C1,jt, . . . , Cijt, . . . C170,jt 6This log is from May 1999. Traffic in May 2000 was double this size; a one-week log was approximately 50 GB compressed. and class 1 if Hj is requested in time slice Tt+1 and 0 if it is not. This can be carried out in real time using modest resources by keeping statistics on the last and current time slices Ct1 and Ct in memory, only keeping counts for hosts that actually appear in a time slice (we never needed more than 30k counts), and outputting the examples for Ct1 as soon as Ct is complete. Using this procedure we obtained a dataset containing 1.89 million examples, 61.1% of which were labeled with the most common class (that the host did not appear again in the next time slice). Testing was carried out on the examples from the last day (276,230 examples). VFDT was run with  = 107,  = 5%, and nmin = 200. All runs were carried out on a 400 MHz Pentium machine. A decision stump (a decision tree with only one node) obtains 64.2% accuracy on this data. The decision stump took 1277 seconds to learn, and VFDT took 1450 seconds to do one pass over the training data (after being initialized with C4.5s over-pruned tree). The majority of this time (983 seconds) was spent reading data from disk. The bootstrap run of C4.5 took 2975 seconds to learn on a subsample of 74.5k examples (as many as would fit in 40 MB of RAM) and achieved 73.3% accuracy. Thus VFDT learned faster on 1.61 million examples than C4.5 did on 75k. We also used a machine with 1 GB of RAM to run C4.5 on the entire 1.61 million training examples; the run took 24 hours and the resulting tree was 75% accurate. Figure 7 shows VFDT-boots performance on this dataset, using 1 GB of RAM. We extended VFDTs run out to 4 million examples by rescanning. The x axis shows the number of examples presented to VFDT after the C4.5 bootstrap phase was complete. Accuracy improves steadily as more examples are seen. VFDT is able to achieve accuracy similar to C4.5s in a small fraction of the time. Further, C4.5s memory requirements and batch nature will not allow it to scale to traces much larger than a week, while VFDT can easily incorporate data indefinitely. The next step is to apply VFDT to predicting page requests from a given host. We also plan to address issues related to time-changing behavior and then set VFDT running permanently, learning and relearning as dictated by the data stream.\nPrevious work on mining large databases using subsampling methods includes the following. Catlett [2] proposed several heuristic methods for extending RAM-based batch decisiontree learners to datasets with up to hundreds of thousands of examples. Musick, Catlett and Russell [13] proposed and tested (but did not implement in a learner) a theoretical model for choosing the size of subsamples to use in comparing attributes. Maron and Moore [9] used Hoeffding bounds to speed selection of instance-based regression models via cross-validation (see also [12]). Gratchs Sequential ID3 [6] used a statistical method to minimize the number of examples needed to choose each split in a decision tree. (Sequential ID3s guarantees of similarity to the batch tree were much looser than those derived here for Hoeffding trees, and it was only tested on repeatedly sampled small datasets.) Gehrke et al.s BOAT [5] learned an approximate tree using a fixed-size subsample, and then refined it by scanning the full database. Provost et al. [14] studied different strategies for mining larger and larger subsamples until accuracy (apparently) asymptotes. In contrast to systems that learn in main memory by subsampling, systems like SLIQ [10] and SPRINT [17] use all the data, and concentrate on optimizing access to disk by always reading examples (more precisely, attribute lists) sequentially. VFDT combines the best of both worlds, accessing data sequentially and using subsampling to potentially require much less than one scan, as opposed to many. This allows it to scale to larger databases than either method alone. VFDT has the additional advantages of being incremental and anytime: new examples can be quickly incorporated as they arrive, and a usable model is available after the first few examples and then progressively refined. As mentioned previously, there is a large literature on incremental learning, which space limitations preclude reviewing here. The system most closely related to ours is Utgoffs [20] ID5R (extended in [21]). ID5R learns the same tree as ID3 (a batch method), by restructuring subtrees as needed. While its learning time is linear in the number of examples, it is worst-case exponential in the number of attributes. On the simple, noise-free problems it was tested on, it was much slower than ID3; noise would presumably aggravate this. Thus ID5R does not appear viable for learning from high-speed data streams. A number of efficient incremental or single-pass algorithms for KDD tasks other than supervised learning have appeared in recent years (e.g., clustering [4] and association rule mining [19]). A substantial theoretical literature on online algorithms exists (e.g., [8]), but it focuses on weak learners (e.g., linear separators), because little can be proved about strong ones like decision trees.\nWe plan to shortly compare VFDT with SPRINT/SLIQ. VFDT may outperform these even in fully disk-resident datasets, because it can learn in less than one scan while the latter require multiple scans, and the dominant component of their cost is often the time required to read examples from disk multiple times. VFDTs speed and anytime character make it ideal for interactive data mining; we plan to also study its application in this context (see [18]). Other directions for future work include: further developing the application of VFDT to Web log data; studying other applications of VFDT (e.g., intrusion detection); using nondiscretized numeric attributes in VFDT; studying the use of post-pruning in VFDT; further optimizing VFDTs computations (e.g., by recomputing Gs exactly when we can tell that the current example may cause the Hoeffding bound to be reached); using adaptive s; studying the use of an example cache in main memory to speed induction by reusing examples at multiple levels; comparing VFDT to ID5R and other incremental algorithms; adapting VFDT to learn evolving concepts in time-changing domains; adapting VFDT to learning with imbalanced classes and asymmetric misclassification costs; adapting VFDT to the extreme case where even the final decision tree (without any stored sufficient statistics) does not fit in main memory; parallelizing VFDT; applying the ideas described here to other types of learning (e.g., rule induction, clustering); etc.\nThis paper introduced Hoeffding trees, a method for learning online from the high-volume data streams that are increasingly common. Hoeffding trees allow learning in very small constant time per example, and have strong guarantees of high asymptotic similarity to the corresponding batch trees. VFDT is a high-performance data mining system based on Hoeffding trees. Empirical studies show its effectiveness in taking advantage of massive numbers of examples. VFDTs application to a high-speed stream of Web log data is under way.\nThis research was partly funded by an NSF CAREER award to the first author.\n[1] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984. [2] J. Catlett. Megainduction: Machine Learning on Very Large Databases. PhD thesis, Basser Department of Computer Science, University of Sydney, Sydney, Australia, 1991. [3] T. G. Dietterich. Overfitting and undercomputing in machine learning. Computing Surveys, 27:326327, 1995. [4] M. Ester, H.-P. Kriegel, J. Sander, M. Wimmer, and X. Xu. Incremental clustering for mining in a data warehousing environment. In Proceedings of the Twenty-Fourth International Conference on Very Large Data Bases, pages 323333, New York, NY, 1998. Morgan Kaufmann. [5] J. Gehrke, V. Ganti, R. Ramakrishnan, and W.-L. Loh. BOAT: optimistic decision tree construction. In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data, pages 169180, Philadelphia, PA, 1999. ACM Press. [6] J. Gratch. Sequential inductive learning. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 779786, Portland, OR, 1996. AAAI Press. [7] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:1330, 1963. [8] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285318, 1997. [9] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classification and function approximation. In J. D. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing Systems 6. Morgan Kaufmann, San Mateo, CA, 1994. [10] M. Mehta, A. Agrawal, and J. Rissanen. SLIQ: A fast scalable classifier for data mining. In Proceedings of the Fifth International Conference on Extending Database Technology, pages 1832, Avignon, France, 1996. Springer. [11] R. G. Miller, Jr. Simultaneous Statistical Inference. Springer, New York, NY, 2nd edition, 1981. [12] A. W. Moore and M. S. Lee. Efficient algorithms for minimizing cross validation error. In Proceedings of the Eleventh International Conference on Machine Learning, pages 190198, New Brunswick, NJ, 1994. Morgan Kaufmann. [13] R. Musick, J. Catlett, and S. Russell. Decision theoretic subsampling for induction on large databases. In Proceedings of the Tenth International Conference on Machine Learning, pages 212219, Amherst, MA, 1993. Morgan Kaufmann. [14] F. Provost, D. Jensen, and T. Oates. Efficient progressive sampling. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2332, San Diego, CA, 1999. ACM Press. [15] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA, 1993. [16] J. R. Quinlan and R. M. Cameron-Jones. Oversearching and layered search in empirical learning. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 10191024, Montreal, Canada, 1995. Morgan Kaufmann. [17] J. C. Shafer, R. Agrawal, and M. Mehta. SPRINT: A scalable parallel classifier for data mining. In Proceedings of the Twenty-Second International Conference on Very Large Databases, pages 544555, Mumbai, India, 1996. Morgan Kaufmann. [18] P. Smyth and D. Wolpert. Anytime exploratory data analysis for massive data sets. In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, pages 5460, Newport Beach, CA, 1997. AAAI Press. [19] H. Toivonen. Sampling large databases for association rules. In Proceedings of the Twenty-Second International Conference on Very Large Data Bases, pages 134145, Mumbai, India, 1996. Morgan Kaufmann. [20] P. E. Utgoff. Incremental induction of decision trees. Machine Learning, 4:161186, 1989. [21] P. E. Utgoff. An improved algorithm for incremental induction of decision trees. In Proceedings of the Eleventh International Conference on Machine Learning, pages 318325, New Brunswick, NJ, 1994. Morgan Kaufmann. [22] G. I. Webb. OPUS: An efficient admissible algorithm for unordered search. Journal of Artificial Intelligence Research, 3:431465, 1995. [23] A. Wolman, G. Voelker, N. Sharma, N. Cardwell, M. Brown, T. Landray, D. Pinnel, A. Karlin, and H. Levy. Organization-based analysis of Web-object sharing and caching. In Proceedings of the Second USENIX Conference on Internet Technologies and Systems, pages 2536, Boulder, CO, 1999.\n", "prediction": "this paper presents a decision tree algorithm that is based on the idea of learning a sequence of trees from a given set of examples.<n> we describe the algorithm, analyze its performance and demonstrate its utility through experiments on synthetic and real data ; we also describe and evaluate an extension of this algorithm for data mining : the continuous stream of access to the whole of a university s main campus is a natural feature of modern data - mining applications ( e.g., finding objects in a database ) and this extension is necessary to overcome the limitations of traditional machine learning and statistics for finding the attributes of data in an efficient manner ( in the limit that the size of the data stream grows exponentially with the distance from the node in which the example is stored and the number of times an attribute is used to find its attributes grows as a power - law with an exponent equal to @xmath0 ( the probability of an example to be found at a node that has the same attributes as it was observed at the previous time step is proportional to this probability ). <n> [ [ section ] ][ [ theorem]lemma [ thm]corollary [ + [ definition]assumption [ -]definitions + the problem of finding an optimal decision trees ( dps)s is of interest in many fields of applications _ in particular in data and information mining_. in this work we present a new algorithm to determine the optimal solution for the classification of classes of two classes ( i = l = ( ii  l - l= ( l lm = @l = ij(l ( @ l_<n> l(i(b ln l(e t ( b = b  l@l @(f(m(n(d = i(h = p(p = a l.(g(@ l is l) @(l. l ( p. @ @e = g = n((s l @ p @ ( n = 2([l (v @ b( l with l to l + l=1 @  @ = = d = j = r = 1 =1 ( g @ i ( ( d @@ b with p = * l for l * @ * * p with @ j  ( j @ x = 0 @ g with (( b @ a @ d  with a ( 2 = m = [ l, p p ( x(t = x @. p) with d with b. *! l b to p is @ r   to b is p@ @ with r. ( r with x ( * g to r @ + @ that = the @ m @)  for p to  b@ p * = to x with n with  in l and p - p i @ n. b ( = y = c = e = that @ is to g is the. d(x = + ( [ p  is in @ which is.) to which to d. to a to ( a. with its @ the ([n = for @t @ whose l will be the first ( that will ( with all to m.. is ( to [ @[d ( + p and @_ l-(r levi  of @... ( which will.  that was to its. for all of p in (... @ and is for a) is with which with [ x. x to all  until the is obtained with any of all @ [ b and l from @m with that to n to e is that ( c. g. [ ( is ]). and to + to * to that"}
{"ground_truth": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.\nDue to advantages such as computational speed, precise manipulation, and exact timing, computers and robots are often superior to humans at performing tasks with well-defined goals and objectives. However, it can be difficult, even for experts, to design reward functions and objectives that lead to desired behaviors when designing autonomous agents (Ng et al., 1999; Amodei et al., 2016). When goals or rewards are difficult for a human to specify, inverse reinforcement learn- *Equal contribution 1Department of Computer Science, University of Texas at Austin, USA 2Preferred Networks, Japan. Correspondence to: Daniel S. Brown <dsbrown@cs.utexas.edu>, Wonjoon Goo <wonjoon@cs.utexas.edu>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). Figure 1. T-REX takes a sequence of ranked demonstrations and learns a reward function from these rankings that allows policy improvement over the demonstrator via reinforcement learning. ing (IRL) (Abbeel & Ng, 2004) techniques can be applied to infer the intrinsic reward function of a user from demonstrations. Unfortunately, high-quality demonstrations are difficult to provide for many tasksfor instance, consider a non-expert user attempting to give kinesthetic demonstrations of a household chore to a robot. Even for relative experts, tasks such as high-frequency stock trading or playing complex video games can be difficult to perform optimally. If a demonstrator is suboptimal, but their intentions can be ascertained, then a learning agent ought to be able to exceed the demonstrators performance in principle. However, existing IRL algorithms fail to do this, typically searching for a reward function that makes the demonstrations appear near-optimal (Ramachandran & Amir, 2007; Ziebart et al., 2008; Finn et al., 2016; Henderson et al., 2018). Thus, when the demonstrator is suboptimal, IRL results in suboptimal behavior as well. Imitation learning approaches (Argall et al., 2009) that mimic behavior directly without reward inference, such as behavioral cloning (Torabi et al., 2018a), also suffer from the same shortcoming. To overcome this critical flaw in current imitation learning methods, we propose a novel IRL algorithm, Trajectoryranked Reward EXtrapolation (T-REX)1 that utilizes ranked demonstrations to extrapolate a users underlying intent be- 1Code available at ICML2019-TREX ar X iv :1 90 4. 06 38 7v 5 [ cs .L G ] 9 J ul 2 01 9 yond the best demonstration, even when all demonstrations are highly suboptimal. This, in turn, enables a reinforcement learning agent to exceed the performance of the demonstrator by learning to optimize this extrapolated reward function. Specifically, we use ranked demonstrations to learn a statebased reward function that assigns greater total return to higher-ranked trajectories. Thus, while standard inverse reinforcement learning approaches seek a reward function that justifies the demonstrations, we instead seek a reward function that explains the ranking over demonstrations, allowing for potentially better-than-demonstrator performance. Utilizing ranking in this way has several advantages. First, rather than imitating suboptimal demonstrations, it allows us to identify features that are correlated with rankings, in a manner that can be extrapolated beyond the demonstrations. Although the learned reward function could potentially overfit to the provided rankings, we demonstrate empirically that it extrapolates well, successfully predicting returns of trajectories that are significantly better than any observed demonstration, likely due to the powerful regularizing effect of having many pairwise ranking constraints between trajectories. For example, the degenerate all-zero reward function (the agent always receives a reward of 0) makes any given set of demonstrations appear optimal. However, such a reward function is eliminated from consideration by any pair of (non-equally) ranked demonstrations. Second, when learning features directly from high-dimensional data, this regularizing effect can also help to prevent overfitting to the small fraction of state space visited by the demonstrator. By utilizing a set of suboptimal, but ranked demonstrations, we provide the neural network with diverse data from multiple areas of the state space, allowing an agent to better learn both what to do and what not to do in a variety of situations. We evaluate T-REX on a variety of standard Atari and MuJoCo benchmark tasks. Our experiments show that T-REX can extrapolate well, achieving performance that is often more than twice as high as the best-performing demonstration, as well as outperforming state-of-the-art imitation learning algorithms. We also show that T-REX performs well even in the presence of significant ranking noise, and provide results showing that T-REX can learn good policies simply by observing a novice demonstrator that noisily improves over time.\nThe goal of our work is to achieve improvements over a suboptimal demonstrator in high-dimensional reinforcement learning tasks without requiring a hand-specified reward function or supervision during policy learning. While there is a large body of research on learning from demonstrations (Argall et al., 2009; Gao et al., 2012; Osa et al., 2018; Arora & Doshi, 2018), most work assumes access to action labels, while we learn only from observations. Additionally, little work has addressed the problem of learning from ranked demonstrations, especially when they are significantly suboptimal. To the best of our knowledge, our work is the first to show better-than-demonstrator performance in highdimensional tasks such as Atari, without requiring active human supervision or access to ground-truth rewards.\nEarly work on learning from demonstration focused on behavioral cloning (Pomerleau, 1991), in which the goal is to learn a policy that imitates the actions taken by the demonstrator; however, without substantial human feedback and correction, this method is known to have large generalization error (Ross et al., 2011). Recent deep learning approaches to imitation learning (Ho & Ermon, 2016) have used Generative Adversarial Networks (Goodfellow et al., 2014) to model the distribution of actions taken by the demonstrator. Rather than directly learn to mimic the demonstrator, inverse reinforcement learning (IRL) (Gao et al., 2012; Arora & Doshi, 2018) seeks to find a reward function that models the intention of the demonstrator, thereby allowing generalization to states that were unvisited during demonstration. Given such a reward function, reinforcement learning (Sutton & Barto, 1998) techniques can be applied to learn an optimal policy. Maximum entropy IRL seeks to find a reward function that makes the demonstrations appear near-optimal, while further disambiguating inference by also maximizing the entropy of the resulting policy (Ziebart et al., 2008; Boularias et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016). While maximum entropy approaches are robust to limited and occasional suboptimality in the demonstrations, they still fundamentally seek a reward function that justifies the demonstrations, resulting in performance that is explicitly tied to the performance of the demonstrator. Syed & Schapire (2008) proved that, given prior knowledge about which features contribute positively or negatively to the true reward, an apprenticeship policy can be found that is guaranteed to outperform the demonstrator. However, their approach requires hand-crafted, linear features, knowledge of the true signs of the rewards features, and also requires repeatedly solving a Markov decision process (MDP). Our proposed method uses deep learning and ranked demonstrations to automatically learn complex features that are positively and negatively correlated with performance, and is able to generate a policy that can outperform the demonstrator via the solution to a single RL problem. Our work can be seen as a form of preference-based policy learning (Akrour et al., 2011) and preference-based IRL (PBIRL) (Wirth et al., 2016; Sugiyama et al., 2012) which both seek to optimize a policy based on preference rankings over demonstrations. However, existing approaches only consider reward functions that are linear in hand-crafted features and have not studied extrapolation capabilities. For a more complete overview survey of preference-based reinforcement learning, see the survey by Wirth et al. (2017). Other methods (Burchfiel et al., 2016; El Asri et al., 2016) have proposed the use of quantitatively scored trajectories as opposed to qualitative pairwise preferences over demonstrations. However, none of the aforementioned methods have been applied to the types of high-dimensional deep inverse reinforcement learning tasks considered in this paper.\nRecently there has been a shift towards learning from observations, in which the actions taken by the demonstrator are unknown. Torabi et al. (2018a) propose a state-of-the-art model-based approach to perform behavioral cloning from observation. Sermanet et al. (2018) and Liu et al. (2018) propose methods to learn directly from a large corpus of videos containing multiple view points of the same task. Yu et al. (2018) and Goo & Niekum (2019) propose metalearning-from-observation approaches that can learn from a single demonstration, but require training on a wide variety of similar tasks. Henderson et al. (2018) and Torabi et al. (2018b) extend Generative Adversarial Imitation Learning (Ho & Ermon, 2016) to remove the need for action labels. However, inverse reinforcement learning methods based on Generative Adversarial Networks (Goodfellow et al., 2014) are notoriously difficult to train and have been shown to fail to scale to high-dimensional imitation learning tasks such as Atari (Tucker et al., 2018).\nVery little work has tried to learn good policies from highly suboptimal demonstrations. Grollman & Billard (2011) propose a method that learns from failed demonstrations where a human attempts, but is unable, to perform a task; however, demonstrations must be labeled as failures and manually clustered into two sets of demonstrations: those that overshoot and those that undershoot the goal. Shiarlis et al. (2016) demonstrate that if successful and failed demonstrations are labeled and the reward function is a linear combination of known features, then maximum entropy IRL can be used to optimize a policy to match the expected feature counts of successful demonstrations while not matching the feature counts of failed demonstrations. Zheng et al. (2014) and Choi et al. (2019) propose methods that are robust to small numbers of unlabeled suboptimal demonstrations, but require a majority of expert demonstrations in order to correctly identify which demonstrations are anomalous. In reinforcement learning, it is common to initialize a policy from suboptimal demonstrations and then improve this policy using the ground truth reward signal (Kober & Peters, 2009; Taylor et al., 2011; Hester et al., 2017; Gao et al., 2018). However, it is often still difficult to perform significantly better than the demonstrator (Hester et al., 2017) and designing reward functions for reinforcement learning can be extremely difficult for non-experts and can easily lead to unintended behaviors (Ng et al., 1999; Amodei et al., 2016).\nMost deep learning-based methods for reward learning require access to demonstrator actions and do not scale to high-dimensional tasks such as video games (e.g. Atari) (Ho & Ermon, 2016; Finn et al., 2016; Fu et al., 2017; Qureshi & Yip, 2018). Tucker et al. (2018) tested state-of-the-art IRL methods on the Atari domain and showed that they are unsuccessful, even with near-optimal demonstrations and extensive parameter tuning. Our work builds on the work of Christiano et al. (2017), who proposed an algorithm that learns to play Atari games via pairwise preferences over trajectories that are actively collected during policy learning. However, this approach requires obtaining thousands of labels through constant human supervision during policy learning. In contrast, our method only requires an initial set of (approximately) ranked demonstrations as input and can learn a better-than-demonstrator policy without any supervision during policy learning. Ibarz et al. (2018) combine deep Q-learning from demonstrations (DQfD) (Hester et al., 2017) and active preference learning (Christiano et al., 2017) to learn to play Atari games using both demonstrations and active queries. However, Ibarz et al. (2018) require access to the demonstrators actions in order to optimize an action-based, large-margin loss (Hester et al., 2017) and to optimize the state-action Q-value function using (s, a, s)-tuples from the demonstrations. Additionally, the large-margin loss encourages Q-values that make the demonstrators actions better than alternative actions, resulting in performance that is often significantly worse than the demonstrator despite using thousands of active queries during policy learning. Aytar et al. (2018) use video demonstrations of experts to learn good policies for the Atari domains of Montezumas Revenge, Pitfall, and Private Eye. Their method first learns a state-embedding and then selects a set of checkpoints from a demonstration. During policy learning, the agent is rewarded only when it reaches these checkpoints. This approach relies on high-performance demonstrations, which their method is unable to outperform. Furthermore, while Aytar et al. (2018) do learn a reward function purely from observations, their method is inherently different from ours in that their learned reward function is designed to only imitate the demonstrations, rather than extrapolate beyond the capabilities of the demonstrator. To the best of our knowledge, our work is the first to sig- nificantly outperform a demonstrator without using ground truth rewards or active preference queries. Furthermore, our approach does not require demonstrator actions and is able to learn a reward function that matches the demonstrators intention without any environmental interactionsgiven rankings, reward learning becomes a binary classification problem and does not require access to an MDP.\nWe model the environment as a Markov decision process (MDP) consisting of a set of states S, actions A, transition probabilities P , reward function r : S  R, and discount factor  (Puterman, 2014). A policy  is a mapping from states to probabilities over actions, (a|s)  [0, 1]. Given a policy and an MDP, the expected discounted return of the policy is given by J() = E[  t=0  trt|]. In this work we are concerned with the problem of inverse reinforcement learning from observation, where we do not have access to the reward function of the MDP nor the actions taken by the demonstrator. Given a sequence of m ranked trajectories t for t = 1, . . . ,m, where i  j if i < j, we wish to find a parameterized reward function r that approximates the true reward function r that the demonstrator is attempting to optimize. Given r, we then seek to optimize a policy  that can outperform the demonstrations. We only assume access to a qualitative ranking over demonstrations. Thus, we only require the demonstrator to have an internal goal or intrinsic reward. The demonstrator can rank trajectories using any method, such as giving pairwise preferences over demonstrations or by rating each demonstration on a scale. Note that even if the relative scores of the demonstrations are used for ranking, it is still necessary to infer why some trajectories are better than others, which is what our proposed method does.\nWe now describe Trajectory-ranked Reward EXtrapolation (T-REX), an algorithm for using ranked suboptimal demonstrations to extrapolate a users underlying intent beyond the best demonstration. Given a sequence of m demonstrations ranked from worst to best, 1, . . . , m, T-REX has two steps: (1) reward inference and (2) policy optimization. Given the ranked demonstrations, T-REX performs reward inference by approximating the reward at state s using a neural network, r(s), such that  si r(s) <  sj r(s) when i  j . The parameterized reward function r can be trained with ranked demonstrations using the generalized loss function: L() = Ei,j [  ( P ( J(i) < J(j) ) , i  j )] , (1) where  is a distribution over demonstrations,  is a binary classification loss function, J is a (discounted) return defined by a parameterized reward function r, and  is an indication of the preference between the demonstrated trajectories. We represent the probability P as a softmax-normalized distribution and we instantiate  using a cross entropy loss: P ( J(i) < J(j) )  exp  sj r(s) exp  si r(s) + exp  sj r(s) , (2) L() =   ij log exp  sj r(s) exp  si r(s) + exp  sj r(s) . (3) This loss function trains a classifier that can predict whether one trajectory is preferable to another based on the predicted returns of each trajectory. This form of loss function follows from the classic Bradley-Terry and Luce-Shephard models of preferences (Bradley & Terry, 1952; Luce, 2012) and has been shown to be effective for training neural networks from preferences (Christiano et al., 2017; Ibarz et al., 2018). To increase the number of training examples, T-REX trains on partial trajectory pairs rather than full trajectory pairs. This results in noisy preference labels that are only weakly supervised; however, using data augmentation to obtain pairwise preferences over many partial trajectories allows T-REX to learn expressive neural network reward functions from only a small number of ranked demonstrations. During training we randomly select pairs of trajectories, i and j . We then randomly select partial trajectories i and j of length L. For each partial trajectory, we take each observation and perform a forward pass through the network r and sum the predicted rewards to compute the cumulative return. We then use the predicted cumulative returns as the logit values in the cross-entropy loss with the label corresponding to the higher ranked demonstration. Given the learned reward function r(s), T-REX then seeks to optimize a policy  with better-than-demonstrator performance through reinforcement learning using r.\n\nWe first evaluated our proposed method on three robotic locomotion tasks using the Mujoco simulator (Todorov et al., 2012) within OpenAI Gym (Brockman et al., 2016), namely HalfCheetah, Hopper, and Ant. In all three tasks, the goal of the robot agent is to move forward as fast as possible without falling to the ground.\nTo generate demonstrations, we trained a Proximal Policy Optimization (PPO) (Schulman et al., 2017) agent with the ground-truth reward for 500 training steps (64,000 simulation steps) and checkpointed its policy after every 5 training steps. For each checkpoint, we generated a trajectory of length 1,000. This provides us with different demonstrations of varying quality which are then ranked based on the ground truth returns. To evaluate the effect of different levels of suboptimality, we divided the trajectories into different overlapping stages. We used 3 stages for HalfCheetah and Hopper. For HalfCheetah, we used the worst 9, 12, and 24 trajectories, respectively. For Hopper, we used the worst 9, 12, and 18 trajectories. For Ant, we used two stages consisting of the worst 12 and 40 trajectories. We used the PPO implementation from OpenAI Baselines (Dhariwal et al., 2017) with the given default hyperparameters.\nWe trained the reward network using 5,000 random pairs of partial trajectories of length 50, with preference labels based on the trajectory rankings, not the ground-truth returns. To prevent overfitting, we represented the reward function using an ensemble of five deep neural networks, trained separately with different random pairs. Each network has 3 fully connected layers of 256 units with ReLU nonlinearities. We train the reward network using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1e-4 and a minibatch size of 64 for 10,000 timesteps. To evaluate the quality of our learned reward, we then trained a policy to maximize the inferred reward function via PPO. The outputs of each the five reward networks in our ensemble, r(s), are normalized by their standard deviation to compensate for any scale differences amongst the models. The reinforcement learning agent receives the average of the ensemble as the reward, plus the control penalty used in OpenAI Gym (Brockman et al., 2016). This control penalty represents a standard safety prior over reward functions for robotics tasks, namely to minimize joint torques. We found that optimizing a policy based solely on this control penalty does not lead to forward locomotion, thus learning a reward function from demonstrations is still necessary.\nLearned Policy Performance We measured the performance of the policy learned by T-REX by measuring the forward distance traveled. We also compared against Behavior Cloning from Observations (BCO) (Torabi et al., 2018a), a state-of-the-art learning-from-observation method, and Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016), a state-of-the-art inverse reinforcement learning algorithm. BCO trains a policy via supervised learning, and has been shown to be competitive with state-of-the-art IRL (Ho & Ermon, 2016) on MuJoCo tasks without requiring action labels, making it one of the strongest baselines when learning from observations. We trained BCO using only the best demonstration among the available suboptimal demonstrations. We trained GAIL with all of the demonstrations. GAIL uses demonstrator actions, while T-REX and BCO do not. We compared against three different levels of suboptimality (Stage 1, 2, and 3), corresponding to increasingly better demonstrations. The results are shown in Figure 2 (see the appendix for full details). The policies learned by T-REX perform significantly better than the provided suboptimal trajectories in all the stages of HalfCheetah and Hopper. This provides evidence that T-REX can discover reward functions that extrapolate beyond the performance of the demonstrator. T-REX also outperforms BCO and GAIL on all tasks and stages except for Stage 2 for Hopper and Ant. BCO and GAIL usually fail to perform better than the average demonstration performance because they explicitly seek to imitate the demonstrator rather than infer the demonstrators intention. Reward Extrapolation We next investigated the ability of T-REX to accurately extrapolate beyond the demonstrator. To do so, we compared ground-truth return and T-REXinferred return across trajectories from a range of performance qualities, including trajectories much better than the best demonstration given to T-REX. The extrapolation of the reward function learned by T-REX is shown in Figure 3. The plots in Figure 3 give insight into the performance of T-REX. When T-REX learns a reward function that has a strong positive correlation with the ground-truth reward function, then it is able to surpass the performance of the suboptimal demonstrations. However, in Ant the correlation is not as strong, resulting in worse-than-demonstrator performance in Stage 2.\n\nWe next evaluated T-REX on eight Atari games shown in Table 1. To obtain a variety of suboptimal demonstrations, we generated 12 full-episode trajectories using PPO policies checkpointed every 50 training updates for all games except for Seaquest and Enduro. For Seaquest, we used every 5th training update due to the ability of PPO to quickly find a good policy. For Enduro, we used every 50th training update starting from step 3,100 since PPO obtained 0 return until after 3,000 steps. We used the OpenAI Baselines implementation of PPO with the default hyperparameters.\nWe used an architecture for reward learning similar to the one proposed in (Ibarz et al., 2018), with four convolutional layers with sizes 7x7, 5x5, 3x3, and 3x3, with strides 3, 2, 1, and 1. Each convolutional layer used 16 filters and LeakyReLU non-linearities. We then used a fully connected layer with 64 hidden units and a single scalar output. We fed in stacks of 4 frames with pixel values normalized between 0 and 1 and masked the game score and number of lives. For all games except Enduro, we subsampled 6,000 trajectory pairs between 50 and 100 observations long. We optimized the reward functions using Adam with a learning rate of 5e-5 for 30,000 steps. Given two full trajectories i and j such that i  j , we first randomly sample a subtrajectory from i. Let ti be the starting timestep for this subtrajectory. We then sample an equal length subtrajectory from j such that ti  tj , where tj is the starting time step of the subtrajectory from j . We found that this resulted in better performance than comparing randomly chosen subtrajectories, likely due to the fact that (1) it eliminates pairings that compare a later part of a worse trajectory with an earlier part of a better trajectory and (2) it encourages reward functions that are monotonically increasing as progress is made in the game. For Enduro, training on short partial trajectories was not sufficient to score any points and instead we used 2,000 pairs of down-sampled full trajectories (see appendix for details). We optimized a policy by training a PPO agent on the learned reward function. To reduce reward scaling issues, we normalized predicted rewards by feeding the output of r(s) through a sigmoid function before passing it to PPO. We trained PPO on the learned reward function for 50 million frames to obtain our final policy. We also compare against Behavioral Cloning from Observation (BCO) (Torabi et al., 2018a) and the state-of-the-art Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016). Note that we give action labels to GAIL, but not to BCO or T-REX. We tuned the hyperparameters for GAIL to maximize performance when using expert demonstrations on Breakout and Pong. We gave the same demonstrations to both BCO and T-REX; however, we found that GAIL was very sensitive to poor demonstrations so we trained GAIL on 10 demonstrations using the policy checkpoint that generated the best demonstration given to T-REX.\nLearned Policy Performance The average performance of T-REX under the ground-truth reward function and the best and average performance of the demonstrator are shown in Table 1. Table 1 shows that T-REX outperformed both BCO and GAIL in 7 out of 8 games. T-REX also outperformed the best demonstration in 7 out of 8 games. On four games (Beam Rider, Breakout, Enduro, and Q*bert) T-REX achieved score that is more than double the score of the best demonstration. In comparison, BCO performed worse than the average performance of the demonstrator in all games, and GAIL only performed better than the average demonstration on Space Invaders. Despite using better training data, GAIL was unable to learn good policies on any of the Atari tasks. These results are consistent with those of Tucker et al. (2018) that show that current GAN-based IRL methods do not perform well on Atari. In the appendix, we compare our results against prior work (Ibarz et al., 2018) that uses demonstrations plus active feedback during policy training to learn control policies for the Atari domain. Reward Extrapolation We also examined the extrapolation of the reward function learned using T-REX. Results are shown in Figure 4. We observed accurate extrapolation for Beam Rider, Breakout, Enduro, Seaquest, and Space Invadersfive games where T-REX significantly outperform the demonstrator. The learned rewards for Pong, Q*bert, and Hero show less correlation. On Pong, T-REX overfits to the suboptimal demonstrations and ends up preferring longer games which do not result in a significant win or loss. T-REX is unable to score any points on Hero, likely due to poor extrapolation and the higher complexity of the game. Surprisingly, the learned reward function for Q*bert shows poor extrapolation, yet T-REX is able to outperform the demonstrator. We analyzed the resulting policy for Q*bert and found that PPO learns a repeatable way to score points by inducing Coily to jump off the edge. This behavior was not seen in the demonstrations. In the appendix, we plot the maximum and minimum predicted observations from the trajectories used to create Figure 4 along with attention maps for the learned reward functions.\nThe above results used synthetic demonstrations generated from an RL agent. We also tested T-REX when given ground-truth rankings over human demonstrations. We used novice human demonstrations from the Atari Grand Challenge Dataset (Kurin et al., 2017) for five Atari tasks. TREX was able to significantly outperform the best human demonstration in Q*bert, Space Invaders, and Video Pinball, but was unable to outperform the human in Montezumas Revenge and Ms Pacman (see the appendix for details).\nAll experiments described thus far have had access to ground-truth rankings. To explore the effects of noisy rankings we first examined the stage 1 Hopper task. We synthetically generated ranking noise by starting with a list of trajectories sorted by ground-truth returns and randomly swapping adjacent trajectories. By varying the number of swaps, we were able to generate different noise levels. Given n trajectories in a ranked list provides ( n 2 ) pairwise preferences over trajectories. The noise level is measured as a total order correctness: the fraction of trajectory pairs whose pairwise ranking after random swapping matches the original ground-truth pairwise preferences. The results of this experiment, averaged over 9 runs per noise level, are shown in Figure 5. We found that T-REX is relatively robust to noise of up to around 15% pairwise errors. To examine the effect of noisy human rankings, we used the synthetic PPO demonstrations that were used in the previous Atari experiments and used Amazon Mechanical Turk to collect human rankings. We presented videos of the demonstrations in pairs along with a brief text description of the goal of the game and asked workers to select which demonstration had better performance, with an option for selecting Not Sure. We collected six labels per demonstration pair and used the most-common label as the label for training the reward function. We removed from the training data any pairings where there was a tie for the most-common label or where Not Sure was the most common label. We found that despite this preprocessing step, human labels added a significant amount of noise and resulted in pair-wise rankings with accuracy between 63% and 88% when compared to ground-truth labels. However, despite significant ranking noise, T-REX outperformed the demonstrator on 5 of the 8 Atari games (see the appendix for full details).\nFinally, we tested whether T-REX has the potential to work without explicit rankings. We took the same demonstrations used for the Mujoco tasks, and rather than sorting them based on ground-truth rankings, we used the order in which they were generated by PPO to produce a ranked list of trajectories, ordered by timestamp from earliest to latest. This provides ranked demonstrations without any need for demonstrator labels, and enables us to test whether simply observing an agent learn over time allows us to extrapolate intention by assuming that later trajectories are preferable to trajectories produced earlier in learning. The results for Hopper are shown in Figure 5 and other task results are shown in the appendix. We found that T-REX is able to infer a meaningful reward function even when noisy, time-based rankings are provided. All the trained policies produced comparable results on most stages to the groundtruth rankings, and those policies outperform BCO and GAIL on all tasks and stages except for Ant Stage 2.\nIn this paper, we introduced T-REX, a reward learning technique for high-dimensional tasks that can learn to extrapolate intent from suboptimal ranked demonstrations. To the best of our knowledge, this is the first IRL algorithm that is able to significantly outperform the demonstrator without additional external knowledge (e.g. signs of feature contributions to reward) and that scales to high-dimensional Atari games. When combined with deep reinforcement learning, we showed that this approach achieves better-thandemonstrator performance as well as outperforming stateof-the-art behavioral cloning and IRL methods. We also demonstrated that T-REX is robust to modest amounts of ranking noise, and can learn from automatically generated labels, obtained by watching a learner noisily improve at a task over time.\nThis work has taken place in the Personal AutonomousRobotics Lab (PeARL) at The University of Texas at Austin. PeARL research is supported in part by the NSF (IIS1724157, IIS-1638107, IIS-1617639, IIS-1749204) and ONR(N00014-18-2243).\nCode as well as supplemental videos are available at ICML2019-TREX.\nB.1. Policy performance Table 1 shows the full results for the MuJoCo experiments. The T-REX (time-ordered) row shows the resulting performance of T-REX when demonstrations come from observing a learning agent and are ranked based on timestamps rather than using explicit preference rankings. B.2. Policy visualization We visualized the T-REX-learned policy for HalfCheetah in Figure 1. Visualizing the demonstrations from different stages shows the specific way the policy evolves over time; an agent learns to crawl first and then begins to attempt to walk in an upright position. The T-REX policy learned from the highly suboptimal Stage 1 demonstrations results in a similar-style crawling gait; however, T-REX captures some of the intent behind the demonstration and is able to optimize a gait that resembles the demonstrator but with increased speed, resulting in a better-than-demonstrator policy. Similarly, given demonstrations from Stage 2, which are still highly suboptimal, T-REX learns a policy that resembles the gait of the best demonstration, but is able to optimize and partially stabilize this gait. Finally, given demonstrations from Stage 3, which are still suboptimal, T-REX is able to learn a near-optimal gait.\nTo build the inverse transition models used by BCO (Torabi et al., 2018a) we used 20,000 steps of a random policy to collect transitions with labeled states. We used the Adam optimizer with learning rate 0.0001 and L2 regularization of 0.0001. We used the DQN architecture (Mnih et al., 2015) for the classification network, using the same architecture to predict actions given state transitions as well as predict actions given states. When predicting P (a|st, st+1), we concatenate the state vectors obtaining an 8x84x84 input consisting of two 4x84x84 frames representing st and st+1. We give both T-REX and BCO the full set of demonstrations. We tried to improve the performance of BCO by running behavioral cloning only on the bestX% of the demonstrations, but were unable to find a parameter setting that performed better than X = 100, likely due to a lack of training data when using very few demonstrations.\nWe used the OpenAI Baselines implementation of PPO with default hyperparameters. We ran all of our experiments on an NVIDIA TITAN V GPU. We used 9 parallel workers when running PPO. When learning and predicting rewards, we mask the score and number of lives left for all games. We did this to avoid having the network learn to only look at the score and recognize, say, the number of significant digits, etc. We additionally masked the sector number and number of enemy ships left on Beam Rider. We masked the bottom half of the dashboard for Enduro to mask the position of the car in the race. We masked the number of divers found and the oxygen meter for Seaquest. We masked the power level and inventory for Hero. To train the reward network for Enduro, we randomly downsampled full trajectories. To create a training set we repeatedly randomly select two full demonstrations, then randomly cropped between 0 and 5 of the initial frames from each trajectory and then downsampled both trajectories by only keeping every xth frame where x is randomly chosen between 3 and 6. We selected 2,000 randomly downsampled demonstrations and trained the reward network for 10,000 steps of Adam with a learning rate of 5e-5.\nIn this section, we examine the ability of prior work on active preference learning to exceed the performance of the demonstrator. In Table 2, we denote the results that surpass the best demonstration with an asterisk (*). DQfD+A only surpasses the demonstrator in 3 out of 9 games tested, even with thousands of active queries. Note that DQfD+A extends the original DQfD algorithm (Hester et al., 2017), which uses demonstrations combined with RL on groundtruth rewards, yet is only able to surpass the best demonstration in 14 out of 41 Atari games. In contrast, we are able to leverage only 12 ranked demos to achieve betterthan-demonstrator performance on 7 out of 8 games tested, without requiring access to true rewards or access to thousands of active queries from an oracle. Ibarz et al. (2018) combine Deep Q-learning from demonstrations and active preference queries (DQfD+A). DQfD+A uses demonstrations consisting of (st, at, st+1)-tuples to initialize a policy using DQfD (Hester et al., 2017). The algorithm then uses the active preference learning algorithm of Christiano et al. (2017) to refine the inferred reward function and initial policy learned from demonstrations. The first two columns of Table 2 compare the demonstration quality given to DQfD+A and T-REX. While our results make use of more demonstrations (12 for T-REX versus 47 for DQfD+A), our demonstrations are typically orders of magnitude worse than the demonstrations used by DQfD+A: on average the demonstrations given to DQfD+A are 38 times better than those used by T-REX. However, despite this large gap in the performance of the demonstrations, TREX surpasses the performance of DQfD+A on Q*Bert, and Seaquest. We achieve these results using 12 ranked demonstrations. This requires only 66 comparisons (n  (n 1)/2) by the demonstrator. In comparison, the DQfD+A results used 3,400 preference labels obtained during policy training using ground-truth rewards.\nF.1. Human demonstrations We used the Atari Grand Challenge data set (Kurin et al., 2017) to collect actual human demonstrations for five Atari games. We used the ground truth returns in the Atari Grand Challenge data set to rank demonstrations. To generate demonstrations we removed duplicate demonstrations (human demonstrations that achieved the same score). We then sorted the remaining demonstrations based on ground truth return and selected 12 of these demonstrations to form our training set. We ran T-REX using the same hyperparameters as described above. The resulting performance of T-REX is shown in Table 3. T-REX is able to outperform the best human demonstration on Q*bert, Space Invaders, and Video Pinball; however, it is not able to learn a good control policy for Montezumas Revenge or Ms Pacman. These games require maze navigation and balancing different objectives, such as collecting objects and avoiding enemies. This matches our results in the main text that show that T-REX is unable to learn a policy for playing Hero, a similar maze navigation task with multiple objectives such as blowing up walls, rescuing people, and destroying enemies. Extending T-REX to work in these types of settings is an interesting area of future work. F.2. Human rankings To measure the effects of human ranking noise, we took the same 12 PPO demonstrations described above in the main text and had humans rank the demonstrations. We used Amazon Mechanical Turk and showed the workers two sideby-side demonstrations and asked them to classify whether video A or video B had better performance or whether they were unsure. We took all 132 possible sequences of two videos across the 12 demonstrations and collected 6 labels for each pair of demonstrations. Because the workers are not actually giving the demonstrations and because some workers may exploit the task by simply selecting choices at random, we expect these labels to be a worst-case lower bound on the accuracy. To ameliorate the noise in the labels we take all 6 labels per pair and use the majority vote as the human label. If there is no majority or if the majority selects the Not Sure label, then we do not include this pair in our training data for T-REX. The resulting accuracy and number of labels that had a majority preference are shown in Table 4. We ran T-REX using the same hyperparameters described in the main text. We ran PPO with 3 different seeds and report the performance of the best final policy averaged over 30 trials. We found that surprisingly, T-REX is able to optimize good policies for many of the games, despite noisy labels. However, we did find cases such as Enduro, where the labels were too noisy to allow successful policy learning.\nWe generated attention maps for the learned rewards for the Atari domains. We use the method proposed by Greydanus et al. (2018), which takes a stack of 4 frames and passes a 3x3 mask over each of the frames with a stride of 1. The mask is set to be the default background color for each game. For each masked 3x3 region, we compute the absolute difference in predicted reward when the 3x3 region is not masked and when it is masked. This allows us to measure the influence of different regions of the image on the predicted reward. The sum total of absolute changes in reward for each pixel is used to generate an attention heatmap. We used the trajectories shown in the extrapolation plots in Figure 4 of the main text and performed a search using the learned reward function to find the observations with minimum and maximum predicted reward. We show the minimum and maximum observations (stacks of four frames) along with the attention heatmaps across all four stacked frames for the learned reward functions in figures 29. The reward function visualizations suggest that our networks are learning relevant features of the reward function.\n", "prediction": "this paper presents a solution to the problem of learning a sequence of commands from a single video sequence.<n> we propose an algorithm that is based on an inverse reinforcement learning ( ij ) approach, in which we first train our demonstrator to play a game of  hide and seek '' and then learn to learn the game s objective from the observed video sequences ; we show that our algorithm is able to achieve state - of - the - art performance for tasks that are difficult to train ( such as directed percolation and learning from incomplete training data in video games and tasks with multiple levels of difficulty ). <n> learning is one of the central problems in the field of artificial intelligence ( ai ), with applications ranging from speech recognition ( see for instance @xcite for a review of this area of research and references therein) to machine learning and robotics : see e.g. the recent paper by abrahams et al ( 2016a,b,c,d,l,e)the first attempt at a supervised learning algorithm for classification was made in 2010 ( epl,2015a)a problem that has since been solved for several tasks in a wide range of tasks including classification of multiple test sequences in natural language processing ( nlp)an example of such an approach is given by kim & tao ( 2011a)(nlp(2011a _npl_,2012a_b(2012_c)and al. etal(al.al ] we provide a policy for state-based policy ( al(e. al-(a (al etm al - al to obtain the policy that t ( in al al in 2012 ( ( 2012 - a alexander et - th al et et ( et a ( to al with al and al, al  al for the method ( we are used in ( which is used to be used by ( the algorithm ( for which  ( [ [ e - [  in [ al [ ( and [[[a et ]) and the [ et [ with the ( j. [ j ]. for [. ( @m - based ( with [ and we do in j - which [ that does not do [ which to do ( that was used with a. to [... [ the results for ranked in that we have the state of [ r.. in addition to which does the ]] while the search and that [ m. and  [ p. while we also that also does  to state. com and is also the. for ranking of which also  for  and which was also used. with which which. that and a state and was the most. which has also to lev and does that ( based based  that which and also for learning of ranked by the top - and p - that for any policy of a ranked from which in one that that in search of that the classification and to add that. by which the ranking from that of one. based. p and while while that to a ranking and for it is not. is the best. from state that makes the analysis of state  levi and it does also is deb ( while which for an. also also by one is that additionally  with one for one from [].<n> [<n> which by that by. j does.[m.... ], while [j.) and an [ it also and]..].  the learning by p, while one - to search for that - for all. does to that did also was  is to is a search that results also with ... and<n> richard and one and in an which - while [ a"}
{"ground_truth": "Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.\nKEYWORDS Reinforcement learning; Task decomposition; Transfer; Lifelong learning ACM Reference Format: Bohan Wu, Jayesh K. Gupta, and Mykel J. Kochenderfer. 2019. Model Primitive Hierarchical Lifelong Reinforcement Learning . In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 1317, 2019, IFAAMAS, 9 pages.\nIn the lifelong learning setting, we want our agent to solve a series of related tasks drawn from some task distribution rather than a single, isolated task. Agents must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning [25, 27, 31] and variousmeta-reinforcement learning settings [7, 8], where the agent jointly trains on multiple task environments. Not only do such nonincremental settings make the problem of discovering common structures between tasks easier, they allow the methods to ignore the problem of catastrophic forgetting [16], which is the inability to solve previous tasks after learning to solve new tasks in a sequential learning setting. Our work takes a step towards solutions for such incremental settings. We draw on the idea of modularity [17]. While learning to perform a complex task, we force the agent to break its solution down into simpler subpolicies instead of learning a single monolithic policy. This decomposition allows our agent to rapidly learn another related task by transferring these subpolicies. We Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 1317, 2019, Montreal, Canada.  2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. hypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agents solution increases if it can capture such shared structure. A key ingredient of our proposal is the idea of world models [10, 12, 14]  transition models that can predict future sensory data given the agents current actions. The world however is complex, and learning models that are consistent enough to plan with is not only hard [24], but planning with such one-step models is suboptimal [11]. We posit that the requirement that these world models be good predictors of the world state is unnecessary, provided we have a multiplicity of such models. We use the termmodel primitives to refer to these suboptimal world models. Since each model primitive is only relatively better at predicting the next states within a certain region of the environment space, we call this area the model primitives region of specialization. Model primitives allow the agent to decompose the task being performed into subtasks according to their regions of specialization and learn a specialized subpolicy for each subtask. The same model primitives are used to learn a gating controller to select, improve, adapt, and sequence the various subpolicies to solve a given task in a manner very similar to a mixture of experts framework [15]. Our framework assumes that at least a subset of model primitives are useful across a range of tasks and environments. This assumption is less restrictive than that of successor representations [3, 5]. Even though successor representations decouple the state transitions from the rewards (representing the task or goals), the transitions learned are policy dependent and can only transfer across tasks with the same environment dynamics. There are alternative approaches to learning hierarchical spatiotemporal decompositions from the rewards seen while interacting with the environment. These approaches include meta-learning algorithms like Meta-learning Shared Hierarchies (MLSH) [8], which require a multiplicity of pretrained subpolicies and joint training on related tasks. Other approaches include the option-critic architecture [1] that allows learning such decompositions in a single task environment. However, this method requires regularization hyperparameters that are tricky to set. As observed by Vezhnevets et al. [30], its learning often collapses to a single subpolicy. Moreover, we posit that capturing the shared structure across task-environments can be more useful in the context of transfer for lifelong learning than reward-based task specific structures. To summarize our contributions:  Given diverse suboptimalworldmodels, we propose amethod to leverage them for task decomposition.  We propose an architecture to jointly train decomposed subpolicies and a gating controller to solve a given task.  We demonstrate the effectiveness of this approach at both single-task and lifelong learning in complex domains with high-dimensional observations and continuous actions. ar X iv :1 90 3. 01 56 7v 1 [ cs .L G ] 4 M ar 2 01 9\nWe assume the standard reinforcement learning (RL) formulation: an agent interacts with an environment to maximize the expected reward [23]. The environment is modeled as a Markov decision process (MDP), which is defined by S,A,R,T ,  with a state space S, an action space A, a reward function R : S  A  R, a dynamics modelT : SA  (S), and a discount factor  [0, 1). Here, () defines a probability distribution over a set. The agent acts according to stationary stochastic policies  : S  (A), which specify action choice probabilities for each state. Each policy  has a corresponding Q : S  A  R function that defines the expected discounted cumulative reward for taking an action a from state s and following the policy  from that point onward. Lifelong Reinforcement Learning: In a lifelong learning setting, the agent must interact with multiple tasks and successfully solve each of them. Adopting the framework from Brunskill and Li [4], in lifelong RL, the agent receives S,A, initial state distribution 0  (S), horizon H , discount factor  , and an unknown distribution over reward-transition function pairs, D. The agent samples (Ri ,Ti )  D and interacts with the MDP S,A,Ri ,Ti ,  for a maximum ofH timesteps, starting according to the initial state distribution 0. After solving the given MDP or after H timesteps, whichever occurs first, the agent resamples from D and repeats. The fundamental question in lifelong learning is to determine what knowledge should be captured by the agent from the tasks it has already solved so that it can improve its performance on future tasks. When learning with functional approximation, this translates to learning the right representation  the one with the right inductive bias for the tasks in the distribution. Given the assumption that the set of related tasks for lifelong learning share a lot of structure, the ideal representation should be able to capture this shared structure. Thrun and Pratt [28] summarized various representation decomposition methods into two major categories. Modern approaches to avoiding catastrophic forgetting during transfer tend to fall into either category. The first category partitions the parameter space into task-specific parameters and general parameters [19]. The second category learns constraints that can be superimposed when learning a new function [13]. A popular approachwithin the first category is to usewhat Thrun and Pratt [28] term as recursive functional decomposition. This approach assumes that solution to tasks can be decomposed into a function of the form fi = hi  , where hi is task-specific whereas  is the same for all fi . This scheme has been particularly effective in computer vision where early convolutional layers in deep convolutional networks trained on ImageNet [6, 22] become a very effective  for a variety of tasks. However, this approach to decomposition often fails in DeepRL because of two main reasons. First, the gradients used to train such networks are noisier as a result of Monte Carlo sampling. Second, the i.i.d. assumption for training data often fails. We instead focus on devising an effective piecewise functional decomposition of the parameter space, as defined by Thrun and Pratt [28]. The assumption behind this decomposition is that each function fi can be represented by a collection of functions h1, . . . ,hm , wherem  N , and N is the number of tasks to learn. Our hypothesis is that this decomposition is much more effective and easier to learn in RL.\nThis section outlines the Model Primitive Hierarchical Reinforcement Learning (MPHRL) framework (Figure 1) to address the problem of effective piecewise functional decomposition for transfer across a distribution of tasks.\nThe key assumption in MPHRL is access to several diverse world models of the environment dynamics. These models can be seen as instances of learned approximations to the true environment dynamics T . In reality, these dynamics can even be non-stationary. Therefore, the task of learning a complete model of the environment dynamics might be too difficult. Instead, it can be much easier to train multiple approximate models that specialize in different parts of the environment. We use the term model primitives to refer to these approximate world models. Suppose we have access to K model primitives: Tk : S  A  (S). For simplicity, we can assign a labelMk to each Tk , such that their predictions of the environments transition probabilities can be denoted by T (st+1 | st ,at ,Mk ). 3.1.1 Subpolicies. The goal of the MPHRL framework is to use these suboptimal predictions from different model primitives to decompose the task space into their regions of specialization, and learn different subpolicies k : S  (A) that can focus on these regions. In the function approximation regime, each subpolicy k belongs to a fixed class of smoothly parameterized stochastic policies {k | k  }, where  is a set of valid parameter vectors. Model primitives are suboptimal and make incorrect predictions about the next state. Therefore we do not use them for planning or model-based learning of subpolicies directly. Instead, model primitives give rise to useful functional decompositions and allow subpolicies to be learned in a model-free way. 3.1.2 Gating Controller. Taking inspiration from the mixture-ofexperts literature [15], where the output from multiple experts can be combined using probabilistic gating functions, MPHRL decomposes the solution for a given task into multiple expert subpolicies and a gating controller that can compose them to solve the task. We want this switching behavior to be probabilistic and continuous to avoid abrupt transitions. During learning, we want this controller to help assign the reward signal to the correct blend of subpolicies to ensure effective learning as well as decomposition. Since the gating controllers goal is to choose the subpolicy whose corresponding model primitive makes the best prediction for a given transition, using Bayes rule we can write: P(Mk | st ,at , st+1)  P(Mk | st )k (at | st )T (st+1 | st ,at ,Mk ) (1) because k (at | st ) =  (at | st ,Mk ). The agent only has access to the current state st during execution. Therefore, the agent needs to marginalize out st+1 and at such that the model choice only depends on the current state st : P(Mk | st ) =  st+1S  at A P(Mk | st ,at , st+1) P(st+1,at )datdst+1 (2) This is equivalent to: P(Mk | st ) = Est+1,atP (st+1,at ) [P(Mk | st ,at , st+1)] (3) Unfortunately, computing these integrals requires expensive Monte Carlomethods. However, we can use an approximatemethod to achieve the same objective with discriminative learning [18]. We parameterize the gating controller (GC) as a categorical distribution P (Mk | st ) = P(Mk | st ;) and minimize the conditional cross entropy loss between Est+1,atP (st+1,at ) [P(Mk | st ,at , st+1)] and P (Mk | st ) for all sampled transitions (st ,at , st+1) in a rollout: minimize  LGC (4) where LGC =  st  k  ( st+1  at P(Mk | st ,at , st+1) )  log P(Mk | st ;) (5) This is equivalent to an implicit Monte Carlo integration to compute the marginal if st+1,at  P(st+1,at ). Although we cannot query or sample from P(st+1,at ) directly, st ,at , and st+1 can be sampled according to their respective distributionswhilewe perform rollouts in the environment. Despite the introduced bias in our estimates, we find Eq. 4 sufficient for achieving task decomposition. 3.1.3 Subpolicy Composition. Taking inspiration from mixtureof-experts, the gating controller composes the subpolicies into a mixture policy:  (at | st ) = K k=1 P (Mk | st )k (at | st ) (6) 3.1.4 Decoupling Cross Entropy from Action Distribution. During a rollout, the agent samples as follows: at   (at | st ) (7) st+1  T(st+1 | st ,at ) (8) The k from Eq. 1 gets coupled with this sampling distribution, making the target distribution in Eq. 5 no longer stationary and the approximation process difficult. We alleviate this issue by ignoring k , effectively treating it as a distribution independent of k . This transforms Eq. 1 into: P(Mk | st ,at , st+1)  P(Mk | st )T (st+1 | st ,at ,Mk ) (9)\nSince the focus of this work is on difficult continuous action problems, we mostly concentrate on the issue of policy optimization and how it integrates with the gating controller. The standard policy (SP) optimization objective is: maximize  LSP = E0, [ (at | st )Q (st ,at )] (10) With baseline subtraction for variance reduction, this turns into [20]: maximize  LPG = E0, [ (at | st )At ] (11) where At is an estimator of the advantage function [2]. In MPHRL, we directly use the mixture policy as defined by Eq. 6. The standard policy gradients (PG) get weighted by the probability outputs of the gating controller, enforcing the required specialization by factorizing into: k = E0,k [ P (Mk | st )k logk (at | st )At ] (12) In practice, we use the Clipped PPO objective [21] instead to perform stable updates by limiting the step size. This includes adding a baseline estimator (BL) parameterized by  for value prediction and variance reduction. We optimize according to the following loss: LBL = E [ V V 2] (13) We summarize this single-task learning algorithm in Algorithm 1, which results in a set of decomposed subpolicies, 1 , . . . K , and a gating controller P that can modulate between them to solve the task under consideration. Algorithm 1 MPHRL: single-task learning 1: Initialize P , = {1 , . . . ,K }, V 2: while not converged do 3: Rollout trajectories   , 4: Compute advantage estimates A 5: Optimize LPG wrt 1, . . . ,K with expectations taken over  6: Optimize LBL wrt with expectations taken over  7: Optimize LGC wrt  with expectations taken over  Lifelong learning: We have shown how MPHRL can decompose a single complex task solution into different functional components. Complex tasks often share structure and can be decomposed into similar sets of subtasks. Different tasks however require different recomposition of similar subtasks. Therefore, we transfer the subpolicies to learn target tasks, but not the gating controller or the baseline estimator. We summarize the lifelong learning algorithm in Algorithm 2, with the global variable RESET set to true. Algorithm 2MPHRL: lifelong learning 1: Initialize P ,  = {1 , . . . ,K },V 2: for Tasks (Ri ,Ti )  D do 3: if RESET then 4: Initialize P , V 5: while not converged do 6: Rollout trajectories   , 7: Compute advantage estimates A 8: Optimize LPG wrt 1, . . . ,K with expectations taken over  9: Optimize LBL wrt with expectations taken over  10: Optimize LGC wrt  with expectations taken over \nOur experiments aim to answer two questions: (a) can model primitives ensure task decomposition? (b) does such decomposition improve transfer for lifelong learning? We evaluate our approach in two challenging domains: a MuJoCo [29] ant navigating different mazes and a Stacker [26] arm picking up and placing different boxes. In our experiments, we use subpolicies that have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input and standard deviations given by a different set of parameters. MPHRLs gating controller outputs a categorical distribution and is parameterized by another multi-layer perceptron. We also use a separate multi-layer perceptron for the baseline estimator. We use the standard PPO algorithm as a baseline to compare against MPHRL. Transferring network weights empirically led to worse performance for standard PPO. Hence, we re-initialize its weights for every task. For fair comparison, we also shrink the hidden layer size of MPHRLs subpolicy networks from 64 to 16. We conduct each experiment across 5 different seeds. Error bars represent the standard deviation from the mean. The focus of this work is on understanding the usefulness of model primitives for task decomposition and the resulting improvement in sample efficiency from transfer. To conduct controlled experiments with interpretable results, we hand-designed model primitives using the true next state provided by the environment simulator. Concretely, we apply distinct multivariate Gaussian noise models with covariance  to the true next state. We then sample from this distribution to obtain the mean of the probability distribution of a model primitives next state prediction, using  as its covariance. Here,  is the noise scaling factor that distinguishes model primitives, while  refers to the empirical covariance of the sampled next states:   N(st+1,k) (14) T (st+1 | st ,at ,Mk ) = N(,k) (15) Using  as opposed to a constant covariance is essential for controlled experiments because different elements of the observation space have different orders of magnitude. Sampling  from a distribution effectively adds random bias to the model primitives next state probability distribution. Hyperparameter details are in Table 1, and our code is freely available at we focus on two single-task learning experiments where MPHRL learns a number of interpretable subpolicies to solve a single task. Both the L-Maze and D-Maze (Figure 2a) tasks require the ant to learn to walk and reach the green goal within a finite 18-Pickup&Place. 2Single task refers to L-Maze and D-Maze; source and target tasks refer to the first task and all subsequent tasks in a lifelong learning taskset, respectively. 3Baseline network hyperparameters apply to both MPHRL and baseline PPO; model primitive networks are for experiments with learned model primitives only. 4The baseline PPO has no subpolicies, so the subpolicy network is the policy network. 5Baseline and subpolicy networks only. horizon. For both tasks, both the goal and the initial ant locations are fixed. For the L-Maze, the agent has access to two model primitives, one specializing in the horizontal (E, W) corridor and the other specializing in the vertical (N, S) corridor of the maze. Similarly for the D-Maze, the agent has access to four model primitives, one specializing in each N, S, E, W corridor of the maze. In their specialized corridors, the noise scaling factor  = 0. Outside of their regions of specialization,  = 0.5. The observation space includes the standard joint angles and positions, lidar information tracking distances from walls on each side, and the Manhattan distance to the goal. Figure 2b shows the experimental results on these environments. Notice that using model primitives can make the learning problem more difficult and increase the sample complexity on a single task. This is expected, since we are forcing the agent to decompose the solution, which could be unnecessary for easy tasks. However, we will observe in the following section that this decomposition can lead to remarkable improvements in transfer performance during lifelong learning.\nTo evaluate our frameworks performance at lifelong learning, we introduce two tasksets. 4.2.1 10-Maze. To evaluate MPHRLs performance in lifelong learning, we generate a family of 10 random mazes for the MuJoCo Ant environment, referred to as the 10-Maze taskset (Figure 4) hereafter. The goal, the observation space, the Gaussian noise models, and the model primitives remain the same as in D-Maze. The agent has a maximum of 3  107 timesteps to reach 80% success rate in each of the 10 tasks. As shown in Figure 3a, MPHRL requires nearly double the number of timesteps to learn the decomposed subpolicies in the first task. However, this cost gets heavily amortized over the entire taskset, with MPHRL taking half the total number of timesteps of the baseline PPO, exhibiting strong subpolicy transfer. 4.2.2 8-Pickup&Place. Wemodify the Stacker task [26] to create the 8-Pickup&Place taskset. As shown in Figure 5, a robotic arm is tasked to bring 2 boxes to their respective goal locations in a certain order. Marked by colors red, green, and blue, the goal locations reside within two short walls forming a stack. Each of the 8 tasks has a maximum of 3 goal locations. The observation space of the agent includes joint angles and positions, box and goal locations, their relative distances to each other, and the current stage of the task encoded as one-hot vectors. The agent has access to six model primitives for each box that specialize in reaching above, lowering to, grasping, picking up, carrying, and dropping a certain box. Similar to 10-Maze, model primitives have  of 0 within their specialized stages and  of 0.5 otherwise. Figure 3b shows MPHRLs experimental performance by learning twelve useful subpolicies for this taskset. We notice again the strong transfer performance due to the decomposition forced by the model primitives. Note that this taskset is much more complex than 10-Maze such that MPHRL even accelerates the learning of the first task.\nWe conduct ablation experiments to answer the following questions: (1) How much gain in sample efficiency is achieved by transfer- ring subpolicies? (2) Can MPHRL learn the task decomposition even when the model primitives are quite noisy or when the source task does not cover all cases? (3) When does MPHRL fail to decompose the solution? (4) What kind of diversity in the model primitives is essential for performance? (5) When does MPHRL lead to negative transfer? (6) Is MPHRLs gain in sample efficiency a result of hand-crafted model primitives and how does it perform with actual learned model primitives? 4.3.1 Model Noise. MPHRL has the ability to decompose the solution even given bad model primitives. Since the learning is done model-free, these suboptimal model primitives should not strongly affect the learning performance so long as they remain sufficiently distinct. To investigate the limitations to this claim, we conduct five experiments using various sets of noisy model primitives. Below, the first value corresponds to the noise scaling factor  within their individual regions of specialization, while the second value corresponds to  outside of their regions of specialization. (a) 0.4 and 0.5: good models with limited distinction (b) 0.5 and 1.0: good models with reasonable distinction (c) 5.0 and 10.0: bad models with reasonable distinction (d) 9.0 and 10.0: bad models with limited distinction (e) 0.5 and 0.5: good models with no distinction Shown in Figure 6a, while (a), (b), (c), and (d) exhibit limited degradation in performance, (d) experiences the most performance degradation on average. On the other hand, in (e) MPHRL took 22.04.6million timesteps to solve the first task and 2.8  1.6 million timesteps to solve the second task, but failed to solve the third task within 30 million timesteps. This is because the model primitives are identical and provide no information about task decomposition. In summary, MPHRL is robust against bad model primitives so long as the they maintain some relative distinction. Similar observations hold true for the 8-Pickup&Place taskset where noise models with distinctive models with large noise of  = 5 and  = 20 show little deterioration in performance, taking 15.8  5.5 million timesteps to reach 75% average success rate. 4.3.2 Overlapping Model Primitives. We next test the condition when there is substantial overlap in regions of specialization between different model primitives. For the 10-Maze taskset, the most plausible region for this confusion is at the corners. In this experiment, within each corner, the two model primitives whose specialized corridors share the corner have  = 0 while the other two have  = 0.5. Figure 6b shows the performance for model primitive confusion against the standard set of model primitives with no confusion. We observe that despite some performance degradation, MPHRL continues to outperform the PPO baseline. 4.3.3 Model Diversity. Having tested MPHRL against noises, we experimented with undesirable model primitives for 10-Maze: (a) Extra: a fifth model primitive that specializes in states where the ant is moving horizontally; (b) H-V corridors: 2 model primitives specializing in horizontal (E, W) and vertical (N, S) corridors respectively; (c) Velocity: 2 model primitives specializing in states where the ant is moving horizontally or vertically; and for 8-Pickup&Place: (a) Box-only: 2 model primitives for all actions on 2 boxes; (b) Action-only: 6 model primitives for 6 actions performed on boxes: reach above, lower to, grasp, pick up, carry, and drop. Table 2 shows MPHRL is susceptible to performance degradation given undesirable sets of model primitives. However, MPHRL still outperforms baseline PPO when given an extra, undesirable model primitive. This indicates that for best transfer, the model primitives need to approximately capture the structure present in the taskset. 4.3.4 Negative Transfer and Catastrophic Forgetting. Lifelong learning agents with neural network function approximators face the problem of negative transfer and catastrophic forgetting. Ideally, they should find the solution quickly if the task has already been seen. More generally, given two sets of tasks T and T  such that T  T , after being exposed to T  the agent should perform no worse, and preferably better, than had it been exposed to T only. In this experiment, we restore the subpolicy checkpoints after solving the 10 tasks and evaluate MPHRLs learning performance for the first 9 tasks. Similarly, we restore the subpolicy checkpoints after solving 6 tasks and evaluate MPHRLs performance on the first 5 tasks. The gating controller is reset for each task as in earlier experiments. We summarize the results in Table 3. Subpolicies trained sequentially on 6 or 10 tasks quickly relearn the required behavior for all previously seen tasks, implying no catastrophic forgetting. Moreover, if we compare the 10-task result to the 6-task result, we see remarkable improvements at transfer. This implies negative transfer is limited with this approach. 4.3.5 Oracle Gating Controller. One might suspect that all gains in sample efficiency come from hand-crafted model primitives because they allow the agent to learn a perfect gating controller. However, Figure 7 shows the reward curves for an experiment where the gating controller is already perfectly known. This setup is unable to learn any 10-Maze task. Since the 10-Maze taskset is composed of sequential subtasks, only one subpolicy will be learned in the first corridor when the gating controller is perfect. When transitioning to the second corridor, the second policy needs to be learned from scratch, making the ants survival rate very low. This discourages the first subpolicy from entering the second corridor and activating the second subpolicy. Eventually, the ant stops moving forward close to the intersection between the first two corridors. In contrast, MPHRLs natural curriculum for gradual specialization allows multiple subpolicies to learn the basic skills for survival initially. 4.3.6 Partial Decomposition. To confirm that the ordering of tasks does not significantly affect MPHRLs performance, we modified 10-Maze to create the 10-Maze-v2 taskset (Figure 9), in which the source task does not allow for complete decomposition into all useful subpolicies for the subsequent tasks. Again, we observe large improvement in sample efficiency over standard PPO (Figure 8). 4.3.7 Learned Model Primitives. This paper focuses on evaluating suboptimal models for task decomposition in controlled experiments using hand-designed model primitives. Here, we show one way to obtain each model primitive for 10-Maze-v2 using three corridor environments demonstrated in Figure 10. Concretely, we parameterize each model primitive using a multivariate Gaussian distribution. We learn the mean of this distribution via a multilayer perceptron using a weighted mean square error in dynamics prediction as the loss. The standard deviation is still derived from the empirical covariance  as described earlier. Even though the diversity in these learned model primitives is much more difficult to quantify and control, their sample efficiency substantially outperforms standard PPO and slightly underperforms hand-designed model primitives with 0 and 0.5 model noises (Figure 8). 4.3.8 Gating Controller Transfer. To explore factors that lead to negative transfer, we tested MPHRL without re-initializing the gating controller in target tasks, as shown in Figure 6c. Although the mean sample efficiency remains stable, its standard deviation increases dramatically, indicating volatility due to negative transfer. 4.3.9 Subpolicy Transfer. To measure how much gain in sample efficiency MPHRL has achieved by transferring subpolicies alone, we conducted a 10-Maze experiment by re-initializing all network weights for every new task. As shown in Figure 6c, sample complexity more than quintuples when subpolicies are re-initialized (in green). 4.3.10 Coupling between Cross Entropy and Action Distribution. To validate using P(Mk | st ,at , st+1) in Eq. 9 as opposed to P(Mk | st ,at , st+1) from Eq. 1, we tested MPHRL with Eq. 1 on 10-Maze. All runs with different seeds failed to solve the first 5 tasks (Table 4). As the gating controller is re-initialized during transfer, most actions were chosen incorrectly. The gating controller is thus presented with the incorrect cross entropy target, which worsens the action distribution. The resulting vicious cycle forces the gating controller to converge to a suboptimal equilibrium against the incorrect target.\nWe showed how imperfect world models can be used to decompose a complex task into simpler ones. We introduced a framework that uses these model primitives to learn piecewise functional decompositions of solutions to complex tasks. The learned decomposed subpolicies can then be used to transfer to a variety of related tasks, reducing the overall sample complexity required to learn complex behaviors. Our experiments showed that such structured decomposition avoids negative transfer and catastrophic interference, a major concern for lifelong learning systems. Our approach does not require access to accurate world models. Neither does it need a well-designed task distribution or the incremental introduction of individual tasks. So long as the set of model primitives are useful across the task distribution, MPHRL is robust to other imperfections. Nevertheless, learning useful and diverse model primitives, subpolicies and task decomposition all simultaneously is left for future work. The recently introduced Neural Processes [9] can potentially be an efficient approach to build upon.\nWe are thankful to Kunal Menda and everyone at SISL for useful comments and suggestions. This work is supported in part by DARPA under agreement number D17AP00032. The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA. We are also grateful for the support from Google Cloud in scaling our experiments.\n", "prediction": "we present a general framework for decomposition of both continuous and discrete tasks.<n> we show how to decompose a single task into multiple subtasks that share the same structure, while still learning to perform decomposition in a desired way : we do this in such a way that we are able to transfer the learned decomposition from one task to another with minimal loss in performance ; we also show that our approach can be used to improve performance in continuous tasks by decomposing them into a mixture of discrete and continuous sub tasks that are indistinguishable in the limit of infinitesimal number of components we demonstrate the effectiveness of the approach through experiments on both synthetic and real world continuous task decompositions and demonstrate that it is effective in comparison to state - of - the - art approaches in terms of improvement in both performance and transfer efficiency _ key words and phrases_.@xmath0 decomposition is an essential component of many machine learning and artificial intelligence ( ai ) tasks @xcite!<n> this is due to the fact that many tasks are difficult to decouple from their underlying structure and the difficulty of finding a decomposition that is both effective and transferable <n> t. schurman has shown that the problem of decomposing a discrete task is equivalent to that of learning an effective function that can capture the structure of a multi - component task  g$ ] ( g[g]f(k]g[ks2(gfk)drtcnvmh[f[d+1bpa$][c[b[n[a](c]g(d]d[p]$i [b] [n]c(s[t[i[s(n(b([[e_s.ss [s, [d [ [[2 ]. [c [ts we prove that [(t, we find [ st [st[(i [... [] we can [ ], [2[ we ]][se ]) we add a  [ p, st_[ [ b. [ d. we make the ...... we have the [ c.[]   we [ we found [ @[...] is the results [ ( st - st] the @  the p. the algorithm [ k....  it  in [ a [ the st. st, the method [ g  for [ 2 [ one is st[@[_] @_ we is we will [ is  to [ it has we has the is found  is a we does we use the policy of st we we - we obtain the solution of @ @... @ [ i  found the function of [. is it does st is @ st... the analysis of  @ -  that  ( [ se  - [ which is not we... is of... st [ sb - @] st)  with the se is one  a is [ with a @,  st-[m  will  which we finds the multi-) is is... it results for the general  has ). we focus of which  whose  from the other is p[. @ the first  results from st and [ j  of we was st @) the] does it will find the sub- ), we derive the probability of all we uses the) we results  if we only [ and we apply [<n> [ in st ( levi  does  finding  and amide  functions  who  they  while the], the we"}
{"ground_truth": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 12 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\nThe promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label [14, 22]. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties. 1 In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles. Jean Pouget-Abadie is visiting Universite de Montreal from Ecole Polytechnique. Sherjil Ozair is visiting Universite de Montreal from Indian Institute of Technology Delhi Yoshua Bengio is a CIFAR Senior Fellow. 1All code and hyperparameters available at ar X iv :1 40 6. 26 61 v1 [ st at .M L ] 1 0 Ju n 20 This framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [17] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.\nAn alternative to directed graphical models with latent variables are undirected graphical models with latent variables, such as restricted Boltzmann machines (RBMs) [27, 16], deep Boltzmann machines (DBMs) [26] and their numerous variants. The interactions within such models are represented as the product of unnormalized potential functions, normalized by a global summation/integration over all states of the random variables. This quantity (the partition function) and its gradient are intractable for all but the most trivial instances, although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses a significant problem for learning algorithms that rely on MCMC [3, 5]. Deep belief networks (DBNs) [16] are hybrid models containing a single undirected layer and several directed layers. While a fast approximate layer-wise training criterion exists, DBNs incur the computational difficulties associated with both undirected and directed models. Alternative criteria that do not approximate or bound the log-likelihood have also been proposed, such as score matching [18] and noise-contrastive estimation (NCE) [13]. Both of these require the learned probability density to be analytically specified up to a normalization constant. Note that in many interesting generative models with several layers of latent variables (such as DBNs and DBMs), it is not even possible to derive a tractable unnormalized probability density. Some models such as denoising auto-encoders [30] and contractive autoencoders have learning rules very similar to score matching applied to RBMs. In NCE, as in this work, a discriminative training criterion is employed to fit a generative model. However, rather than fitting a separate discriminative model, the generative model itself is used to discriminate generated data from samples a fixed noise distribution. Because NCE uses a fixed noise distribution, learning slows dramatically after the model has learned even an approximately correct distribution over a small subset of the observed variables. Finally, some techniques do not involve defining a probability distribution explicitly, but rather train a generative machine to draw samples from the desired distribution. This approach has the advantage that such machines can be designed to be trained by back-propagation. Prominent recent work in this area includes the generative stochastic network (GSN) framework [5], which extends generalized denoising auto-encoders [4]: both can be seen as defining a parameterized Markov chain, i.e., one learns the parameters of a machine that performs one step of a generative Markov chain. Compared to GSNs, the adversarial nets framework does not require a Markov chain for sampling. Because adversarial nets do not require feedback loops during generation, they are better able to leverage piecewise linear units [19, 9, 10], which improve the performance of backpropagation but have problems with unbounded activation when used ina feedback loop. More recent examples of training a generative machine by back-propagating into it include recent work on auto-encoding variational Bayes [20] and stochastic backpropagation [24].\nThe adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generators distribution pg over data x, we define a prior on input noise variables pz(z), then represent a mapping to data space as G(z; g), where G is a differentiable function represented by a multilayer perceptron with parameters g . We also define a second multilayer perceptron D(x; d) that outputs a single scalar. D(x) represents the probability that x came from the data rather than pg . We train D to maximize the probability of assigning the correct label to both training examples and samples fromG. We simultaneously trainG to minimize log(1D(G(z))): In other words,D andG play the following two-player minimax game with value function V (G,D): min G max D V (D,G) = Expdata(x)[logD(x)] + Ezpz(z)[log(1D(G(z)))]. (1) In the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as G and D are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing D to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between k steps of optimizing D and one step of optimizing G. This results in D being maintained near its optimal solution, so long as G changes slowly enough. This strategy is analogous to the way that SML/PCD [31, 29] training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in Algorithm 1. In practice, equation 1 may not provide sufficient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data. In this case, log(1  D(G(z))) saturates. Rather than training G to minimize log(1D(G(z))) we can train G to maximize logD(G(z)). This objective function results in the same fixed point of the dynamics ofG andD but provides much stronger gradients early in learning. pdata(x) pdata(x)+pg(x) . (c) After an update to G, gradient of D has guided G(z) to flow to regions that are more likely to be classified as data. (d) After several steps of training, if G and D have enough capacity, they will reach a point at which both cannot improve because pg = pdata. The discriminator is unable to differentiate between the two distributions, i.e. D(x) = 1 2 .\nThe generator G implicitly defines a probability distribution pg as the distribution of the samples G(z) obtained when z  pz . Therefore, we would like Algorithm 1 to converge to a good estimator of pdata, if given enough capacity and training time. The results of this section are done in a nonparametric setting, e.g. we represent a model with infinite capacity by studying convergence in the space of probability density functions. We will show in section 4.1 that this minimax game has a global optimum for pg = pdata. We will then show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result. Algorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, k, is a hyperparameter. We used k = 1, the least expensive option, in our experiments. for number of training iterations do for k steps do  Sample minibatch of m noise samples {z(1), . . . ,z(m)} from noise prior pg(z).  Sample minibatch of m examples {x(1), . . . ,x(m)} from data generating distribution pdata(x).  Update the discriminator by ascending its stochastic gradient: d 1 m m i=1 [ logD ( x(i) ) + log ( 1D ( G ( z(i) )))] . end for  Sample minibatch of m noise samples {z(1), . . . ,z(m)} from noise prior pg(z).  Update the generator by descending its stochastic gradient: g 1 m m i=1 log ( 1D ( G ( z(i) ))) . end for The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments. 4.1 Global Optimality of pg = pdata We first consider the optimal discriminator D for any given generator G. Proposition 1. For G fixed, the optimal discriminator D is DG(x) = pdata(x) pdata(x) + pg(x) (2) Proof. The training criterion for the discriminator D, given any generator G, is to maximize the quantity V (G,D) V (G,D) =  x pdata(x) log(D(x))dx+  z pz(z) log(1D(g(z)))dz =  x pdata(x) log(D(x)) + pg(x) log(1D(x))dx (3) For any (a, b)  R2 \\ {0, 0}, the function y  a log(y) + b log(1  y) achieves its maximum in [0, 1] at aa+b . The discriminator does not need to be defined outside of Supp(pdata)  Supp(pg), concluding the proof. Note that the training objective for D can be interpreted as maximizing the log-likelihood for estimating the conditional probability P (Y = y|x), where Y indicates whether x comes from pdata (with y = 1) or from pg (with y = 0). The minimax game in Eq. 1 can now be reformulated as: C(G) =max D V (G,D) =Expdata [logDG(x)] + Ezpz [log(1DG(G(z)))] (4) =Expdata [logDG(x)] + Expg [log(1DG(x))] =Expdata [ log pdata(x) Pdata(x) + pg(x) ] + Expg [ log pg(x) pdata(x) + pg(x) ] Theorem 1. The global minimum of the virtual training criterion C(G) is achieved if and only if pg = pdata. At that point, C(G) achieves the value  log 4. Proof. For pg = pdata, DG(x) = 1 2 , (consider Eq. 2). Hence, by inspecting Eq. 4 atD  G(x) = 1 2 , we find C(G) = log 12 + log 1 2 =  log 4. To see that this is the best possible value of C(G), reached only for pg = pdata, observe that Expdata [ log 2] + Expg [ log 2] =  log 4 and that by subtracting this expression from C(G) = V (DG, G), we obtain: C(G) =  log(4) +KL ( pdata pdata + pg2 ) +KL ( pg pdata + pg2 ) (5) where KL is the KullbackLeibler divergence. We recognize in the previous expression the Jensen Shannon divergence between the models distribution and the data generating process: C(G) =  log(4) + 2  JSD (pdata pg ) (6) Since the JensenShannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that C =  log(4) is the global minimum of C(G) and that the only solution is pg = pdata, i.e., the generative model perfectly replicating the data generating process.\nProposition 2. IfG andD have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given G, and pg is updated so as to improve the criterion Expdata [logDG(x)] + Expg [log(1DG(x))] then pg converges to pdata Proof. Consider V (G,D) = U(pg, D) as a function of pg as done in the above criterion. Note that U(pg, D) is convex in pg . The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if f(x) = supA f(x) and f(x) is convex in x for every , then f(x)  f if  = arg supA f(x). This is equivalent to computing a gradient descent update for pg at the optimal D given the corresponding G. supD U(pg, D) is convex in pg with a unique global optima as proven in Thm 1, therefore with sufficiently small updates of pg , pg converges to px, concluding the proof. In practice, adversarial nets represent a limited family of pg distributions via the function G(z; g), and we optimize g rather than pg itself. Using a multilayer perceptron to define G introduces multiple critical points in parameter space. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees.\nWe trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of rectifier linear activations [19, 9] and sigmoid activations, while the discriminator net used maxout [10] activations. Dropout [17] was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network. We estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The  parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [8] and used for various generative models for which the exact likelihood is not tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating the likelihood has somewhat high variance and does not perform well in high dimensional spaces but it is the best method available to our knowledge. Advances in generative models that can sample but not estimate likelihood directly motivate further research into how to evaluate such models. In Figures 2 and 3 we show samples drawn from the generator net after training. While we make no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework.\nThis new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of pg(x), and that D must be synchronized well with G during training (in particular, G must not be trained too much without updatingD, in order to avoid the Helvetica scenario in whichG collapses too many values of z to the same value of x to have enough diversity to model pdata), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches. The aforementioned advantages are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data examples, but only with gradients flowing through the discriminator. This means that components of the input are not copied directly into the generators parameters. Another advantage of adversarial networks is that they can represent very sharp, even degenerate distributions, while methods based on Markov chains require that the distribution be somewhat blurry in order for the chains to be able to mix between modes.\nThis framework admits many straightforward extensions: 1. A conditional generative model p(x | c) can be obtained by adding c as input to both G and D. 2. Learned approximate inference can be performed by training an auxiliary network to predict z given x. This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a fixed generator net after the generator net has finished training. 3. One can approximately model all conditionals p(xS | x 6S) where S is a subset of the indices of x by training a family of conditional models that share parameters. Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM [11]. 4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available. 5. Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training. This paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.\nWe would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2 [12] and Theano [7, 1], particularly Frederic Bastien who rushed a Theano feature specifically to benefit this project. Arnaud Bergeron provided much-needed support with LATEX typesetting. We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Quebec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity.\n", "prediction": "this paper proposes a new framework for estimating generative models.<n> the framework consists of two parts : the first part is the training of a discriminator, which is a stochastic process that generates a subset of data from a given model and estimates the probability distribution of the data based on the obtained subset s characteristics ; the second part uses an adversarial learning algorithm that is designed to learn a priori the model parameters of interest from the given data set in order to maximize the likelihood function of this model with respect to the set of parameters to be estimated<n> this framework can be applied to a variety of problems in machine learning such as data compression and classification _ keywords and phrases_. <n> [ section ] [ theorem]corollary [section]proposition [ sections]lemma [ subsection]assumption [ chapter]definition [ chapters]remark # 1#2#3#4#1 * # 2 * ( # 4 ) # 3 * the problem of estimating stochastic models is important in many areas of science and engineering!<n> for example  classification of systems '' is an important field of study in computer science @xcite and computer vision?<n> while there are a number of methods for the estimation of models in one or the other field constituting a basis for this problem and a lot of work has been done in the area of model estimation gaussian models are the most popular and widely used models for classification and inference in artificial intelligence and machine vision and neural networks in this field for many of machine recognition of these models and dna and the models that have been used in most of many models to generate a model in a machine and we have to obtain data in an algorithm for any model for a matrix in any data ( p(s of p and p ( pg(g(p(d ( g( g ( for p. g in g and g g for g. p p ng p g is p for all g of g t g - g to p is g p in p - p[s g [ g[g g @ g with g d([[(( p [ ps  g as p with p to gs p @  p) and [ ( [ [ 1 ]. g) ]) and @ ( ( d  for [ x(e g, p = g-[p ], g = p, g_ gg ( x and d ( b( [g and x - [[d g]. pg [e (t(z(m g] is [  [ z th g], p] and z(] to [ d[]  to x[t   ( z - x  z. [() is of x g]). z[z[n g * g was g i  in [] g + p_ p-(f( z) for x. d - d [ b g[m ... p])  the p]. [... g by g we  that p was p * p... [ e  is for d and is to ]] and] for which p by p which g which to d) to all p as g while p], the g that []. to ( the [ * * x].  with a g). g has p we are p + g e. ([f  and to z is ). p of [ a.]]. the x ( f - to we is in which we was the analysis of d. * [.[. x] with the]) is that g... ( to] in x to b. the]. for  of  a "}
{"ground_truth": "Among the now numerous alternative cryptocurrencies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcashs transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.\ncies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcashs transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.\nSince the introduction of Bitcoin in 2008 [34], cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. While the broader positive potential of blockchain (i.e., the public decentralized ledger underlying almost all cryptocurrencies) is still unclear, despite the growing number of legitimate users there are today still many people using these cryptocurrencies for less legitimate purposes. These range from the purchase of drugs or other illicit goods on so-called dark markets such as Dream Market, to the payments from victims in ransomware attacks such as WannaCry, with many other crimes in between. Criminals engaged in these activities may be drawn to Bitcoin due to the relatively low friction of making international payments using only pseudonyms as identifiers, but the public nature of its ledger of transactions raises the question of how much anonymity is actually being achieved. Indeed, a long line of research [37, 38, 12, 27, 40] has by now demonstrated that the use of pseudonymous ad- dresses in Bitcoin does not provide any meaningful level of anonymity. Beyond academic research, companies now provide analysis of the Bitcoin blockchain as a business [19]. This type of analysis was used in several arrests associated with the takedown of Silk Road [20], and to identify the attempts of the WannaCry hackers to move their ransom earnings from Bitcoin into Monero [17]. Perhaps in response to this growing awareness that most cryptocurrencies do not have strong anonymity guarantees, a number of alternative cryptocurrencies or other privacy-enhancing techniques have been deployed with the goal of improving on these guarantees. The most notable cryptocurrencies that fall into this former category are Dash [2] (launched in January 2014), Monero [3] (April 2014), and Zcash [7] (October 2016). At the time of this writing all have a market capitalization of over 1 billion USD [1], although this figure is notoriously volatile, so should be taken with a grain of salt. Even within this category of privacy-enhanced cryptocurrencies, and despite its relative youth, Zcash stands somewhat on its own. From an academic perspective, Zcash is backed by highly regarded research [28, 13], and thus comes with seemingly strong anonymity guarantees. Indeed, the original papers cryptographically prove the security of the main privacy feature of Zcash (known as the shielded pool), in which users can spend shielded coins without revealing which coins they have spent. These strong guarantees have attracted at least some criminal attention to Zcash: the underground marketplace AlphaBay was on the verge of accepting it before their shutdown in July 2017 [11], and the Shadow Brokers hacking group started accepting Zcash in May 2017 (and in fact for their monthly dumps accepted exclusively Zcash in September 2017) [16]. Despite these theoretical privacy guarantees, the deployed version of Zcash does not require all transactions to take place within the shielded pool itself: it also supports so-called transparent transactions, which are essentially the same as transactions in Bitcoin in USENIX Association 27th USENIX Security Symposium 463 that they reveal the pseudonymous addresses of both the senders and recipients, and the amount being sent. It does require, however, that all newly generated coins pass through the shielded pool before being spent further, thus ensuring that all coins have been shielded at least once. This requirement led the Zcash developers to conclude that the anonymity set for users spending shielded coins is in fact all generated coins, and thus that the mixing strategies that other cryptocurrencies use for anonymity provide a rather small [anonymity set] in comparison to Zcash and that Zcash has a distinct advantage in terms of transaction privacy [9]. In this paper, we provide the first in-depth empirical analysis of anonymity in Zcash, in order to examine these claims and more generally provide a longitudinal study of how Zcash has evolved and who its main participants are. We begin in Section 4 by providing a general examination of the Zcash blockchain, from which we observe that the vast majority of Zcash activity is in the transparent part of the blockchain, meaning it does not engage with the shielded pool at all. In Section 5, we explore this aspect of Zcash by adapting the analysis that has already been developed for Bitcoin, and find that exchanges typically dominate this part of the blockchain. We then move in Section 6 to examining interactions with the shielded pool. We find that, unsurprisingly, the main actors doing so are the founders and miners, who are required to put all newly generated coins directly into it. Using newly developed heuristics for attributing transactions to founders and miners, we find that 65.6% of the value withdrawn from the pool can be linked back to deposits made by either founders or miners. We also implement a general heuristic for linking together other types of transactions, and capture an additional 3.5% of the value using this. Our relatively simple heuristics thus reduce the size of the overall anonymity set by 69.1%. In Section 7, we then look at the relatively small percentage of transactions that have taken place within the shielded pool. Here, we find (perhaps unsurprisingly) that relatively little information can be inferred, although we do identify certain patterns that may warrant further investigation. Finally, we perform a small case study of the activities of the Shadow Brokers within Zcash in Section 8, and in Section 9 we conclude. All of our results have been disclosed, at the time of the papers submission, to the creators of Zcash, and discussed extensively with them since. This has resulted in changes to both their public communication about Zcashs anonymity as well as the transactional behavior of the founders. Additionally, all the code for our analysis is available as an open-source repository.1 consider as related all work that has focused on the anonymity of cryptocurrencies, either by building solutions to achieve stronger anonymity guarantees or by demonstrating its limits. In terms of the former, there has been a significant volume of research in providing solutions for existing cryptocurrencies that allow interested users to mix their coins in a way that achieves better anonymity than regular transactions [15, 41, 21, 24, 39, 14, 22, 25]. Another line of research has focused on producing alternative privacy-enhanced cryptocurrencies. Most notably, Dash [2] incorporates the techniques of CoinJoin [24] in its PrivateSpend transactions; Monero [3, 35] uses ring signatures to allow users to create mix-ins (i.e., include the keys of other users in their own transactions as a way of providing a larger anonymity set); and Zcash [7, 13] uses zero-knowledge proofs to allow users to spend coins without revealing which coins are being spent. In terms of the latter, there has also been a significant volume of research on de-anonymizing Bitcoin [37, 38, 12, 27, 40]. Almost all of these attacks follow the same pattern: they first apply so-called clustering heuristics that associate multiple different addresses with one single entity, based on some evidence of shared ownership. The most common assumption is that all input addresses in a transaction belong to the same entity, with some papers [12, 27] also incorporating an additional heuristic in which output addresses receiving change are also linked. Once these clusters are formed, a re-identification attack [27] then tags specific addresses and thus the clusters in which they are contained. These techniques have also been applied to alternative cryptocurrencies with similar types of transactions, such as Ripple [30]. The work that is perhaps closest to our own focuses on de-anonymizing the privacy solutions described above, rather than just on Bitcoin. Here, several papers have focused on analyzing so-called privacy overlays or mixing services for Bitcoin [33, 26, 31, 32], and considered both their level of anonymity and the extent to which participants must trust each other. Some of this analysis [32, 26] also has implications for anonymity in Dash, due to its focus on CoinJoin. More recently, Miller et al. [29] and Kumar et al. [23] looked at Monero. They both found that it was possible to link together transactions based on temporal patterns, and also based on certain patterns of usage, such as users who choose to do transactions with 0 mix-ins (in which case their ring signature provides no anonymity, which in turns affects other users who may have included their key in their own mix-ins). Finally, we are aware of one effort to de-anonymize Zcash, by Quesnelle [36]. This article focuses on linking together the transactions used to shield 464 27th USENIX Security Symposium USENIX Association and deshield coins, based on their timing and the amount sent in the transactions. In comparison, our paper implements this heuristic but also provides a broader perspective on the entire Zcash ecosystem, as well as a more in-depth analysis of all interactions with (and within) the shielded pool.\n\nZcash (ZEC) is an alternative cryptocurrency developed as a (code) fork of Bitcoin that aims to break the link between senders and recipients in a transaction. In Bitcoin, recipients receive funds into addresses (referred to as the vOut in a transaction), and when they spend them they do so from these addresses (referred to as the vIn in a transaction). The act of spending bitcoins thus creates a link between the sender and recipient, and these links can be followed as bitcoins continue to change hands. It is thus possible to track any given bitcoin from its creation to its current owner. Any transaction which interacts with the so-called shielded pool in Zcash does so through the inclusion of a vJoinSplit, which specifies where the coins are coming from and where they are going. To receive funds, users can provide either a transparent address (t-address) or a shielded address (z-address). Coins that are held in z-addresses are said to be in the shielded pool. To specify where the funds are going, a vJoinSplit contains (1) a list of output t-addresses with funds assigned to them (called zOut), (2) two shielded outputs, and (3) an encrypted memo field. The zOut can be empty, in which case the transaction is either shielded (tto-z) or private (z-to-z), depending on the inputs. If the zOut list contains a quantity of ZEC not assigned to any address, then we still consider it to be empty (as this is simply the allocation of the miners fee). Each shielded output contains an unknown quantity of ZEC as well as a hidden double-spending token. The shielded output can be a dummy output (i.e., it contains zero ZEC) to hide the fact that there is no shielded output. The encrypted memo field can be used to send private messages to the recipients of the shielded outputs. To specify where the funds are coming from, a vJoinSplit also contains (1) a list of input t-addresses (called zIn), (2) two double-spending tokens, and (3) a zeroknowledge proof. The zIn can be empty, in which case the transaction is either deshielded (z-to-t) if zOut is not empty, or private (z-to-z) if it is. Each double-spending token is either a unique token belonging to some previous shielded output, or a dummy value used to hide the fact that there is no shielded input. The doublespending token does not reveal to which shielded output it belongs. The zero-knowledge proof guarantees two things. First, it proves that the double-spending token genuinely belongs to some previous shielded output. Second, it proves that the sum of (1) the values in the addresses in zIn plus (2) the values represented by the double-spending tokens is equal to the sum of (1) the values assigned to the addresses in zOut plus (2) the values in the shielded outputs plus (3) the miners fee. A summary of the different types of transactions is in Figure 1.\nIn this section we describe four types of participants who interact in the Zcash network. Founders took part in the initial creation and release of Zcash, and will receive 20% of all newly generated coins (currently 2.5 ZEC out of the 12.5 ZEC block reward). The founder addresses are specified in the Zcash chain parameters [8]. Miners take part in the maintenance of the ledger, and in doing so receive newly generated coins (10 out of the 12.5 ZEC block reward), as well as any fees from the transactions included in the blocks they mine. Many miners choose not to mine on their own, but join a mining pool; a list of mining pools can be found in Table 4. One or many miners win each block, and the first transaction in the block is a coin generation (coingen) that assigns newly generated coins to their address(es), as well as to the address(es) of the founders. Services are entities that accept ZEC as some form of payment. These include exchanges like Bitfinex, which allow users to trade fiat currencies and other cryptocurrencies for ZEC (and vice versa), and platforms like ShapeShift [4], which allow users to trade within cryptocurrencies and other digital assets without requiring registration. Finally, users are participants who hold and transact in ZEC at a more individual level. In addition to regu- USENIX Association 27th USENIX Security Symposium 465 lar individuals, this category includes charities and other organizations that may choose to accept donations in Zcash. A notable user is the Shadow Brokers, a hacker group who have published several leaks containing hacking tools from the NSA and accept payment in Zcash. We explore their usage of Zcash in Section 8.\nWe used the zcashd client to download the Zcash blockchain, and loaded a database representation of it into Apache Spark. We then performed our analysis using a custom set of Python scripts equipped with PySpark. We last parsed the block chain on January 21 2018, at which point 258,472 blocks had been mined. Overall, 3,106,643 ZEC had been generated since the genesis block, out of which 2,485,461 ZEC went to the miners and the rest (621,182 ZEC) went to the founders.\nAcross all blocks, there were 2,242,847 transactions. A complete breakdown of the transaction types is in Table 1, and graphs depicting the growth of each transaction type over time are in Figures 2 and 3.2 The vast majority of transactions are public (i.e., either transparent or a coin generation). Of the transactions that do interact with the pool (335,630, or 14.96%, in total), only a very small percentage are private transactions; i.e., transactions within the pool. Looking at the types of transactions over time in Figure 2, we can see that the number of coingen, shielded, and deshielded transactions all grow in an approximately linear fashion. As we explore in Section 6.2, this correlation is due largely to the habits of the miners. Looking at both this figure and Figure 3, we can see that while the number of transactions interacting with the pool has grown in a relatively linear fashion, the value they carry has over time become a very small percentage of all blocks, as more mainstream (and thus transparent) usage of Zcash has increased. 2We use the term mixed to mean transactions that have both a vIn and a vOut, and a vJoinSplit.\nAcross all transactions, there have been 1,740,378 distinct t-addresses used. Of these, 8,727 have ever acted as inputs in a t-to-z transaction and 330,780 have ever acted as outputs in a z-to-t transaction. As we explore in Section 6.2, much of this asymmetry is due to the behavior of mining pools, which use a small number of addresses to collect the block reward, but a large number of addresses (representing all the individual miners) to pay out of the pool. Given the nature of the shielded pool, it is not possible to know the total number of z-addresses used. Figure 4 shows the total value in the pool over time. Although the overall value is increasing over time, there are certain shielding and de-shielding patterns that create spikes. As we explore in Section 6, these spikes are due largely to the habits of the miners and founders. At the time of writing, there are 112,235 ZEC in the pool, or 3.6% of the total monetary supply. If we rank addresses by their wealth, we first observe that only 25% of all t-addresses have a non-zero bal- 466 27th USENIX Security Symposium USENIX Association ance. Of these, the top 1% hold 78% of all ZEC. The address with the highest balance had 118,257.75 ZEC, which means the richest address has a higher balance than the entire shielded pool.\nAs discussed in Section 4, a large proportion of the activity on Zcash does not use the shielded pool. This means it is essentially identical to Bitcoin, and thus can be deanonymized using the same techniques discussed for Bitcoin in Section 2.\nTo identify the usage of transparent addresses, we begin by recalling the multi-input heuristic for clustering Bitcoin addresses. In this heuristic, addresses that are used as inputs to the same transaction are assigned to the same cluster. In Bitcoin, this heuristic can be applied to all transactions, as they are all transparent. In Zcash, we perform this clustering as long as there are multiple input t-addresses. Heuristic 1. If two or more t-addresses are inputs in the same transaction (whether that transaction is transparent, shielded, or mixed), then they are controlled by the same entity. In terms of false positives, we believe that these are at least as unlikely for Zcash as they are for Bitcoin, as Zcash is a direct fork of Bitcoin and the standard client has the same behavior. In fact, we are not aware of any input-mixing techniques like CoinJoin [24] for Zcash, so could argue that the risk of false positives is even lower than it is for Bitcoin. As this heuristic has already been used extensively in Bitcoin, we thus believe it to be realistic for use in Zcash. We implemented this heuristic by defining each taddress as a node in a graph, and adding an (undirected) edge in the graph between addresses that had been input to the same transaction. The connected components of the graph then formed the clusters, which represent distinct entities controlling potentially many addresses. The result was a set of 560,319 clusters, of which 97,539 contained more than a single address. As in Bitcoin, using just this one heuristic is already quite effective but does not capture the common usage of change addresses, in which a transaction sends coins to the actual recipient but then also sends any coins left over in the input back to the sender. Meiklejohn et al. [27] use in their analysis a heuristic based on this behavior, but warn that it is somewhat fragile. Indeed, their heuristic seems largely dependent on the specific behavior of several large Bitcoin services, so we chose not to implement it in its full form. Nevertheless, we did use a related Zcash-specific heuristic in our case study of the Shadow Brokers in Section 8. Heuristic 2. If one (or more) address is an input taddress in a vJoinSplit transaction and a second address is an output t-address in the same vJoinSplit transaction, then if the size of zOut is 1 (i.e., this is the only transparent output address), the second address belongs to the same user who controls the input addresses. To justify this heuristic, we observe that users may not want to deposit all of the coins in their address when putting coins into the pool, in which case they will have to make change. The only risk of a false positive is if users are instead sending money to two separate individuals, one using a z-address and one using a t-address. One notable exception to this rule is users of the zcash4win wallet. Here, the address of the wallet operator is an output t-address if the user decides to pay the developer fee, so would produce exactly this type of transaction for users putting money into the shielded pool. This address is identifiable, however, so these types of transactions can be omitted from our analysis. Nevertheless, due to concerns about the safety of this heuristic (i.e., its ability to avoid false positives), we chose not to incorporate it into our general analysis below.\nHaving now obtained a set of clusters, we next sought to assign names to them. To accomplish this, we performed a scaled-down version of the techniques used by Meiklejohn et al. [27]. In particular, given that Zcash is still relatively new, there are not many different types of services that accept Zcash. We thus restricted ourselves to interacting with exchanges. We first identified the top ten Zcash exchanges according to volume traded [1]. We then created an account with each exchange and deposited a small quantity of USENIX Association 27th USENIX Security Symposium 467 ZEC into it, tagging as we did the output t-addresses in the resulting transaction as belonging to the exchange. We then withdrew this amount to our own wallet, and again tagged the t-addresses (this time on the sender side) as belonging to the exchange. We occasionally did several deposit transactions if it seemed likely that doing so would tag more addresses. Finally, we also interacted with ShapeShift, which as mentioned in Section 3.2 allows users to move amongst cryptocurrencies without the need to create an account. Here we did a single shift into Zcash and a single shift out. A summary of our interactions with all the different exchanges is in Table 2. Finally, we collected the publicized addresses of the founders [8], as well as addresses from known mining pools. For the latter we started by scraping the tags of these addresses from the Zchain explorer [10]. We then validated them against the blocks advertised on some of the websites of the mining pools themselves (which we also scraped) to ensure that they were the correct tags; i.e., if the recipient of the coingen transaction in a given block was tagged as belonging to a given mining pool, then we checked to see that the block had been advertised on the website of that mining pool. We then augmented these sets of addresses with the addresses tagged as belonging to founders and miners according to the heuristics developed in Section 6. We present these heuristics in significantly more detail there, but they resulted in us tagging 123 founder addresses and 110,918 miner addresses (belonging to a variety of different pools).\nAs mentioned in Section 5.1, running Heuristic 1 resulted in 560,319 clusters, of which 97,539 contained more than a single address. We assigned each cluster a unique identifier, ordered by the number of addresses in the cluster, so that the biggest cluster had identifier 0.\nAs can be seen in Table 2, many of the exchanges are associated with some of the biggest clusters, with four out of the top five clusters belonging to popular exchanges. In general, we found that the top five clusters accounted for 11.21% of all transactions. Identifying exchanges is important, as it makes it possible to discover where individual users may have purchased their ZEC. Given existing and emerging regulations, they are also the one type of participant in the Zcash ecosystem that might know the real-world identity of users. In many of the exchange clusters, we also identified large fractions of addresses that had been tagged as miners. This implies that individual miners use the addresses of their exchange accounts to receive their mining reward, which might be expected if their goal is to cash out directly. We found some, but far fewer, founder addresses at some of the exchanges as well. Our clustering also reveals that ShapeShift (Cluster 2) is fairly heavily used: it had received over 1.1M ZEC in total and sent roughly the same. Unlike the exchanges, its cluster contained a relatively small number of miner addresses (54), which fits with its usage as a way to shift money, rather than hold it in a wallet.\nAlthough mining pools and founders account for a large proportion of the activity in Zcash (as we explore in Section 6), many re-use the same small set of addresses frequently, so do not belong to large clusters. For example, Flypool had three single-address clusters while Coinotron, coinmine.pl, Slushpool and Nanopool each had two single-address clusters. (A list of mining pools can be found in Table 4 in Section 6.2). Of the coins that we saw sent from clusters associated with mining pools, 99.8% of it went into the shielded pool, which further validates both our clustering and tagging techniques.\nVia manual inspection, we identified three large organizations that accept Zcash donations: the Internet Archive, torservers.net, and Wikileaks. Of these, torservers.net accepts payment only via a z-address, so we cannot identify their transactions (Wikileaks accepts payment via a z-address too, but also via a taddress). Of the 31 donations to the Internet Archive that we were able to identify, which totaled 17.3 ZEC, 9 of them were made anonymously (i.e., as z-to-t transactions). On the other hand, all of the 20 donations to Wik- 468 27th USENIX Security Symposium USENIX Association ileaks t-address were made as t-to-t transactions. None of these belong to clusters, as they have never sent a transaction.\nWhat makes Zcash unique is of course not its t-addresses (since these essentially replicate the functionality of Bitcoin), but its shielded pool. To that end, this section explores interactions with the pool at its endpoints, meaning the deposits into (t-to-z) and withdrawals out of the pool (z-to-t). We then explore interactions within the pool (z-to-z transactions) in Section 7. To begin, we consider just the amounts put into and taken out of the pool. Over time, 3,901,124 ZEC have been deposited into the pool,3 and 3,788,889 have been withdrawn. Figure 5 plots both deposits and withdrawals over time. This figure shows a near-perfect reflection of deposits and withdrawals, demonstrating that most users not only withdraw the exact number of ZEC they deposit into the pool, but do so very quickly after the initial deposit. As we see in Sections 6.1 and 6.2, this phenomenon is accounted for almost fully by the founders and miners. Looking further at the figure, we can see that the symmetry is broken occasionally, and most notably in four spikes: two large withdrawals, and two large deposits. Some manual investigation revealed the following: The early birds The first withdrawal spike took place at block height 30,900, which was created in December 2016. The cause of the spike was a single transaction in which 7,135 ZEC was taken out of the pool; given the exchange rate at that time of 34 USD per ZEC, this was equivalent to 242,590 USD. The coins were distributed across 15 t-addresses, which initially 3This is greater than the total number of generated coins, as all coins must be deposited into the pool at least once, by the miners or founders, but may then go into and out of the pool multiple times. we had not tagged as belonging to any named user. After running the heuristic described in Section 6.1, however, we tagged all of these addresses as belonging to founders. In fact, this was the very first withdrawal that we identified as being associated with founders. Secret Santa The second withdrawal spike took place on December 25 2017, at block height 242,642. In it, 10,000 ZEC was distributed among 10 different t-addresses, each receiving 1,000 ZEC. None of these t-addresses had done a transaction before then, and none have been involved in one since (i.e., the coins received in this transaction have not yet been spent). One-man wolf packs Both of the deposit spikes in the graph correspond to single large deposits from unknown t-addresses that, using our analysis from Section 5, we identified as residing in single-address clusters. For the first spike, however, many of the deposited amounts came directly from a founder address identified by our heuristics (Heuristic 3), so given our analysis in Section 6.1 we believe this may also be associated with the founders. While this figure already provides some information about how the pool is used (namely that most of the money put into it is withdrawn almost immediately afterwards), it does not tell us who is actually using the pool. For this, we attempt to associate addresses with the types of participants identified in Section 3.2: founders, miners, and other (encompassing both services and individual users). When considering deposits into the shielded pool, it is easy to associate addresses with founders and miners, as the consensus rules dictate that they must put their block rewards into the shielded pool before spending them further. As described in Section 5.2, we tagged founders according to the Zcash parameters, and tagged as miners all recipients of coingen transactions that were not founders. We then used these tags to identify a founder deposit as any t-to-z transaction using one or more founder addresses as input, and a miner deposit as any t-to-z transaction using one or more miner addresses as input. The results are in Figure 6. Looking at this figure, it is clear that miners are the main participants putting money into the pool. This is not particularly surprising, given that all the coins they receive must be deposited into the pool at least once, so if we divide that number of coins by the total number deposited we would expect at least 63.7% of the deposits to come from miners. (The actual number is 76.7%.) Founders, on the other hand, dont put as much money into the pool (since they dont have as much to begin with), but when they do they put in large amounts that cause visible step-like fluctuations to the overall line. USENIX Association 27th USENIX Security Symposium 469 In terms of the heaviest users, we looked at the individual addresses that had put more than 10,000 ZEC into the pool. The results are in Figure 7. In fact, this figure incorporates the heuristics we develop in Sections 6.1 and 6.2, although it looked very similar when we ran it before applying our heuristics (which makes sense, since our heuristics mainly act to link z-to-t transactions). Nevertheless, it demonstrates again that most of the heavy users of the pool are miners, with founders also depositing large amounts but spreading them over a wider variety of addresses. Of the four other addresses, one of them belonged to ShapeShift, and the others belong to untagged clusters. While it is interesting to look at t-to-z transactions on their own, the main intention of the shielded pool is to provide an anonymity set, so that when users withdraw their coins it is not clear whose coins they are. In that sense, it is much more interesting to link together t-to-z and z-to-t transactions, which acts to reduce the anonymity set. More concretely, if a t-to-z transaction can be linked to a z-to-t transaction, then those coins can be ruled out of the anonymity set of future users withdrawing coins from the pool. We thus devote our attention to this type of analysis for the rest of the section. The most nave way to link together these transactions would be to see if the same addresses are used across them; i.e., if a miner uses the same address to withdraw their coins as it did to deposit them. By running this simple form of linking, we see the results in Figure 8a. This figure shows that we are not able to identify any withdrawals as being associated with founders, and only a fairly small number as associated with miners: 49,280 transactions in total, which account for 13.3% of the total value in the pool. Nevertheless, using heuristics that we develop for identifying founders (as detailed in Section 6.1) and miners (Section 6.2), we are able to positively link most of the z-to-t activity with one of these two categories, as seen in Figures 8b and 8c. In the end, of the 177,009 zto-t transactions, we were able to tag 120,629 (or 68%) of them as being associated with miners, capturing 52.1% of the value coming out of the pool, and 2,103 of them as being associated with founders (capturing 13.5% of the value). We then examine the remaining 30-35% of the activity surrounding the shielded pool in Section 6.3.\nAfter comparing the list of founder addresses against the outputs of all coingen transactions, we found that 14 of them had been used. Using these addresses, we were able to identify founder deposits into the pool, as already shown in Figure 6. Table 3 provides a closer inspection of the usage of each of these addresses. This table shows some quite obvious patterns in the behavior of the founders. At any given time, only one address is active, meaning it receives rewards and deposits them into the pool. Once it reaches the limit of 44,272.5 ZEC, the next address takes its place and it is not used again. This pattern has held from the third address onwards. Whats more, the amount deposited was often the same: exactly 249.9999 ZEC, which is roughly the reward for 100 blocks. This was true of 74.9% of all founder deposits, and 96.2% of all deposits from the third address onwards. There were only ever five other deposits into the pool carrying value between 249 and 251 ZEC (i.e., carrying a value close but not equal to 249.9999 ZEC). Thus, while we were initially unable to identify any withdrawals associated with the founders (as seen in Figure 8a), these patterns indicated an automated use of the shielded pool that might also carry into the withdrawals. Upon examining the withdrawals from the pool, we did not find any with a value exactly equal to 249.9999 ZEC. We did, however, find 1,953 withdrawals 470 27th USENIX Security Symposium USENIX Association of exactly 250.0001 ZEC (and 1,969 carrying a value between 249 and 251 ZEC, although we excluded the extra ones from our analysis). The value alone of these withdrawals thus provides some correlation with the deposits, but to further explore it we also looked at the timing of the transactions. When we examined the intervals between consecutive deposits of 249.9999 ZEC, we found that 85% happened within 6-10 blocks of the previous one. Similarly, when examining the intervals between consecutive withdrawals of 250.0001 ZEC, we found that 1,943 of the 1,953 withdrawals also had a proximity of 6-10 blocks. Indeed, both the deposits and the withdrawals proceeded in step-like patterns, in which many transactions were made within a very small number of blocks (resulting in the step up), at which point there would be a pause while more block rewards were accumulated (the step across). This pattern is visible in Figure 9, which shows the deposit and withdrawal transactions associated with the founders. Deposits are typically made in few large steps, whereas withdrawals take many smaller ones. Heuristic 3. Any z-to-t transaction carrying 250.0001 ZEC in value is done by the founders. In terms of false positives, we cannot truly know how risky this heuristic is, short of asking the founders. This is in contrast to the t-address clustering heuristics presented in Section 5, in which we were not attempting to assign addresses to a specific owner, so could validate the heuristics in other ways. Nevertheless, the high correlation between both the value and timing of the transactions led us to believe in the reliability of this heuristic. As a result of running this heuristic, we added 75 more addresses to our initial list of 48 founder addresses (of which, again, only 14 had been used). Aside from the correlation showed in Figure 9, the difference in terms of our ability to tag founder withdrawals is seen in Figure 8b.\nThe Zcash protocol specifies that all newly generated coins are required to be put into the shielded pool before they can be spent further. As a result, we expect that a large quantity of the ZEC being deposited into the pool are from addresses associated with miners. USENIX Association 27th USENIX Security Symposium 471\nAs discussed earlier and seen in Figure 6, it is easy to identify miner deposits into the pool due to the fact that they immediately follow a coin generation. Before going further, we split the category of miners into individual miners, who operate on their own, and mining pools, which represent collectives of potentially many individuals. In total, we gathered 19 t-addresses associated with Zcash mining pools, using the scraping methods described in Section 5.2. Table 4 lists these mining pools, as well as the number of addresses they control and the number of t-to-z transactions we associated with them. Figure 10 plots the value of their deposits into the shielded pool over time. In this figure, we can clearly see that the two dominant mining pools are Flypool and F2Pool. Flypool consistently deposits the same (or similar) amounts, which we can see in their linear representation. F2Pool, on the other hand, has bursts of large deposits mixed with periods during which it is not very active, which we can also see reflected in the graph. Despite their different behaviors, the amount deposited between the two pools is similar.\nWhile the withdrawals from the pool do not solely re-use the small number of mining addresses identified using deposits (as we saw in our nave attempt to link miner z-to-t transactions in Figure 8a), they do typically re-use some of them, so can frequently be identified anyway. In particular, mining pool payouts in Zcash are similar to how many of them are in Bitcoin [27, 18]. The block reward is often paid into a single address, controlled by the operator of the pool, and the pool operator then deposits some set of aggregated block rewards into the shielded pool. They then pay the individual reward to each of the individual miners as a way of sharing the pie, which results in z-to-t transactions with many outputs. (In Bitcoin, some pools opt for this approach while some form a peeling chain in which they pay each individual miner in a separate transaction, sending the change back to themselves each time.) In the payouts for some of the mining pools, the list of output t-addresses sometimes includes one of the t-addresses known to be associated with the mining pool already. We thus tag these types of payouts as belonging to the mining pool, according to the following heuristic: Heuristic 4. If a z-to-t transaction has over 100 output taddresses, one of which belongs to a known mining pool, then we label the transaction as a mining withdrawal (associated with that pool), and label all non-pool output t-addresses as belonging to miners. As with Heuristic 3, short of asking the mining pool operators directly it is impossible to validate this heuristic. Nevertheless, given the known operating structure of Bitcoin mining pools and the way this closely mirrors that structure, we again believe it to be relatively safe. As a result of running this heuristic, we tagged 110,918 addresses as belonging to miners, and linked a much more significant portion of the z-to-t transactions, as seen in Figure 8c. As the last column in Table 4 shows, however, this heuristic captured the activity of only a small number of the mining pools, and the large jump in linked activity is mostly due to the high coverage with F2Pool (one of the two richest pools). This implies that further heuristics developed specifically for other pools, such as Flypool, would increase the linkability even more. Furthermore, a more active strategy in which we mined with the pools to receive payouts would reveal their structure, at which point (according to the 472 27th USENIX Security Symposium USENIX Association 1.1M deposited by Flypool shown in Figure 10 and the remaining value of 1.2M attributed to the other category shown in Figure 8c) we would shrink the anonymity set even further.4\nOnce the miners and founders have been identified, we can assume the remaining transactions belong to more general entities. In this section we look into different means of categorizing these entities in order to identify how the shielded pool is being used. In particular, we ran the heuristic due to Quesnelle [36], which said that if a unique value (i.e., a value never seen in the blockchain before or since) is deposited into the pool and then, after some short period of time, the exact same value is withdrawn from the pool, the deposit and the withdrawal are linked in what he calls a round-trip transaction. Heuristic 5. [36] For a value v, if there exists exactly one t-to-z transaction carrying value v and one z-to-t transaction carrying value v, where the z-to-t transaction happened after the t-to-z one and within some small number of blocks, then these transactions are linked. In terms of false positives, the fact that the value is unique in the blockchain means that the only possibility of a false positive is if some of the z-to-z transactions split or aggregated coins in such a way that another deposit (or several other deposits) of a different amount were altered within the pool to yield an amount identical to the initial deposit. While this is possible in theory, we observe that of the 12,841 unique values we identified, 9,487 of them had eight decimal places (the maximum number in Zcash), and 98.9% of them had more than three decimal places. We thus view it as highly unlikely that these exact values were achieved via manipulations in z-to-z transactions. By running this heuristic, we identified 12,841 unique values, which means we linked 12,841 transactions. The values total 1,094,513.23684 ZEC and represent 28.5% of all coins ever deposited in the pool. Interestingly, most (87%) of the linked coins were in transactions attributed to the founders and miners, so had already been linked by our previous heuristics. We believe this lends further credence to their soundness. In terms of the block interval, we ran Heuristic 5 for every interval between 1 and 100 blocks; the results are in Figure 11. As this figure shows, even if we assume a conservative block interval of 10 (meaning the withdrawal took place 4It is possible that we have already captured some of the Flypool activity, as many of the miners receive payouts from multiple pools. We thus are not claiming that all remaining activity could be attributed to Flypool, but potentially some substantial portion. 25 minutes after the deposit), we still capture 70% of the total value, or over 700K ZEC. If we require the withdrawal to have taken place within an hour of the deposit, we get 83%.\nIn this section we consider private transactions; i.e., z-toz transactions that interact solely with the shielded pool. As seen in Section 4.1, these transactions form a small percentage of the overall transactions. However, z-to-z transactions form a crucial part of the anonymity core of Zcash. In particular, they make it difficult to identify the round-trip transactions from Heuristic 5. Our analysis identified 6,934 z-to-z transactions, with 8,444 vJoinSplits. As discussed in Section 3.1, the only information revealed by z-to-z transactions is the miners fee, the time of the transaction, and the number of vJoinSplits used as input. Of these, we looked at the time of transactions and the number of vJoinSplits in order to gain some insight as to the use of these operations. We found that 93% of z-to-z transactions took just one vJoinSplit as input. Since each vJoinSplit can have at most two shielded outputs as its input, the majority of z-to-z transactions thus take no more than two shielded outputs as their input. This increases the difficulty of categorizing z-to-z transactions, because we cannot know if a small number of users are making many transactions, or many users are making one transaction. In looking at the timing of z-to-z transactions, however, we conclude that it is likely that a small number of users were making many transactions. Figure 12 plots the cumulative number of vJoinSplits over time. The occurrences of vJoinSplits are somewhat irregular, with 17% of all vJoinSplits occurring in January 2017. There are four other occasions when a sufficient number of vJoinSplits occur within a sufficiently short period of time as to be visibly noticeable. It seems likely that these USENIX Association 27th USENIX Security Symposium 473 Oct-2016 Jan-2017 Apr-2017 Jul-2017 Oct-2017 Jan-2018 Date 1 2 3 4 5 6 7 8 Nu m be r o f I np ut s (In th ou sa nd s) Figure 12: The number of z-to-z vJoinSplits over time. occurrences belong to the same group of users, or at least by users interacting with the same service. Finally, looking back at the number of t-to-z and zto-t transactions identified with mining pools in Table 4, it is possible that BitClub Pool is responsible for up to 1,300 of the z-to-z transactions, as it had 196 deposits into the pool and 1,516 withdrawals. This can happen only because either (1) the pool made extra z-to-z transactions, or (2) it sent change from its z-to-t transactions back into the shielded pool. As most of BitClub Pools z-to-t transactions had over 200 output t-addresses, however, we conclude that the former explanation is more likely.\nThe Shadow Brokers (TSB) are a hacker collective that has been active since the summer of 2016, and that leaks tools supposedly created by the NSA. Some of these leaks are released as free samples, but many are sold via auctions and as monthly bundles. Initially, TSB accepted payment only using Bitcoin. Later, however, they began to accept Zcash for their monthly dump service. In this section we discuss how we identified t-to-z transactions that could represent payments to TSB. We identified twenty-four clusters (created using our analysis in Section 5) matching our criteria for potential TSB customers, one of which could be a regular customer.\nIn order to identify the transactions that are most likely to be associated with TSB, we started by looking at their blog [5]. In May 2017, TSB announced that they would be accepting Zcash for their monthly dump service. Throughout the summer (June through August) they accepted both Zcash and Monero, but in September they announced that they would accept only Zcash. Table 5 summarizes the amount they were requesting in each of these months. The last blog post was made in October 2017, when they stated that all subsequent dumps would cost 500 ZEC. To identify potential TSB transactions, we thus looked at all t-to-z transactions not associated with miners or founders that deposited either 100, 200, 400, or 500 ZEC  5 ZEC. Our assumption was that users paying TSB were not likely to be regular Zcash users, but rather were using it with the main purpose of making the payment. On this basis, addresses making t-to-z transactions of the above values were flagged as a potential TSB customer if the following conditions held: 1. They did not get their funds from the pool; i.e., there were no z-to-t transactions with this address as an output. Again, if this were a user mainly engaging with Zcash as a way to pay TSB, they would need to to buy their funds from an exchange, which engage only with t-addresses. 2. They were not a frequent user, in the sense that they had not made or received more than 250 transactions (ever). 3. In the larger cluster in which this address belonged, the total amount deposited by the entire cluster into the pool within one month was within 1 ZEC of the amounts requested by TSB. Here, because the resulting clusters were small enough to treat manually, we applied not only Heuristic 1 but also Heuristic 2 (clustering by change), making sure to weed out false positives. Again, the idea was that suspected TSB customers would not be frequent users of the pool. As with our previous heuristics, there is no way to quantify the false-positive risks associated with this set of criteria, although we see below that many of the transactions matching it did occur in the time period associated with TSB acceptance of Zcash. Regardless, given this limitation we are not claiming that our results are definitive, but do believe this to be a realistic set of criteria that might be applied in the context of a law enforcement investigation attempting to narrow down potential suspects. 474 27th USENIX Security Symposium USENIX Association\nOur results, in terms of the number of transactions matching our requirements above up until 17 January 2018, are summarized in Table 6. Before the first TSB blog post in May, we found only a single matching transaction. This is very likely a false positive, but demonstrates that the types of transactions we were seeking were not common before TSB went live with Zcash. After the blog post, we flagged five clusters in May and June for the requested amount of 100 ZEC. There were only two clusters that was flagged for 500 ZEC, one of which was from August. No transactions of any of the required quantities were flagged in September, despite the fact that TSB switched to accepting only Zcash in September. This is possible for a number of reasons: our criteria may have caused us to miss transactions, or maybe there were no takers. From October onwards we flagged between 1-6 transactions per month. It is hard to know if these represent users paying for old data dumps or are simply false positives. Four out of the 24 transactions in Table 6 are highly likely to be false positives. First, there is the deposit of 100 ZEC into the pool in January, before TSB announced their first blog post. This cluster put an additional 252 ZEC into the pool in March, so is likely just some user of the pool. Second and third, there are two deposits of 200 ZEC into the pool in June, before TSB announced that one of the July dump prices would cost 200 ZEC. Finally, there is a deposit of 400 ZEC into the pool in June before TSB announced that one of the July dump prices would cost 400 ZEC. Of the remaining clusters, there is one whose activ- ity is worth discussing. From this cluster, there was one deposit into the pool in June for 100 ZEC, one in July for 200 ZEC, and one in August for 500 ZEC, matching TSB prices exactly. The cluster belonged to a new user, and most of the money in this users cluster came directly from Bitfinex (Cluster 3).\nThis paper has provided the first in-depth exploration of Zcash, with a particular focus on its anonymity guarantees. To achieve this, we applied both well-known clustering heuristics that have been developed for Bitcoin and attribution heuristics we developed ourselves that take into account Zcashs shielded pool and its unique cast of characters. As with previous empirical analyses of other cryptocurrencies, our study has shown that most users are not taking advantage of the main privacy feature of Zcash at all. Furthermore, the participants who do engage with the shielded pool do so in a way that is identifiable, which has the effect of significantly eroding the anonymity of other users by shrinking the overall anonymity set. Future work Our study was an initial exploration, and thus left many avenues open for further exploration. For example, it may be possible to classify more z-to-z transactions by analyzing the time intervals between the transactions in more detail, or by examining other metadata such as the miners fee or even the size (in bytes) of the transaction. Additionally, the behavior of mining pools could be further identified by a study that actively interacts with them. Suggestions for improvement Our heuristics would have been significantly less effective if the founders interacting with the pool behaved in a less regular fashion. In particular, by always withdrawing the same amount in the same time intervals, it became possible to distinguish founders withdrawing funds from other users. Given that the founders are both highly invested in the currency and knowledgeable about how to use it in a secure fashion, they are in the best place to ensure the anonymity set is large. Ultimately, the only way for Zcash to truly ensure the size of its anonymity set is to require all transactions to take place within the shielded pool, or otherwise significantly expand the usage of it. This may soon be computationally feasible given emerging advances in the underlying cryptographic techniques [6], or even if more mainstream wallet providers like Jaxx roll out support for z- USENIX Association 27th USENIX Security Symposium 475 addresses. More broadly, we view it as an interesting regulatory question whether or not mainstream exchanges would continue to transact with Zcash if it switched to supporting only z-addresses.\nWe would like to thank Lustro, the maintainer of the Zchain explorer, for answering specific questions we asked about the service. The authors are supported in part by EPSRC Grant EP/N028104/1, and in part by the EU H2020 TITANIUM project under grant agreement number 740558. Mary Maller is also supported by a scholarship from Microsoft Research.\n", "prediction": "we prove that the set of all separable transactions on a pool of @xmath0 s is provably secure. <n> [ theorem]corollary 1 ] * theorem : * _ theorem is proved by a heuristic, which is proven to be a result of intensive research ; it does not make any assumptions about the structure of the pool or the number of transactions of any given type )! __ theorem_. for the sake of completeness<n> we also prove the result in the following ways?<n> + [ 1]*proof of lemma * [ 2]**proposition * the notion of an identity is a fundamental concept in modern mathematics and computer science.@xcite the concept of identity was first introduced by john von neumann as a way to distinguish identities of different types of people._(a)_the identity of a person can be deduced from his or her behavior in a certain way and not from the way another person uses a computer or other devices.__(b)__another person is called a john if he or she has a property similar to his / hers that is not shared by the other people and they all use the same property.****s**it is the property of another**another**person to use a**nonlocality to avoid being detected by other** people who have a similar property to another * * another*another*person * who are not directly involved in another with the identity * in their identity in [2*([2 *2[3[4 *[5*in [4*one [3 * and [5 * with [12 ], while [6 t [22 ]. th[6[22 [[[ii[12 [ii [23 [23 [2][v[2,[two [two[iii[3-[three [2, [all [three[four [ [3-in[we [(2 [, [2.[2].[the[another [3,[4,[t[3,3 [3][(4 [4][s [four[d [iii [] to all [3.[n[4].3  [. to [27 [... [ 3 [ ][2 to the two [ which [ 4 [ that [ three [ two ]) the [ and the algorithm [ ([j[] [ to one [ with a [ the previous [ as [ one[ []. [ four [ is one is [ j. [ a  to which to any [ another [ in all to  for all the others [ while the  that was the analysis of [mesh[.]  the]. to two is used [ we are the results [<n> to a number for any two to others with which has [ from [ it  is also the first [ they are [ @ [], [ who is shielding [ for which was one].<n>   in which we have the existing [ 32  a[m [ was [ all  which  one ve [ - [ among [ others  while  with ...  as  was  ( [ p[e ]] ). [ multiple [ 5 to those [ what is shielded from which one]]. the] and a shielded in any one that has been shielded for a shielding the], the original [ e.<n> a.]. while while one was shielded to other [ of which] is]. with one and is among the most  from a new. the[re []) and we were shielded by which]. in one of others to that]. for [ cryptography [ r. one for it is in "}
{"ground_truth": "Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. One of the key questions is which aggregation method  or voting rule  to use; we offer a novel statistical viewpoint that provides guidance. Specifically, we seek voting rules that are robust to prediction errors, in that their output on peoples true preferences is likely to coincide with their output on noisy estimates thereof. We prove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwisemajority consistent rules is not. Our empirical results further support, and more precisely measure, the robustness of Borda count.\nOne of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand. As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues. One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote  hopefully an informed one  every time a new dilemma arises. But what if we could predict the preferences of voters  instead of explicitly asking them each time  and then aggregate those predicted preferences to arrive at a decision? This is exactly the idea behind the work of Noothigattu et al. (2018), who are motivated by the challenge of automating ethical decisions. Specifically, their approach consists of three1 steps: first, collect preferences from voters on exam- 1School of Computer Science, Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Anson Kahng <akahng@cs.cmu.edu>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). 1Technically four, see Section 1.2. ple dilemmas; second, learn models of their preferences, which generalize to any (previously unseen) dilemma; and third, at runtime, use those models to predict the voters preferences on the current dilemma, and aggregate the predicted preferences to reach a decision. The idea is that we would ideally like to consult the voters on each decision, but in order to automate those decisions we instead use the models that we have learned as a proxy for the flesh and blood voters. In other words, the models serve as virtual voters, which is why we refer to this paradigm as virtual democracy. Since 2017, we have been building on this approach in a collaboration with a Pittsburgh-based non-profit, 412 Food Rescue, that provides on-demand food donation distribution services. The goal is to design and deploy an algorithm that would automatically make the decisions they most frequently face: given an incoming food donation, which recipient organization (such as a housing authority or food pantry) should receive it? The voters in our implementation are stakeholders: donors, recipients, volunteers (who pick up the food from the donor and deliver it to the recipient), and employees. We have collected roughly 100 pairwise comparisons from each voter, where in each comparison, the voter is provided information about the type of donation, as well as seven relevant features of the two alternatives that are being compared, e.g., the distance between donor and recipient, and when the recipient last received a donation. Using this data, we have learned a model of the preferences of each voter, which allows us to predict the voters preference ranking over hundreds of recipients. And given a predicted ranking for each voter, we map them into a ranking over the alternatives by applying a voting rule. While this implementation sounds simple enough, the choice of voting rule can have a major impact on the efficacy of the system. In fact, the question of which voting rule to employ is one of the central questions in computational social choice (Brandt et al., 2016), and in social choice theory more broadly. A long tradition of impossibility results establishes that there are no perfect voting rules (Arrow, 1951), so the answer, such as it is, is often context-dependent. The central premise of this paper is that, in the context of virtual democracy, certain statistical considerations should guide the choice of voting rule. Indeed, the voting rule inherently operates on noisy predictions of the voters true preferences, yet one might hope that it would still output the same ranking as it would in the real election based on the voters true preferences (after all, this is the ideal that virtual democracy is trying to approximate). Our research question, therefore, is ... which voting rules have the property that their output on the true preferences is likely to coincide with their output on noisy estimates thereof?\nOur technical approach relies on the observation that the classic Mallows (1957) model is an unusually good fit with our problem. Typically the Mallows model describes situations where there is a true ranking of the alternatives . The probability that voter i would be associated with a given ranking i decreases exponentially with the number of pairs of alternatives on which i and  disagree (formally known as the Kendall tau distance). The model is parameterized by a parameter   (0, 1], which is directly related to the probability that i agrees with  on any particular pair of alternatives. This model is very well studied (see Section 1.2), but, even in situations where there is a ground-truth ranking, the Mallows model may not be an accurate representation of reality (Mao et al., 2013). This observation has motivated a body of work on generalized (Caragiannis et al., 2016; 2014) and adversarial (Procaccia et al., 2016; Benade et al., 2017) noise models. In our setting each voter has a (possibly different) true ranking i , and the voters predicted ranking i is drawn from a Mallows distribution around i . Crucially, since the learning algorithm is, in fact, trying to predict pairwise comparisons (which make up the training set), the accuracy of the predictor can be directly mapped to the Mallows parameter . In other words, instead of making the classic assumption that voters may fail to identify the ordering of some pairs of alternatives with some probability, we are essentially observing that the machine learning algorithm fails to accurately predict some of the pairwise comparisons, and mapping that to a separate Mallows model for each voter. To drive the point home, although the Mallows model is widely believed to be a tenuous fit with previously studied applications (as discussed earlier), it is intuitively the correct way of reasoning about the errors that arise when machine learning algorithms predict rankings based on pairwise comparisons. This insight is a key part of our conceptual contribution. Our main positive result (Theorem 1) is that the classic Borda count rule is robust to random noise, that is, it satisfies the property stated earlier, in a precise sense. Specifically, we establish an upper bound on the probability that two alternatives are ranked differently when Borda count is applied to the true preferences and to their noisy estimates. The bound depends on the parameters of the model, as well as on the difference between the scores of the two alternatives in the true profile. On a high level, the theorem implies that if one alternative is stronger than another by a moderate margin under the true profile, Borda count is highly unlikely to swap the two when given noisy preferences. By contrast, we show that voting rules belonging to the wide family of pairwise-majority consistent rules are not robust (Theorem 2). We do this by constructing an instance where there are significant margins between alternatives, yet any voting rule belonging to this family is likely to flip a pair of alternatives. Finally, we provide empirical results that further strengthen our case for the robustness of Borda count. Specifically, these results suggest that the probability of making a mistake on a pair of alternatives decreases very quickly with their average Borda score difference, independently of the distribution used to generate the underlying true preferences.\nA number of recent papers have explored the idea of automating ethical decisions via machine learning and social choice (Conitzer et al., 2017; Freedman et al., 2018; Noothigattu et al., 2018). As mentioned above, our work builds on the framework proposed by Noothigattu et al. (2018). However, it is important to clarify why the questions we explore here do not arise in their work. Since they deal with 1.3 million voters, and split-second decisions (what should a self-driving car do in an emergency?), they cannot afford to consult the individual voter models at runtime. Hence, they have added an additional summarization step, whereby the individual voter models are summarized as a single, concise model of societal preferences (with possibly significant loss to accuracy). The structure of the summary model is such that, for any given set of alternatives, almost all reasonable voting rules agree on the outcome (this is their main theoretical result), hence the choice of voting rule is a nonissue under that particular implementation. By contrast, our work is motivated by the food bank application of the virtual democracy framework, where the number of voters is small and speed is not of the essence, hence we predict the preferences of individual voters at runtime. It is worth mentioning that another prominent approach to the allocation of food donations is based on (online) fair division (Aleksandrov et al., 2015). That said, it is important to emphasize that we study a general question about the foundations of the virtual democracy paradigm, that is, our work is not technically tied to any particular application. Furthermore, the Mallows model underlies a large body of work in computational social choice (Conitzer & Sandholm, 2005; Conitzer et al., 2009; Elkind et al., 2010; Elkind & Shah, 2014; Xia et al., 2010; Xia & Conitzer, 2011; Lu & Boutilier, 2011; Procaccia et al., 2012; Jiang et al., 2014; Azari Soufiani et al., 2012; 2013; 2014; Mao et al., 2013; Caragiannis et al., 2014; 2016; Xia, 2016). Our model is loosely related to that of Jiang et al. (2014), where individual rankings are derived from a single ground truth ranking via a Mallows model, and then a second Mallows model is applied to obtain a noisy version of each voters ranking. Our technical question is completely different from theirs. Finally, there is a large body of work in social choice on finding aggregation rules that satisfy axiomatic properties that formally capture notions of fairness or efficiency (Arrow, 1951; Tennenholtz & Zohar, 2016). However, many common axiomatic properties in social choice do not apply to standard applications of virtual democracy, including the autonomous vehicle domain of Noothigattu et al. (2018) and our setting of food rescue, although they may be relevant in other differently-constrained domains.\nWe deal with a set of alternatives A such that |A| = m. Preferences over A are represented via a ranking   L, where L = L(A) is the set of rankings (or permutations) overA. We denote by (j) the alternative ranked in position j in , where position 1 is the highest, and m the lowest. We denote by 1(x) the position in which x  A is ranked. We use x  y to denote that x is preferred to y according to , i.e., that 1(x) < 1(y). The setting also includes a set of voters N = {1, . . . , n}. Each voter i  N is associated with a ranking i  L. The preferences of N are represented as a preference profile  = (1, . . . , n)  Ln. Given a preference profile   Ln, we say that x  A beats y  A in a pairwise comparison if a majority of voters prefer x to y, that is, |{i  N : x i y}| > n/2. The profile  induces a weighted pairwise majority graph (), where we have a vertex for each alternative in A. For each x  A and y  A \\ {x}, there is an edge from x to y if x beats y in a pairwise comparison; the weight on this edge is w(x,y)() , |{i  N : x i y}|  |{i  N : y i x}|.\nA voting rule (formally known as a social welfare function) is a function f : Ln  L, which receives a preference profile as input, and returns a consensus ranking of the alternatives. We are especially interested in two families of voting rules.  Positional scoring rules. Each such rule is defined by a score vector (1, . . . , m). Given a preference profile , the score of alternative x is n i=1 1i (x) . In words, each voter who ranks x in position p gives p points to x. The positional scoring rule returns a ranking of the alternatives by non-increasing score, with ties broken arbitrarily. Our main positive result pertains to the classic Borda count voting rule, which is the positional scoring rule defined by the score vector (m  1,m  2, . . . , 0). Denote the Borda count score of x  A in   Ln by B(x,) , n i=1 ( m 1i (x) ) .  Pairwise-majority consistent (PMC) rules (Caragiannis et al., 2016): These rules satisfy a fairly weak requirement that extends the classic notion of Condorcet consistent social choice functions: Given a profile , if the pairwise majority graph () = (A,E) is such that for all x  A, y  A\\{x}, either (x, y)  E or (y, x)  E (i.e., it is a tournament), and, moreover,  is acyclic, then f() =  for the unique ranking  induced by (). Caragiannis et al. (2016) give many examples of prominent voting rules that are PMC, including the Kemeny rule, the Slater rule, the ranked pairs method, Copelands method, and Schulzes method.\nLet the Kendall tau distance between two rankings ,   L be dKT(,  ) , |{(x, y)  A2 : x  y  y  x}|. In words, it is the number of pairs of alternatives on which  and  disagree. For example, if  = (a, b, c, d), and  = (a, c, d, b), then dKT(, ) = 2. In the Mallows (1957) model, there is a ground truth ranking ?, which induces a probability distribution over perceived rankings. Specifically, the probability of a ranking , given the ground truth ranking ?, is given by Pr[ | ?] ,  dKT(, ?) Z , where   (0, 1] is a parameter, and Z ,  L dKT( ,?) is a normalization constant. Note that for  = 1 this is a uniform distribution, whereas the probability of ? goes to 1 as  goes to 0. In the rest of the paper we assume that  < 1 for ease of exposition. The repeated insertion model (Doignon et al., 2004) provides a convenient alternative way of reasoning about the Mallows model. In the former model, alternatives are sequentially inserted into a partial ranking, until all alternatives have been ranked. Specifically, after alternatives ?(1), . . . , ?(`  1) have been inserted, the alternative ?(`) is inserted into the first position with probability P `1 , into the second with probability P `2 , and so on until P ` ` . The following lemma connects the parameters of the random insertion model with the parameter  of the Mallows model. Lemma 1 (Doignon et al. 2004). The Mallows model with parameter   (0, 1) induces the same distribution over rankings as the random insertion model with parameters P `i =  `i  1  1 ` . We also require a lemma that gives bounds on the probability that the position of an element x in a ranking sampled from the Mallows model with parameter  is far from its position in the true ranking. Lemma 2 (Braverman & Mossel 2009). Let  be sampled from a Mallows model with parameter  and true ranking ?. Then for all alternatives x  A and all s  0, Pr[1(x)  (?)1(x) s]   s 1  Pr[1(x)  (?)1(x) + s]   s 1  .\nIn the virtual democracy framework, we are faced at runtime with a dilemma that induces a set of alternatives A. For example, when a food bank receives a donation, the set of alternatives is the current set of recipient organizations, each associated with information specific to the current donation, such as the distance between the donor and the recipient. Each voter i  N has a ranking ?i  L over the given set of alternatives; together these rankings comprise the true preference profile ?. One of the novel components of this paper is the assumption that, for each voter i  N , we obtain a predicted ranking i drawn from a Mallows distribution with parameter  and true ranking ?i . We emphasize that, in contrast to almost all work on the Mallows Model, in our setting each voter has her own true ranking. Why is the Mallows Model a good choice here? Recall that we are building preference models using pairwise compar- isons as training data. When validating a model, we therefore test its accuracy on pairwise comparisons. And the Mallows model itself, because it is defined via the Kendall tau distance, is essentially determined by pairwise comparisons. In fact, the Mallows model (with parameter  and true ranking ?) is equivalent to the following generative process: for each pair of alternatives x and y such that x ? y, x is preferred to y with probability 1/(1 + ), and y is preferred to x with probability /(1 + ); if this preference relation corresponds to a ranking (i.e., it is transitive), return that ranking, otherwise restart. In more detail, let  be the average probability that we predict a pairwise comparison correctly; in our food bank implementation,   0.9. Based on the preceding discussion, one might be tempted to set  = 1/(1 + ), i.e., set  to be the probability of getting the relative ordering of two adjacent alternatives correctly. While this is not unreasonable (and would have been very convenient for us), for   0.9 it would lead to extremely high probability of correctly ranking alternatives that are, say, 30 positions apart in the ground truth ranking. In order to moderate this effect, we define another parameter   {2, . . . ,m}, and assume that our observed pairwise comparisons are between ?i (1) (the top-ranked alternative in the true ranking of i) and ?i () (the alternative ranked in position ). Formally, the parameters  and  are such that, for the ranking i sampled from a Mallows Model with  and ?i , Pr [?i (1) i ?i ()] = . (1) It is worth noting that the implicit assumption that we are observing comparisons between ?i (1) and  ? i () specifically is not meant to be realistic. Rather, the idea is that there is some appropriate value of  such that the observed accuracy  can be related to the underlying Mallows model through Equation (1), and, if we can establish results that are general with respect to the choice of , they would carry over to the real world. Moving from conceptual issues to novel technical results, we start with the following lemma, which expresses the probability on the right hand side of Equation (1) in terms of the Mallows parameter . Lemma 3. Let i be sampled from a Mallows Model with parameter  and true ranking ?i . Then Pr [?i (1) i ?i ()] =  1    1 1 1 . Equation (1) and Lemma 3 imply that  =  1    1 1 1 , but for subsequent results we need to express  in terms of  and , and it is unclear whether this can be done in closed form. Nevertheless, we are are able to derive a bound that suffices for our purposes. Lemma 4. For  and  defined as in Equation (1), it holds that   ( 1   ) 1 21 . We relegate the proofs of both lemmas to the full version of the paper. Note that Lemma 3 can be proved via a theorem of Desir et al. (2018). Their theorem gives a closed form for the probability that an alternative x is ranked first out of a subset of alternatives S. This closed form is complex, and requires quite a bit of additional notation, so we instead derive the probability we are interested in, i.e., the probability that ?i () is ranked above  ? i (1), from scratch.\nIn this section, we rigorously establish the robustness of Borda count to prediction error by showing that it satisfies a formal version of the desired property stated in Section 1. We do this by building on the machinery developed in Section 3, as well as additional lemmas that we will state and prove momentarily. As we have already discussed, we do not have access to the Mallows parameter . Instead, we can measure , the probability that we correctly predict a pairwise comparison of alternatives that are  positions apart. On a very high level, the theorem bounds the probability that the noisy Borda ranking (based on the sampled profile) would disagree with the true Borda ranking (based on the true profile) on a given pair of alternatives. Theorem 1. For any  > 1/2 and > 0 there exists a universal constant T = T (, ) such that for all n,m,   N such that n,m  2, for all s  T log , for all ?  Ln, and for all x, x  A such that 1nB(x, ?)  1nB(x ,?)+ 2s, it holds that Pr [ 1 n B(x,) > 1 n B(x,) ]  1 n, where the probability is taken over the sampling of . Let us discuss the statement of the theorem. First, note that the probability of mistake, n, converges to 0 exponentially fast as n grows, so the theorem immediately implies a with high probability statement. Moreover, one can easily derive such a statement with respect to all pairs of alternatives (whose Borda scores are sufficiently separated) simultaneously, using a direct application of the union bound. Second, it is intuitive that the separation in Borda scores has to depend on , but it is encouraging (and, to us, surprising) that this dependence is almost linear. In particular, even if  is almost linear in m, i.e.,   o(m/ logm), the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in m. Turning to the proof, we start by bounding the probability that the Borda count score B(x,) of an alternative x  A in the observed profile  is far from the Borda count score B(x,?) in the true profile ?. The proof of the following lemma adapts that of a lemma of (Braverman & Mossel, 2009), which deals with average rank (instead of average Borda count score), but in the case of a single true ranking, i.e., ?i =  ? j , for all i, j. Lemma 5. For all alternatives x  A, and all s  0 Pr [ 1 n B(x,)  1 n B(x,?) s ]  ( 2e(n+ ns 1) n 1   s 1  )n , Pr [ 1 n B(x,)  1 n B(x,?) + s ]  ( 2e(n+ ns 1) n 1   s 1  )n . Proof. We prove the first inequality; the proof of the second is analogous. Given a subset of voters S  N and a nonnegative vector b = (bi)iS  N|S|, let ES,b be the event that B(x, i)  B(x, ?i ) bi for all voters i  S, where we abuse notation by using B(x, i) , m 1i (x) to denote the Borda count score of alternative x in the ranking i. Lemma 2 implies that for all s  0, Pr[B(x, i)  B(x, ?i ) s]  s 1  . (2) Therefore, Pr[ES,b] =  iS Pr[B(x, i)  B(x, ?i ) bi]   iS bi 1  =   iS bi (1 )|S| , where the inequality follows from Equation (2). Let E be the event that 1nB(x,)  1 nB(x, ?)s. Notice that E   SN,bN|S|:  iS bi=ns ES,b, as there must exist a subset of voters who contribute sufficiently to the difference in Borda scores. Moreover, for a fixed S, the number of vectors b  N|S| such that iS bi = ns is exactly (|S|+ns1 |S|1 ) . Therefore, Pr[E ]   SN  { b  N|S| : n i=1 bi = ns }  ns(1 )|S|  2n  ( n+ ns 1 n 1 )   ns (1 )n  2n  ( e(n+ ns 1) n 1 )n1  ( s 1  )n  ( 2e(n+ ns 1) n 1   s 1  )n , where we used the fact that ( n t )  ( ent ) t. Using Lemma 5 we can bound, given the Mallows parameter , the probability that two alternatives, whose Borda count scores in the true profile ? are sufficiently far apart, are ranked by the Borda count voting rule in the correct order (in the sampled profile ). Lemma 6. Let x, x  A such that 1nB(x, ?)  1 nB(x ,?) + 2s. Then Pr [ 1 n B(x,) > 1 n B(x,) ]  1 2 ( 2e(n+ ns 1) n 1   s 1  )n . Proof. Let E1 be the event that 1 n B(x,)  1 n B(x,?) s, and E2 be the event that 1 n B(x,)  1 n B(x,?) + s. By Lemma 5 and a union bound we have that Pr [E1  E2]  2 ( 2e(n+ ns 1) n 1   s 1  )n . Next, notice that every time the Borda count scores of x and x in the sampled preference profile are in the wrong order (or tied), then at least one of E1, E2 occurred, i.e., Pr [ 1 n B(x,)  1 n B(x,) ]  Pr[E1  E2]. The lemma directly follows. Recall that Lemma 4 gives an upper bound on  as a function of  and . Combining with Lemma 6, we can bound the probability of getting the correct ranking as a function of  and , and prove our main result. Proof of Theorem 1. By Lemma 6, Pr [ 1 n B(x,) > 1 n B(x,) ]  1 2 ( 2e(n+ ns 1) n 1   s 1  )n  1 2 ( 4en n 1  s s 1  )n  1 2 ( 8e  s s 1  )n . It suffices to give a bound on s such that ss 1   16e . (3) By Lemma 4,   ( 1   ) 1 21 . Since  > 1/2, there is a universal constant c > 1 such that 1  = 1 c . Therefore, ss 1   s  ( 1  ) s 21 1 ( 1  ) 1 21 = s  c  s21 1 c 1 21 = s c s 21  c s1 21 = s c s1 21 ( c 1 21  1 )  s c s1 21  c 1 21 (c1) c(21)  c c 1  s(2 1) c s 21 , where for the penultimate inequality we use the inequality rz(z1/r  1) > z1/r(z  1), which holds for all z, r  1,2 with z = c and r = 2 1. It is now easy to verify that there is a universal constant T > 0 such that if s  T log  then Equation (3) holds. 2To see this, let f(z, r) , rz(z1/r  1) z1/r(z  1) z = (r  1)z1/r + z1/r1  r. Taking the partial derivative with respect to z, we have  z f(z, r) = (r  1)(z  1)z1/r2 r , which is clearly non-negative for z, r  1. Also, f(1, r) = 0. So, we have shown that f(z, r)  0 for all z, r  1, which implies the claim. It is important to note that it should be possible to extend Theorem 1 to other positional scoring rules defined by a score vector (1, . . . , m) where j > j+1 for all j = 1, . . . ,m 1. However, Borda count is especially practical and easy to explain (see Section 7 for more on this), which is why we focus on it for our positive result.\nTheorem 1 shows that Borda count is robust against noisy perturbations of the preference profile. It is natural to ask whether many voting rules satisfy a similar property. In this section we answer this question in the negative, by proving that any voting rule that belongs to the important family of PMC rules is not robust in a similar sense. Specifically, recall that under a PMC rule, when the weighted pairwise majority graph is acyclic, the output ranking is the topological ordering of the pairwise majority graph. We show that there exist profiles in which the pairwise majority graph is acyclic and all edge weights are large, but, with high probability, the noisy profile also has an acyclic pairwise majority graph which induces a different ranking. This means that any PMC rule would return different rankings when applied to the true profile and the noisy profile. Theorem 2. For all  > 0,   (0, 1), and m  N such that m  3, there exists n0  N such that for all n  n0, there exists a profile ?  Ln such that (?) is acyclic and all edges have weight (n), but with probability at least 1 () is acyclic and there is a pair of alternatives on which the unique rankings induced by (?) and () disagree, where the probability is taken over the sampling of . It is instructive to contrast our positive result, Theorem 1, with this negative result. On a very high level, the former result asserts that if Borda count says that the gaps between alternatives are significant, then the alternatives will not flip under Borda count, whereas the latter says even if a PMC rule says that the gaps between alternatives are very significant, some alternatives are likely to flip under that rule. On a technical level, a subtle difference is that Theorem 1 is stated for  and , whereas Theorem 2 is stated directly for . This actually strengthens the negative result, because a constant  and   (1) lead to  = 1  o(1), i.e., very noisy distributions  and still the positive result of Theorem 1 holds. By contrast, the negative result of Theorem 2 is true even when  is constant, i.e., for settings that are not nearly as noisy. That said, the two results are not directly comparable, as Borda count and PMC rules deal with very different notions of score or weight. Nevertheless, the takehome message is that the notion of score that defines Borda count is inherently more robust to random perturbations of the preference profile. The proof of Theorem 2 is rather technical, and appears in the full version of the paper. In a nutshell, we construct a preference profile ? with n voters whose preferences are x? x1    , and (1  )n voters whose preferences are x1    x?, for  > 1/2. This profile induces a ranking where x? is first and x1 is second. However, it can be seen that, in the sampled profile , many voters from the first group would flip x? and x1, leading to a majority who prefer x1 to x?. Furthermore, we prove the nontrivial claim that () is likely to be acyclic (nontrivial because it is unclear there would not be a cycle involving x?), which completes the argument.\nIn Section 4 we have established that Borda count is robust to prediction error. However, our positive theoretical result, Theorem 1, only provides asymptotic guarantees. In this section, we evaluate the performance of Borda count on profiles of size that is more representative of real-world instances. For our evaluation metric, we consider the probability of the rule flipping alternatives when aggregating noisy rankings against their difference in Borda score in the underlying true profile. All of our code is open-source and can be found at n voters, m alternatives, a Mallows parameter   (0, 1), and a probability p  [0, 1], we generate a true profile ? = (?1 , . . . ,  ? n) from a mixture of Mallows models. Specifically, each ranking is drawn with probability p from a Mallows model with base ranking x1 x2    xm and parameter , and with probability 1p from a Mallows model with base ranking xm xm1    x1 and parameter . We then repeatedly generate noisy profiles  = (1, . . . , n) where each i is generated by a Mallows model centered at ?i with parameter . For every pair of alternatives (xi, xj) such that B(xi,?) > B(xj ,?)  that is, xi beat xj when Borda count was applied to the true profile  we calculate the percentage of noisy profiles that flipped the order of xi and xj , i.e., those where B(xj ,) > B(xi,). Based on the true difference in Borda scores B(xi,?)  B(xj ,?), we place this data point in the appropriate bucket, where the width of each bucket corresponds to an average Borda score difference of 1. This way we can relate the Borda score difference to the probability of making a pairwise prediction error. Note that starting from a mixture of opposite ranking models allows us to vary the distribution over score differences in ? by varying p.\nThroughout our experiments, we let n = 100, m = 40,   {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, and p  {1, 0.7, 0.5}. Our results for p = 1, shown in Figure 1, plot the average probability of flipping the order of alternatives as a function of the difference in average Borda scores of the alternatives, where comparisons are bucketed by the difference in average Borda score. For   {0.1, 0.2, 0.3}, the observed probability of flipping any two alternatives, regardless of average Borda score difference, is 0; i.e., there are no mistakes. At a high level, error rate decreases with true average Borda score distance in all experiments. Note that the maximum observed error rate increases with the Mallows parameter , which is intuitive because higher values of  imply noisier (more uniformly random) rankings, so the probability of swapping alternatives should increase. However, for all values of  and under all methods of generating profiles, the probability of making errors quickly decreases with average Borda score difference in the true profile. Similar plots for p = 0.7 and p = 0.5 are included in the full version of the paper; these plots support the observation that the probability of making a mistake depends on the average Borda score difference, and not on the particular methods used to sample the underlying true profile.\nOur theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. However, Borda count is also compelling in terms of usability and explainability. In more detail, in our implemented donor-recipient matching system, clicking on a recommended alternative displays an explanation for why it was ranked highly by Borda count, which consists of two components. First, we show the alternatives average position in the predicted preferences of each of the four stakeholder groups. Note that this information determines the Borda score of the alternative, given the weight of each stakeholder group.3 Second  this is the more novel component  we show specific features in which the recommended alternative stands out. This is interesting because classic social choice theory does not have features for alternatives, and we are able to give this type of explanation precisely because our alternatives are represented as vectors of features (which is crucial for the application of learning-to-rank algorithms). Based on the results presented in this paper, as well as these additional insights, we use Borda count in our implemented virtual-democracy-based system. 3These weights were decided by the stakeholders themselves.\n", "prediction": "we consider the problem of determining which subset of @xmath0 people have the same preferences.<n> we show that this problem is np - hard, even if we start with a set of voters with similar preferences that have been observed to be distinct in some measure ( e.g. the fraction of people with distinct preferences in a city s election ) ; we prove the existence of such a distinction and give a complete solution for the class of cities for which this distinction is true ( in terms of the amount of noise in the election of which the voters choose from a given list of alternatives and the number of rounds of election in which each voter decides which of these two choices she is most interested in making is the most likely outcome of this choice ). <n> * keywords : * _ online voting ( one way voting is similar to offline voting in classical systems such as the u.s.t?<n> electoral system and in many other democratic systems where one can vote in an online fashion ), online decision making ( machine learning based on the preferences of individual voters in addition to voting for a representative of a party in elections where the voting rule is known in advance and can be easily checked from the internet )(even though we do nt know in what sense this is a priori how to decide on which choices are ranked by a pairwise comparisons ( the classic example is used in our setting ( we are the classical voting of ( a classic ( but we have a similar results ( ( j. et al etal ( al al com ( to the paper ( ! ] ( m th et t ( and al - al(a. al-(s et - j) and ( d. jacobs & al ( y tam et et e et and j et. j and m et ( g et d et i et in j(em al. ( x - m - y - b. d - e - g - x- e) ( b - d ei et b et m. e and y and b ( which j - c - p) with the j  the e hereinafter b(x et c-) in x. g. b and x(y et with x and d(e ])(t j in this  in p  ii  for x ... ]]   to x with j ( p. i (i  a -2  that is p(m  with p and p with y. m) while the ranking of x in these  until the results with  ( i. p in [ x ( c  while m with m([[x ].<n> ( @ x which ], while j with this ( this in ( it is not  as a ranking ( 2((n  it  which is based ( while p that  y with one of p ( [ y in  i) the rank ( as p for p which in all the analysis of j of  whose analysis for this analysis ( that the other ie  analysis by the top  is in one  when the  @ @  if the result of one ( with which which we  of m and sym (... (<n> ng  p is of it was the corresponding  based  by p while while  its analysis to which was based based in that was a classification of [  from x to p by  this for these ( based to a new analysis  unless the first  and a)<n> j for any pair of that we does  was [(f ( until p until a. while we is one for all  who "}
